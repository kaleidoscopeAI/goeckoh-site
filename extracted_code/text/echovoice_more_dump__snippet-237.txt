• Ollama (LLM): Communication with the Ollama server (running as a separate process, typically on http://localhost:11434) via its HTTP REST API is the standard and appropriate method [User Plan]. This involves sending prompts (likely as JSON payloads) via POST requests and receiving the generated text. While technically IPC, it uses well-established web protocols and is relatively lightweight, especially over the loopback interface.
    ◦ Handling LLM Latency: The significant latency of CPU-based LLM inference is a primary concern [User Plan].
        ▪ Synchronous Approach: Calling the Ollama API and waiting for the response before proceeding with the simulation loop is the simplest implementation [User Plan]. It guarantees state consistency, as the simulation is paused during the LLM call. However, this pause halts physics updates and UI responsiveness, which could be disruptive depending on the inference time (potentially several seconds).
        ▪ Asynchronous Approach: Using Python's asyncio or threading to make the HTTP request to Ollama without blocking the main loop allows the simulation or UI updates to potentially continue concurrently [User Plan]. This improves perceived responsiveness but introduces complexity in managing state (ensuring feedback is applied relevantly) and handling callbacks or futures. While Python's Global Interpreter Lock (GIL) limits true CPU-bound parallelism in Python code, the LLM inference itself is handled by Ollama's backend (likely C++ based llama.cpp), which runs outside the GIL and can utilize CPU cores effectively. I/O operations like WebSocket communication can also run concurrently.
        ▪ Recommendation: Begin with the synchronous approach due to its simplicity in implementation and state management. To mitigate the user experience impact of the pause, implement a clear visual indicator in the UI (e.g., a "Cube is thinking..." message or animation) during the LLM call. If the pause proves detrimental to the simulation's dynamics or the user experience after testing, refactor to use an asynchronous approach (asyncio is generally preferred for I/O-bound tasks like HTTP requests and WebSocket handling). Careful state management will be required in the asynchronous case to ensure that actions triggered by the LLM's response are applied appropriately to the potentially evolved simulation state.
    ◦ The choice between synchronous and asynchronous handling also subtly influences the simulation's character. A synchronous pause gives the "thinking" process a distinct, time-consuming presence. An asynchronous approach makes the inner voice feel more like a background or parallel process. This qualitative difference should be considered in relation to the project's goal of simulating cognition.
• UI (Browser): Using WebSockets for real-time, bidirectional communication between the Python server and the Three.js front-end is the correct choice for streaming state updates and potentially receiving user commands.43 The plan to use a single Python server process (e.g., based on FastAPI or Flask) to serve the static HTML/JS/CSS files and handle the WebSocket connection on a single port is efficient and minimizes system complexity [User Plan]. JSON is a suitable format for the messages exchanged over the WebSocket [User Plan].
• Internal Communication: The suggestion to use in-process mechanisms like callbacks or a dedicated event/signaling library (e.g., Python's blinker) for communication between the different modules (cube_simulator, inner_voice, visualization, etc.) within the main Python process is sound [User Plan]. This promotes logical decoupling and modularity without incurring the overhead and complexity of OS-level IPC (like pipes or shared memory segments), aligning with the unified architecture goal.

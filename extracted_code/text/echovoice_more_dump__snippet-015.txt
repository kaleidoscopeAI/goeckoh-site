    • LLM CPU Inference Latency: Achieving acceptable performance for the "inner voice" generation using models like Mistral 7B or even Llama 3.2 3B on CPU hardware via Ollama requires careful model selection (smaller, instruction-tuned models), aggressive quantization (e.g., Q4_K_M), and empirical testing on the target hardware. The system design must accommodate potentially slow response times, and the choice between synchronous and asynchronous handling of LLM calls needs careful consideration regarding both user experience and simulation dynamics.
    • Visualization Performance: Rendering potentially hundreds of dynamic nodes, bonds, and especially dynamically updated text labels (thought bubbles) in Three.js requires optimization techniques like instanced rendering and texture atlasing to maintain smooth frame rates.

import React, { useRef, useEffect, useState } from 'react';

interface NodeData {
  vector: number[]; // e.g., length 8-16 (cognitive embedding)
  arousal: number;
  valence: number;
}

interface ThoughtImageProps {
  nodes: NodeData[];
  width: number;
  height: number;
}

const ThoughtImage: React.FC<ThoughtImageProps> = ({ nodes, width, height }) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const [imageData, setImageData] = useState<Uint8ClampedArray>();

  useEffect(() => {
    if (!nodes.length) return;
    
    // Example: Map 1 node per pixel, assuming width * height == nodes.length
    // Aggregate vector features into colors (normalized)
    const data = new Uint8ClampedArray(width * height * 4);
    
    // Example PCA-like feature extraction (simplified)
    nodes.forEach((node, i) => {
      const baseIdx = i * 4;
      // Use first 3 dims of vector as RGB after normalizing [0,1]
      for (let j = 0; j < 3; j++) {
        data[baseIdx + j] = Math.min(255, Math.max(0, node.vector[j] * 255));
      }
      // Alpha channel based on arousal or valence
      data[baseIdx + 3] = Math.min(255, Math.max(50, node.arousal * 255));
    });
    
    setImageData(data);
  }, [nodes, width, height]);

  useEffect(() => {
    if (!canvasRef.current || !imageData) return;
    const ctx = canvasRef.current.getContext("2d");
    if (!ctx) return;
    
    const imgData = new ImageData(imageData, width, height);
    ctx.putImageData(imgData, 0, 0);
  }, [imageData, width, height]);

  return <canvas ref={canvasRef} width={width} height={height} style={{ imageRendering: 'pixelated', border: '1px solid #222' }} />;
};

export default ThoughtImage;

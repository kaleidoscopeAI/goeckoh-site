    • Phase 2: Basic Visualization: Develop the visualization.py module. Set up the WebSocket server and the basic Three.js front-end. Implement rendering of the nodes and bonds based on data from CubeState streamed via WebSockets. Add the HTML overlay for displaying real-time metrics. Verify that the visualization accurately reflects the simulation state.
    • Phase 3: LLM Integration (Synchronous): Integrate the inner_voice.py module. Establish communication with the Ollama server using the recommended initial models (Mistral 7B Instruct Q4_K_M or Llama 3.2 3B Instruct Q4_K_M). Implement synchronous LLM calls triggered periodically within the main loop. Display the generated thoughts in the Global Thought Panel in the UI. Add a visual indicator for when the LLM is processing. Test basic reflection capabilities.
    • Phase 4: Memory Implementation: Add the memory_store.py module. Implement basic functionality to log LLM thoughts and potentially key state transitions (e.g., high stress events) to a simple persistent store (e.g., JSON file). Modify the LLM prompt generation to include relevant recent history from the memory store.
    • Phase 5: Transformation Engine & Feedback: Develop the transformation_engine.py module. Implement initial, simple rules for interpreting LLM output and applying feedback to the CubeState (e.g., adjusting a physics parameter based on an LLM suggestion). Ensure the engine logs any actions taken. Observe the effects of this basic feedback loop.
    • Phase 6: Visualization Refinement: Enhance the visualization. Implement node-attached thought bubbles using the optimized Sprite + CanvasTexture (texture atlas) approach. Add any desired interactive controls (e.g., pause/resume, camera controls). Refine visual styling.
    • Phase 7: Optimization, Evaluation & Experimentation: Profile the application, paying close attention to LLM inference latency and visualization rendering performance. If the synchronous LLM calls cause unacceptable pauses, refactor to an asynchronous model. Use the configuration file extensively to experiment with different LLM models, generation parameters, prompt strategies, simulation parameters, and transformation rules to explore emergent behaviors.

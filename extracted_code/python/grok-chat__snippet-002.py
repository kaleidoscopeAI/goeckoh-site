The human brain distinguishes self-generated speech from external speech via N1 Suppression. When a neurotypical individual speaks, the motor cortex sends a copy of the command (Corollary Discharge) to the auditory cortex, effectively saying, "Expect this sound." If the sound arrives within a tight temporal window (<300ms) and matches the prediction, the brain tags it as "Self" and suppresses the neural response.
For many non-speaking autistic individuals, this loop is theorized to be fractured. The internal prediction is weak, noisy, or temporally desynchronized. Consequently, when the individual attempts to speak, the auditory feedback is perceived as "foreign" or "unexpected"â€”a prediction error. This creates a state of chronic sensory surprise, leading to the inhibition of speech motor plans. Traditional AI voice assistants (Siri, ChatGPT) operate on a "Turn-Taking" cadence with latencies ranging from 1000ms to 3000ms. This delay fundamentally categorizes the AI as "Other," preventing any therapeutic integration into the user's cognitive loop. To bridge this gap, an AI system must operate simultaneously with the user, functioning as a real-time neuro-acoustic mirror.

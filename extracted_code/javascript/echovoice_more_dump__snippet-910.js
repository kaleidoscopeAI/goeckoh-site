• Sample Efficiency: RL often requires a large number of interactions (potentially millions 53) to learn effective policies, which might be costly or slow in real-world data streams. Model-based RL or incorporating prior knowledge can help.58
• Stability and Convergence: Training RL agents, especially DRL, can be unstable and sensitive to hyperparameter choices.56
• Reward Definition: As mentioned, defining a reward function that accurately captures the abstract goal of "understanding" is non-trivial and crucial for success.60
• Complexity: Implementing and debugging RL systems can be complex.58 Alternatives like Decision Transformers, which frame RL as a sequence modeling problem, have been proposed.58



# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/__init__.py
# ========================================================
from typing import List, Optional

__version__ = "24.0"


def main(args: Optional[List[str]] = None) -> int:
    """This is an internal API only meant for use by pip's own console scripts.

    For additional details, see https://github.com/pypa/pip/issues/7498.
    """
    from pip._internal.utils.entrypoints import _wrapper

    return _wrapper(args)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/__main__.py
# ========================================================
import os
import sys

# Remove '' and current working directory from the first entry
# of sys.path, if present to avoid using current directory
# in pip commands check, freeze, install, list and show,
# when invoked as python -m pip <command>
if sys.path[0] in ("", os.getcwd()):
    sys.path.pop(0)

# If we are running from a wheel, add the wheel to sys.path
# This allows the usage python pip-*.whl/pip install pip-*.whl
if __package__ == "":
    # __file__ is pip-*.whl/pip/__main__.py
    # first dirname call strips of '/__main__.py', second strips off '/pip'
    # Resulting path is the name of the wheel itself
    # Add that to sys.path so we can import pip
    path = os.path.dirname(os.path.dirname(__file__))
    sys.path.insert(0, path)

if __name__ == "__main__":
    from pip._internal.cli.main import main as _main

    sys.exit(_main())


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/android.py
# ========================================================
"""Android."""
from __future__ import annotations

import os
import re
import sys
from functools import lru_cache
from typing import cast

from .api import PlatformDirsABC


class Android(PlatformDirsABC):
    """
    Follows the guidance `from here <https://android.stackexchange.com/a/216132>`_. Makes use of the
    `appname <platformdirs.api.PlatformDirsABC.appname>`,
    `version <platformdirs.api.PlatformDirsABC.version>`,
    `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    """

    @property
    def user_data_dir(self) -> str:
        """:return: data directory tied to the user, e.g. ``/data/user/<userid>/<packagename>/files/<AppName>``"""
        return self._append_app_name_and_version(cast(str, _android_folder()), "files")

    @property
    def site_data_dir(self) -> str:
        """:return: data directory shared by users, same as `user_data_dir`"""
        return self.user_data_dir

    @property
    def user_config_dir(self) -> str:
        """
        :return: config directory tied to the user, e.g. \
        ``/data/user/<userid>/<packagename>/shared_prefs/<AppName>``
        """
        return self._append_app_name_and_version(cast(str, _android_folder()), "shared_prefs")

    @property
    def site_config_dir(self) -> str:
        """:return: config directory shared by the users, same as `user_config_dir`"""
        return self.user_config_dir

    @property
    def user_cache_dir(self) -> str:
        """:return: cache directory tied to the user, e.g. e.g. ``/data/user/<userid>/<packagename>/cache/<AppName>``"""
        return self._append_app_name_and_version(cast(str, _android_folder()), "cache")

    @property
    def site_cache_dir(self) -> str:
        """:return: cache directory shared by users, same as `user_cache_dir`"""
        return self.user_cache_dir

    @property
    def user_state_dir(self) -> str:
        """:return: state directory tied to the user, same as `user_data_dir`"""
        return self.user_data_dir

    @property
    def user_log_dir(self) -> str:
        """
        :return: log directory tied to the user, same as `user_cache_dir` if not opinionated else ``log`` in it,
          e.g. ``/data/user/<userid>/<packagename>/cache/<AppName>/log``
        """
        path = self.user_cache_dir
        if self.opinion:
            path = os.path.join(path, "log")  # noqa: PTH118
        return path

    @property
    def user_documents_dir(self) -> str:
        """:return: documents directory tied to the user e.g. ``/storage/emulated/0/Documents``"""
        return _android_documents_folder()

    @property
    def user_downloads_dir(self) -> str:
        """:return: downloads directory tied to the user e.g. ``/storage/emulated/0/Downloads``"""
        return _android_downloads_folder()

    @property
    def user_pictures_dir(self) -> str:
        """:return: pictures directory tied to the user e.g. ``/storage/emulated/0/Pictures``"""
        return _android_pictures_folder()

    @property
    def user_videos_dir(self) -> str:
        """:return: videos directory tied to the user e.g. ``/storage/emulated/0/DCIM/Camera``"""
        return _android_videos_folder()

    @property
    def user_music_dir(self) -> str:
        """:return: music directory tied to the user e.g. ``/storage/emulated/0/Music``"""
        return _android_music_folder()

    @property
    def user_runtime_dir(self) -> str:
        """
        :return: runtime directory tied to the user, same as `user_cache_dir` if not opinionated else ``tmp`` in it,
          e.g. ``/data/user/<userid>/<packagename>/cache/<AppName>/tmp``
        """
        path = self.user_cache_dir
        if self.opinion:
            path = os.path.join(path, "tmp")  # noqa: PTH118
        return path


@lru_cache(maxsize=1)
def _android_folder() -> str | None:
    """:return: base folder for the Android OS or None if it cannot be found"""
    try:
        # First try to get path to android app via pyjnius
        from jnius import autoclass

        context = autoclass("android.content.Context")
        result: str | None = context.getFilesDir().getParentFile().getAbsolutePath()
    except Exception:  # noqa: BLE001
        # if fails find an android folder looking path on the sys.path
        pattern = re.compile(r"/data/(data|user/\d+)/(.+)/files")
        for path in sys.path:
            if pattern.match(path):
                result = path.split("/files")[0]
                break
        else:
            result = None
    return result


@lru_cache(maxsize=1)
def _android_documents_folder() -> str:
    """:return: documents folder for the Android OS"""
    # Get directories with pyjnius
    try:
        from jnius import autoclass

        context = autoclass("android.content.Context")
        environment = autoclass("android.os.Environment")
        documents_dir: str = context.getExternalFilesDir(environment.DIRECTORY_DOCUMENTS).getAbsolutePath()
    except Exception:  # noqa: BLE001
        documents_dir = "/storage/emulated/0/Documents"

    return documents_dir


@lru_cache(maxsize=1)
def _android_downloads_folder() -> str:
    """:return: downloads folder for the Android OS"""
    # Get directories with pyjnius
    try:
        from jnius import autoclass

        context = autoclass("android.content.Context")
        environment = autoclass("android.os.Environment")
        downloads_dir: str = context.getExternalFilesDir(environment.DIRECTORY_DOWNLOADS).getAbsolutePath()
    except Exception:  # noqa: BLE001
        downloads_dir = "/storage/emulated/0/Downloads"

    return downloads_dir


@lru_cache(maxsize=1)
def _android_pictures_folder() -> str:
    """:return: pictures folder for the Android OS"""
    # Get directories with pyjnius
    try:
        from jnius import autoclass

        context = autoclass("android.content.Context")
        environment = autoclass("android.os.Environment")
        pictures_dir: str = context.getExternalFilesDir(environment.DIRECTORY_PICTURES).getAbsolutePath()
    except Exception:  # noqa: BLE001
        pictures_dir = "/storage/emulated/0/Pictures"

    return pictures_dir


@lru_cache(maxsize=1)
def _android_videos_folder() -> str:
    """:return: videos folder for the Android OS"""
    # Get directories with pyjnius
    try:
        from jnius import autoclass

        context = autoclass("android.content.Context")
        environment = autoclass("android.os.Environment")
        videos_dir: str = context.getExternalFilesDir(environment.DIRECTORY_DCIM).getAbsolutePath()
    except Exception:  # noqa: BLE001
        videos_dir = "/storage/emulated/0/DCIM/Camera"

    return videos_dir


@lru_cache(maxsize=1)
def _android_music_folder() -> str:
    """:return: music folder for the Android OS"""
    # Get directories with pyjnius
    try:
        from jnius import autoclass

        context = autoclass("android.content.Context")
        environment = autoclass("android.os.Environment")
        music_dir: str = context.getExternalFilesDir(environment.DIRECTORY_MUSIC).getAbsolutePath()
    except Exception:  # noqa: BLE001
        music_dir = "/storage/emulated/0/Music"

    return music_dir


__all__ = [
    "Android",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/macos.py
# ========================================================
"""macOS."""
from __future__ import annotations

import os.path

from .api import PlatformDirsABC


class MacOS(PlatformDirsABC):
    """
    Platform directories for the macOS operating system. Follows the guidance from `Apple documentation
    <https://developer.apple.com/library/archive/documentation/FileManagement/Conceptual/FileSystemProgrammingGuide/MacOSXDirectories/MacOSXDirectories.html>`_.
    Makes use of the `appname <platformdirs.api.PlatformDirsABC.appname>`,
    `version <platformdirs.api.PlatformDirsABC.version>`,
    `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    """

    @property
    def user_data_dir(self) -> str:
        """:return: data directory tied to the user, e.g. ``~/Library/Application Support/$appname/$version``"""
        return self._append_app_name_and_version(os.path.expanduser("~/Library/Application Support"))  # noqa: PTH111

    @property
    def site_data_dir(self) -> str:
        """:return: data directory shared by users, e.g. ``/Library/Application Support/$appname/$version``"""
        return self._append_app_name_and_version("/Library/Application Support")

    @property
    def user_config_dir(self) -> str:
        """:return: config directory tied to the user, same as `user_data_dir`"""
        return self.user_data_dir

    @property
    def site_config_dir(self) -> str:
        """:return: config directory shared by the users, same as `site_data_dir`"""
        return self.site_data_dir

    @property
    def user_cache_dir(self) -> str:
        """:return: cache directory tied to the user, e.g. ``~/Library/Caches/$appname/$version``"""
        return self._append_app_name_and_version(os.path.expanduser("~/Library/Caches"))  # noqa: PTH111

    @property
    def site_cache_dir(self) -> str:
        """:return: cache directory shared by users, e.g. ``/Library/Caches/$appname/$version``"""
        return self._append_app_name_and_version("/Library/Caches")

    @property
    def user_state_dir(self) -> str:
        """:return: state directory tied to the user, same as `user_data_dir`"""
        return self.user_data_dir

    @property
    def user_log_dir(self) -> str:
        """:return: log directory tied to the user, e.g. ``~/Library/Logs/$appname/$version``"""
        return self._append_app_name_and_version(os.path.expanduser("~/Library/Logs"))  # noqa: PTH111

    @property
    def user_documents_dir(self) -> str:
        """:return: documents directory tied to the user, e.g. ``~/Documents``"""
        return os.path.expanduser("~/Documents")  # noqa: PTH111

    @property
    def user_downloads_dir(self) -> str:
        """:return: downloads directory tied to the user, e.g. ``~/Downloads``"""
        return os.path.expanduser("~/Downloads")  # noqa: PTH111

    @property
    def user_pictures_dir(self) -> str:
        """:return: pictures directory tied to the user, e.g. ``~/Pictures``"""
        return os.path.expanduser("~/Pictures")  # noqa: PTH111

    @property
    def user_videos_dir(self) -> str:
        """:return: videos directory tied to the user, e.g. ``~/Movies``"""
        return os.path.expanduser("~/Movies")  # noqa: PTH111

    @property
    def user_music_dir(self) -> str:
        """:return: music directory tied to the user, e.g. ``~/Music``"""
        return os.path.expanduser("~/Music")  # noqa: PTH111

    @property
    def user_runtime_dir(self) -> str:
        """:return: runtime directory tied to the user, e.g. ``~/Library/Caches/TemporaryItems/$appname/$version``"""
        return self._append_app_name_and_version(os.path.expanduser("~/Library/Caches/TemporaryItems"))  # noqa: PTH111


__all__ = [
    "MacOS",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/version.py
# ========================================================
# file generated by setuptools_scm
# don't change, don't track in version control
__version__ = version = '3.8.1'
__version_tuple__ = version_tuple = (3, 8, 1)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/__init__.py
# ========================================================
"""
Utilities for determining application-specific dirs. See <https://github.com/platformdirs/platformdirs> for details and
usage.
"""
from __future__ import annotations

import os
import sys
from typing import TYPE_CHECKING

from .api import PlatformDirsABC
from .version import __version__
from .version import __version_tuple__ as __version_info__

if TYPE_CHECKING:
    from pathlib import Path

    if sys.version_info >= (3, 8):  # pragma: no cover (py38+)
        from typing import Literal
    else:  # pragma: no cover (py38+)
        from pip._vendor.typing_extensions import Literal


def _set_platform_dir_class() -> type[PlatformDirsABC]:
    if sys.platform == "win32":
        from pip._vendor.platformdirs.windows import Windows as Result
    elif sys.platform == "darwin":
        from pip._vendor.platformdirs.macos import MacOS as Result
    else:
        from pip._vendor.platformdirs.unix import Unix as Result

    if os.getenv("ANDROID_DATA") == "/data" and os.getenv("ANDROID_ROOT") == "/system":
        if os.getenv("SHELL") or os.getenv("PREFIX"):
            return Result

        from pip._vendor.platformdirs.android import _android_folder

        if _android_folder() is not None:
            from pip._vendor.platformdirs.android import Android

            return Android  # return to avoid redefinition of result

    return Result


PlatformDirs = _set_platform_dir_class()  #: Currently active platform
AppDirs = PlatformDirs  #: Backwards compatibility with appdirs


def user_data_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param roaming: See `roaming <platformdirs.api.PlatformDirsABC.roaming>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: data directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        roaming=roaming,
        ensure_exists=ensure_exists,
    ).user_data_dir


def site_data_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    multipath: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param multipath: See `roaming <platformdirs.api.PlatformDirsABC.multipath>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: data directory shared by users
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        multipath=multipath,
        ensure_exists=ensure_exists,
    ).site_data_dir


def user_config_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param roaming: See `roaming <platformdirs.api.PlatformDirsABC.roaming>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: config directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        roaming=roaming,
        ensure_exists=ensure_exists,
    ).user_config_dir


def site_config_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    multipath: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param multipath: See `roaming <platformdirs.api.PlatformDirsABC.multipath>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: config directory shared by the users
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        multipath=multipath,
        ensure_exists=ensure_exists,
    ).site_config_dir


def user_cache_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `roaming <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: cache directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).user_cache_dir


def site_cache_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `opinion <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: cache directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).site_cache_dir


def user_state_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param roaming: See `roaming <platformdirs.api.PlatformDirsABC.roaming>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: state directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        roaming=roaming,
        ensure_exists=ensure_exists,
    ).user_state_dir


def user_log_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `roaming <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: log directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).user_log_dir


def user_documents_dir() -> str:
    """:returns: documents directory tied to the user"""
    return PlatformDirs().user_documents_dir


def user_downloads_dir() -> str:
    """:returns: downloads directory tied to the user"""
    return PlatformDirs().user_downloads_dir


def user_pictures_dir() -> str:
    """:returns: pictures directory tied to the user"""
    return PlatformDirs().user_pictures_dir


def user_videos_dir() -> str:
    """:returns: videos directory tied to the user"""
    return PlatformDirs().user_videos_dir


def user_music_dir() -> str:
    """:returns: music directory tied to the user"""
    return PlatformDirs().user_music_dir


def user_runtime_dir(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> str:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `opinion <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: runtime directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).user_runtime_dir


def user_data_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param roaming: See `roaming <platformdirs.api.PlatformDirsABC.roaming>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: data path tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        roaming=roaming,
        ensure_exists=ensure_exists,
    ).user_data_path


def site_data_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    multipath: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param multipath: See `multipath <platformdirs.api.PlatformDirsABC.multipath>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: data path shared by users
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        multipath=multipath,
        ensure_exists=ensure_exists,
    ).site_data_path


def user_config_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param roaming: See `roaming <platformdirs.api.PlatformDirsABC.roaming>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: config path tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        roaming=roaming,
        ensure_exists=ensure_exists,
    ).user_config_path


def site_config_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    multipath: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param multipath: See `roaming <platformdirs.api.PlatformDirsABC.multipath>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: config path shared by the users
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        multipath=multipath,
        ensure_exists=ensure_exists,
    ).site_config_path


def site_cache_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `opinion <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: cache directory tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).site_cache_path


def user_cache_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `roaming <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: cache path tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).user_cache_path


def user_state_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    roaming: bool = False,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param roaming: See `roaming <platformdirs.api.PlatformDirsABC.roaming>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: state path tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        roaming=roaming,
        ensure_exists=ensure_exists,
    ).user_state_path


def user_log_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `roaming <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: log path tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).user_log_path


def user_documents_path() -> Path:
    """:returns: documents path tied to the user"""
    return PlatformDirs().user_documents_path


def user_downloads_path() -> Path:
    """:returns: downloads path tied to the user"""
    return PlatformDirs().user_downloads_path


def user_pictures_path() -> Path:
    """:returns: pictures path tied to the user"""
    return PlatformDirs().user_pictures_path


def user_videos_path() -> Path:
    """:returns: videos path tied to the user"""
    return PlatformDirs().user_videos_path


def user_music_path() -> Path:
    """:returns: music path tied to the user"""
    return PlatformDirs().user_music_path


def user_runtime_path(
    appname: str | None = None,
    appauthor: str | None | Literal[False] = None,
    version: str | None = None,
    opinion: bool = True,  # noqa: FBT001, FBT002
    ensure_exists: bool = False,  # noqa: FBT001, FBT002
) -> Path:
    """
    :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
    :param appauthor: See `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`.
    :param version: See `version <platformdirs.api.PlatformDirsABC.version>`.
    :param opinion: See `opinion <platformdirs.api.PlatformDirsABC.opinion>`.
    :param ensure_exists: See `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    :returns: runtime path tied to the user
    """
    return PlatformDirs(
        appname=appname,
        appauthor=appauthor,
        version=version,
        opinion=opinion,
        ensure_exists=ensure_exists,
    ).user_runtime_path


__all__ = [
    "__version__",
    "__version_info__",
    "PlatformDirs",
    "AppDirs",
    "PlatformDirsABC",
    "user_data_dir",
    "user_config_dir",
    "user_cache_dir",
    "user_state_dir",
    "user_log_dir",
    "user_documents_dir",
    "user_downloads_dir",
    "user_pictures_dir",
    "user_videos_dir",
    "user_music_dir",
    "user_runtime_dir",
    "site_data_dir",
    "site_config_dir",
    "site_cache_dir",
    "user_data_path",
    "user_config_path",
    "user_cache_path",
    "user_state_path",
    "user_log_path",
    "user_documents_path",
    "user_downloads_path",
    "user_pictures_path",
    "user_videos_path",
    "user_music_path",
    "user_runtime_path",
    "site_data_path",
    "site_config_path",
    "site_cache_path",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/api.py
# ========================================================
"""Base API."""
from __future__ import annotations

import os
from abc import ABC, abstractmethod
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    import sys

    if sys.version_info >= (3, 8):  # pragma: no cover (py38+)
        from typing import Literal
    else:  # pragma: no cover (py38+)
        from pip._vendor.typing_extensions import Literal


class PlatformDirsABC(ABC):
    """Abstract base class for platform directories."""

    def __init__(  # noqa: PLR0913
        self,
        appname: str | None = None,
        appauthor: str | None | Literal[False] = None,
        version: str | None = None,
        roaming: bool = False,  # noqa: FBT001, FBT002
        multipath: bool = False,  # noqa: FBT001, FBT002
        opinion: bool = True,  # noqa: FBT001, FBT002
        ensure_exists: bool = False,  # noqa: FBT001, FBT002
    ) -> None:
        """
        Create a new platform directory.

        :param appname: See `appname`.
        :param appauthor: See `appauthor`.
        :param version: See `version`.
        :param roaming: See `roaming`.
        :param multipath: See `multipath`.
        :param opinion: See `opinion`.
        :param ensure_exists: See `ensure_exists`.
        """
        self.appname = appname  #: The name of application.
        self.appauthor = appauthor
        """
        The name of the app author or distributing body for this application. Typically, it is the owning company name.
        Defaults to `appname`. You may pass ``False`` to disable it.
        """
        self.version = version
        """
        An optional version path element to append to the path. You might want to use this if you want multiple versions
        of your app to be able to run independently. If used, this would typically be ``<major>.<minor>``.
        """
        self.roaming = roaming
        """
        Whether to use the roaming appdata directory on Windows. That means that for users on a Windows network setup
        for roaming profiles, this user data will be synced on login (see
        `here <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>`_).
        """
        self.multipath = multipath
        """
        An optional parameter only applicable to Unix/Linux which indicates that the entire list of data dirs should be
        returned. By default, the first item would only be returned.
        """
        self.opinion = opinion  #: A flag to indicating to use opinionated values.
        self.ensure_exists = ensure_exists
        """
        Optionally create the directory (and any missing parents) upon access if it does not exist.
        By default, no directories are created.
        """

    def _append_app_name_and_version(self, *base: str) -> str:
        params = list(base[1:])
        if self.appname:
            params.append(self.appname)
            if self.version:
                params.append(self.version)
        path = os.path.join(base[0], *params)  # noqa: PTH118
        self._optionally_create_directory(path)
        return path

    def _optionally_create_directory(self, path: str) -> None:
        if self.ensure_exists:
            Path(path).mkdir(parents=True, exist_ok=True)

    @property
    @abstractmethod
    def user_data_dir(self) -> str:
        """:return: data directory tied to the user"""

    @property
    @abstractmethod
    def site_data_dir(self) -> str:
        """:return: data directory shared by users"""

    @property
    @abstractmethod
    def user_config_dir(self) -> str:
        """:return: config directory tied to the user"""

    @property
    @abstractmethod
    def site_config_dir(self) -> str:
        """:return: config directory shared by the users"""

    @property
    @abstractmethod
    def user_cache_dir(self) -> str:
        """:return: cache directory tied to the user"""

    @property
    @abstractmethod
    def site_cache_dir(self) -> str:
        """:return: cache directory shared by users"""

    @property
    @abstractmethod
    def user_state_dir(self) -> str:
        """:return: state directory tied to the user"""

    @property
    @abstractmethod
    def user_log_dir(self) -> str:
        """:return: log directory tied to the user"""

    @property
    @abstractmethod
    def user_documents_dir(self) -> str:
        """:return: documents directory tied to the user"""

    @property
    @abstractmethod
    def user_downloads_dir(self) -> str:
        """:return: downloads directory tied to the user"""

    @property
    @abstractmethod
    def user_pictures_dir(self) -> str:
        """:return: pictures directory tied to the user"""

    @property
    @abstractmethod
    def user_videos_dir(self) -> str:
        """:return: videos directory tied to the user"""

    @property
    @abstractmethod
    def user_music_dir(self) -> str:
        """:return: music directory tied to the user"""

    @property
    @abstractmethod
    def user_runtime_dir(self) -> str:
        """:return: runtime directory tied to the user"""

    @property
    def user_data_path(self) -> Path:
        """:return: data path tied to the user"""
        return Path(self.user_data_dir)

    @property
    def site_data_path(self) -> Path:
        """:return: data path shared by users"""
        return Path(self.site_data_dir)

    @property
    def user_config_path(self) -> Path:
        """:return: config path tied to the user"""
        return Path(self.user_config_dir)

    @property
    def site_config_path(self) -> Path:
        """:return: config path shared by the users"""
        return Path(self.site_config_dir)

    @property
    def user_cache_path(self) -> Path:
        """:return: cache path tied to the user"""
        return Path(self.user_cache_dir)

    @property
    def site_cache_path(self) -> Path:
        """:return: cache path shared by users"""
        return Path(self.site_cache_dir)

    @property
    def user_state_path(self) -> Path:
        """:return: state path tied to the user"""
        return Path(self.user_state_dir)

    @property
    def user_log_path(self) -> Path:
        """:return: log path tied to the user"""
        return Path(self.user_log_dir)

    @property
    def user_documents_path(self) -> Path:
        """:return: documents path tied to the user"""
        return Path(self.user_documents_dir)

    @property
    def user_downloads_path(self) -> Path:
        """:return: downloads path tied to the user"""
        return Path(self.user_downloads_dir)

    @property
    def user_pictures_path(self) -> Path:
        """:return: pictures path tied to the user"""
        return Path(self.user_pictures_dir)

    @property
    def user_videos_path(self) -> Path:
        """:return: videos path tied to the user"""
        return Path(self.user_videos_dir)

    @property
    def user_music_path(self) -> Path:
        """:return: music path tied to the user"""
        return Path(self.user_music_dir)

    @property
    def user_runtime_path(self) -> Path:
        """:return: runtime path tied to the user"""
        return Path(self.user_runtime_dir)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/__main__.py
# ========================================================
"""Main entry point."""
from __future__ import annotations

from pip._vendor.platformdirs import PlatformDirs, __version__

PROPS = (
    "user_data_dir",
    "user_config_dir",
    "user_cache_dir",
    "user_state_dir",
    "user_log_dir",
    "user_documents_dir",
    "user_downloads_dir",
    "user_pictures_dir",
    "user_videos_dir",
    "user_music_dir",
    "user_runtime_dir",
    "site_data_dir",
    "site_config_dir",
    "site_cache_dir",
)


def main() -> None:
    """Run main entry point."""
    app_name = "MyApp"
    app_author = "MyCompany"

    print(f"-- platformdirs {__version__} --")  # noqa: T201

    print("-- app dirs (with optional 'version')")  # noqa: T201
    dirs = PlatformDirs(app_name, app_author, version="1.0")
    for prop in PROPS:
        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201

    print("\n-- app dirs (without optional 'version')")  # noqa: T201
    dirs = PlatformDirs(app_name, app_author)
    for prop in PROPS:
        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201

    print("\n-- app dirs (without optional 'appauthor')")  # noqa: T201
    dirs = PlatformDirs(app_name)
    for prop in PROPS:
        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201

    print("\n-- app dirs (with disabled 'appauthor')")  # noqa: T201
    dirs = PlatformDirs(app_name, appauthor=False)
    for prop in PROPS:
        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201


if __name__ == "__main__":
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/windows.py
# ========================================================
"""Windows."""
from __future__ import annotations

import ctypes
import os
import sys
from functools import lru_cache
from typing import TYPE_CHECKING

from .api import PlatformDirsABC

if TYPE_CHECKING:
    from collections.abc import Callable


class Windows(PlatformDirsABC):
    """
    `MSDN on where to store app data files
    <http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120>`_.
    Makes use of the
    `appname <platformdirs.api.PlatformDirsABC.appname>`,
    `appauthor <platformdirs.api.PlatformDirsABC.appauthor>`,
    `version <platformdirs.api.PlatformDirsABC.version>`,
    `roaming <platformdirs.api.PlatformDirsABC.roaming>`,
    `opinion <platformdirs.api.PlatformDirsABC.opinion>`,
    `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    """

    @property
    def user_data_dir(self) -> str:
        """
        :return: data directory tied to the user, e.g.
         ``%USERPROFILE%\\AppData\\Local\\$appauthor\\$appname`` (not roaming) or
         ``%USERPROFILE%\\AppData\\Roaming\\$appauthor\\$appname`` (roaming)
        """
        const = "CSIDL_APPDATA" if self.roaming else "CSIDL_LOCAL_APPDATA"
        path = os.path.normpath(get_win_folder(const))
        return self._append_parts(path)

    def _append_parts(self, path: str, *, opinion_value: str | None = None) -> str:
        params = []
        if self.appname:
            if self.appauthor is not False:
                author = self.appauthor or self.appname
                params.append(author)
            params.append(self.appname)
            if opinion_value is not None and self.opinion:
                params.append(opinion_value)
            if self.version:
                params.append(self.version)
        path = os.path.join(path, *params)  # noqa: PTH118
        self._optionally_create_directory(path)
        return path

    @property
    def site_data_dir(self) -> str:
        """:return: data directory shared by users, e.g. ``C:\\ProgramData\\$appauthor\\$appname``"""
        path = os.path.normpath(get_win_folder("CSIDL_COMMON_APPDATA"))
        return self._append_parts(path)

    @property
    def user_config_dir(self) -> str:
        """:return: config directory tied to the user, same as `user_data_dir`"""
        return self.user_data_dir

    @property
    def site_config_dir(self) -> str:
        """:return: config directory shared by the users, same as `site_data_dir`"""
        return self.site_data_dir

    @property
    def user_cache_dir(self) -> str:
        """
        :return: cache directory tied to the user (if opinionated with ``Cache`` folder within ``$appname``) e.g.
         ``%USERPROFILE%\\AppData\\Local\\$appauthor\\$appname\\Cache\\$version``
        """
        path = os.path.normpath(get_win_folder("CSIDL_LOCAL_APPDATA"))
        return self._append_parts(path, opinion_value="Cache")

    @property
    def site_cache_dir(self) -> str:
        """:return: cache directory shared by users, e.g. ``C:\\ProgramData\\$appauthor\\$appname\\Cache\\$version``"""
        path = os.path.normpath(get_win_folder("CSIDL_COMMON_APPDATA"))
        return self._append_parts(path, opinion_value="Cache")

    @property
    def user_state_dir(self) -> str:
        """:return: state directory tied to the user, same as `user_data_dir`"""
        return self.user_data_dir

    @property
    def user_log_dir(self) -> str:
        """:return: log directory tied to the user, same as `user_data_dir` if not opinionated else ``Logs`` in it"""
        path = self.user_data_dir
        if self.opinion:
            path = os.path.join(path, "Logs")  # noqa: PTH118
            self._optionally_create_directory(path)
        return path

    @property
    def user_documents_dir(self) -> str:
        """:return: documents directory tied to the user e.g. ``%USERPROFILE%\\Documents``"""
        return os.path.normpath(get_win_folder("CSIDL_PERSONAL"))

    @property
    def user_downloads_dir(self) -> str:
        """:return: downloads directory tied to the user e.g. ``%USERPROFILE%\\Downloads``"""
        return os.path.normpath(get_win_folder("CSIDL_DOWNLOADS"))

    @property
    def user_pictures_dir(self) -> str:
        """:return: pictures directory tied to the user e.g. ``%USERPROFILE%\\Pictures``"""
        return os.path.normpath(get_win_folder("CSIDL_MYPICTURES"))

    @property
    def user_videos_dir(self) -> str:
        """:return: videos directory tied to the user e.g. ``%USERPROFILE%\\Videos``"""
        return os.path.normpath(get_win_folder("CSIDL_MYVIDEO"))

    @property
    def user_music_dir(self) -> str:
        """:return: music directory tied to the user e.g. ``%USERPROFILE%\\Music``"""
        return os.path.normpath(get_win_folder("CSIDL_MYMUSIC"))

    @property
    def user_runtime_dir(self) -> str:
        """
        :return: runtime directory tied to the user, e.g.
         ``%USERPROFILE%\\AppData\\Local\\Temp\\$appauthor\\$appname``
        """
        path = os.path.normpath(os.path.join(get_win_folder("CSIDL_LOCAL_APPDATA"), "Temp"))  # noqa: PTH118
        return self._append_parts(path)


def get_win_folder_from_env_vars(csidl_name: str) -> str:
    """Get folder from environment variables."""
    result = get_win_folder_if_csidl_name_not_env_var(csidl_name)
    if result is not None:
        return result

    env_var_name = {
        "CSIDL_APPDATA": "APPDATA",
        "CSIDL_COMMON_APPDATA": "ALLUSERSPROFILE",
        "CSIDL_LOCAL_APPDATA": "LOCALAPPDATA",
    }.get(csidl_name)
    if env_var_name is None:
        msg = f"Unknown CSIDL name: {csidl_name}"
        raise ValueError(msg)
    result = os.environ.get(env_var_name)
    if result is None:
        msg = f"Unset environment variable: {env_var_name}"
        raise ValueError(msg)
    return result


def get_win_folder_if_csidl_name_not_env_var(csidl_name: str) -> str | None:
    """Get folder for a CSIDL name that does not exist as an environment variable."""
    if csidl_name == "CSIDL_PERSONAL":
        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Documents")  # noqa: PTH118

    if csidl_name == "CSIDL_DOWNLOADS":
        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Downloads")  # noqa: PTH118

    if csidl_name == "CSIDL_MYPICTURES":
        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Pictures")  # noqa: PTH118

    if csidl_name == "CSIDL_MYVIDEO":
        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Videos")  # noqa: PTH118

    if csidl_name == "CSIDL_MYMUSIC":
        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Music")  # noqa: PTH118
    return None


def get_win_folder_from_registry(csidl_name: str) -> str:
    """
    Get folder from the registry.

    This is a fallback technique at best. I'm not sure if using the registry for these guarantees us the correct answer
    for all CSIDL_* names.
    """
    shell_folder_name = {
        "CSIDL_APPDATA": "AppData",
        "CSIDL_COMMON_APPDATA": "Common AppData",
        "CSIDL_LOCAL_APPDATA": "Local AppData",
        "CSIDL_PERSONAL": "Personal",
        "CSIDL_DOWNLOADS": "{374DE290-123F-4565-9164-39C4925E467B}",
        "CSIDL_MYPICTURES": "My Pictures",
        "CSIDL_MYVIDEO": "My Video",
        "CSIDL_MYMUSIC": "My Music",
    }.get(csidl_name)
    if shell_folder_name is None:
        msg = f"Unknown CSIDL name: {csidl_name}"
        raise ValueError(msg)
    if sys.platform != "win32":  # only needed for mypy type checker to know that this code runs only on Windows
        raise NotImplementedError
    import winreg

    key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r"Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders")
    directory, _ = winreg.QueryValueEx(key, shell_folder_name)
    return str(directory)


def get_win_folder_via_ctypes(csidl_name: str) -> str:
    """Get folder with ctypes."""
    # There is no 'CSIDL_DOWNLOADS'.
    # Use 'CSIDL_PROFILE' (40) and append the default folder 'Downloads' instead.
    # https://learn.microsoft.com/en-us/windows/win32/shell/knownfolderid

    csidl_const = {
        "CSIDL_APPDATA": 26,
        "CSIDL_COMMON_APPDATA": 35,
        "CSIDL_LOCAL_APPDATA": 28,
        "CSIDL_PERSONAL": 5,
        "CSIDL_MYPICTURES": 39,
        "CSIDL_MYVIDEO": 14,
        "CSIDL_MYMUSIC": 13,
        "CSIDL_DOWNLOADS": 40,
    }.get(csidl_name)
    if csidl_const is None:
        msg = f"Unknown CSIDL name: {csidl_name}"
        raise ValueError(msg)

    buf = ctypes.create_unicode_buffer(1024)
    windll = getattr(ctypes, "windll")  # noqa: B009 # using getattr to avoid false positive with mypy type checker
    windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)

    # Downgrade to short path name if it has highbit chars.
    if any(ord(c) > 255 for c in buf):  # noqa: PLR2004
        buf2 = ctypes.create_unicode_buffer(1024)
        if windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):
            buf = buf2

    if csidl_name == "CSIDL_DOWNLOADS":
        return os.path.join(buf.value, "Downloads")  # noqa: PTH118

    return buf.value


def _pick_get_win_folder() -> Callable[[str], str]:
    if hasattr(ctypes, "windll"):
        return get_win_folder_via_ctypes
    try:
        import winreg  # noqa: F401
    except ImportError:
        return get_win_folder_from_env_vars
    else:
        return get_win_folder_from_registry


get_win_folder = lru_cache(maxsize=None)(_pick_get_win_folder())

__all__ = [
    "Windows",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/platformdirs/unix.py
# ========================================================
"""Unix."""
from __future__ import annotations

import os
import sys
from configparser import ConfigParser
from pathlib import Path

from .api import PlatformDirsABC

if sys.platform == "win32":

    def getuid() -> int:
        msg = "should only be used on Unix"
        raise RuntimeError(msg)

else:
    from os import getuid


class Unix(PlatformDirsABC):
    """
    On Unix/Linux, we follow the
    `XDG Basedir Spec <https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html>`_. The spec allows
    overriding directories with environment variables. The examples show are the default values, alongside the name of
    the environment variable that overrides them. Makes use of the
    `appname <platformdirs.api.PlatformDirsABC.appname>`,
    `version <platformdirs.api.PlatformDirsABC.version>`,
    `multipath <platformdirs.api.PlatformDirsABC.multipath>`,
    `opinion <platformdirs.api.PlatformDirsABC.opinion>`,
    `ensure_exists <platformdirs.api.PlatformDirsABC.ensure_exists>`.
    """

    @property
    def user_data_dir(self) -> str:
        """
        :return: data directory tied to the user, e.g. ``~/.local/share/$appname/$version`` or
         ``$XDG_DATA_HOME/$appname/$version``
        """
        path = os.environ.get("XDG_DATA_HOME", "")
        if not path.strip():
            path = os.path.expanduser("~/.local/share")  # noqa: PTH111
        return self._append_app_name_and_version(path)

    @property
    def site_data_dir(self) -> str:
        """
        :return: data directories shared by users (if `multipath <platformdirs.api.PlatformDirsABC.multipath>` is
         enabled and ``XDG_DATA_DIR`` is set and a multi path the response is also a multi path separated by the OS
         path separator), e.g. ``/usr/local/share/$appname/$version`` or ``/usr/share/$appname/$version``
        """
        # XDG default for $XDG_DATA_DIRS; only first, if multipath is False
        path = os.environ.get("XDG_DATA_DIRS", "")
        if not path.strip():
            path = f"/usr/local/share{os.pathsep}/usr/share"
        return self._with_multi_path(path)

    def _with_multi_path(self, path: str) -> str:
        path_list = path.split(os.pathsep)
        if not self.multipath:
            path_list = path_list[0:1]
        path_list = [self._append_app_name_and_version(os.path.expanduser(p)) for p in path_list]  # noqa: PTH111
        return os.pathsep.join(path_list)

    @property
    def user_config_dir(self) -> str:
        """
        :return: config directory tied to the user, e.g. ``~/.config/$appname/$version`` or
         ``$XDG_CONFIG_HOME/$appname/$version``
        """
        path = os.environ.get("XDG_CONFIG_HOME", "")
        if not path.strip():
            path = os.path.expanduser("~/.config")  # noqa: PTH111
        return self._append_app_name_and_version(path)

    @property
    def site_config_dir(self) -> str:
        """
        :return: config directories shared by users (if `multipath <platformdirs.api.PlatformDirsABC.multipath>`
         is enabled and ``XDG_DATA_DIR`` is set and a multi path the response is also a multi path separated by the OS
         path separator), e.g. ``/etc/xdg/$appname/$version``
        """
        # XDG default for $XDG_CONFIG_DIRS only first, if multipath is False
        path = os.environ.get("XDG_CONFIG_DIRS", "")
        if not path.strip():
            path = "/etc/xdg"
        return self._with_multi_path(path)

    @property
    def user_cache_dir(self) -> str:
        """
        :return: cache directory tied to the user, e.g. ``~/.cache/$appname/$version`` or
         ``~/$XDG_CACHE_HOME/$appname/$version``
        """
        path = os.environ.get("XDG_CACHE_HOME", "")
        if not path.strip():
            path = os.path.expanduser("~/.cache")  # noqa: PTH111
        return self._append_app_name_and_version(path)

    @property
    def site_cache_dir(self) -> str:
        """:return: cache directory shared by users, e.g. ``/var/tmp/$appname/$version``"""
        return self._append_app_name_and_version("/var/tmp")  # noqa: S108

    @property
    def user_state_dir(self) -> str:
        """
        :return: state directory tied to the user, e.g. ``~/.local/state/$appname/$version`` or
         ``$XDG_STATE_HOME/$appname/$version``
        """
        path = os.environ.get("XDG_STATE_HOME", "")
        if not path.strip():
            path = os.path.expanduser("~/.local/state")  # noqa: PTH111
        return self._append_app_name_and_version(path)

    @property
    def user_log_dir(self) -> str:
        """:return: log directory tied to the user, same as `user_state_dir` if not opinionated else ``log`` in it"""
        path = self.user_state_dir
        if self.opinion:
            path = os.path.join(path, "log")  # noqa: PTH118
        return path

    @property
    def user_documents_dir(self) -> str:
        """:return: documents directory tied to the user, e.g. ``~/Documents``"""
        return _get_user_media_dir("XDG_DOCUMENTS_DIR", "~/Documents")

    @property
    def user_downloads_dir(self) -> str:
        """:return: downloads directory tied to the user, e.g. ``~/Downloads``"""
        return _get_user_media_dir("XDG_DOWNLOAD_DIR", "~/Downloads")

    @property
    def user_pictures_dir(self) -> str:
        """:return: pictures directory tied to the user, e.g. ``~/Pictures``"""
        return _get_user_media_dir("XDG_PICTURES_DIR", "~/Pictures")

    @property
    def user_videos_dir(self) -> str:
        """:return: videos directory tied to the user, e.g. ``~/Videos``"""
        return _get_user_media_dir("XDG_VIDEOS_DIR", "~/Videos")

    @property
    def user_music_dir(self) -> str:
        """:return: music directory tied to the user, e.g. ``~/Music``"""
        return _get_user_media_dir("XDG_MUSIC_DIR", "~/Music")

    @property
    def user_runtime_dir(self) -> str:
        """
        :return: runtime directory tied to the user, e.g. ``/run/user/$(id -u)/$appname/$version`` or
         ``$XDG_RUNTIME_DIR/$appname/$version``.

         For FreeBSD/OpenBSD/NetBSD, it would return ``/var/run/user/$(id -u)/$appname/$version`` if
         exists, otherwise ``/tmp/runtime-$(id -u)/$appname/$version``, if``$XDG_RUNTIME_DIR``
         is not set.
        """
        path = os.environ.get("XDG_RUNTIME_DIR", "")
        if not path.strip():
            if sys.platform.startswith(("freebsd", "openbsd", "netbsd")):
                path = f"/var/run/user/{getuid()}"
                if not Path(path).exists():
                    path = f"/tmp/runtime-{getuid()}"  # noqa: S108
            else:
                path = f"/run/user/{getuid()}"
        return self._append_app_name_and_version(path)

    @property
    def site_data_path(self) -> Path:
        """:return: data path shared by users. Only return first item, even if ``multipath`` is set to ``True``"""
        return self._first_item_as_path_if_multipath(self.site_data_dir)

    @property
    def site_config_path(self) -> Path:
        """:return: config path shared by the users. Only return first item, even if ``multipath`` is set to ``True``"""
        return self._first_item_as_path_if_multipath(self.site_config_dir)

    @property
    def site_cache_path(self) -> Path:
        """:return: cache path shared by users. Only return first item, even if ``multipath`` is set to ``True``"""
        return self._first_item_as_path_if_multipath(self.site_cache_dir)

    def _first_item_as_path_if_multipath(self, directory: str) -> Path:
        if self.multipath:
            # If multipath is True, the first path is returned.
            directory = directory.split(os.pathsep)[0]
        return Path(directory)


def _get_user_media_dir(env_var: str, fallback_tilde_path: str) -> str:
    media_dir = _get_user_dirs_folder(env_var)
    if media_dir is None:
        media_dir = os.environ.get(env_var, "").strip()
        if not media_dir:
            media_dir = os.path.expanduser(fallback_tilde_path)  # noqa: PTH111

    return media_dir


def _get_user_dirs_folder(key: str) -> str | None:
    """Return directory from user-dirs.dirs config file. See https://freedesktop.org/wiki/Software/xdg-user-dirs/."""
    user_dirs_config_path = Path(Unix().user_config_dir) / "user-dirs.dirs"
    if user_dirs_config_path.exists():
        parser = ConfigParser()

        with user_dirs_config_path.open() as stream:
            # Add fake section header, so ConfigParser doesn't complain
            parser.read_string(f"[top]\n{stream.read()}")

        if key not in parser["top"]:
            return None

        path = parser["top"][key].strip('"')
        # Handle relative home paths
        return path.replace("$HOME", os.path.expanduser("~"))  # noqa: PTH111

    return None


__all__ = [
    "Unix",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/poolmanager.py
# ========================================================
from __future__ import absolute_import

import collections
import functools
import logging

from ._collections import HTTPHeaderDict, RecentlyUsedContainer
from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, port_by_scheme
from .exceptions import (
    LocationValueError,
    MaxRetryError,
    ProxySchemeUnknown,
    ProxySchemeUnsupported,
    URLSchemeUnknown,
)
from .packages import six
from .packages.six.moves.urllib.parse import urljoin
from .request import RequestMethods
from .util.proxy import connection_requires_http_tunnel
from .util.retry import Retry
from .util.url import parse_url

__all__ = ["PoolManager", "ProxyManager", "proxy_from_url"]


log = logging.getLogger(__name__)

SSL_KEYWORDS = (
    "key_file",
    "cert_file",
    "cert_reqs",
    "ca_certs",
    "ssl_version",
    "ca_cert_dir",
    "ssl_context",
    "key_password",
    "server_hostname",
)

# All known keyword arguments that could be provided to the pool manager, its
# pools, or the underlying connections. This is used to construct a pool key.
_key_fields = (
    "key_scheme",  # str
    "key_host",  # str
    "key_port",  # int
    "key_timeout",  # int or float or Timeout
    "key_retries",  # int or Retry
    "key_strict",  # bool
    "key_block",  # bool
    "key_source_address",  # str
    "key_key_file",  # str
    "key_key_password",  # str
    "key_cert_file",  # str
    "key_cert_reqs",  # str
    "key_ca_certs",  # str
    "key_ssl_version",  # str
    "key_ca_cert_dir",  # str
    "key_ssl_context",  # instance of ssl.SSLContext or urllib3.util.ssl_.SSLContext
    "key_maxsize",  # int
    "key_headers",  # dict
    "key__proxy",  # parsed proxy url
    "key__proxy_headers",  # dict
    "key__proxy_config",  # class
    "key_socket_options",  # list of (level (int), optname (int), value (int or str)) tuples
    "key__socks_options",  # dict
    "key_assert_hostname",  # bool or string
    "key_assert_fingerprint",  # str
    "key_server_hostname",  # str
)

#: The namedtuple class used to construct keys for the connection pool.
#: All custom key schemes should include the fields in this key at a minimum.
PoolKey = collections.namedtuple("PoolKey", _key_fields)

_proxy_config_fields = ("ssl_context", "use_forwarding_for_https")
ProxyConfig = collections.namedtuple("ProxyConfig", _proxy_config_fields)


def _default_key_normalizer(key_class, request_context):
    """
    Create a pool key out of a request context dictionary.

    According to RFC 3986, both the scheme and host are case-insensitive.
    Therefore, this function normalizes both before constructing the pool
    key for an HTTPS request. If you wish to change this behaviour, provide
    alternate callables to ``key_fn_by_scheme``.

    :param key_class:
        The class to use when constructing the key. This should be a namedtuple
        with the ``scheme`` and ``host`` keys at a minimum.
    :type  key_class: namedtuple
    :param request_context:
        A dictionary-like object that contain the context for a request.
    :type  request_context: dict

    :return: A namedtuple that can be used as a connection pool key.
    :rtype:  PoolKey
    """
    # Since we mutate the dictionary, make a copy first
    context = request_context.copy()
    context["scheme"] = context["scheme"].lower()
    context["host"] = context["host"].lower()

    # These are both dictionaries and need to be transformed into frozensets
    for key in ("headers", "_proxy_headers", "_socks_options"):
        if key in context and context[key] is not None:
            context[key] = frozenset(context[key].items())

    # The socket_options key may be a list and needs to be transformed into a
    # tuple.
    socket_opts = context.get("socket_options")
    if socket_opts is not None:
        context["socket_options"] = tuple(socket_opts)

    # Map the kwargs to the names in the namedtuple - this is necessary since
    # namedtuples can't have fields starting with '_'.
    for key in list(context.keys()):
        context["key_" + key] = context.pop(key)

    # Default to ``None`` for keys missing from the context
    for field in key_class._fields:
        if field not in context:
            context[field] = None

    return key_class(**context)


#: A dictionary that maps a scheme to a callable that creates a pool key.
#: This can be used to alter the way pool keys are constructed, if desired.
#: Each PoolManager makes a copy of this dictionary so they can be configured
#: globally here, or individually on the instance.
key_fn_by_scheme = {
    "http": functools.partial(_default_key_normalizer, PoolKey),
    "https": functools.partial(_default_key_normalizer, PoolKey),
}

pool_classes_by_scheme = {"http": HTTPConnectionPool, "https": HTTPSConnectionPool}


class PoolManager(RequestMethods):
    """
    Allows for arbitrary requests while transparently keeping track of
    necessary connection pools for you.

    :param num_pools:
        Number of connection pools to cache before discarding the least
        recently used pool.

    :param headers:
        Headers to include with all requests, unless other headers are given
        explicitly.

    :param \\**connection_pool_kw:
        Additional parameters are used to create fresh
        :class:`urllib3.connectionpool.ConnectionPool` instances.

    Example::

        >>> manager = PoolManager(num_pools=2)
        >>> r = manager.request('GET', 'http://google.com/')
        >>> r = manager.request('GET', 'http://google.com/mail')
        >>> r = manager.request('GET', 'http://yahoo.com/')
        >>> len(manager.pools)
        2

    """

    proxy = None
    proxy_config = None

    def __init__(self, num_pools=10, headers=None, **connection_pool_kw):
        RequestMethods.__init__(self, headers)
        if "retries" in connection_pool_kw:
            retries = connection_pool_kw["retries"]
            if not isinstance(retries, Retry):
                # When Retry is initialized, raise_on_redirect is based
                # on a redirect boolean value.
                # But requests made via a pool manager always set
                # redirect to False, and raise_on_redirect always ends
                # up being False consequently.
                # Here we fix the issue by setting raise_on_redirect to
                # a value needed by the pool manager without considering
                # the redirect boolean.
                raise_on_redirect = retries is not False
                retries = Retry.from_int(retries, redirect=False)
                retries.raise_on_redirect = raise_on_redirect
                connection_pool_kw = connection_pool_kw.copy()
                connection_pool_kw["retries"] = retries
        self.connection_pool_kw = connection_pool_kw
        self.pools = RecentlyUsedContainer(num_pools)

        # Locally set the pool classes and keys so other PoolManagers can
        # override them.
        self.pool_classes_by_scheme = pool_classes_by_scheme
        self.key_fn_by_scheme = key_fn_by_scheme.copy()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.clear()
        # Return False to re-raise any potential exceptions
        return False

    def _new_pool(self, scheme, host, port, request_context=None):
        """
        Create a new :class:`urllib3.connectionpool.ConnectionPool` based on host, port, scheme, and
        any additional pool keyword arguments.

        If ``request_context`` is provided, it is provided as keyword arguments
        to the pool class used. This method is used to actually create the
        connection pools handed out by :meth:`connection_from_url` and
        companion methods. It is intended to be overridden for customization.
        """
        pool_cls = self.pool_classes_by_scheme[scheme]
        if request_context is None:
            request_context = self.connection_pool_kw.copy()

        # Although the context has everything necessary to create the pool,
        # this function has historically only used the scheme, host, and port
        # in the positional args. When an API change is acceptable these can
        # be removed.
        for key in ("scheme", "host", "port"):
            request_context.pop(key, None)

        if scheme == "http":
            for kw in SSL_KEYWORDS:
                request_context.pop(kw, None)

        return pool_cls(host, port, **request_context)

    def clear(self):
        """
        Empty our store of pools and direct them all to close.

        This will not affect in-flight connections, but they will not be
        re-used after completion.
        """
        self.pools.clear()

    def connection_from_host(self, host, port=None, scheme="http", pool_kwargs=None):
        """
        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the host, port, and scheme.

        If ``port`` isn't given, it will be derived from the ``scheme`` using
        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is
        provided, it is merged with the instance's ``connection_pool_kw``
        variable and used to create the new connection pool, if one is
        needed.
        """

        if not host:
            raise LocationValueError("No host specified.")

        request_context = self._merge_pool_kwargs(pool_kwargs)
        request_context["scheme"] = scheme or "http"
        if not port:
            port = port_by_scheme.get(request_context["scheme"].lower(), 80)
        request_context["port"] = port
        request_context["host"] = host

        return self.connection_from_context(request_context)

    def connection_from_context(self, request_context):
        """
        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the request context.

        ``request_context`` must at least contain the ``scheme`` key and its
        value must be a key in ``key_fn_by_scheme`` instance variable.
        """
        scheme = request_context["scheme"].lower()
        pool_key_constructor = self.key_fn_by_scheme.get(scheme)
        if not pool_key_constructor:
            raise URLSchemeUnknown(scheme)
        pool_key = pool_key_constructor(request_context)

        return self.connection_from_pool_key(pool_key, request_context=request_context)

    def connection_from_pool_key(self, pool_key, request_context=None):
        """
        Get a :class:`urllib3.connectionpool.ConnectionPool` based on the provided pool key.

        ``pool_key`` should be a namedtuple that only contains immutable
        objects. At a minimum it must have the ``scheme``, ``host``, and
        ``port`` fields.
        """
        with self.pools.lock:
            # If the scheme, host, or port doesn't match existing open
            # connections, open a new ConnectionPool.
            pool = self.pools.get(pool_key)
            if pool:
                return pool

            # Make a fresh ConnectionPool of the desired type
            scheme = request_context["scheme"]
            host = request_context["host"]
            port = request_context["port"]
            pool = self._new_pool(scheme, host, port, request_context=request_context)
            self.pools[pool_key] = pool

        return pool

    def connection_from_url(self, url, pool_kwargs=None):
        """
        Similar to :func:`urllib3.connectionpool.connection_from_url`.

        If ``pool_kwargs`` is not provided and a new pool needs to be
        constructed, ``self.connection_pool_kw`` is used to initialize
        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``
        is provided, it is used instead. Note that if a new pool does not
        need to be created for the request, the provided ``pool_kwargs`` are
        not used.
        """
        u = parse_url(url)
        return self.connection_from_host(
            u.host, port=u.port, scheme=u.scheme, pool_kwargs=pool_kwargs
        )

    def _merge_pool_kwargs(self, override):
        """
        Merge a dictionary of override values for self.connection_pool_kw.

        This does not modify self.connection_pool_kw and returns a new dict.
        Any keys in the override dictionary with a value of ``None`` are
        removed from the merged dictionary.
        """
        base_pool_kwargs = self.connection_pool_kw.copy()
        if override:
            for key, value in override.items():
                if value is None:
                    try:
                        del base_pool_kwargs[key]
                    except KeyError:
                        pass
                else:
                    base_pool_kwargs[key] = value
        return base_pool_kwargs

    def _proxy_requires_url_absolute_form(self, parsed_url):
        """
        Indicates if the proxy requires the complete destination URL in the
        request.  Normally this is only needed when not using an HTTP CONNECT
        tunnel.
        """
        if self.proxy is None:
            return False

        return not connection_requires_http_tunnel(
            self.proxy, self.proxy_config, parsed_url.scheme
        )

    def _validate_proxy_scheme_url_selection(self, url_scheme):
        """
        Validates that were not attempting to do TLS in TLS connections on
        Python2 or with unsupported SSL implementations.
        """
        if self.proxy is None or url_scheme != "https":
            return

        if self.proxy.scheme != "https":
            return

        if six.PY2 and not self.proxy_config.use_forwarding_for_https:
            raise ProxySchemeUnsupported(
                "Contacting HTTPS destinations through HTTPS proxies "
                "'via CONNECT tunnels' is not supported in Python 2"
            )

    def urlopen(self, method, url, redirect=True, **kw):
        """
        Same as :meth:`urllib3.HTTPConnectionPool.urlopen`
        with custom cross-host redirect logic and only sends the request-uri
        portion of the ``url``.

        The given ``url`` parameter must be absolute, such that an appropriate
        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.
        """
        u = parse_url(url)
        self._validate_proxy_scheme_url_selection(u.scheme)

        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)

        kw["assert_same_host"] = False
        kw["redirect"] = False

        if "headers" not in kw:
            kw["headers"] = self.headers.copy()

        if self._proxy_requires_url_absolute_form(u):
            response = conn.urlopen(method, url, **kw)
        else:
            response = conn.urlopen(method, u.request_uri, **kw)

        redirect_location = redirect and response.get_redirect_location()
        if not redirect_location:
            return response

        # Support relative URLs for redirecting.
        redirect_location = urljoin(url, redirect_location)

        if response.status == 303:
            # Change the method according to RFC 9110, Section 15.4.4.
            method = "GET"
            # And lose the body not to transfer anything sensitive.
            kw["body"] = None
            kw["headers"] = HTTPHeaderDict(kw["headers"])._prepare_for_method_change()

        retries = kw.get("retries", response.retries)
        if not isinstance(retries, Retry):
            retries = Retry.from_int(retries, redirect=redirect)

        # Strip headers marked as unsafe to forward to the redirected location.
        # Check remove_headers_on_redirect to avoid a potential network call within
        # conn.is_same_host() which may use socket.gethostbyname() in the future.
        if retries.remove_headers_on_redirect and not conn.is_same_host(
            redirect_location
        ):
            headers = list(six.iterkeys(kw["headers"]))
            for header in headers:
                if header.lower() in retries.remove_headers_on_redirect:
                    kw["headers"].pop(header, None)

        try:
            retries = retries.increment(method, url, response=response, _pool=conn)
        except MaxRetryError:
            if retries.raise_on_redirect:
                response.drain_conn()
                raise
            return response

        kw["retries"] = retries
        kw["redirect"] = redirect

        log.info("Redirecting %s -> %s", url, redirect_location)

        response.drain_conn()
        return self.urlopen(method, redirect_location, **kw)


class ProxyManager(PoolManager):
    """
    Behaves just like :class:`PoolManager`, but sends all requests through
    the defined proxy, using the CONNECT method for HTTPS URLs.

    :param proxy_url:
        The URL of the proxy to be used.

    :param proxy_headers:
        A dictionary containing headers that will be sent to the proxy. In case
        of HTTP they are being sent with each request, while in the
        HTTPS/CONNECT case they are sent only once. Could be used for proxy
        authentication.

    :param proxy_ssl_context:
        The proxy SSL context is used to establish the TLS connection to the
        proxy when using HTTPS proxies.

    :param use_forwarding_for_https:
        (Defaults to False) If set to True will forward requests to the HTTPS
        proxy to be made on behalf of the client instead of creating a TLS
        tunnel via the CONNECT method. **Enabling this flag means that request
        and response headers and content will be visible from the HTTPS proxy**
        whereas tunneling keeps request and response headers and content
        private.  IP address, target hostname, SNI, and port are always visible
        to an HTTPS proxy even when this flag is disabled.

    Example:
        >>> proxy = urllib3.ProxyManager('http://localhost:3128/')
        >>> r1 = proxy.request('GET', 'http://google.com/')
        >>> r2 = proxy.request('GET', 'http://httpbin.org/')
        >>> len(proxy.pools)
        1
        >>> r3 = proxy.request('GET', 'https://httpbin.org/')
        >>> r4 = proxy.request('GET', 'https://twitter.com/')
        >>> len(proxy.pools)
        3

    """

    def __init__(
        self,
        proxy_url,
        num_pools=10,
        headers=None,
        proxy_headers=None,
        proxy_ssl_context=None,
        use_forwarding_for_https=False,
        **connection_pool_kw
    ):

        if isinstance(proxy_url, HTTPConnectionPool):
            proxy_url = "%s://%s:%i" % (
                proxy_url.scheme,
                proxy_url.host,
                proxy_url.port,
            )
        proxy = parse_url(proxy_url)

        if proxy.scheme not in ("http", "https"):
            raise ProxySchemeUnknown(proxy.scheme)

        if not proxy.port:
            port = port_by_scheme.get(proxy.scheme, 80)
            proxy = proxy._replace(port=port)

        self.proxy = proxy
        self.proxy_headers = proxy_headers or {}
        self.proxy_ssl_context = proxy_ssl_context
        self.proxy_config = ProxyConfig(proxy_ssl_context, use_forwarding_for_https)

        connection_pool_kw["_proxy"] = self.proxy
        connection_pool_kw["_proxy_headers"] = self.proxy_headers
        connection_pool_kw["_proxy_config"] = self.proxy_config

        super(ProxyManager, self).__init__(num_pools, headers, **connection_pool_kw)

    def connection_from_host(self, host, port=None, scheme="http", pool_kwargs=None):
        if scheme == "https":
            return super(ProxyManager, self).connection_from_host(
                host, port, scheme, pool_kwargs=pool_kwargs
            )

        return super(ProxyManager, self).connection_from_host(
            self.proxy.host, self.proxy.port, self.proxy.scheme, pool_kwargs=pool_kwargs
        )

    def _set_proxy_headers(self, url, headers=None):
        """
        Sets headers needed by proxies: specifically, the Accept and Host
        headers. Only sets headers not provided by the user.
        """
        headers_ = {"Accept": "*/*"}

        netloc = parse_url(url).netloc
        if netloc:
            headers_["Host"] = netloc

        if headers:
            headers_.update(headers)
        return headers_

    def urlopen(self, method, url, redirect=True, **kw):
        "Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute."
        u = parse_url(url)
        if not connection_requires_http_tunnel(self.proxy, self.proxy_config, u.scheme):
            # For connections using HTTP CONNECT, httplib sets the necessary
            # headers on the CONNECT to the proxy. If we're not using CONNECT,
            # we'll definitely need to set 'Host' at the very least.
            headers = kw.get("headers", self.headers)
            kw["headers"] = self._set_proxy_headers(url, headers)

        return super(ProxyManager, self).urlopen(method, url, redirect=redirect, **kw)


def proxy_from_url(url, **kw):
    return ProxyManager(proxy_url=url, **kw)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/backports/__init__.py
# ========================================================


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/backports/weakref_finalize.py
# ========================================================
# -*- coding: utf-8 -*-
"""
backports.weakref_finalize
~~~~~~~~~~~~~~~~~~

Backports the Python 3 ``weakref.finalize`` method.
"""
from __future__ import absolute_import

import itertools
import sys
from weakref import ref

__all__ = ["weakref_finalize"]


class weakref_finalize(object):
    """Class for finalization of weakrefable objects
    finalize(obj, func, *args, **kwargs) returns a callable finalizer
    object which will be called when obj is garbage collected. The
    first time the finalizer is called it evaluates func(*arg, **kwargs)
    and returns the result. After this the finalizer is dead, and
    calling it just returns None.
    When the program exits any remaining finalizers for which the
    atexit attribute is true will be run in reverse order of creation.
    By default atexit is true.
    """

    # Finalizer objects don't have any state of their own.  They are
    # just used as keys to lookup _Info objects in the registry.  This
    # ensures that they cannot be part of a ref-cycle.

    __slots__ = ()
    _registry = {}
    _shutdown = False
    _index_iter = itertools.count()
    _dirty = False
    _registered_with_atexit = False

    class _Info(object):
        __slots__ = ("weakref", "func", "args", "kwargs", "atexit", "index")

    def __init__(self, obj, func, *args, **kwargs):
        if not self._registered_with_atexit:
            # We may register the exit function more than once because
            # of a thread race, but that is harmless
            import atexit

            atexit.register(self._exitfunc)
            weakref_finalize._registered_with_atexit = True
        info = self._Info()
        info.weakref = ref(obj, self)
        info.func = func
        info.args = args
        info.kwargs = kwargs or None
        info.atexit = True
        info.index = next(self._index_iter)
        self._registry[self] = info
        weakref_finalize._dirty = True

    def __call__(self, _=None):
        """If alive then mark as dead and return func(*args, **kwargs);
        otherwise return None"""
        info = self._registry.pop(self, None)
        if info and not self._shutdown:
            return info.func(*info.args, **(info.kwargs or {}))

    def detach(self):
        """If alive then mark as dead and return (obj, func, args, kwargs);
        otherwise return None"""
        info = self._registry.get(self)
        obj = info and info.weakref()
        if obj is not None and self._registry.pop(self, None):
            return (obj, info.func, info.args, info.kwargs or {})

    def peek(self):
        """If alive then return (obj, func, args, kwargs);
        otherwise return None"""
        info = self._registry.get(self)
        obj = info and info.weakref()
        if obj is not None:
            return (obj, info.func, info.args, info.kwargs or {})

    @property
    def alive(self):
        """Whether finalizer is alive"""
        return self in self._registry

    @property
    def atexit(self):
        """Whether finalizer should be called at exit"""
        info = self._registry.get(self)
        return bool(info) and info.atexit

    @atexit.setter
    def atexit(self, value):
        info = self._registry.get(self)
        if info:
            info.atexit = bool(value)

    def __repr__(self):
        info = self._registry.get(self)
        obj = info and info.weakref()
        if obj is None:
            return "<%s object at %#x; dead>" % (type(self).__name__, id(self))
        else:
            return "<%s object at %#x; for %r at %#x>" % (
                type(self).__name__,
                id(self),
                type(obj).__name__,
                id(obj),
            )

    @classmethod
    def _select_for_exit(cls):
        # Return live finalizers marked for exit, oldest first
        L = [(f, i) for (f, i) in cls._registry.items() if i.atexit]
        L.sort(key=lambda item: item[1].index)
        return [f for (f, i) in L]

    @classmethod
    def _exitfunc(cls):
        # At shutdown invoke finalizers for which atexit is true.
        # This is called once all other non-daemonic threads have been
        # joined.
        reenable_gc = False
        try:
            if cls._registry:
                import gc

                if gc.isenabled():
                    reenable_gc = True
                    gc.disable()
                pending = None
                while True:
                    if pending is None or weakref_finalize._dirty:
                        pending = cls._select_for_exit()
                        weakref_finalize._dirty = False
                    if not pending:
                        break
                    f = pending.pop()
                    try:
                        # gc is disabled, so (assuming no daemonic
                        # threads) the following is the only line in
                        # this function which might trigger creation
                        # of a new finalizer
                        f()
                    except Exception:
                        sys.excepthook(*sys.exc_info())
                    assert f not in cls._registry
        finally:
            # prevent any more finalizers from executing during shutdown
            weakref_finalize._shutdown = True
            if reenable_gc:
                gc.enable()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/backports/makefile.py
# ========================================================
# -*- coding: utf-8 -*-
"""
backports.makefile
~~~~~~~~~~~~~~~~~~

Backports the Python 3 ``socket.makefile`` method for use with anything that
wants to create a "fake" socket object.
"""
import io
from socket import SocketIO


def backport_makefile(
    self, mode="r", buffering=None, encoding=None, errors=None, newline=None
):
    """
    Backport of ``socket.makefile`` from Python 3.5.
    """
    if not set(mode) <= {"r", "w", "b"}:
        raise ValueError("invalid mode %r (only r, w, b allowed)" % (mode,))
    writing = "w" in mode
    reading = "r" in mode or not writing
    assert reading or writing
    binary = "b" in mode
    rawmode = ""
    if reading:
        rawmode += "r"
    if writing:
        rawmode += "w"
    raw = SocketIO(self, rawmode)
    self._makefile_refs += 1
    if buffering is None:
        buffering = -1
    if buffering < 0:
        buffering = io.DEFAULT_BUFFER_SIZE
    if buffering == 0:
        if not binary:
            raise ValueError("unbuffered streams must be binary")
        return raw
    if reading and writing:
        buffer = io.BufferedRWPair(raw, raw, buffering)
    elif reading:
        buffer = io.BufferedReader(raw, buffering)
    else:
        assert writing
        buffer = io.BufferedWriter(raw, buffering)
    if binary:
        return buffer
    text = io.TextIOWrapper(buffer, encoding, errors, newline)
    text.mode = mode
    return text


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/__init__.py
# ========================================================


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py
# ========================================================
# Copyright (c) 2010-2020 Benjamin Peterson
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""Utilities for writing code that runs on Python 2 and 3"""

from __future__ import absolute_import

import functools
import itertools
import operator
import sys
import types

__author__ = "Benjamin Peterson <benjamin@python.org>"
__version__ = "1.16.0"


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3
PY34 = sys.version_info[0:2] >= (3, 4)

if PY3:
    string_types = (str,)
    integer_types = (int,)
    class_types = (type,)
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize
else:
    string_types = (basestring,)
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str

    if sys.platform.startswith("java"):
        # Jython always uses 32 bits.
        MAXSIZE = int((1 << 31) - 1)
    else:
        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
        class X(object):
            def __len__(self):
                return 1 << 31

        try:
            len(X())
        except OverflowError:
            # 32-bit
            MAXSIZE = int((1 << 31) - 1)
        else:
            # 64-bit
            MAXSIZE = int((1 << 63) - 1)
        del X

if PY34:
    from importlib.util import spec_from_loader
else:
    spec_from_loader = None


def _add_doc(func, doc):
    """Add documentation to a function."""
    func.__doc__ = doc


def _import_module(name):
    """Import module, returning the module after the last dot."""
    __import__(name)
    return sys.modules[name]


class _LazyDescr(object):
    def __init__(self, name):
        self.name = name

    def __get__(self, obj, tp):
        result = self._resolve()
        setattr(obj, self.name, result)  # Invokes __set__.
        try:
            # This is a bit ugly, but it avoids running this again by
            # removing this descriptor.
            delattr(obj.__class__, self.name)
        except AttributeError:
            pass
        return result


class MovedModule(_LazyDescr):
    def __init__(self, name, old, new=None):
        super(MovedModule, self).__init__(name)
        if PY3:
            if new is None:
                new = name
            self.mod = new
        else:
            self.mod = old

    def _resolve(self):
        return _import_module(self.mod)

    def __getattr__(self, attr):
        _module = self._resolve()
        value = getattr(_module, attr)
        setattr(self, attr, value)
        return value


class _LazyModule(types.ModuleType):
    def __init__(self, name):
        super(_LazyModule, self).__init__(name)
        self.__doc__ = self.__class__.__doc__

    def __dir__(self):
        attrs = ["__doc__", "__name__"]
        attrs += [attr.name for attr in self._moved_attributes]
        return attrs

    # Subclasses should override this
    _moved_attributes = []


class MovedAttribute(_LazyDescr):
    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
        super(MovedAttribute, self).__init__(name)
        if PY3:
            if new_mod is None:
                new_mod = name
            self.mod = new_mod
            if new_attr is None:
                if old_attr is None:
                    new_attr = name
                else:
                    new_attr = old_attr
            self.attr = new_attr
        else:
            self.mod = old_mod
            if old_attr is None:
                old_attr = name
            self.attr = old_attr

    def _resolve(self):
        module = _import_module(self.mod)
        return getattr(module, self.attr)


class _SixMetaPathImporter(object):

    """
    A meta path importer to import six.moves and its submodules.

    This class implements a PEP302 finder and loader. It should be compatible
    with Python 2.5 and all existing versions of Python3
    """

    def __init__(self, six_module_name):
        self.name = six_module_name
        self.known_modules = {}

    def _add_module(self, mod, *fullnames):
        for fullname in fullnames:
            self.known_modules[self.name + "." + fullname] = mod

    def _get_module(self, fullname):
        return self.known_modules[self.name + "." + fullname]

    def find_module(self, fullname, path=None):
        if fullname in self.known_modules:
            return self
        return None

    def find_spec(self, fullname, path, target=None):
        if fullname in self.known_modules:
            return spec_from_loader(fullname, self)
        return None

    def __get_module(self, fullname):
        try:
            return self.known_modules[fullname]
        except KeyError:
            raise ImportError("This loader does not know module " + fullname)

    def load_module(self, fullname):
        try:
            # in case of a reload
            return sys.modules[fullname]
        except KeyError:
            pass
        mod = self.__get_module(fullname)
        if isinstance(mod, MovedModule):
            mod = mod._resolve()
        else:
            mod.__loader__ = self
        sys.modules[fullname] = mod
        return mod

    def is_package(self, fullname):
        """
        Return true, if the named module is a package.

        We need this method to get correct spec objects with
        Python 3.4 (see PEP451)
        """
        return hasattr(self.__get_module(fullname), "__path__")

    def get_code(self, fullname):
        """Return None

        Required, if is_package is implemented"""
        self.__get_module(fullname)  # eventually raises ImportError
        return None

    get_source = get_code  # same as get_code

    def create_module(self, spec):
        return self.load_module(spec.name)

    def exec_module(self, module):
        pass


_importer = _SixMetaPathImporter(__name__)


class _MovedItems(_LazyModule):

    """Lazy loading of moved objects"""

    __path__ = []  # mark as package


_moved_attributes = [
    MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
    MovedAttribute("filter", "itertools", "builtins", "ifilter", "filter"),
    MovedAttribute(
        "filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"
    ),
    MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
    MovedAttribute("intern", "__builtin__", "sys"),
    MovedAttribute("map", "itertools", "builtins", "imap", "map"),
    MovedAttribute("getcwd", "os", "os", "getcwdu", "getcwd"),
    MovedAttribute("getcwdb", "os", "os", "getcwd", "getcwdb"),
    MovedAttribute("getoutput", "commands", "subprocess"),
    MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
    MovedAttribute(
        "reload_module", "__builtin__", "importlib" if PY34 else "imp", "reload"
    ),
    MovedAttribute("reduce", "__builtin__", "functools"),
    MovedAttribute("shlex_quote", "pipes", "shlex", "quote"),
    MovedAttribute("StringIO", "StringIO", "io"),
    MovedAttribute("UserDict", "UserDict", "collections"),
    MovedAttribute("UserList", "UserList", "collections"),
    MovedAttribute("UserString", "UserString", "collections"),
    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
    MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
    MovedAttribute(
        "zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"
    ),
    MovedModule("builtins", "__builtin__"),
    MovedModule("configparser", "ConfigParser"),
    MovedModule(
        "collections_abc",
        "collections",
        "collections.abc" if sys.version_info >= (3, 3) else "collections",
    ),
    MovedModule("copyreg", "copy_reg"),
    MovedModule("dbm_gnu", "gdbm", "dbm.gnu"),
    MovedModule("dbm_ndbm", "dbm", "dbm.ndbm"),
    MovedModule(
        "_dummy_thread",
        "dummy_thread",
        "_dummy_thread" if sys.version_info < (3, 9) else "_thread",
    ),
    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
    MovedModule("http_cookies", "Cookie", "http.cookies"),
    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
    MovedModule("html_parser", "HTMLParser", "html.parser"),
    MovedModule("http_client", "httplib", "http.client"),
    MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
    MovedModule("email_mime_image", "email.MIMEImage", "email.mime.image"),
    MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
    MovedModule(
        "email_mime_nonmultipart", "email.MIMENonMultipart", "email.mime.nonmultipart"
    ),
    MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
    MovedModule("cPickle", "cPickle", "pickle"),
    MovedModule("queue", "Queue"),
    MovedModule("reprlib", "repr"),
    MovedModule("socketserver", "SocketServer"),
    MovedModule("_thread", "thread", "_thread"),
    MovedModule("tkinter", "Tkinter"),
    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
    MovedModule("tkinter_ttk", "ttk", "tkinter.ttk"),
    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
    MovedModule("tkinter_colorchooser", "tkColorChooser", "tkinter.colorchooser"),
    MovedModule("tkinter_commondialog", "tkCommonDialog", "tkinter.commondialog"),
    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog", "tkinter.simpledialog"),
    MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
    MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
    MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
    MovedModule("xmlrpc_client", "xmlrpclib", "xmlrpc.client"),
    MovedModule("xmlrpc_server", "SimpleXMLRPCServer", "xmlrpc.server"),
]
# Add windows specific modules.
if sys.platform == "win32":
    _moved_attributes += [
        MovedModule("winreg", "_winreg"),
    ]

for attr in _moved_attributes:
    setattr(_MovedItems, attr.name, attr)
    if isinstance(attr, MovedModule):
        _importer._add_module(attr, "moves." + attr.name)
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = _MovedItems(__name__ + ".moves")
_importer._add_module(moves, "moves")


class Module_six_moves_urllib_parse(_LazyModule):

    """Lazy loading of moved objects in six.moves.urllib_parse"""


_urllib_parse_moved_attributes = [
    MovedAttribute("ParseResult", "urlparse", "urllib.parse"),
    MovedAttribute("SplitResult", "urlparse", "urllib.parse"),
    MovedAttribute("parse_qs", "urlparse", "urllib.parse"),
    MovedAttribute("parse_qsl", "urlparse", "urllib.parse"),
    MovedAttribute("urldefrag", "urlparse", "urllib.parse"),
    MovedAttribute("urljoin", "urlparse", "urllib.parse"),
    MovedAttribute("urlparse", "urlparse", "urllib.parse"),
    MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
    MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
    MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
    MovedAttribute("quote", "urllib", "urllib.parse"),
    MovedAttribute("quote_plus", "urllib", "urllib.parse"),
    MovedAttribute("unquote", "urllib", "urllib.parse"),
    MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
    MovedAttribute(
        "unquote_to_bytes", "urllib", "urllib.parse", "unquote", "unquote_to_bytes"
    ),
    MovedAttribute("urlencode", "urllib", "urllib.parse"),
    MovedAttribute("splitquery", "urllib", "urllib.parse"),
    MovedAttribute("splittag", "urllib", "urllib.parse"),
    MovedAttribute("splituser", "urllib", "urllib.parse"),
    MovedAttribute("splitvalue", "urllib", "urllib.parse"),
    MovedAttribute("uses_fragment", "urlparse", "urllib.parse"),
    MovedAttribute("uses_netloc", "urlparse", "urllib.parse"),
    MovedAttribute("uses_params", "urlparse", "urllib.parse"),
    MovedAttribute("uses_query", "urlparse", "urllib.parse"),
    MovedAttribute("uses_relative", "urlparse", "urllib.parse"),
]
for attr in _urllib_parse_moved_attributes:
    setattr(Module_six_moves_urllib_parse, attr.name, attr)
del attr

Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes

_importer._add_module(
    Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse"),
    "moves.urllib_parse",
    "moves.urllib.parse",
)


class Module_six_moves_urllib_error(_LazyModule):

    """Lazy loading of moved objects in six.moves.urllib_error"""


_urllib_error_moved_attributes = [
    MovedAttribute("URLError", "urllib2", "urllib.error"),
    MovedAttribute("HTTPError", "urllib2", "urllib.error"),
    MovedAttribute("ContentTooShortError", "urllib", "urllib.error"),
]
for attr in _urllib_error_moved_attributes:
    setattr(Module_six_moves_urllib_error, attr.name, attr)
del attr

Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes

_importer._add_module(
    Module_six_moves_urllib_error(__name__ + ".moves.urllib.error"),
    "moves.urllib_error",
    "moves.urllib.error",
)


class Module_six_moves_urllib_request(_LazyModule):

    """Lazy loading of moved objects in six.moves.urllib_request"""


_urllib_request_moved_attributes = [
    MovedAttribute("urlopen", "urllib2", "urllib.request"),
    MovedAttribute("install_opener", "urllib2", "urllib.request"),
    MovedAttribute("build_opener", "urllib2", "urllib.request"),
    MovedAttribute("pathname2url", "urllib", "urllib.request"),
    MovedAttribute("url2pathname", "urllib", "urllib.request"),
    MovedAttribute("getproxies", "urllib", "urllib.request"),
    MovedAttribute("Request", "urllib2", "urllib.request"),
    MovedAttribute("OpenerDirector", "urllib2", "urllib.request"),
    MovedAttribute("HTTPDefaultErrorHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPRedirectHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPCookieProcessor", "urllib2", "urllib.request"),
    MovedAttribute("ProxyHandler", "urllib2", "urllib.request"),
    MovedAttribute("BaseHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
    MovedAttribute("AbstractBasicAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPBasicAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("ProxyBasicAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("AbstractDigestAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPDigestAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("ProxyDigestAuthHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPSHandler", "urllib2", "urllib.request"),
    MovedAttribute("FileHandler", "urllib2", "urllib.request"),
    MovedAttribute("FTPHandler", "urllib2", "urllib.request"),
    MovedAttribute("CacheFTPHandler", "urllib2", "urllib.request"),
    MovedAttribute("UnknownHandler", "urllib2", "urllib.request"),
    MovedAttribute("HTTPErrorProcessor", "urllib2", "urllib.request"),
    MovedAttribute("urlretrieve", "urllib", "urllib.request"),
    MovedAttribute("urlcleanup", "urllib", "urllib.request"),
    MovedAttribute("URLopener", "urllib", "urllib.request"),
    MovedAttribute("FancyURLopener", "urllib", "urllib.request"),
    MovedAttribute("proxy_bypass", "urllib", "urllib.request"),
    MovedAttribute("parse_http_list", "urllib2", "urllib.request"),
    MovedAttribute("parse_keqv_list", "urllib2", "urllib.request"),
]
for attr in _urllib_request_moved_attributes:
    setattr(Module_six_moves_urllib_request, attr.name, attr)
del attr

Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes

_importer._add_module(
    Module_six_moves_urllib_request(__name__ + ".moves.urllib.request"),
    "moves.urllib_request",
    "moves.urllib.request",
)


class Module_six_moves_urllib_response(_LazyModule):

    """Lazy loading of moved objects in six.moves.urllib_response"""


_urllib_response_moved_attributes = [
    MovedAttribute("addbase", "urllib", "urllib.response"),
    MovedAttribute("addclosehook", "urllib", "urllib.response"),
    MovedAttribute("addinfo", "urllib", "urllib.response"),
    MovedAttribute("addinfourl", "urllib", "urllib.response"),
]
for attr in _urllib_response_moved_attributes:
    setattr(Module_six_moves_urllib_response, attr.name, attr)
del attr

Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes

_importer._add_module(
    Module_six_moves_urllib_response(__name__ + ".moves.urllib.response"),
    "moves.urllib_response",
    "moves.urllib.response",
)


class Module_six_moves_urllib_robotparser(_LazyModule):

    """Lazy loading of moved objects in six.moves.urllib_robotparser"""


_urllib_robotparser_moved_attributes = [
    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser"),
]
for attr in _urllib_robotparser_moved_attributes:
    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
del attr

Module_six_moves_urllib_robotparser._moved_attributes = (
    _urllib_robotparser_moved_attributes
)

_importer._add_module(
    Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser"),
    "moves.urllib_robotparser",
    "moves.urllib.robotparser",
)


class Module_six_moves_urllib(types.ModuleType):

    """Create a six.moves.urllib namespace that resembles the Python 3 namespace"""

    __path__ = []  # mark as package
    parse = _importer._get_module("moves.urllib_parse")
    error = _importer._get_module("moves.urllib_error")
    request = _importer._get_module("moves.urllib_request")
    response = _importer._get_module("moves.urllib_response")
    robotparser = _importer._get_module("moves.urllib_robotparser")

    def __dir__(self):
        return ["parse", "error", "request", "response", "robotparser"]


_importer._add_module(
    Module_six_moves_urllib(__name__ + ".moves.urllib"), "moves.urllib"
)


def add_move(move):
    """Add an item to six.moves."""
    setattr(_MovedItems, move.name, move)


def remove_move(name):
    """Remove item from six.moves."""
    try:
        delattr(_MovedItems, name)
    except AttributeError:
        try:
            del moves.__dict__[name]
        except KeyError:
            raise AttributeError("no such move, %r" % (name,))


if PY3:
    _meth_func = "__func__"
    _meth_self = "__self__"

    _func_closure = "__closure__"
    _func_code = "__code__"
    _func_defaults = "__defaults__"
    _func_globals = "__globals__"
else:
    _meth_func = "im_func"
    _meth_self = "im_self"

    _func_closure = "func_closure"
    _func_code = "func_code"
    _func_defaults = "func_defaults"
    _func_globals = "func_globals"


try:
    advance_iterator = next
except NameError:

    def advance_iterator(it):
        return it.next()


next = advance_iterator


try:
    callable = callable
except NameError:

    def callable(obj):
        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)


if PY3:

    def get_unbound_function(unbound):
        return unbound

    create_bound_method = types.MethodType

    def create_unbound_method(func, cls):
        return func

    Iterator = object
else:

    def get_unbound_function(unbound):
        return unbound.im_func

    def create_bound_method(func, obj):
        return types.MethodType(func, obj, obj.__class__)

    def create_unbound_method(func, cls):
        return types.MethodType(func, None, cls)

    class Iterator(object):
        def next(self):
            return type(self).__next__(self)

    callable = callable
_add_doc(
    get_unbound_function, """Get the function out of a possibly unbound function"""
)


get_method_function = operator.attrgetter(_meth_func)
get_method_self = operator.attrgetter(_meth_self)
get_function_closure = operator.attrgetter(_func_closure)
get_function_code = operator.attrgetter(_func_code)
get_function_defaults = operator.attrgetter(_func_defaults)
get_function_globals = operator.attrgetter(_func_globals)


if PY3:

    def iterkeys(d, **kw):
        return iter(d.keys(**kw))

    def itervalues(d, **kw):
        return iter(d.values(**kw))

    def iteritems(d, **kw):
        return iter(d.items(**kw))

    def iterlists(d, **kw):
        return iter(d.lists(**kw))

    viewkeys = operator.methodcaller("keys")

    viewvalues = operator.methodcaller("values")

    viewitems = operator.methodcaller("items")
else:

    def iterkeys(d, **kw):
        return d.iterkeys(**kw)

    def itervalues(d, **kw):
        return d.itervalues(**kw)

    def iteritems(d, **kw):
        return d.iteritems(**kw)

    def iterlists(d, **kw):
        return d.iterlists(**kw)

    viewkeys = operator.methodcaller("viewkeys")

    viewvalues = operator.methodcaller("viewvalues")

    viewitems = operator.methodcaller("viewitems")

_add_doc(iterkeys, "Return an iterator over the keys of a dictionary.")
_add_doc(itervalues, "Return an iterator over the values of a dictionary.")
_add_doc(iteritems, "Return an iterator over the (key, value) pairs of a dictionary.")
_add_doc(
    iterlists, "Return an iterator over the (key, [values]) pairs of a dictionary."
)


if PY3:

    def b(s):
        return s.encode("latin-1")

    def u(s):
        return s

    unichr = chr
    import struct

    int2byte = struct.Struct(">B").pack
    del struct
    byte2int = operator.itemgetter(0)
    indexbytes = operator.getitem
    iterbytes = iter
    import io

    StringIO = io.StringIO
    BytesIO = io.BytesIO
    del io
    _assertCountEqual = "assertCountEqual"
    if sys.version_info[1] <= 1:
        _assertRaisesRegex = "assertRaisesRegexp"
        _assertRegex = "assertRegexpMatches"
        _assertNotRegex = "assertNotRegexpMatches"
    else:
        _assertRaisesRegex = "assertRaisesRegex"
        _assertRegex = "assertRegex"
        _assertNotRegex = "assertNotRegex"
else:

    def b(s):
        return s

    # Workaround for standalone backslash

    def u(s):
        return unicode(s.replace(r"\\", r"\\\\"), "unicode_escape")

    unichr = unichr
    int2byte = chr

    def byte2int(bs):
        return ord(bs[0])

    def indexbytes(buf, i):
        return ord(buf[i])

    iterbytes = functools.partial(itertools.imap, ord)
    import StringIO

    StringIO = BytesIO = StringIO.StringIO
    _assertCountEqual = "assertItemsEqual"
    _assertRaisesRegex = "assertRaisesRegexp"
    _assertRegex = "assertRegexpMatches"
    _assertNotRegex = "assertNotRegexpMatches"
_add_doc(b, """Byte literal""")
_add_doc(u, """Text literal""")


def assertCountEqual(self, *args, **kwargs):
    return getattr(self, _assertCountEqual)(*args, **kwargs)


def assertRaisesRegex(self, *args, **kwargs):
    return getattr(self, _assertRaisesRegex)(*args, **kwargs)


def assertRegex(self, *args, **kwargs):
    return getattr(self, _assertRegex)(*args, **kwargs)


def assertNotRegex(self, *args, **kwargs):
    return getattr(self, _assertNotRegex)(*args, **kwargs)


if PY3:
    exec_ = getattr(moves.builtins, "exec")

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
            raise value
        finally:
            value = None
            tb = None

else:

    def exec_(_code_, _globs_=None, _locs_=None):
        """Execute code in a namespace."""
        if _globs_ is None:
            frame = sys._getframe(1)
            _globs_ = frame.f_globals
            if _locs_ is None:
                _locs_ = frame.f_locals
            del frame
        elif _locs_ is None:
            _locs_ = _globs_
        exec ("""exec _code_ in _globs_, _locs_""")

    exec_(
        """def reraise(tp, value, tb=None):
    try:
        raise tp, value, tb
    finally:
        tb = None
"""
    )


if sys.version_info[:2] > (3,):
    exec_(
        """def raise_from(value, from_value):
    try:
        raise value from from_value
    finally:
        value = None
"""
    )
else:

    def raise_from(value, from_value):
        raise value


print_ = getattr(moves.builtins, "print", None)
if print_ is None:

    def print_(*args, **kwargs):
        """The new-style print function for Python 2.4 and 2.5."""
        fp = kwargs.pop("file", sys.stdout)
        if fp is None:
            return

        def write(data):
            if not isinstance(data, basestring):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (
                isinstance(fp, file)
                and isinstance(data, unicode)
                and fp.encoding is not None
            ):
                errors = getattr(fp, "errors", None)
                if errors is None:
                    errors = "strict"
                data = data.encode(fp.encoding, errors)
            fp.write(data)

        want_unicode = False
        sep = kwargs.pop("sep", None)
        if sep is not None:
            if isinstance(sep, unicode):
                want_unicode = True
            elif not isinstance(sep, str):
                raise TypeError("sep must be None or a string")
        end = kwargs.pop("end", None)
        if end is not None:
            if isinstance(end, unicode):
                want_unicode = True
            elif not isinstance(end, str):
                raise TypeError("end must be None or a string")
        if kwargs:
            raise TypeError("invalid keyword arguments to print()")
        if not want_unicode:
            for arg in args:
                if isinstance(arg, unicode):
                    want_unicode = True
                    break
        if want_unicode:
            newline = unicode("\n")
            space = unicode(" ")
        else:
            newline = "\n"
            space = " "
        if sep is None:
            sep = space
        if end is None:
            end = newline
        for i, arg in enumerate(args):
            if i:
                write(sep)
            write(arg)
        write(end)


if sys.version_info[:2] < (3, 3):
    _print = print_

    def print_(*args, **kwargs):
        fp = kwargs.get("file", sys.stdout)
        flush = kwargs.pop("flush", False)
        _print(*args, **kwargs)
        if flush and fp is not None:
            fp.flush()


_add_doc(reraise, """Reraise an exception.""")

if sys.version_info[0:2] < (3, 4):
    # This does exactly the same what the :func:`py3:functools.update_wrapper`
    # function does on Python versions after 3.2. It sets the ``__wrapped__``
    # attribute on ``wrapper`` object and it doesn't raise an error if any of
    # the attributes mentioned in ``assigned`` and ``updated`` are missing on
    # ``wrapped`` object.
    def _update_wrapper(
        wrapper,
        wrapped,
        assigned=functools.WRAPPER_ASSIGNMENTS,
        updated=functools.WRAPPER_UPDATES,
    ):
        for attr in assigned:
            try:
                value = getattr(wrapped, attr)
            except AttributeError:
                continue
            else:
                setattr(wrapper, attr, value)
        for attr in updated:
            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))
        wrapper.__wrapped__ = wrapped
        return wrapper

    _update_wrapper.__doc__ = functools.update_wrapper.__doc__

    def wraps(
        wrapped,
        assigned=functools.WRAPPER_ASSIGNMENTS,
        updated=functools.WRAPPER_UPDATES,
    ):
        return functools.partial(
            _update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated
        )

    wraps.__doc__ = functools.wraps.__doc__

else:
    wraps = functools.wraps


def with_metaclass(meta, *bases):
    """Create a base class with a metaclass."""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(type):
        def __new__(cls, name, this_bases, d):
            if sys.version_info[:2] >= (3, 7):
                # This version introduced PEP 560 that requires a bit
                # of extra care (we mimic what is done by __build_class__).
                resolved_bases = types.resolve_bases(bases)
                if resolved_bases is not bases:
                    d["__orig_bases__"] = bases
            else:
                resolved_bases = bases
            return meta(name, resolved_bases, d)

        @classmethod
        def __prepare__(cls, name, this_bases):
            return meta.__prepare__(name, bases)

    return type.__new__(metaclass, "temporary_class", (), {})


def add_metaclass(metaclass):
    """Class decorator for creating a class with a metaclass."""

    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        slots = orig_vars.get("__slots__")
        if slots is not None:
            if isinstance(slots, str):
                slots = [slots]
            for slots_var in slots:
                orig_vars.pop(slots_var)
        orig_vars.pop("__dict__", None)
        orig_vars.pop("__weakref__", None)
        if hasattr(cls, "__qualname__"):
            orig_vars["__qualname__"] = cls.__qualname__
        return metaclass(cls.__name__, cls.__bases__, orig_vars)

    return wrapper


def ensure_binary(s, encoding="utf-8", errors="strict"):
    """Coerce **s** to six.binary_type.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> encoded to `bytes`
      - `bytes` -> `bytes`
    """
    if isinstance(s, binary_type):
        return s
    if isinstance(s, text_type):
        return s.encode(encoding, errors)
    raise TypeError("not expecting type '%s'" % type(s))


def ensure_str(s, encoding="utf-8", errors="strict"):
    """Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """
    # Optimization: Fast return for the common case.
    if type(s) is str:
        return s
    if PY2 and isinstance(s, text_type):
        return s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (text_type, binary_type)):
        raise TypeError("not expecting type '%s'" % type(s))
    return s


def ensure_text(s, encoding="utf-8", errors="strict"):
    """Coerce *s* to six.text_type.

    For Python 2:
      - `unicode` -> `unicode`
      - `str` -> `unicode`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """
    if isinstance(s, binary_type):
        return s.decode(encoding, errors)
    elif isinstance(s, text_type):
        return s
    else:
        raise TypeError("not expecting type '%s'" % type(s))


def python_2_unicode_compatible(klass):
    """
    A class decorator that defines __unicode__ and __str__ methods under Python 2.
    Under Python 3 it does nothing.

    To support Python 2 and 3 with a single code base, define a __str__ method
    returning text and apply this decorator to the class.
    """
    if PY2:
        if "__str__" not in klass.__dict__:
            raise ValueError(
                "@python_2_unicode_compatible cannot be applied "
                "to %s because it doesn't define __str__()." % klass.__name__
            )
        klass.__unicode__ = klass.__str__
        klass.__str__ = lambda self: self.__unicode__().encode("utf-8")
    return klass


# Complete the moves implementation.
# This code is at the end of this module to speed up module loading.
# Turn this module into a package.
__path__ = []  # required for PEP 302 and PEP 451
__package__ = __name__  # see PEP 366 @ReservedAssignment
if globals().get("__spec__") is not None:
    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
# Remove other six meta path importers, since they cause problems. This can
# happen if six is removed from sys.modules and then reloaded. (Setuptools does
# this for some reason.)
if sys.meta_path:
    for i, importer in enumerate(sys.meta_path):
        # Here's some real nastiness: Another "instance" of the six module might
        # be floating around. Therefore, we can't use isinstance() to check for
        # the six meta path importer, since the other six instance will have
        # inserted an importer with different class.
        if (
            type(importer).__name__ == "_SixMetaPathImporter"
            and importer.name == __name__
        ):
            del sys.meta_path[i]
            break
    del i, importer
# Finally, add the importer to the meta path import hook.
sys.meta_path.append(_importer)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_match_hostname.py
# ========================================================
"""The match_hostname() function from Python 3.3.3, essential when using SSL."""

# Note: This file is under the PSF license as the code comes from the python
# stdlib.   http://docs.python.org/3/license.html

import re
import sys

# ipaddress has been backported to 2.6+ in pypi.  If it is installed on the
# system, use it to handle IPAddress ServerAltnames (this was added in
# python-3.5) otherwise only do DNS matching.  This allows
# util.ssl_match_hostname to continue to be used in Python 2.7.
try:
    import ipaddress
except ImportError:
    ipaddress = None

__version__ = "3.5.0.1"


class CertificateError(ValueError):
    pass


def _dnsname_match(dn, hostname, max_wildcards=1):
    """Matching according to RFC 6125, section 6.4.3

    http://tools.ietf.org/html/rfc6125#section-6.4.3
    """
    pats = []
    if not dn:
        return False

    # Ported from python3-syntax:
    # leftmost, *remainder = dn.split(r'.')
    parts = dn.split(r".")
    leftmost = parts[0]
    remainder = parts[1:]

    wildcards = leftmost.count("*")
    if wildcards > max_wildcards:
        # Issue #17980: avoid denials of service by refusing more
        # than one wildcard per fragment.  A survey of established
        # policy among SSL implementations showed it to be a
        # reasonable choice.
        raise CertificateError(
            "too many wildcards in certificate DNS name: " + repr(dn)
        )

    # speed up common case w/o wildcards
    if not wildcards:
        return dn.lower() == hostname.lower()

    # RFC 6125, section 6.4.3, subitem 1.
    # The client SHOULD NOT attempt to match a presented identifier in which
    # the wildcard character comprises a label other than the left-most label.
    if leftmost == "*":
        # When '*' is a fragment by itself, it matches a non-empty dotless
        # fragment.
        pats.append("[^.]+")
    elif leftmost.startswith("xn--") or hostname.startswith("xn--"):
        # RFC 6125, section 6.4.3, subitem 3.
        # The client SHOULD NOT attempt to match a presented identifier
        # where the wildcard character is embedded within an A-label or
        # U-label of an internationalized domain name.
        pats.append(re.escape(leftmost))
    else:
        # Otherwise, '*' matches any dotless string, e.g. www*
        pats.append(re.escape(leftmost).replace(r"\*", "[^.]*"))

    # add the remaining fragments, ignore any wildcards
    for frag in remainder:
        pats.append(re.escape(frag))

    pat = re.compile(r"\A" + r"\.".join(pats) + r"\Z", re.IGNORECASE)
    return pat.match(hostname)


def _to_unicode(obj):
    if isinstance(obj, str) and sys.version_info < (3,):
        # ignored flake8 # F821 to support python 2.7 function
        obj = unicode(obj, encoding="ascii", errors="strict")  # noqa: F821
    return obj


def _ipaddress_match(ipname, host_ip):
    """Exact matching of IP addresses.

    RFC 6125 explicitly doesn't define an algorithm for this
    (section 1.7.2 - "Out of Scope").
    """
    # OpenSSL may add a trailing newline to a subjectAltName's IP address
    # Divergence from upstream: ipaddress can't handle byte str
    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())
    return ip == host_ip


def match_hostname(cert, hostname):
    """Verify that *cert* (in decoded format as returned by
    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125
    rules are followed, but IP addresses are not accepted for *hostname*.

    CertificateError is raised on failure. On success, the function
    returns nothing.
    """
    if not cert:
        raise ValueError(
            "empty or no certificate, match_hostname needs a "
            "SSL socket or SSL context with either "
            "CERT_OPTIONAL or CERT_REQUIRED"
        )
    try:
        # Divergence from upstream: ipaddress can't handle byte str
        host_ip = ipaddress.ip_address(_to_unicode(hostname))
    except (UnicodeError, ValueError):
        # ValueError: Not an IP address (common case)
        # UnicodeError: Divergence from upstream: Have to deal with ipaddress not taking
        # byte strings.  addresses should be all ascii, so we consider it not
        # an ipaddress in this case
        host_ip = None
    except AttributeError:
        # Divergence from upstream: Make ipaddress library optional
        if ipaddress is None:
            host_ip = None
        else:  # Defensive
            raise
    dnsnames = []
    san = cert.get("subjectAltName", ())
    for key, value in san:
        if key == "DNS":
            if host_ip is None and _dnsname_match(value, hostname):
                return
            dnsnames.append(value)
        elif key == "IP Address":
            if host_ip is not None and _ipaddress_match(value, host_ip):
                return
            dnsnames.append(value)
    if not dnsnames:
        # The subject is only checked when there is no dNSName entry
        # in subjectAltName
        for sub in cert.get("subject", ()):
            for key, value in sub:
                # XXX according to RFC 2818, the most specific Common Name
                # must be used.
                if key == "commonName":
                    if _dnsname_match(value, hostname):
                        return
                    dnsnames.append(value)
    if len(dnsnames) > 1:
        raise CertificateError(
            "hostname %r "
            "doesn't match either of %s" % (hostname, ", ".join(map(repr, dnsnames)))
        )
    elif len(dnsnames) == 1:
        raise CertificateError("hostname %r doesn't match %r" % (hostname, dnsnames[0]))
    else:
        raise CertificateError(
            "no appropriate commonName or subjectAltName fields were found"
        )


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/url.py
# ========================================================
from __future__ import absolute_import

import re
from collections import namedtuple

from ..exceptions import LocationParseError
from ..packages import six

url_attrs = ["scheme", "auth", "host", "port", "path", "query", "fragment"]

# We only want to normalize urls with an HTTP(S) scheme.
# urllib3 infers URLs without a scheme (None) to be http.
NORMALIZABLE_SCHEMES = ("http", "https", None)

# Almost all of these patterns were derived from the
# 'rfc3986' module: https://github.com/python-hyper/rfc3986
PERCENT_RE = re.compile(r"%[a-fA-F0-9]{2}")
SCHEME_RE = re.compile(r"^(?:[a-zA-Z][a-zA-Z0-9+-]*:|/)")
URI_RE = re.compile(
    r"^(?:([a-zA-Z][a-zA-Z0-9+.-]*):)?"
    r"(?://([^\\/?#]*))?"
    r"([^?#]*)"
    r"(?:\?([^#]*))?"
    r"(?:#(.*))?$",
    re.UNICODE | re.DOTALL,
)

IPV4_PAT = r"(?:[0-9]{1,3}\.){3}[0-9]{1,3}"
HEX_PAT = "[0-9A-Fa-f]{1,4}"
LS32_PAT = "(?:{hex}:{hex}|{ipv4})".format(hex=HEX_PAT, ipv4=IPV4_PAT)
_subs = {"hex": HEX_PAT, "ls32": LS32_PAT}
_variations = [
    #                            6( h16 ":" ) ls32
    "(?:%(hex)s:){6}%(ls32)s",
    #                       "::" 5( h16 ":" ) ls32
    "::(?:%(hex)s:){5}%(ls32)s",
    # [               h16 ] "::" 4( h16 ":" ) ls32
    "(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)s",
    # [ *1( h16 ":" ) h16 ] "::" 3( h16 ":" ) ls32
    "(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)s",
    # [ *2( h16 ":" ) h16 ] "::" 2( h16 ":" ) ls32
    "(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)s",
    # [ *3( h16 ":" ) h16 ] "::"    h16 ":"   ls32
    "(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s",
    # [ *4( h16 ":" ) h16 ] "::"              ls32
    "(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)s",
    # [ *5( h16 ":" ) h16 ] "::"              h16
    "(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)s",
    # [ *6( h16 ":" ) h16 ] "::"
    "(?:(?:%(hex)s:){0,6}%(hex)s)?::",
]

UNRESERVED_PAT = r"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._\-~"
IPV6_PAT = "(?:" + "|".join([x % _subs for x in _variations]) + ")"
ZONE_ID_PAT = "(?:%25|%)(?:[" + UNRESERVED_PAT + "]|%[a-fA-F0-9]{2})+"
IPV6_ADDRZ_PAT = r"\[" + IPV6_PAT + r"(?:" + ZONE_ID_PAT + r")?\]"
REG_NAME_PAT = r"(?:[^\[\]%:/?#]|%[a-fA-F0-9]{2})*"
TARGET_RE = re.compile(r"^(/[^?#]*)(?:\?([^#]*))?(?:#.*)?$")

IPV4_RE = re.compile("^" + IPV4_PAT + "$")
IPV6_RE = re.compile("^" + IPV6_PAT + "$")
IPV6_ADDRZ_RE = re.compile("^" + IPV6_ADDRZ_PAT + "$")
BRACELESS_IPV6_ADDRZ_RE = re.compile("^" + IPV6_ADDRZ_PAT[2:-2] + "$")
ZONE_ID_RE = re.compile("(" + ZONE_ID_PAT + r")\]$")

_HOST_PORT_PAT = ("^(%s|%s|%s)(?::0*?(|0|[1-9][0-9]{0,4}))?$") % (
    REG_NAME_PAT,
    IPV4_PAT,
    IPV6_ADDRZ_PAT,
)
_HOST_PORT_RE = re.compile(_HOST_PORT_PAT, re.UNICODE | re.DOTALL)

UNRESERVED_CHARS = set(
    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._-~"
)
SUB_DELIM_CHARS = set("!$&'()*+,;=")
USERINFO_CHARS = UNRESERVED_CHARS | SUB_DELIM_CHARS | {":"}
PATH_CHARS = USERINFO_CHARS | {"@", "/"}
QUERY_CHARS = FRAGMENT_CHARS = PATH_CHARS | {"?"}


class Url(namedtuple("Url", url_attrs)):
    """
    Data structure for representing an HTTP URL. Used as a return value for
    :func:`parse_url`. Both the scheme and host are normalized as they are
    both case-insensitive according to RFC 3986.
    """

    __slots__ = ()

    def __new__(
        cls,
        scheme=None,
        auth=None,
        host=None,
        port=None,
        path=None,
        query=None,
        fragment=None,
    ):
        if path and not path.startswith("/"):
            path = "/" + path
        if scheme is not None:
            scheme = scheme.lower()
        return super(Url, cls).__new__(
            cls, scheme, auth, host, port, path, query, fragment
        )

    @property
    def hostname(self):
        """For backwards-compatibility with urlparse. We're nice like that."""
        return self.host

    @property
    def request_uri(self):
        """Absolute path including the query string."""
        uri = self.path or "/"

        if self.query is not None:
            uri += "?" + self.query

        return uri

    @property
    def netloc(self):
        """Network location including host and port"""
        if self.port:
            return "%s:%d" % (self.host, self.port)
        return self.host

    @property
    def url(self):
        """
        Convert self into a url

        This function should more or less round-trip with :func:`.parse_url`. The
        returned url may not be exactly the same as the url inputted to
        :func:`.parse_url`, but it should be equivalent by the RFC (e.g., urls
        with a blank port will have : removed).

        Example: ::

            >>> U = parse_url('http://google.com/mail/')
            >>> U.url
            'http://google.com/mail/'
            >>> Url('http', 'username:password', 'host.com', 80,
            ... '/path', 'query', 'fragment').url
            'http://username:password@host.com:80/path?query#fragment'
        """
        scheme, auth, host, port, path, query, fragment = self
        url = u""

        # We use "is not None" we want things to happen with empty strings (or 0 port)
        if scheme is not None:
            url += scheme + u"://"
        if auth is not None:
            url += auth + u"@"
        if host is not None:
            url += host
        if port is not None:
            url += u":" + str(port)
        if path is not None:
            url += path
        if query is not None:
            url += u"?" + query
        if fragment is not None:
            url += u"#" + fragment

        return url

    def __str__(self):
        return self.url


def split_first(s, delims):
    """
    .. deprecated:: 1.25

    Given a string and an iterable of delimiters, split on the first found
    delimiter. Return two split parts and the matched delimiter.

    If not found, then the first part is the full input string.

    Example::

        >>> split_first('foo/bar?baz', '?/=')
        ('foo', 'bar?baz', '/')
        >>> split_first('foo/bar?baz', '123')
        ('foo/bar?baz', '', None)

    Scales linearly with number of delims. Not ideal for large number of delims.
    """
    min_idx = None
    min_delim = None
    for d in delims:
        idx = s.find(d)
        if idx < 0:
            continue

        if min_idx is None or idx < min_idx:
            min_idx = idx
            min_delim = d

    if min_idx is None or min_idx < 0:
        return s, "", None

    return s[:min_idx], s[min_idx + 1 :], min_delim


def _encode_invalid_chars(component, allowed_chars, encoding="utf-8"):
    """Percent-encodes a URI component without reapplying
    onto an already percent-encoded component.
    """
    if component is None:
        return component

    component = six.ensure_text(component)

    # Normalize existing percent-encoded bytes.
    # Try to see if the component we're encoding is already percent-encoded
    # so we can skip all '%' characters but still encode all others.
    component, percent_encodings = PERCENT_RE.subn(
        lambda match: match.group(0).upper(), component
    )

    uri_bytes = component.encode("utf-8", "surrogatepass")
    is_percent_encoded = percent_encodings == uri_bytes.count(b"%")
    encoded_component = bytearray()

    for i in range(0, len(uri_bytes)):
        # Will return a single character bytestring on both Python 2 & 3
        byte = uri_bytes[i : i + 1]
        byte_ord = ord(byte)
        if (is_percent_encoded and byte == b"%") or (
            byte_ord < 128 and byte.decode() in allowed_chars
        ):
            encoded_component += byte
            continue
        encoded_component.extend(b"%" + (hex(byte_ord)[2:].encode().zfill(2).upper()))

    return encoded_component.decode(encoding)


def _remove_path_dot_segments(path):
    # See http://tools.ietf.org/html/rfc3986#section-5.2.4 for pseudo-code
    segments = path.split("/")  # Turn the path into a list of segments
    output = []  # Initialize the variable to use to store output

    for segment in segments:
        # '.' is the current directory, so ignore it, it is superfluous
        if segment == ".":
            continue
        # Anything other than '..', should be appended to the output
        elif segment != "..":
            output.append(segment)
        # In this case segment == '..', if we can, we should pop the last
        # element
        elif output:
            output.pop()

    # If the path starts with '/' and the output is empty or the first string
    # is non-empty
    if path.startswith("/") and (not output or output[0]):
        output.insert(0, "")

    # If the path starts with '/.' or '/..' ensure we add one more empty
    # string to add a trailing '/'
    if path.endswith(("/.", "/..")):
        output.append("")

    return "/".join(output)


def _normalize_host(host, scheme):
    if host:
        if isinstance(host, six.binary_type):
            host = six.ensure_str(host)

        if scheme in NORMALIZABLE_SCHEMES:
            is_ipv6 = IPV6_ADDRZ_RE.match(host)
            if is_ipv6:
                # IPv6 hosts of the form 'a::b%zone' are encoded in a URL as
                # such per RFC 6874: 'a::b%25zone'. Unquote the ZoneID
                # separator as necessary to return a valid RFC 4007 scoped IP.
                match = ZONE_ID_RE.search(host)
                if match:
                    start, end = match.span(1)
                    zone_id = host[start:end]

                    if zone_id.startswith("%25") and zone_id != "%25":
                        zone_id = zone_id[3:]
                    else:
                        zone_id = zone_id[1:]
                    zone_id = "%" + _encode_invalid_chars(zone_id, UNRESERVED_CHARS)
                    return host[:start].lower() + zone_id + host[end:]
                else:
                    return host.lower()
            elif not IPV4_RE.match(host):
                return six.ensure_str(
                    b".".join([_idna_encode(label) for label in host.split(".")])
                )
    return host


def _idna_encode(name):
    if name and any(ord(x) >= 128 for x in name):
        try:
            from pip._vendor import idna
        except ImportError:
            six.raise_from(
                LocationParseError("Unable to parse URL without the 'idna' module"),
                None,
            )
        try:
            return idna.encode(name.lower(), strict=True, std3_rules=True)
        except idna.IDNAError:
            six.raise_from(
                LocationParseError(u"Name '%s' is not a valid IDNA label" % name), None
            )
    return name.lower().encode("ascii")


def _encode_target(target):
    """Percent-encodes a request target so that there are no invalid characters"""
    path, query = TARGET_RE.match(target).groups()
    target = _encode_invalid_chars(path, PATH_CHARS)
    query = _encode_invalid_chars(query, QUERY_CHARS)
    if query is not None:
        target += "?" + query
    return target


def parse_url(url):
    """
    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
    performed to parse incomplete urls. Fields not provided will be None.
    This parser is RFC 3986 and RFC 6874 compliant.

    The parser logic and helper functions are based heavily on
    work done in the ``rfc3986`` module.

    :param str url: URL to parse into a :class:`.Url` namedtuple.

    Partly backwards-compatible with :mod:`urlparse`.

    Example::

        >>> parse_url('http://google.com/mail/')
        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
        >>> parse_url('google.com:80')
        Url(scheme=None, host='google.com', port=80, path=None, ...)
        >>> parse_url('/foo?bar')
        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
    """
    if not url:
        # Empty
        return Url()

    source_url = url
    if not SCHEME_RE.search(url):
        url = "//" + url

    try:
        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES

        if scheme:
            scheme = scheme.lower()

        if authority:
            auth, _, host_port = authority.rpartition("@")
            auth = auth or None
            host, port = _HOST_PORT_RE.match(host_port).groups()
            if auth and normalize_uri:
                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
            if port == "":
                port = None
        else:
            auth, host, port = None, None, None

        if port is not None:
            port = int(port)
            if not (0 <= port <= 65535):
                raise LocationParseError(url)

        host = _normalize_host(host, scheme)

        if normalize_uri and path:
            path = _remove_path_dot_segments(path)
            path = _encode_invalid_chars(path, PATH_CHARS)
        if normalize_uri and query:
            query = _encode_invalid_chars(query, QUERY_CHARS)
        if normalize_uri and fragment:
            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)

    except (ValueError, AttributeError):
        return six.raise_from(LocationParseError(source_url), None)

    # For the sake of backwards compatibility we put empty
    # string values for path if there are any defined values
    # beyond the path in the URL.
    # TODO: Remove this when we break backwards compatibility.
    if not path:
        if query is not None or fragment is not None:
            path = ""
        else:
            path = None

    # Ensure that each part of the URL is a `str` for
    # backwards compatibility.
    if isinstance(url, six.text_type):
        ensure_func = six.ensure_text
    else:
        ensure_func = six.ensure_str

    def ensure_type(x):
        return x if x is None else ensure_func(x)

    return Url(
        scheme=ensure_type(scheme),
        auth=ensure_type(auth),
        host=ensure_type(host),
        port=port,
        path=ensure_type(path),
        query=ensure_type(query),
        fragment=ensure_type(fragment),
    )


def get_host(url):
    """
    Deprecated. Use :func:`parse_url` instead.
    """
    p = parse_url(url)
    return p.scheme or "http", p.hostname, p.port


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/connection.py
# ========================================================
from __future__ import absolute_import

import socket

from ..contrib import _appengine_environ
from ..exceptions import LocationParseError
from ..packages import six
from .wait import NoWayToWaitForSocketError, wait_for_read


def is_connection_dropped(conn):  # Platform-specific
    """
    Returns True if the connection is dropped and should be closed.

    :param conn:
        :class:`http.client.HTTPConnection` object.

    Note: For platforms like AppEngine, this will always return ``False`` to
    let the platform handle connection recycling transparently for us.
    """
    sock = getattr(conn, "sock", False)
    if sock is False:  # Platform-specific: AppEngine
        return False
    if sock is None:  # Connection already closed (such as by httplib).
        return True
    try:
        # Returns True if readable, which here means it's been dropped
        return wait_for_read(sock, timeout=0.0)
    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine
        return False


# This function is copied from socket.py in the Python 2.7 standard
# library test suite. Added to its signature is only `socket_options`.
# One additional modification is that we avoid binding to IPv6 servers
# discovered in DNS if the system doesn't have IPv6 functionality.
def create_connection(
    address,
    timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
    source_address=None,
    socket_options=None,
):
    """Connect to *address* and return the socket object.

    Convenience function.  Connect to *address* (a 2-tuple ``(host,
    port)``) and return the socket object.  Passing the optional
    *timeout* parameter will set the timeout on the socket instance
    before attempting to connect.  If no *timeout* is supplied, the
    global default timeout setting returned by :func:`socket.getdefaulttimeout`
    is used.  If *source_address* is set it must be a tuple of (host, port)
    for the socket to bind as a source address before making the connection.
    An host of '' or port 0 tells the OS to use the default.
    """

    host, port = address
    if host.startswith("["):
        host = host.strip("[]")
    err = None

    # Using the value from allowed_gai_family() in the context of getaddrinfo lets
    # us select whether to work with IPv4 DNS records, IPv6 records, or both.
    # The original create_connection function always returns all records.
    family = allowed_gai_family()

    try:
        host.encode("idna")
    except UnicodeError:
        return six.raise_from(
            LocationParseError(u"'%s', label empty or too long" % host), None
        )

    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
        af, socktype, proto, canonname, sa = res
        sock = None
        try:
            sock = socket.socket(af, socktype, proto)

            # If provided, set socket level options before connecting.
            _set_socket_options(sock, socket_options)

            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:
                sock.settimeout(timeout)
            if source_address:
                sock.bind(source_address)
            sock.connect(sa)
            return sock

        except socket.error as e:
            err = e
            if sock is not None:
                sock.close()
                sock = None

    if err is not None:
        raise err

    raise socket.error("getaddrinfo returns an empty list")


def _set_socket_options(sock, options):
    if options is None:
        return

    for opt in options:
        sock.setsockopt(*opt)


def allowed_gai_family():
    """This function is designed to work in the context of
    getaddrinfo, where family=socket.AF_UNSPEC is the default and
    will perform a DNS search for both IPv6 and IPv4 records."""

    family = socket.AF_INET
    if HAS_IPV6:
        family = socket.AF_UNSPEC
    return family


def _has_ipv6(host):
    """Returns True if the system can bind an IPv6 address."""
    sock = None
    has_ipv6 = False

    # App Engine doesn't support IPV6 sockets and actually has a quota on the
    # number of sockets that can be used, so just early out here instead of
    # creating a socket needlessly.
    # See https://github.com/urllib3/urllib3/issues/1446
    if _appengine_environ.is_appengine_sandbox():
        return False

    if socket.has_ipv6:
        # has_ipv6 returns true if cPython was compiled with IPv6 support.
        # It does not tell us if the system has IPv6 support enabled. To
        # determine that we must bind to an IPv6 address.
        # https://github.com/urllib3/urllib3/pull/611
        # https://bugs.python.org/issue658327
        try:
            sock = socket.socket(socket.AF_INET6)
            sock.bind((host, 0))
            has_ipv6 = True
        except Exception:
            pass

    if sock:
        sock.close()
    return has_ipv6


HAS_IPV6 = _has_ipv6("::1")


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/__init__.py
# ========================================================
from __future__ import absolute_import

# For backwards compatibility, provide imports that used to be here.
from .connection import is_connection_dropped
from .request import SKIP_HEADER, SKIPPABLE_HEADERS, make_headers
from .response import is_fp_closed
from .retry import Retry
from .ssl_ import (
    ALPN_PROTOCOLS,
    HAS_SNI,
    IS_PYOPENSSL,
    IS_SECURETRANSPORT,
    PROTOCOL_TLS,
    SSLContext,
    assert_fingerprint,
    resolve_cert_reqs,
    resolve_ssl_version,
    ssl_wrap_socket,
)
from .timeout import Timeout, current_time
from .url import Url, get_host, parse_url, split_first
from .wait import wait_for_read, wait_for_write

__all__ = (
    "HAS_SNI",
    "IS_PYOPENSSL",
    "IS_SECURETRANSPORT",
    "SSLContext",
    "PROTOCOL_TLS",
    "ALPN_PROTOCOLS",
    "Retry",
    "Timeout",
    "Url",
    "assert_fingerprint",
    "current_time",
    "is_connection_dropped",
    "is_fp_closed",
    "get_host",
    "parse_url",
    "make_headers",
    "resolve_cert_reqs",
    "resolve_ssl_version",
    "split_first",
    "ssl_wrap_socket",
    "wait_for_read",
    "wait_for_write",
    "SKIP_HEADER",
    "SKIPPABLE_HEADERS",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/wait.py
# ========================================================
import errno
import select
import sys
from functools import partial

try:
    from time import monotonic
except ImportError:
    from time import time as monotonic

__all__ = ["NoWayToWaitForSocketError", "wait_for_read", "wait_for_write"]


class NoWayToWaitForSocketError(Exception):
    pass


# How should we wait on sockets?
#
# There are two types of APIs you can use for waiting on sockets: the fancy
# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
# select/poll. The stateful APIs are more efficient when you have a lots of
# sockets to keep track of, because you can set them up once and then use them
# lots of times. But we only ever want to wait on a single socket at a time
# and don't want to keep track of state, so the stateless APIs are actually
# more efficient. So we want to use select() or poll().
#
# Now, how do we choose between select() and poll()? On traditional Unixes,
# select() has a strange calling convention that makes it slow, or fail
# altogether, for high-numbered file descriptors. The point of poll() is to fix
# that, so on Unixes, we prefer poll().
#
# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
# for it), but that's OK, because on Windows, select() doesn't have this
# strange calling convention; plain select() works fine.
#
# So: on Windows we use select(), and everywhere else we use poll(). We also
# fall back to select() in case poll() is somehow broken or missing.

if sys.version_info >= (3, 5):
    # Modern Python, that retries syscalls by default
    def _retry_on_intr(fn, timeout):
        return fn(timeout)

else:
    # Old and broken Pythons.
    def _retry_on_intr(fn, timeout):
        if timeout is None:
            deadline = float("inf")
        else:
            deadline = monotonic() + timeout

        while True:
            try:
                return fn(timeout)
            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
            except (OSError, select.error) as e:
                # 'e.args[0]' incantation works for both OSError and select.error
                if e.args[0] != errno.EINTR:
                    raise
                else:
                    timeout = deadline - monotonic()
                    if timeout < 0:
                        timeout = 0
                    if timeout == float("inf"):
                        timeout = None
                    continue


def select_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError("must specify at least one of read=True, write=True")
    rcheck = []
    wcheck = []
    if read:
        rcheck.append(sock)
    if write:
        wcheck.append(sock)
    # When doing a non-blocking connect, most systems signal success by
    # marking the socket writable. Windows, though, signals success by marked
    # it as "exceptional". We paper over the difference by checking the write
    # sockets for both conditions. (The stdlib selectors module does the same
    # thing.)
    fn = partial(select.select, rcheck, wcheck, wcheck)
    rready, wready, xready = _retry_on_intr(fn, timeout)
    return bool(rready or wready or xready)


def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
    if not read and not write:
        raise RuntimeError("must specify at least one of read=True, write=True")
    mask = 0
    if read:
        mask |= select.POLLIN
    if write:
        mask |= select.POLLOUT
    poll_obj = select.poll()
    poll_obj.register(sock, mask)

    # For some reason, poll() takes timeout in milliseconds
    def do_poll(t):
        if t is not None:
            t *= 1000
        return poll_obj.poll(t)

    return bool(_retry_on_intr(do_poll, timeout))


def null_wait_for_socket(*args, **kwargs):
    raise NoWayToWaitForSocketError("no select-equivalent available")


def _have_working_poll():
    # Apparently some systems have a select.poll that fails as soon as you try
    # to use it, either due to strange configuration or broken monkeypatching
    # from libraries like eventlet/greenlet.
    try:
        poll_obj = select.poll()
        _retry_on_intr(poll_obj.poll, 0)
    except (AttributeError, OSError):
        return False
    else:
        return True


def wait_for_socket(*args, **kwargs):
    # We delay choosing which implementation to use until the first time we're
    # called. We could do it at import time, but then we might make the wrong
    # decision if someone goes wild with monkeypatching select.poll after
    # we're imported.
    global wait_for_socket
    if _have_working_poll():
        wait_for_socket = poll_wait_for_socket
    elif hasattr(select, "select"):
        wait_for_socket = select_wait_for_socket
    else:  # Platform-specific: Appengine.
        wait_for_socket = null_wait_for_socket
    return wait_for_socket(*args, **kwargs)


def wait_for_read(sock, timeout=None):
    """Waits for reading to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """
    return wait_for_socket(sock, read=True, timeout=timeout)


def wait_for_write(sock, timeout=None):
    """Waits for writing to be available on a given socket.
    Returns True if the socket is readable, or False if the timeout expired.
    """
    return wait_for_socket(sock, write=True, timeout=timeout)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/timeout.py
# ========================================================
from __future__ import absolute_import

import time

# The default socket timeout, used by httplib to indicate that no timeout was; specified by the user
from socket import _GLOBAL_DEFAULT_TIMEOUT, getdefaulttimeout

from ..exceptions import TimeoutStateError

# A sentinel value to indicate that no timeout was specified by the user in
# urllib3
_Default = object()


# Use time.monotonic if available.
current_time = getattr(time, "monotonic", time.time)


class Timeout(object):
    """Timeout configuration.

    Timeouts can be defined as a default for a pool:

    .. code-block:: python

       timeout = Timeout(connect=2.0, read=7.0)
       http = PoolManager(timeout=timeout)
       response = http.request('GET', 'http://example.com/')

    Or per-request (which overrides the default for the pool):

    .. code-block:: python

       response = http.request('GET', 'http://example.com/', timeout=Timeout(10))

    Timeouts can be disabled by setting all the parameters to ``None``:

    .. code-block:: python

       no_timeout = Timeout(connect=None, read=None)
       response = http.request('GET', 'http://example.com/, timeout=no_timeout)


    :param total:
        This combines the connect and read timeouts into one; the read timeout
        will be set to the time leftover from the connect attempt. In the
        event that both a connect timeout and a total are specified, or a read
        timeout and a total are specified, the shorter timeout will be applied.

        Defaults to None.

    :type total: int, float, or None

    :param connect:
        The maximum amount of time (in seconds) to wait for a connection
        attempt to a server to succeed. Omitting the parameter will default the
        connect timeout to the system default, probably `the global default
        timeout in socket.py
        <http://hg.python.org/cpython/file/603b4d593758/Lib/socket.py#l535>`_.
        None will set an infinite timeout for connection attempts.

    :type connect: int, float, or None

    :param read:
        The maximum amount of time (in seconds) to wait between consecutive
        read operations for a response from the server. Omitting the parameter
        will default the read timeout to the system default, probably `the
        global default timeout in socket.py
        <http://hg.python.org/cpython/file/603b4d593758/Lib/socket.py#l535>`_.
        None will set an infinite timeout.

    :type read: int, float, or None

    .. note::

        Many factors can affect the total amount of time for urllib3 to return
        an HTTP response.

        For example, Python's DNS resolver does not obey the timeout specified
        on the socket. Other factors that can affect total request time include
        high CPU load, high swap, the program running at a low priority level,
        or other behaviors.

        In addition, the read and total timeouts only measure the time between
        read operations on the socket connecting the client and the server,
        not the total amount of time for the request to return a complete
        response. For most requests, the timeout is raised because the server
        has not sent the first byte in the specified time. This is not always
        the case; if a server streams one byte every fifteen seconds, a timeout
        of 20 seconds will not trigger, even though the request will take
        several minutes to complete.

        If your goal is to cut off any request after a set amount of wall clock
        time, consider having a second "watcher" thread to cut off a slow
        request.
    """

    #: A sentinel object representing the default timeout value
    DEFAULT_TIMEOUT = _GLOBAL_DEFAULT_TIMEOUT

    def __init__(self, total=None, connect=_Default, read=_Default):
        self._connect = self._validate_timeout(connect, "connect")
        self._read = self._validate_timeout(read, "read")
        self.total = self._validate_timeout(total, "total")
        self._start_connect = None

    def __repr__(self):
        return "%s(connect=%r, read=%r, total=%r)" % (
            type(self).__name__,
            self._connect,
            self._read,
            self.total,
        )

    # __str__ provided for backwards compatibility
    __str__ = __repr__

    @classmethod
    def resolve_default_timeout(cls, timeout):
        return getdefaulttimeout() if timeout is cls.DEFAULT_TIMEOUT else timeout

    @classmethod
    def _validate_timeout(cls, value, name):
        """Check that a timeout attribute is valid.

        :param value: The timeout value to validate
        :param name: The name of the timeout attribute to validate. This is
            used to specify in error messages.
        :return: The validated and casted version of the given value.
        :raises ValueError: If it is a numeric value less than or equal to
            zero, or the type is not an integer, float, or None.
        """
        if value is _Default:
            return cls.DEFAULT_TIMEOUT

        if value is None or value is cls.DEFAULT_TIMEOUT:
            return value

        if isinstance(value, bool):
            raise ValueError(
                "Timeout cannot be a boolean value. It must "
                "be an int, float or None."
            )
        try:
            float(value)
        except (TypeError, ValueError):
            raise ValueError(
                "Timeout value %s was %s, but it must be an "
                "int, float or None." % (name, value)
            )

        try:
            if value <= 0:
                raise ValueError(
                    "Attempted to set %s timeout to %s, but the "
                    "timeout cannot be set to a value less "
                    "than or equal to 0." % (name, value)
                )
        except TypeError:
            # Python 3
            raise ValueError(
                "Timeout value %s was %s, but it must be an "
                "int, float or None." % (name, value)
            )

        return value

    @classmethod
    def from_float(cls, timeout):
        """Create a new Timeout from a legacy timeout value.

        The timeout value used by httplib.py sets the same timeout on the
        connect(), and recv() socket requests. This creates a :class:`Timeout`
        object that sets the individual timeouts to the ``timeout`` value
        passed to this function.

        :param timeout: The legacy timeout value.
        :type timeout: integer, float, sentinel default object, or None
        :return: Timeout object
        :rtype: :class:`Timeout`
        """
        return Timeout(read=timeout, connect=timeout)

    def clone(self):
        """Create a copy of the timeout object

        Timeout properties are stored per-pool but each request needs a fresh
        Timeout object to ensure each one has its own start/stop configured.

        :return: a copy of the timeout object
        :rtype: :class:`Timeout`
        """
        # We can't use copy.deepcopy because that will also create a new object
        # for _GLOBAL_DEFAULT_TIMEOUT, which socket.py uses as a sentinel to
        # detect the user default.
        return Timeout(connect=self._connect, read=self._read, total=self.total)

    def start_connect(self):
        """Start the timeout clock, used during a connect() attempt

        :raises urllib3.exceptions.TimeoutStateError: if you attempt
            to start a timer that has been started already.
        """
        if self._start_connect is not None:
            raise TimeoutStateError("Timeout timer has already been started.")
        self._start_connect = current_time()
        return self._start_connect

    def get_connect_duration(self):
        """Gets the time elapsed since the call to :meth:`start_connect`.

        :return: Elapsed time in seconds.
        :rtype: float
        :raises urllib3.exceptions.TimeoutStateError: if you attempt
            to get duration for a timer that hasn't been started.
        """
        if self._start_connect is None:
            raise TimeoutStateError(
                "Can't get connect duration for timer that has not started."
            )
        return current_time() - self._start_connect

    @property
    def connect_timeout(self):
        """Get the value to use when setting a connection timeout.

        This will be a positive float or integer, the value None
        (never timeout), or the default system timeout.

        :return: Connect timeout.
        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None
        """
        if self.total is None:
            return self._connect

        if self._connect is None or self._connect is self.DEFAULT_TIMEOUT:
            return self.total

        return min(self._connect, self.total)

    @property
    def read_timeout(self):
        """Get the value for the read timeout.

        This assumes some time has elapsed in the connection timeout and
        computes the read timeout appropriately.

        If self.total is set, the read timeout is dependent on the amount of
        time taken by the connect timeout. If the connection time has not been
        established, a :exc:`~urllib3.exceptions.TimeoutStateError` will be
        raised.

        :return: Value to use for the read timeout.
        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None
        :raises urllib3.exceptions.TimeoutStateError: If :meth:`start_connect`
            has not yet been called on this object.
        """
        if (
            self.total is not None
            and self.total is not self.DEFAULT_TIMEOUT
            and self._read is not None
            and self._read is not self.DEFAULT_TIMEOUT
        ):
            # In case the connect timeout has not yet been established.
            if self._start_connect is None:
                return self._read
            return max(0, min(self.total - self.get_connect_duration(), self._read))
        elif self.total is not None and self.total is not self.DEFAULT_TIMEOUT:
            return max(0, self.total - self.get_connect_duration())
        else:
            return self._read


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/response.py
# ========================================================
from __future__ import absolute_import

from email.errors import MultipartInvariantViolationDefect, StartBoundaryNotFoundDefect

from ..exceptions import HeaderParsingError
from ..packages.six.moves import http_client as httplib


def is_fp_closed(obj):
    """
    Checks whether a given file-like object is closed.

    :param obj:
        The file-like object to check.
    """

    try:
        # Check `isclosed()` first, in case Python3 doesn't set `closed`.
        # GH Issue #928
        return obj.isclosed()
    except AttributeError:
        pass

    try:
        # Check via the official file-like-object way.
        return obj.closed
    except AttributeError:
        pass

    try:
        # Check if the object is a container for another file-like object that
        # gets released on exhaustion (e.g. HTTPResponse).
        return obj.fp is None
    except AttributeError:
        pass

    raise ValueError("Unable to determine whether fp is closed.")


def assert_header_parsing(headers):
    """
    Asserts whether all headers have been successfully parsed.
    Extracts encountered errors from the result of parsing headers.

    Only works on Python 3.

    :param http.client.HTTPMessage headers: Headers to verify.

    :raises urllib3.exceptions.HeaderParsingError:
        If parsing errors are found.
    """

    # This will fail silently if we pass in the wrong kind of parameter.
    # To make debugging easier add an explicit check.
    if not isinstance(headers, httplib.HTTPMessage):
        raise TypeError("expected httplib.Message, got {0}.".format(type(headers)))

    defects = getattr(headers, "defects", None)
    get_payload = getattr(headers, "get_payload", None)

    unparsed_data = None
    if get_payload:
        # get_payload is actually email.message.Message.get_payload;
        # we're only interested in the result if it's not a multipart message
        if not headers.is_multipart():
            payload = get_payload()

            if isinstance(payload, (bytes, str)):
                unparsed_data = payload
    if defects:
        # httplib is assuming a response body is available
        # when parsing headers even when httplib only sends
        # header data to parse_headers() This results in
        # defects on multipart responses in particular.
        # See: https://github.com/urllib3/urllib3/issues/800

        # So we ignore the following defects:
        # - StartBoundaryNotFoundDefect:
        #     The claimed start boundary was never found.
        # - MultipartInvariantViolationDefect:
        #     A message claimed to be a multipart but no subparts were found.
        defects = [
            defect
            for defect in defects
            if not isinstance(
                defect, (StartBoundaryNotFoundDefect, MultipartInvariantViolationDefect)
            )
        ]

    if defects or unparsed_data:
        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)


def is_response_to_head(response):
    """
    Checks whether the request of a response has been a HEAD-request.
    Handles the quirks of AppEngine.

    :param http.client.HTTPResponse response:
        Response to check if the originating request
        used 'HEAD' as a method.
    """
    # FIXME: Can we do this somehow without accessing private httplib _method?
    method = response._method
    if isinstance(method, int):  # Platform-specific: Appengine
        return method == 3
    return method.upper() == "HEAD"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssl_.py
# ========================================================
from __future__ import absolute_import

import hmac
import os
import sys
import warnings
from binascii import hexlify, unhexlify
from hashlib import md5, sha1, sha256

from ..exceptions import (
    InsecurePlatformWarning,
    ProxySchemeUnsupported,
    SNIMissingWarning,
    SSLError,
)
from ..packages import six
from .url import BRACELESS_IPV6_ADDRZ_RE, IPV4_RE

SSLContext = None
SSLTransport = None
HAS_SNI = False
IS_PYOPENSSL = False
IS_SECURETRANSPORT = False
ALPN_PROTOCOLS = ["http/1.1"]

# Maps the length of a digest to a possible hash function producing this digest
HASHFUNC_MAP = {32: md5, 40: sha1, 64: sha256}


def _const_compare_digest_backport(a, b):
    """
    Compare two digests of equal length in constant time.

    The digests must be of type str/bytes.
    Returns True if the digests match, and False otherwise.
    """
    result = abs(len(a) - len(b))
    for left, right in zip(bytearray(a), bytearray(b)):
        result |= left ^ right
    return result == 0


_const_compare_digest = getattr(hmac, "compare_digest", _const_compare_digest_backport)

try:  # Test for SSL features
    import ssl
    from ssl import CERT_REQUIRED, wrap_socket
except ImportError:
    pass

try:
    from ssl import HAS_SNI  # Has SNI?
except ImportError:
    pass

try:
    from .ssltransport import SSLTransport
except ImportError:
    pass


try:  # Platform-specific: Python 3.6
    from ssl import PROTOCOL_TLS

    PROTOCOL_SSLv23 = PROTOCOL_TLS
except ImportError:
    try:
        from ssl import PROTOCOL_SSLv23 as PROTOCOL_TLS

        PROTOCOL_SSLv23 = PROTOCOL_TLS
    except ImportError:
        PROTOCOL_SSLv23 = PROTOCOL_TLS = 2

try:
    from ssl import PROTOCOL_TLS_CLIENT
except ImportError:
    PROTOCOL_TLS_CLIENT = PROTOCOL_TLS


try:
    from ssl import OP_NO_COMPRESSION, OP_NO_SSLv2, OP_NO_SSLv3
except ImportError:
    OP_NO_SSLv2, OP_NO_SSLv3 = 0x1000000, 0x2000000
    OP_NO_COMPRESSION = 0x20000


try:  # OP_NO_TICKET was added in Python 3.6
    from ssl import OP_NO_TICKET
except ImportError:
    OP_NO_TICKET = 0x4000


# A secure default.
# Sources for more information on TLS ciphers:
#
# - https://wiki.mozilla.org/Security/Server_Side_TLS
# - https://www.ssllabs.com/projects/best-practices/index.html
# - https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
#
# The general intent is:
# - prefer cipher suites that offer perfect forward secrecy (DHE/ECDHE),
# - prefer ECDHE over DHE for better performance,
# - prefer any AES-GCM and ChaCha20 over any AES-CBC for better performance and
#   security,
# - prefer AES-GCM over ChaCha20 because hardware-accelerated AES is common,
# - disable NULL authentication, MD5 MACs, DSS, and other
#   insecure ciphers for security reasons.
# - NOTE: TLS 1.3 cipher suites are managed through a different interface
#   not exposed by CPython (yet!) and are enabled by default if they're available.
DEFAULT_CIPHERS = ":".join(
    [
        "ECDHE+AESGCM",
        "ECDHE+CHACHA20",
        "DHE+AESGCM",
        "DHE+CHACHA20",
        "ECDH+AESGCM",
        "DH+AESGCM",
        "ECDH+AES",
        "DH+AES",
        "RSA+AESGCM",
        "RSA+AES",
        "!aNULL",
        "!eNULL",
        "!MD5",
        "!DSS",
    ]
)

try:
    from ssl import SSLContext  # Modern SSL?
except ImportError:

    class SSLContext(object):  # Platform-specific: Python 2
        def __init__(self, protocol_version):
            self.protocol = protocol_version
            # Use default values from a real SSLContext
            self.check_hostname = False
            self.verify_mode = ssl.CERT_NONE
            self.ca_certs = None
            self.options = 0
            self.certfile = None
            self.keyfile = None
            self.ciphers = None

        def load_cert_chain(self, certfile, keyfile):
            self.certfile = certfile
            self.keyfile = keyfile

        def load_verify_locations(self, cafile=None, capath=None, cadata=None):
            self.ca_certs = cafile

            if capath is not None:
                raise SSLError("CA directories not supported in older Pythons")

            if cadata is not None:
                raise SSLError("CA data not supported in older Pythons")

        def set_ciphers(self, cipher_suite):
            self.ciphers = cipher_suite

        def wrap_socket(self, socket, server_hostname=None, server_side=False):
            warnings.warn(
                "A true SSLContext object is not available. This prevents "
                "urllib3 from configuring SSL appropriately and may cause "
                "certain SSL connections to fail. You can upgrade to a newer "
                "version of Python to solve this. For more information, see "
                "https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
                "#ssl-warnings",
                InsecurePlatformWarning,
            )
            kwargs = {
                "keyfile": self.keyfile,
                "certfile": self.certfile,
                "ca_certs": self.ca_certs,
                "cert_reqs": self.verify_mode,
                "ssl_version": self.protocol,
                "server_side": server_side,
            }
            return wrap_socket(socket, ciphers=self.ciphers, **kwargs)


def assert_fingerprint(cert, fingerprint):
    """
    Checks if given fingerprint matches the supplied certificate.

    :param cert:
        Certificate as bytes object.
    :param fingerprint:
        Fingerprint as string of hexdigits, can be interspersed by colons.
    """

    fingerprint = fingerprint.replace(":", "").lower()
    digest_length = len(fingerprint)
    hashfunc = HASHFUNC_MAP.get(digest_length)
    if not hashfunc:
        raise SSLError("Fingerprint of invalid length: {0}".format(fingerprint))

    # We need encode() here for py32; works on py2 and p33.
    fingerprint_bytes = unhexlify(fingerprint.encode())

    cert_digest = hashfunc(cert).digest()

    if not _const_compare_digest(cert_digest, fingerprint_bytes):
        raise SSLError(
            'Fingerprints did not match. Expected "{0}", got "{1}".'.format(
                fingerprint, hexlify(cert_digest)
            )
        )


def resolve_cert_reqs(candidate):
    """
    Resolves the argument to a numeric constant, which can be passed to
    the wrap_socket function/method from the ssl module.
    Defaults to :data:`ssl.CERT_REQUIRED`.
    If given a string it is assumed to be the name of the constant in the
    :mod:`ssl` module or its abbreviation.
    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
    If it's neither `None` nor a string we assume it is already the numeric
    constant which can directly be passed to wrap_socket.
    """
    if candidate is None:
        return CERT_REQUIRED

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, "CERT_" + candidate)
        return res

    return candidate


def resolve_ssl_version(candidate):
    """
    like resolve_cert_reqs
    """
    if candidate is None:
        return PROTOCOL_TLS

    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, "PROTOCOL_" + candidate)
        return res

    return candidate


def create_urllib3_context(
    ssl_version=None, cert_reqs=None, options=None, ciphers=None
):
    """All arguments have the same meaning as ``ssl_wrap_socket``.

    By default, this function does a lot of the same work that
    ``ssl.create_default_context`` does on Python 3.4+. It:

    - Disables SSLv2, SSLv3, and compression
    - Sets a restricted set of server ciphers

    If you wish to enable SSLv3, you can do::

        from pip._vendor.urllib3.util import ssl_
        context = ssl_.create_urllib3_context()
        context.options &= ~ssl_.OP_NO_SSLv3

    You can do the same to enable compression (substituting ``COMPRESSION``
    for ``SSLv3`` in the last line above).

    :param ssl_version:
        The desired protocol version to use. This will default to
        PROTOCOL_SSLv23 which will negotiate the highest protocol that both
        the server and your installation of OpenSSL support.
    :param cert_reqs:
        Whether to require the certificate verification. This defaults to
        ``ssl.CERT_REQUIRED``.
    :param options:
        Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``,
        ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``, and ``ssl.OP_NO_TICKET``.
    :param ciphers:
        Which cipher suites to allow the server to select.
    :returns:
        Constructed SSLContext object with specified options
    :rtype: SSLContext
    """
    # PROTOCOL_TLS is deprecated in Python 3.10
    if not ssl_version or ssl_version == PROTOCOL_TLS:
        ssl_version = PROTOCOL_TLS_CLIENT

    context = SSLContext(ssl_version)

    context.set_ciphers(ciphers or DEFAULT_CIPHERS)

    # Setting the default here, as we may have no ssl module on import
    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs

    if options is None:
        options = 0
        # SSLv2 is easily broken and is considered harmful and dangerous
        options |= OP_NO_SSLv2
        # SSLv3 has several problems and is now dangerous
        options |= OP_NO_SSLv3
        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+
        # (issue #309)
        options |= OP_NO_COMPRESSION
        # TLSv1.2 only. Unless set explicitly, do not request tickets.
        # This may save some bandwidth on wire, and although the ticket is encrypted,
        # there is a risk associated with it being on wire,
        # if the server is not rotating its ticketing keys properly.
        options |= OP_NO_TICKET

    context.options |= options

    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is
    # necessary for conditional client cert authentication with TLS 1.3.
    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older
    # versions of Python.  We only enable on Python 3.7.4+ or if certificate
    # verification is enabled to work around Python issue #37428
    # See: https://bugs.python.org/issue37428
    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(
        context, "post_handshake_auth", None
    ) is not None:
        context.post_handshake_auth = True

    def disable_check_hostname():
        if (
            getattr(context, "check_hostname", None) is not None
        ):  # Platform-specific: Python 3.2
            # We do our own verification, including fingerprints and alternative
            # hostnames. So disable it here
            context.check_hostname = False

    # The order of the below lines setting verify_mode and check_hostname
    # matter due to safe-guards SSLContext has to prevent an SSLContext with
    # check_hostname=True, verify_mode=NONE/OPTIONAL. This is made even more
    # complex because we don't know whether PROTOCOL_TLS_CLIENT will be used
    # or not so we don't know the initial state of the freshly created SSLContext.
    if cert_reqs == ssl.CERT_REQUIRED:
        context.verify_mode = cert_reqs
        disable_check_hostname()
    else:
        disable_check_hostname()
        context.verify_mode = cert_reqs

    # Enable logging of TLS session keys via defacto standard environment variable
    # 'SSLKEYLOGFILE', if the feature is available (Python 3.8+). Skip empty values.
    if hasattr(context, "keylog_filename"):
        sslkeylogfile = os.environ.get("SSLKEYLOGFILE")
        if sslkeylogfile:
            context.keylog_filename = sslkeylogfile

    return context


def ssl_wrap_socket(
    sock,
    keyfile=None,
    certfile=None,
    cert_reqs=None,
    ca_certs=None,
    server_hostname=None,
    ssl_version=None,
    ciphers=None,
    ssl_context=None,
    ca_cert_dir=None,
    key_password=None,
    ca_cert_data=None,
    tls_in_tls=False,
):
    """
    All arguments except for server_hostname, ssl_context, and ca_cert_dir have
    the same meaning as they do when using :func:`ssl.wrap_socket`.

    :param server_hostname:
        When SNI is supported, the expected hostname of the certificate
    :param ssl_context:
        A pre-made :class:`SSLContext` object. If none is provided, one will
        be created using :func:`create_urllib3_context`.
    :param ciphers:
        A string of ciphers we wish the client to support.
    :param ca_cert_dir:
        A directory containing CA certificates in multiple separate files, as
        supported by OpenSSL's -CApath flag or the capath argument to
        SSLContext.load_verify_locations().
    :param key_password:
        Optional password if the keyfile is encrypted.
    :param ca_cert_data:
        Optional string containing CA certificates in PEM format suitable for
        passing as the cadata parameter to SSLContext.load_verify_locations()
    :param tls_in_tls:
        Use SSLTransport to wrap the existing socket.
    """
    context = ssl_context
    if context is None:
        # Note: This branch of code and all the variables in it are no longer
        # used by urllib3 itself. We should consider deprecating and removing
        # this code.
        context = create_urllib3_context(ssl_version, cert_reqs, ciphers=ciphers)

    if ca_certs or ca_cert_dir or ca_cert_data:
        try:
            context.load_verify_locations(ca_certs, ca_cert_dir, ca_cert_data)
        except (IOError, OSError) as e:
            raise SSLError(e)

    elif ssl_context is None and hasattr(context, "load_default_certs"):
        # try to load OS default certs; works well on Windows (require Python3.4+)
        context.load_default_certs()

    # Attempt to detect if we get the goofy behavior of the
    # keyfile being encrypted and OpenSSL asking for the
    # passphrase via the terminal and instead error out.
    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
        raise SSLError("Client private key is encrypted, password is required")

    if certfile:
        if key_password is None:
            context.load_cert_chain(certfile, keyfile)
        else:
            context.load_cert_chain(certfile, keyfile, key_password)

    try:
        if hasattr(context, "set_alpn_protocols"):
            context.set_alpn_protocols(ALPN_PROTOCOLS)
    except NotImplementedError:  # Defensive: in CI, we always have set_alpn_protocols
        pass

    # If we detect server_hostname is an IP address then the SNI
    # extension should not be used according to RFC3546 Section 3.1
    use_sni_hostname = server_hostname and not is_ipaddress(server_hostname)
    # SecureTransport uses server_hostname in certificate verification.
    send_sni = (use_sni_hostname and HAS_SNI) or (
        IS_SECURETRANSPORT and server_hostname
    )
    # Do not warn the user if server_hostname is an invalid SNI hostname.
    if not HAS_SNI and use_sni_hostname:
        warnings.warn(
            "An HTTPS request has been made, but the SNI (Server Name "
            "Indication) extension to TLS is not available on this platform. "
            "This may cause the server to present an incorrect TLS "
            "certificate, which can cause validation failures. You can upgrade to "
            "a newer version of Python to solve this. For more information, see "
            "https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
            "#ssl-warnings",
            SNIMissingWarning,
        )

    if send_sni:
        ssl_sock = _ssl_wrap_socket_impl(
            sock, context, tls_in_tls, server_hostname=server_hostname
        )
    else:
        ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls)
    return ssl_sock


def is_ipaddress(hostname):
    """Detects whether the hostname given is an IPv4 or IPv6 address.
    Also detects IPv6 addresses with Zone IDs.

    :param str hostname: Hostname to examine.
    :return: True if the hostname is an IP address, False otherwise.
    """
    if not six.PY2 and isinstance(hostname, bytes):
        # IDN A-label bytes are ASCII compatible.
        hostname = hostname.decode("ascii")
    return bool(IPV4_RE.match(hostname) or BRACELESS_IPV6_ADDRZ_RE.match(hostname))


def _is_key_file_encrypted(key_file):
    """Detects if a key file is encrypted or not."""
    with open(key_file, "r") as f:
        for line in f:
            # Look for Proc-Type: 4,ENCRYPTED
            if "ENCRYPTED" in line:
                return True

    return False


def _ssl_wrap_socket_impl(sock, ssl_context, tls_in_tls, server_hostname=None):
    if tls_in_tls:
        if not SSLTransport:
            # Import error, ssl is not available.
            raise ProxySchemeUnsupported(
                "TLS in TLS requires support for the 'ssl' module"
            )

        SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)
        return SSLTransport(sock, ssl_context, server_hostname)

    if server_hostname:
        return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
    else:
        return ssl_context.wrap_socket(sock)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/retry.py
# ========================================================
from __future__ import absolute_import

import email
import logging
import re
import time
import warnings
from collections import namedtuple
from itertools import takewhile

from ..exceptions import (
    ConnectTimeoutError,
    InvalidHeader,
    MaxRetryError,
    ProtocolError,
    ProxyError,
    ReadTimeoutError,
    ResponseError,
)
from ..packages import six

log = logging.getLogger(__name__)


# Data structure for representing the metadata of requests that result in a retry.
RequestHistory = namedtuple(
    "RequestHistory", ["method", "url", "error", "status", "redirect_location"]
)


# TODO: In v2 we can remove this sentinel and metaclass with deprecated options.
_Default = object()


class _RetryMeta(type):
    @property
    def DEFAULT_METHOD_WHITELIST(cls):
        warnings.warn(
            "Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and "
            "will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead",
            DeprecationWarning,
        )
        return cls.DEFAULT_ALLOWED_METHODS

    @DEFAULT_METHOD_WHITELIST.setter
    def DEFAULT_METHOD_WHITELIST(cls, value):
        warnings.warn(
            "Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and "
            "will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead",
            DeprecationWarning,
        )
        cls.DEFAULT_ALLOWED_METHODS = value

    @property
    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls):
        warnings.warn(
            "Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and "
            "will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead",
            DeprecationWarning,
        )
        return cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT

    @DEFAULT_REDIRECT_HEADERS_BLACKLIST.setter
    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls, value):
        warnings.warn(
            "Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and "
            "will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead",
            DeprecationWarning,
        )
        cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT = value

    @property
    def BACKOFF_MAX(cls):
        warnings.warn(
            "Using 'Retry.BACKOFF_MAX' is deprecated and "
            "will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead",
            DeprecationWarning,
        )
        return cls.DEFAULT_BACKOFF_MAX

    @BACKOFF_MAX.setter
    def BACKOFF_MAX(cls, value):
        warnings.warn(
            "Using 'Retry.BACKOFF_MAX' is deprecated and "
            "will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead",
            DeprecationWarning,
        )
        cls.DEFAULT_BACKOFF_MAX = value


@six.add_metaclass(_RetryMeta)
class Retry(object):
    """Retry configuration.

    Each retry attempt will create a new Retry object with updated values, so
    they can be safely reused.

    Retries can be defined as a default for a pool::

        retries = Retry(connect=5, read=2, redirect=5)
        http = PoolManager(retries=retries)
        response = http.request('GET', 'http://example.com/')

    Or per-request (which overrides the default for the pool)::

        response = http.request('GET', 'http://example.com/', retries=Retry(10))

    Retries can be disabled by passing ``False``::

        response = http.request('GET', 'http://example.com/', retries=False)

    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless
    retries are disabled, in which case the causing exception will be raised.

    :param int total:
        Total number of retries to allow. Takes precedence over other counts.

        Set to ``None`` to remove this constraint and fall back on other
        counts.

        Set to ``0`` to fail on the first retry.

        Set to ``False`` to disable and imply ``raise_on_redirect=False``.

    :param int connect:
        How many connection-related errors to retry on.

        These are errors raised before the request is sent to the remote server,
        which we assume has not triggered the server to process the request.

        Set to ``0`` to fail on the first retry of this type.

    :param int read:
        How many times to retry on read errors.

        These errors are raised after the request was sent to the server, so the
        request may have side-effects.

        Set to ``0`` to fail on the first retry of this type.

    :param int redirect:
        How many redirects to perform. Limit this to avoid infinite redirect
        loops.

        A redirect is a HTTP response with a status code 301, 302, 303, 307 or
        308.

        Set to ``0`` to fail on the first retry of this type.

        Set to ``False`` to disable and imply ``raise_on_redirect=False``.

    :param int status:
        How many times to retry on bad status codes.

        These are retries made on responses, where status code matches
        ``status_forcelist``.

        Set to ``0`` to fail on the first retry of this type.

    :param int other:
        How many times to retry on other errors.

        Other errors are errors that are not connect, read, redirect or status errors.
        These errors might be raised after the request was sent to the server, so the
        request might have side-effects.

        Set to ``0`` to fail on the first retry of this type.

        If ``total`` is not set, it's a good idea to set this to 0 to account
        for unexpected edge cases and avoid infinite retry loops.

    :param iterable allowed_methods:
        Set of uppercased HTTP method verbs that we should retry on.

        By default, we only retry on methods which are considered to be
        idempotent (multiple requests with the same parameters end with the
        same state). See :attr:`Retry.DEFAULT_ALLOWED_METHODS`.

        Set to a ``False`` value to retry on any verb.

        .. warning::

            Previously this parameter was named ``method_whitelist``, that
            usage is deprecated in v1.26.0 and will be removed in v2.0.

    :param iterable status_forcelist:
        A set of integer HTTP status codes that we should force a retry on.
        A retry is initiated if the request method is in ``allowed_methods``
        and the response status code is in ``status_forcelist``.

        By default, this is disabled with ``None``.

    :param float backoff_factor:
        A backoff factor to apply between attempts after the second try
        (most errors are resolved immediately by a second try without a
        delay). urllib3 will sleep for::

            {backoff factor} * (2 ** ({number of total retries} - 1))

        seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep
        for [0.0s, 0.2s, 0.4s, ...] between retries. It will never be longer
        than :attr:`Retry.DEFAULT_BACKOFF_MAX`.

        By default, backoff is disabled (set to 0).

    :param bool raise_on_redirect: Whether, if the number of redirects is
        exhausted, to raise a MaxRetryError, or to return a response with a
        response code in the 3xx range.

    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:
        whether we should raise an exception, or return a response,
        if status falls in ``status_forcelist`` range and retries have
        been exhausted.

    :param tuple history: The history of the request encountered during
        each call to :meth:`~Retry.increment`. The list is in the order
        the requests occurred. Each list item is of class :class:`RequestHistory`.

    :param bool respect_retry_after_header:
        Whether to respect Retry-After header on status codes defined as
        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.

    :param iterable remove_headers_on_redirect:
        Sequence of headers to remove from the request when a response
        indicating a redirect is returned before firing off the redirected
        request.
    """

    #: Default methods to be used for ``allowed_methods``
    DEFAULT_ALLOWED_METHODS = frozenset(
        ["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE"]
    )

    #: Default status codes to be used for ``status_forcelist``
    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])

    #: Default headers to be used for ``remove_headers_on_redirect``
    DEFAULT_REMOVE_HEADERS_ON_REDIRECT = frozenset(
        ["Cookie", "Authorization", "Proxy-Authorization"]
    )

    #: Maximum backoff time.
    DEFAULT_BACKOFF_MAX = 120

    def __init__(
        self,
        total=10,
        connect=None,
        read=None,
        redirect=None,
        status=None,
        other=None,
        allowed_methods=_Default,
        status_forcelist=None,
        backoff_factor=0,
        raise_on_redirect=True,
        raise_on_status=True,
        history=None,
        respect_retry_after_header=True,
        remove_headers_on_redirect=_Default,
        # TODO: Deprecated, remove in v2.0
        method_whitelist=_Default,
    ):

        if method_whitelist is not _Default:
            if allowed_methods is not _Default:
                raise ValueError(
                    "Using both 'allowed_methods' and "
                    "'method_whitelist' together is not allowed. "
                    "Instead only use 'allowed_methods'"
                )
            warnings.warn(
                "Using 'method_whitelist' with Retry is deprecated and "
                "will be removed in v2.0. Use 'allowed_methods' instead",
                DeprecationWarning,
                stacklevel=2,
            )
            allowed_methods = method_whitelist
        if allowed_methods is _Default:
            allowed_methods = self.DEFAULT_ALLOWED_METHODS
        if remove_headers_on_redirect is _Default:
            remove_headers_on_redirect = self.DEFAULT_REMOVE_HEADERS_ON_REDIRECT

        self.total = total
        self.connect = connect
        self.read = read
        self.status = status
        self.other = other

        if redirect is False or total is False:
            redirect = 0
            raise_on_redirect = False

        self.redirect = redirect
        self.status_forcelist = status_forcelist or set()
        self.allowed_methods = allowed_methods
        self.backoff_factor = backoff_factor
        self.raise_on_redirect = raise_on_redirect
        self.raise_on_status = raise_on_status
        self.history = history or tuple()
        self.respect_retry_after_header = respect_retry_after_header
        self.remove_headers_on_redirect = frozenset(
            [h.lower() for h in remove_headers_on_redirect]
        )

    def new(self, **kw):
        params = dict(
            total=self.total,
            connect=self.connect,
            read=self.read,
            redirect=self.redirect,
            status=self.status,
            other=self.other,
            status_forcelist=self.status_forcelist,
            backoff_factor=self.backoff_factor,
            raise_on_redirect=self.raise_on_redirect,
            raise_on_status=self.raise_on_status,
            history=self.history,
            remove_headers_on_redirect=self.remove_headers_on_redirect,
            respect_retry_after_header=self.respect_retry_after_header,
        )

        # TODO: If already given in **kw we use what's given to us
        # If not given we need to figure out what to pass. We decide
        # based on whether our class has the 'method_whitelist' property
        # and if so we pass the deprecated 'method_whitelist' otherwise
        # we use 'allowed_methods'. Remove in v2.0
        if "method_whitelist" not in kw and "allowed_methods" not in kw:
            if "method_whitelist" in self.__dict__:
                warnings.warn(
                    "Using 'method_whitelist' with Retry is deprecated and "
                    "will be removed in v2.0. Use 'allowed_methods' instead",
                    DeprecationWarning,
                )
                params["method_whitelist"] = self.allowed_methods
            else:
                params["allowed_methods"] = self.allowed_methods

        params.update(kw)
        return type(self)(**params)

    @classmethod
    def from_int(cls, retries, redirect=True, default=None):
        """Backwards-compatibility for the old retries format."""
        if retries is None:
            retries = default if default is not None else cls.DEFAULT

        if isinstance(retries, Retry):
            return retries

        redirect = bool(redirect) and None
        new_retries = cls(retries, redirect=redirect)
        log.debug("Converted retries value: %r -> %r", retries, new_retries)
        return new_retries

    def get_backoff_time(self):
        """Formula for computing the current backoff

        :rtype: float
        """
        # We want to consider only the last consecutive errors sequence (Ignore redirects).
        consecutive_errors_len = len(
            list(
                takewhile(lambda x: x.redirect_location is None, reversed(self.history))
            )
        )
        if consecutive_errors_len <= 1:
            return 0

        backoff_value = self.backoff_factor * (2 ** (consecutive_errors_len - 1))
        return min(self.DEFAULT_BACKOFF_MAX, backoff_value)

    def parse_retry_after(self, retry_after):
        # Whitespace: https://tools.ietf.org/html/rfc7230#section-3.2.4
        if re.match(r"^\s*[0-9]+\s*$", retry_after):
            seconds = int(retry_after)
        else:
            retry_date_tuple = email.utils.parsedate_tz(retry_after)
            if retry_date_tuple is None:
                raise InvalidHeader("Invalid Retry-After header: %s" % retry_after)
            if retry_date_tuple[9] is None:  # Python 2
                # Assume UTC if no timezone was specified
                # On Python2.7, parsedate_tz returns None for a timezone offset
                # instead of 0 if no timezone is given, where mktime_tz treats
                # a None timezone offset as local time.
                retry_date_tuple = retry_date_tuple[:9] + (0,) + retry_date_tuple[10:]

            retry_date = email.utils.mktime_tz(retry_date_tuple)
            seconds = retry_date - time.time()

        if seconds < 0:
            seconds = 0

        return seconds

    def get_retry_after(self, response):
        """Get the value of Retry-After in seconds."""

        retry_after = response.headers.get("Retry-After")

        if retry_after is None:
            return None

        return self.parse_retry_after(retry_after)

    def sleep_for_retry(self, response=None):
        retry_after = self.get_retry_after(response)
        if retry_after:
            time.sleep(retry_after)
            return True

        return False

    def _sleep_backoff(self):
        backoff = self.get_backoff_time()
        if backoff <= 0:
            return
        time.sleep(backoff)

    def sleep(self, response=None):
        """Sleep between retry attempts.

        This method will respect a server's ``Retry-After`` response header
        and sleep the duration of the time requested. If that is not present, it
        will use an exponential backoff. By default, the backoff factor is 0 and
        this method will return immediately.
        """

        if self.respect_retry_after_header and response:
            slept = self.sleep_for_retry(response)
            if slept:
                return

        self._sleep_backoff()

    def _is_connection_error(self, err):
        """Errors when we're fairly sure that the server did not receive the
        request, so it should be safe to retry.
        """
        if isinstance(err, ProxyError):
            err = err.original_error
        return isinstance(err, ConnectTimeoutError)

    def _is_read_error(self, err):
        """Errors that occur after the request has been started, so we should
        assume that the server began processing it.
        """
        return isinstance(err, (ReadTimeoutError, ProtocolError))

    def _is_method_retryable(self, method):
        """Checks if a given HTTP method should be retried upon, depending if
        it is included in the allowed_methods
        """
        # TODO: For now favor if the Retry implementation sets its own method_whitelist
        # property outside of our constructor to avoid breaking custom implementations.
        if "method_whitelist" in self.__dict__:
            warnings.warn(
                "Using 'method_whitelist' with Retry is deprecated and "
                "will be removed in v2.0. Use 'allowed_methods' instead",
                DeprecationWarning,
            )
            allowed_methods = self.method_whitelist
        else:
            allowed_methods = self.allowed_methods

        if allowed_methods and method.upper() not in allowed_methods:
            return False
        return True

    def is_retry(self, method, status_code, has_retry_after=False):
        """Is this method/status code retryable? (Based on allowlists and control
        variables such as the number of total retries to allow, whether to
        respect the Retry-After header, whether this header is present, and
        whether the returned status code is on the list of status codes to
        be retried upon on the presence of the aforementioned header)
        """
        if not self._is_method_retryable(method):
            return False

        if self.status_forcelist and status_code in self.status_forcelist:
            return True

        return (
            self.total
            and self.respect_retry_after_header
            and has_retry_after
            and (status_code in self.RETRY_AFTER_STATUS_CODES)
        )

    def is_exhausted(self):
        """Are we out of retries?"""
        retry_counts = (
            self.total,
            self.connect,
            self.read,
            self.redirect,
            self.status,
            self.other,
        )
        retry_counts = list(filter(None, retry_counts))
        if not retry_counts:
            return False

        return min(retry_counts) < 0

    def increment(
        self,
        method=None,
        url=None,
        response=None,
        error=None,
        _pool=None,
        _stacktrace=None,
    ):
        """Return a new Retry object with incremented retry counters.

        :param response: A response object, or None, if the server did not
            return a response.
        :type response: :class:`~urllib3.response.HTTPResponse`
        :param Exception error: An error encountered during the request, or
            None if the response was received successfully.

        :return: A new ``Retry`` object.
        """
        if self.total is False and error:
            # Disabled, indicate to re-raise the error.
            raise six.reraise(type(error), error, _stacktrace)

        total = self.total
        if total is not None:
            total -= 1

        connect = self.connect
        read = self.read
        redirect = self.redirect
        status_count = self.status
        other = self.other
        cause = "unknown"
        status = None
        redirect_location = None

        if error and self._is_connection_error(error):
            # Connect retry?
            if connect is False:
                raise six.reraise(type(error), error, _stacktrace)
            elif connect is not None:
                connect -= 1

        elif error and self._is_read_error(error):
            # Read retry?
            if read is False or not self._is_method_retryable(method):
                raise six.reraise(type(error), error, _stacktrace)
            elif read is not None:
                read -= 1

        elif error:
            # Other retry?
            if other is not None:
                other -= 1

        elif response and response.get_redirect_location():
            # Redirect retry?
            if redirect is not None:
                redirect -= 1
            cause = "too many redirects"
            redirect_location = response.get_redirect_location()
            status = response.status

        else:
            # Incrementing because of a server error like a 500 in
            # status_forcelist and the given method is in the allowed_methods
            cause = ResponseError.GENERIC_ERROR
            if response and response.status:
                if status_count is not None:
                    status_count -= 1
                cause = ResponseError.SPECIFIC_ERROR.format(status_code=response.status)
                status = response.status

        history = self.history + (
            RequestHistory(method, url, error, status, redirect_location),
        )

        new_retry = self.new(
            total=total,
            connect=connect,
            read=read,
            redirect=redirect,
            status=status_count,
            other=other,
            history=history,
        )

        if new_retry.is_exhausted():
            raise MaxRetryError(_pool, url, error or ResponseError(cause))

        log.debug("Incremented Retry for (url='%s'): %r", url, new_retry)

        return new_retry

    def __repr__(self):
        return (
            "{cls.__name__}(total={self.total}, connect={self.connect}, "
            "read={self.read}, redirect={self.redirect}, status={self.status})"
        ).format(cls=type(self), self=self)

    def __getattr__(self, item):
        if item == "method_whitelist":
            # TODO: Remove this deprecated alias in v2.0
            warnings.warn(
                "Using 'method_whitelist' with Retry is deprecated and "
                "will be removed in v2.0. Use 'allowed_methods' instead",
                DeprecationWarning,
            )
            return self.allowed_methods
        try:
            return getattr(super(Retry, self), item)
        except AttributeError:
            return getattr(Retry, item)


# For backwards compatibility (equivalent to pre-v1.9):
Retry.DEFAULT = Retry(3)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/ssltransport.py
# ========================================================
import io
import socket
import ssl

from ..exceptions import ProxySchemeUnsupported
from ..packages import six

SSL_BLOCKSIZE = 16384


class SSLTransport:
    """
    The SSLTransport wraps an existing socket and establishes an SSL connection.

    Contrary to Python's implementation of SSLSocket, it allows you to chain
    multiple TLS connections together. It's particularly useful if you need to
    implement TLS within TLS.

    The class supports most of the socket API operations.
    """

    @staticmethod
    def _validate_ssl_context_for_tls_in_tls(ssl_context):
        """
        Raises a ProxySchemeUnsupported if the provided ssl_context can't be used
        for TLS in TLS.

        The only requirement is that the ssl_context provides the 'wrap_bio'
        methods.
        """

        if not hasattr(ssl_context, "wrap_bio"):
            if six.PY2:
                raise ProxySchemeUnsupported(
                    "TLS in TLS requires SSLContext.wrap_bio() which isn't "
                    "supported on Python 2"
                )
            else:
                raise ProxySchemeUnsupported(
                    "TLS in TLS requires SSLContext.wrap_bio() which isn't "
                    "available on non-native SSLContext"
                )

    def __init__(
        self, socket, ssl_context, server_hostname=None, suppress_ragged_eofs=True
    ):
        """
        Create an SSLTransport around socket using the provided ssl_context.
        """
        self.incoming = ssl.MemoryBIO()
        self.outgoing = ssl.MemoryBIO()

        self.suppress_ragged_eofs = suppress_ragged_eofs
        self.socket = socket

        self.sslobj = ssl_context.wrap_bio(
            self.incoming, self.outgoing, server_hostname=server_hostname
        )

        # Perform initial handshake.
        self._ssl_io_loop(self.sslobj.do_handshake)

    def __enter__(self):
        return self

    def __exit__(self, *_):
        self.close()

    def fileno(self):
        return self.socket.fileno()

    def read(self, len=1024, buffer=None):
        return self._wrap_ssl_read(len, buffer)

    def recv(self, len=1024, flags=0):
        if flags != 0:
            raise ValueError("non-zero flags not allowed in calls to recv")
        return self._wrap_ssl_read(len)

    def recv_into(self, buffer, nbytes=None, flags=0):
        if flags != 0:
            raise ValueError("non-zero flags not allowed in calls to recv_into")
        if buffer and (nbytes is None):
            nbytes = len(buffer)
        elif nbytes is None:
            nbytes = 1024
        return self.read(nbytes, buffer)

    def sendall(self, data, flags=0):
        if flags != 0:
            raise ValueError("non-zero flags not allowed in calls to sendall")
        count = 0
        with memoryview(data) as view, view.cast("B") as byte_view:
            amount = len(byte_view)
            while count < amount:
                v = self.send(byte_view[count:])
                count += v

    def send(self, data, flags=0):
        if flags != 0:
            raise ValueError("non-zero flags not allowed in calls to send")
        response = self._ssl_io_loop(self.sslobj.write, data)
        return response

    def makefile(
        self, mode="r", buffering=None, encoding=None, errors=None, newline=None
    ):
        """
        Python's httpclient uses makefile and buffered io when reading HTTP
        messages and we need to support it.

        This is unfortunately a copy and paste of socket.py makefile with small
        changes to point to the socket directly.
        """
        if not set(mode) <= {"r", "w", "b"}:
            raise ValueError("invalid mode %r (only r, w, b allowed)" % (mode,))

        writing = "w" in mode
        reading = "r" in mode or not writing
        assert reading or writing
        binary = "b" in mode
        rawmode = ""
        if reading:
            rawmode += "r"
        if writing:
            rawmode += "w"
        raw = socket.SocketIO(self, rawmode)
        self.socket._io_refs += 1
        if buffering is None:
            buffering = -1
        if buffering < 0:
            buffering = io.DEFAULT_BUFFER_SIZE
        if buffering == 0:
            if not binary:
                raise ValueError("unbuffered streams must be binary")
            return raw
        if reading and writing:
            buffer = io.BufferedRWPair(raw, raw, buffering)
        elif reading:
            buffer = io.BufferedReader(raw, buffering)
        else:
            assert writing
            buffer = io.BufferedWriter(raw, buffering)
        if binary:
            return buffer
        text = io.TextIOWrapper(buffer, encoding, errors, newline)
        text.mode = mode
        return text

    def unwrap(self):
        self._ssl_io_loop(self.sslobj.unwrap)

    def close(self):
        self.socket.close()

    def getpeercert(self, binary_form=False):
        return self.sslobj.getpeercert(binary_form)

    def version(self):
        return self.sslobj.version()

    def cipher(self):
        return self.sslobj.cipher()

    def selected_alpn_protocol(self):
        return self.sslobj.selected_alpn_protocol()

    def selected_npn_protocol(self):
        return self.sslobj.selected_npn_protocol()

    def shared_ciphers(self):
        return self.sslobj.shared_ciphers()

    def compression(self):
        return self.sslobj.compression()

    def settimeout(self, value):
        self.socket.settimeout(value)

    def gettimeout(self):
        return self.socket.gettimeout()

    def _decref_socketios(self):
        self.socket._decref_socketios()

    def _wrap_ssl_read(self, len, buffer=None):
        try:
            return self._ssl_io_loop(self.sslobj.read, len, buffer)
        except ssl.SSLError as e:
            if e.errno == ssl.SSL_ERROR_EOF and self.suppress_ragged_eofs:
                return 0  # eof, return 0.
            else:
                raise

    def _ssl_io_loop(self, func, *args):
        """Performs an I/O loop between incoming/outgoing and the socket."""
        should_loop = True
        ret = None

        while should_loop:
            errno = None
            try:
                ret = func(*args)
            except ssl.SSLError as e:
                if e.errno not in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE):
                    # WANT_READ, and WANT_WRITE are expected, others are not.
                    raise e
                errno = e.errno

            buf = self.outgoing.read()
            self.socket.sendall(buf)

            if errno is None:
                should_loop = False
            elif errno == ssl.SSL_ERROR_WANT_READ:
                buf = self.socket.recv(SSL_BLOCKSIZE)
                if buf:
                    self.incoming.write(buf)
                else:
                    self.incoming.write_eof()
        return ret


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/proxy.py
# ========================================================
from .ssl_ import create_urllib3_context, resolve_cert_reqs, resolve_ssl_version


def connection_requires_http_tunnel(
    proxy_url=None, proxy_config=None, destination_scheme=None
):
    """
    Returns True if the connection requires an HTTP CONNECT through the proxy.

    :param URL proxy_url:
        URL of the proxy.
    :param ProxyConfig proxy_config:
        Proxy configuration from poolmanager.py
    :param str destination_scheme:
        The scheme of the destination. (i.e https, http, etc)
    """
    # If we're not using a proxy, no way to use a tunnel.
    if proxy_url is None:
        return False

    # HTTP destinations never require tunneling, we always forward.
    if destination_scheme == "http":
        return False

    # Support for forwarding with HTTPS proxies and HTTPS destinations.
    if (
        proxy_url.scheme == "https"
        and proxy_config
        and proxy_config.use_forwarding_for_https
    ):
        return False

    # Otherwise always use a tunnel.
    return True


def create_proxy_ssl_context(
    ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None
):
    """
    Generates a default proxy ssl context if one hasn't been provided by the
    user.
    """
    ssl_context = create_urllib3_context(
        ssl_version=resolve_ssl_version(ssl_version),
        cert_reqs=resolve_cert_reqs(cert_reqs),
    )

    if (
        not ca_certs
        and not ca_cert_dir
        and not ca_cert_data
        and hasattr(ssl_context, "load_default_certs")
    ):
        ssl_context.load_default_certs()

    return ssl_context


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/queue.py
# ========================================================
import collections

from ..packages import six
from ..packages.six.moves import queue

if six.PY2:
    # Queue is imported for side effects on MS Windows. See issue #229.
    import Queue as _unused_module_Queue  # noqa: F401


class LifoQueue(queue.Queue):
    def _init(self, _):
        self.queue = collections.deque()

    def _qsize(self, len=len):
        return len(self.queue)

    def _put(self, item):
        self.queue.append(item)

    def _get(self):
        return self.queue.pop()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/util/request.py
# ========================================================
from __future__ import absolute_import

from base64 import b64encode

from ..exceptions import UnrewindableBodyError
from ..packages.six import b, integer_types

# Pass as a value within ``headers`` to skip
# emitting some HTTP headers that are added automatically.
# The only headers that are supported are ``Accept-Encoding``,
# ``Host``, and ``User-Agent``.
SKIP_HEADER = "@@@SKIP_HEADER@@@"
SKIPPABLE_HEADERS = frozenset(["accept-encoding", "host", "user-agent"])

ACCEPT_ENCODING = "gzip,deflate"

_FAILEDTELL = object()


def make_headers(
    keep_alive=None,
    accept_encoding=None,
    user_agent=None,
    basic_auth=None,
    proxy_basic_auth=None,
    disable_cache=None,
):
    """
    Shortcuts for generating request headers.

    :param keep_alive:
        If ``True``, adds 'connection: keep-alive' header.

    :param accept_encoding:
        Can be a boolean, list, or string.
        ``True`` translates to 'gzip,deflate'.
        List will get joined by comma.
        String will be used as provided.

    :param user_agent:
        String representing the user-agent you want, such as
        "python-urllib3/0.6"

    :param basic_auth:
        Colon-separated username:password string for 'authorization: basic ...'
        auth header.

    :param proxy_basic_auth:
        Colon-separated username:password string for 'proxy-authorization: basic ...'
        auth header.

    :param disable_cache:
        If ``True``, adds 'cache-control: no-cache' header.

    Example::

        >>> make_headers(keep_alive=True, user_agent="Batman/1.0")
        {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}
        >>> make_headers(accept_encoding=True)
        {'accept-encoding': 'gzip,deflate'}
    """
    headers = {}
    if accept_encoding:
        if isinstance(accept_encoding, str):
            pass
        elif isinstance(accept_encoding, list):
            accept_encoding = ",".join(accept_encoding)
        else:
            accept_encoding = ACCEPT_ENCODING
        headers["accept-encoding"] = accept_encoding

    if user_agent:
        headers["user-agent"] = user_agent

    if keep_alive:
        headers["connection"] = "keep-alive"

    if basic_auth:
        headers["authorization"] = "Basic " + b64encode(b(basic_auth)).decode("utf-8")

    if proxy_basic_auth:
        headers["proxy-authorization"] = "Basic " + b64encode(
            b(proxy_basic_auth)
        ).decode("utf-8")

    if disable_cache:
        headers["cache-control"] = "no-cache"

    return headers


def set_file_position(body, pos):
    """
    If a position is provided, move file to that point.
    Otherwise, we'll attempt to record a position for future use.
    """
    if pos is not None:
        rewind_body(body, pos)
    elif getattr(body, "tell", None) is not None:
        try:
            pos = body.tell()
        except (IOError, OSError):
            # This differentiates from None, allowing us to catch
            # a failed `tell()` later when trying to rewind the body.
            pos = _FAILEDTELL

    return pos


def rewind_body(body, body_pos):
    """
    Attempt to rewind body to a certain position.
    Primarily used for request redirects and retries.

    :param body:
        File-like object that supports seek.

    :param int pos:
        Position to seek to in file.
    """
    body_seek = getattr(body, "seek", None)
    if body_seek is not None and isinstance(body_pos, integer_types):
        try:
            body_seek(body_pos)
        except (IOError, OSError):
            raise UnrewindableBodyError(
                "An error occurred when rewinding request body for redirect/retry."
            )
    elif body_pos is _FAILEDTELL:
        raise UnrewindableBodyError(
            "Unable to record file position for rewinding "
            "request body during a redirect/retry."
        )
    else:
        raise ValueError(
            "body_pos must be of type integer, instead it was %s." % type(body_pos)
        )


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/connection.py
# ========================================================
from __future__ import absolute_import

import datetime
import logging
import os
import re
import socket
import warnings
from socket import error as SocketError
from socket import timeout as SocketTimeout

from .packages import six
from .packages.six.moves.http_client import HTTPConnection as _HTTPConnection
from .packages.six.moves.http_client import HTTPException  # noqa: F401
from .util.proxy import create_proxy_ssl_context

try:  # Compiled with SSL?
    import ssl

    BaseSSLError = ssl.SSLError
except (ImportError, AttributeError):  # Platform-specific: No SSL.
    ssl = None

    class BaseSSLError(BaseException):
        pass


try:
    # Python 3: not a no-op, we're adding this to the namespace so it can be imported.
    ConnectionError = ConnectionError
except NameError:
    # Python 2
    class ConnectionError(Exception):
        pass


try:  # Python 3:
    # Not a no-op, we're adding this to the namespace so it can be imported.
    BrokenPipeError = BrokenPipeError
except NameError:  # Python 2:

    class BrokenPipeError(Exception):
        pass


from ._collections import HTTPHeaderDict  # noqa (historical, removed in v2)
from ._version import __version__
from .exceptions import (
    ConnectTimeoutError,
    NewConnectionError,
    SubjectAltNameWarning,
    SystemTimeWarning,
)
from .util import SKIP_HEADER, SKIPPABLE_HEADERS, connection
from .util.ssl_ import (
    assert_fingerprint,
    create_urllib3_context,
    is_ipaddress,
    resolve_cert_reqs,
    resolve_ssl_version,
    ssl_wrap_socket,
)
from .util.ssl_match_hostname import CertificateError, match_hostname

log = logging.getLogger(__name__)

port_by_scheme = {"http": 80, "https": 443}

# When it comes time to update this value as a part of regular maintenance
# (ie test_recent_date is failing) update it to ~6 months before the current date.
RECENT_DATE = datetime.date(2022, 1, 1)

_CONTAINS_CONTROL_CHAR_RE = re.compile(r"[^-!#$%&'*+.^_`|~0-9a-zA-Z]")


class HTTPConnection(_HTTPConnection, object):
    """
    Based on :class:`http.client.HTTPConnection` but provides an extra constructor
    backwards-compatibility layer between older and newer Pythons.

    Additional keyword parameters are used to configure attributes of the connection.
    Accepted parameters include:

    - ``strict``: See the documentation on :class:`urllib3.connectionpool.HTTPConnectionPool`
    - ``source_address``: Set the source address for the current connection.
    - ``socket_options``: Set specific options on the underlying socket. If not specified, then
      defaults are loaded from ``HTTPConnection.default_socket_options`` which includes disabling
      Nagle's algorithm (sets TCP_NODELAY to 1) unless the connection is behind a proxy.

      For example, if you wish to enable TCP Keep Alive in addition to the defaults,
      you might pass:

      .. code-block:: python

         HTTPConnection.default_socket_options + [
             (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),
         ]

      Or you may want to disable the defaults by passing an empty list (e.g., ``[]``).
    """

    default_port = port_by_scheme["http"]

    #: Disable Nagle's algorithm by default.
    #: ``[(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]``
    default_socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]

    #: Whether this connection verifies the host's certificate.
    is_verified = False

    #: Whether this proxy connection (if used) verifies the proxy host's
    #: certificate.
    proxy_is_verified = None

    def __init__(self, *args, **kw):
        if not six.PY2:
            kw.pop("strict", None)

        # Pre-set source_address.
        self.source_address = kw.get("source_address")

        #: The socket options provided by the user. If no options are
        #: provided, we use the default options.
        self.socket_options = kw.pop("socket_options", self.default_socket_options)

        # Proxy options provided by the user.
        self.proxy = kw.pop("proxy", None)
        self.proxy_config = kw.pop("proxy_config", None)

        _HTTPConnection.__init__(self, *args, **kw)

    @property
    def host(self):
        """
        Getter method to remove any trailing dots that indicate the hostname is an FQDN.

        In general, SSL certificates don't include the trailing dot indicating a
        fully-qualified domain name, and thus, they don't validate properly when
        checked against a domain name that includes the dot. In addition, some
        servers may not expect to receive the trailing dot when provided.

        However, the hostname with trailing dot is critical to DNS resolution; doing a
        lookup with the trailing dot will properly only resolve the appropriate FQDN,
        whereas a lookup without a trailing dot will search the system's search domain
        list. Thus, it's important to keep the original host around for use only in
        those cases where it's appropriate (i.e., when doing DNS lookup to establish the
        actual TCP connection across which we're going to send HTTP requests).
        """
        return self._dns_host.rstrip(".")

    @host.setter
    def host(self, value):
        """
        Setter for the `host` property.

        We assume that only urllib3 uses the _dns_host attribute; httplib itself
        only uses `host`, and it seems reasonable that other libraries follow suit.
        """
        self._dns_host = value

    def _new_conn(self):
        """Establish a socket connection and set nodelay settings on it.

        :return: New socket connection.
        """
        extra_kw = {}
        if self.source_address:
            extra_kw["source_address"] = self.source_address

        if self.socket_options:
            extra_kw["socket_options"] = self.socket_options

        try:
            conn = connection.create_connection(
                (self._dns_host, self.port), self.timeout, **extra_kw
            )

        except SocketTimeout:
            raise ConnectTimeoutError(
                self,
                "Connection to %s timed out. (connect timeout=%s)"
                % (self.host, self.timeout),
            )

        except SocketError as e:
            raise NewConnectionError(
                self, "Failed to establish a new connection: %s" % e
            )

        return conn

    def _is_using_tunnel(self):
        # Google App Engine's httplib does not define _tunnel_host
        return getattr(self, "_tunnel_host", None)

    def _prepare_conn(self, conn):
        self.sock = conn
        if self._is_using_tunnel():
            # TODO: Fix tunnel so it doesn't depend on self.sock state.
            self._tunnel()
            # Mark this connection as not reusable
            self.auto_open = 0

    def connect(self):
        conn = self._new_conn()
        self._prepare_conn(conn)

    def putrequest(self, method, url, *args, **kwargs):
        """ """
        # Empty docstring because the indentation of CPython's implementation
        # is broken but we don't want this method in our documentation.
        match = _CONTAINS_CONTROL_CHAR_RE.search(method)
        if match:
            raise ValueError(
                "Method cannot contain non-token characters %r (found at least %r)"
                % (method, match.group())
            )

        return _HTTPConnection.putrequest(self, method, url, *args, **kwargs)

    def putheader(self, header, *values):
        """ """
        if not any(isinstance(v, str) and v == SKIP_HEADER for v in values):
            _HTTPConnection.putheader(self, header, *values)
        elif six.ensure_str(header.lower()) not in SKIPPABLE_HEADERS:
            raise ValueError(
                "urllib3.util.SKIP_HEADER only supports '%s'"
                % ("', '".join(map(str.title, sorted(SKIPPABLE_HEADERS))),)
            )

    def request(self, method, url, body=None, headers=None):
        # Update the inner socket's timeout value to send the request.
        # This only triggers if the connection is re-used.
        if getattr(self, "sock", None) is not None:
            self.sock.settimeout(self.timeout)

        if headers is None:
            headers = {}
        else:
            # Avoid modifying the headers passed into .request()
            headers = headers.copy()
        if "user-agent" not in (six.ensure_str(k.lower()) for k in headers):
            headers["User-Agent"] = _get_default_user_agent()
        super(HTTPConnection, self).request(method, url, body=body, headers=headers)

    def request_chunked(self, method, url, body=None, headers=None):
        """
        Alternative to the common request method, which sends the
        body with chunked encoding and not as one block
        """
        headers = headers or {}
        header_keys = set([six.ensure_str(k.lower()) for k in headers])
        skip_accept_encoding = "accept-encoding" in header_keys
        skip_host = "host" in header_keys
        self.putrequest(
            method, url, skip_accept_encoding=skip_accept_encoding, skip_host=skip_host
        )
        if "user-agent" not in header_keys:
            self.putheader("User-Agent", _get_default_user_agent())
        for header, value in headers.items():
            self.putheader(header, value)
        if "transfer-encoding" not in header_keys:
            self.putheader("Transfer-Encoding", "chunked")
        self.endheaders()

        if body is not None:
            stringish_types = six.string_types + (bytes,)
            if isinstance(body, stringish_types):
                body = (body,)
            for chunk in body:
                if not chunk:
                    continue
                if not isinstance(chunk, bytes):
                    chunk = chunk.encode("utf8")
                len_str = hex(len(chunk))[2:]
                to_send = bytearray(len_str.encode())
                to_send += b"\r\n"
                to_send += chunk
                to_send += b"\r\n"
                self.send(to_send)

        # After the if clause, to always have a closed body
        self.send(b"0\r\n\r\n")


class HTTPSConnection(HTTPConnection):
    """
    Many of the parameters to this constructor are passed to the underlying SSL
    socket by means of :py:func:`urllib3.util.ssl_wrap_socket`.
    """

    default_port = port_by_scheme["https"]

    cert_reqs = None
    ca_certs = None
    ca_cert_dir = None
    ca_cert_data = None
    ssl_version = None
    assert_fingerprint = None
    tls_in_tls_required = False

    def __init__(
        self,
        host,
        port=None,
        key_file=None,
        cert_file=None,
        key_password=None,
        strict=None,
        timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
        ssl_context=None,
        server_hostname=None,
        **kw
    ):

        HTTPConnection.__init__(self, host, port, strict=strict, timeout=timeout, **kw)

        self.key_file = key_file
        self.cert_file = cert_file
        self.key_password = key_password
        self.ssl_context = ssl_context
        self.server_hostname = server_hostname

        # Required property for Google AppEngine 1.9.0 which otherwise causes
        # HTTPS requests to go out as HTTP. (See Issue #356)
        self._protocol = "https"

    def set_cert(
        self,
        key_file=None,
        cert_file=None,
        cert_reqs=None,
        key_password=None,
        ca_certs=None,
        assert_hostname=None,
        assert_fingerprint=None,
        ca_cert_dir=None,
        ca_cert_data=None,
    ):
        """
        This method should only be called once, before the connection is used.
        """
        # If cert_reqs is not provided we'll assume CERT_REQUIRED unless we also
        # have an SSLContext object in which case we'll use its verify_mode.
        if cert_reqs is None:
            if self.ssl_context is not None:
                cert_reqs = self.ssl_context.verify_mode
            else:
                cert_reqs = resolve_cert_reqs(None)

        self.key_file = key_file
        self.cert_file = cert_file
        self.cert_reqs = cert_reqs
        self.key_password = key_password
        self.assert_hostname = assert_hostname
        self.assert_fingerprint = assert_fingerprint
        self.ca_certs = ca_certs and os.path.expanduser(ca_certs)
        self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir)
        self.ca_cert_data = ca_cert_data

    def connect(self):
        # Add certificate verification
        self.sock = conn = self._new_conn()
        hostname = self.host
        tls_in_tls = False

        if self._is_using_tunnel():
            if self.tls_in_tls_required:
                self.sock = conn = self._connect_tls_proxy(hostname, conn)
                tls_in_tls = True

            # Calls self._set_hostport(), so self.host is
            # self._tunnel_host below.
            self._tunnel()
            # Mark this connection as not reusable
            self.auto_open = 0

            # Override the host with the one we're requesting data from.
            hostname = self._tunnel_host

        server_hostname = hostname
        if self.server_hostname is not None:
            server_hostname = self.server_hostname

        is_time_off = datetime.date.today() < RECENT_DATE
        if is_time_off:
            warnings.warn(
                (
                    "System time is way off (before {0}). This will probably "
                    "lead to SSL verification errors"
                ).format(RECENT_DATE),
                SystemTimeWarning,
            )

        # Wrap socket using verification with the root certs in
        # trusted_root_certs
        default_ssl_context = False
        if self.ssl_context is None:
            default_ssl_context = True
            self.ssl_context = create_urllib3_context(
                ssl_version=resolve_ssl_version(self.ssl_version),
                cert_reqs=resolve_cert_reqs(self.cert_reqs),
            )

        context = self.ssl_context
        context.verify_mode = resolve_cert_reqs(self.cert_reqs)

        # Try to load OS default certs if none are given.
        # Works well on Windows (requires Python3.4+)
        if (
            not self.ca_certs
            and not self.ca_cert_dir
            and not self.ca_cert_data
            and default_ssl_context
            and hasattr(context, "load_default_certs")
        ):
            context.load_default_certs()

        self.sock = ssl_wrap_socket(
            sock=conn,
            keyfile=self.key_file,
            certfile=self.cert_file,
            key_password=self.key_password,
            ca_certs=self.ca_certs,
            ca_cert_dir=self.ca_cert_dir,
            ca_cert_data=self.ca_cert_data,
            server_hostname=server_hostname,
            ssl_context=context,
            tls_in_tls=tls_in_tls,
        )

        # If we're using all defaults and the connection
        # is TLSv1 or TLSv1.1 we throw a DeprecationWarning
        # for the host.
        if (
            default_ssl_context
            and self.ssl_version is None
            and hasattr(self.sock, "version")
            and self.sock.version() in {"TLSv1", "TLSv1.1"}
        ):
            warnings.warn(
                "Negotiating TLSv1/TLSv1.1 by default is deprecated "
                "and will be disabled in urllib3 v2.0.0. Connecting to "
                "'%s' with '%s' can be enabled by explicitly opting-in "
                "with 'ssl_version'" % (self.host, self.sock.version()),
                DeprecationWarning,
            )

        if self.assert_fingerprint:
            assert_fingerprint(
                self.sock.getpeercert(binary_form=True), self.assert_fingerprint
            )
        elif (
            context.verify_mode != ssl.CERT_NONE
            and not getattr(context, "check_hostname", False)
            and self.assert_hostname is not False
        ):
            # While urllib3 attempts to always turn off hostname matching from
            # the TLS library, this cannot always be done. So we check whether
            # the TLS Library still thinks it's matching hostnames.
            cert = self.sock.getpeercert()
            if not cert.get("subjectAltName", ()):
                warnings.warn(
                    (
                        "Certificate for {0} has no `subjectAltName`, falling back to check for a "
                        "`commonName` for now. This feature is being removed by major browsers and "
                        "deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 "
                        "for details.)".format(hostname)
                    ),
                    SubjectAltNameWarning,
                )
            _match_hostname(cert, self.assert_hostname or server_hostname)

        self.is_verified = (
            context.verify_mode == ssl.CERT_REQUIRED
            or self.assert_fingerprint is not None
        )

    def _connect_tls_proxy(self, hostname, conn):
        """
        Establish a TLS connection to the proxy using the provided SSL context.
        """
        proxy_config = self.proxy_config
        ssl_context = proxy_config.ssl_context
        if ssl_context:
            # If the user provided a proxy context, we assume CA and client
            # certificates have already been set
            return ssl_wrap_socket(
                sock=conn,
                server_hostname=hostname,
                ssl_context=ssl_context,
            )

        ssl_context = create_proxy_ssl_context(
            self.ssl_version,
            self.cert_reqs,
            self.ca_certs,
            self.ca_cert_dir,
            self.ca_cert_data,
        )

        # If no cert was provided, use only the default options for server
        # certificate validation
        socket = ssl_wrap_socket(
            sock=conn,
            ca_certs=self.ca_certs,
            ca_cert_dir=self.ca_cert_dir,
            ca_cert_data=self.ca_cert_data,
            server_hostname=hostname,
            ssl_context=ssl_context,
        )

        if ssl_context.verify_mode != ssl.CERT_NONE and not getattr(
            ssl_context, "check_hostname", False
        ):
            # While urllib3 attempts to always turn off hostname matching from
            # the TLS library, this cannot always be done. So we check whether
            # the TLS Library still thinks it's matching hostnames.
            cert = socket.getpeercert()
            if not cert.get("subjectAltName", ()):
                warnings.warn(
                    (
                        "Certificate for {0} has no `subjectAltName`, falling back to check for a "
                        "`commonName` for now. This feature is being removed by major browsers and "
                        "deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 "
                        "for details.)".format(hostname)
                    ),
                    SubjectAltNameWarning,
                )
            _match_hostname(cert, hostname)

        self.proxy_is_verified = ssl_context.verify_mode == ssl.CERT_REQUIRED
        return socket


def _match_hostname(cert, asserted_hostname):
    # Our upstream implementation of ssl.match_hostname()
    # only applies this normalization to IP addresses so it doesn't
    # match DNS SANs so we do the same thing!
    stripped_hostname = asserted_hostname.strip("u[]")
    if is_ipaddress(stripped_hostname):
        asserted_hostname = stripped_hostname

    try:
        match_hostname(cert, asserted_hostname)
    except CertificateError as e:
        log.warning(
            "Certificate did not match expected hostname: %s. Certificate: %s",
            asserted_hostname,
            cert,
        )
        # Add cert to exception and reraise so client code can inspect
        # the cert when catching the exception, if they want to
        e._peer_cert = cert
        raise


def _get_default_user_agent():
    return "python-urllib3/%s" % __version__


class DummyConnection(object):
    """Used to detect a failed ConnectionCls import."""

    pass


if not ssl:
    HTTPSConnection = DummyConnection  # noqa: F811


VerifiedHTTPSConnection = HTTPSConnection


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/exceptions.py
# ========================================================
from __future__ import absolute_import

from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead

# Base Exceptions


class HTTPError(Exception):
    """Base exception used by this module."""

    pass


class HTTPWarning(Warning):
    """Base warning used by this module."""

    pass


class PoolError(HTTPError):
    """Base exception for errors caused within a pool."""

    def __init__(self, pool, message):
        self.pool = pool
        HTTPError.__init__(self, "%s: %s" % (pool, message))

    def __reduce__(self):
        # For pickling purposes.
        return self.__class__, (None, None)


class RequestError(PoolError):
    """Base exception for PoolErrors that have associated URLs."""

    def __init__(self, pool, url, message):
        self.url = url
        PoolError.__init__(self, pool, message)

    def __reduce__(self):
        # For pickling purposes.
        return self.__class__, (None, self.url, None)


class SSLError(HTTPError):
    """Raised when SSL certificate fails in an HTTPS connection."""

    pass


class ProxyError(HTTPError):
    """Raised when the connection to a proxy fails."""

    def __init__(self, message, error, *args):
        super(ProxyError, self).__init__(message, error, *args)
        self.original_error = error


class DecodeError(HTTPError):
    """Raised when automatic decoding based on Content-Type fails."""

    pass


class ProtocolError(HTTPError):
    """Raised when something unexpected happens mid-request/response."""

    pass


#: Renamed to ProtocolError but aliased for backwards compatibility.
ConnectionError = ProtocolError


# Leaf Exceptions


class MaxRetryError(RequestError):
    """Raised when the maximum number of retries is exceeded.

    :param pool: The connection pool
    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`
    :param string url: The requested Url
    :param exceptions.Exception reason: The underlying error

    """

    def __init__(self, pool, url, reason=None):
        self.reason = reason

        message = "Max retries exceeded with url: %s (Caused by %r)" % (url, reason)

        RequestError.__init__(self, pool, url, message)


class HostChangedError(RequestError):
    """Raised when an existing pool gets a request for a foreign host."""

    def __init__(self, pool, url, retries=3):
        message = "Tried to open a foreign host with url: %s" % url
        RequestError.__init__(self, pool, url, message)
        self.retries = retries


class TimeoutStateError(HTTPError):
    """Raised when passing an invalid state to a timeout"""

    pass


class TimeoutError(HTTPError):
    """Raised when a socket timeout error occurs.

    Catching this error will catch both :exc:`ReadTimeoutErrors
    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.
    """

    pass


class ReadTimeoutError(TimeoutError, RequestError):
    """Raised when a socket timeout occurs while receiving data from a server"""

    pass


# This timeout error does not have a URL attached and needs to inherit from the
# base HTTPError
class ConnectTimeoutError(TimeoutError):
    """Raised when a socket timeout occurs while connecting to a server"""

    pass


class NewConnectionError(ConnectTimeoutError, PoolError):
    """Raised when we fail to establish a new connection. Usually ECONNREFUSED."""

    pass


class EmptyPoolError(PoolError):
    """Raised when a pool runs out of connections and no more are allowed."""

    pass


class ClosedPoolError(PoolError):
    """Raised when a request enters a pool after the pool has been closed."""

    pass


class LocationValueError(ValueError, HTTPError):
    """Raised when there is something wrong with a given URL input."""

    pass


class LocationParseError(LocationValueError):
    """Raised when get_host or similar fails to parse the URL input."""

    def __init__(self, location):
        message = "Failed to parse: %s" % location
        HTTPError.__init__(self, message)

        self.location = location


class URLSchemeUnknown(LocationValueError):
    """Raised when a URL input has an unsupported scheme."""

    def __init__(self, scheme):
        message = "Not supported URL scheme %s" % scheme
        super(URLSchemeUnknown, self).__init__(message)

        self.scheme = scheme


class ResponseError(HTTPError):
    """Used as a container for an error reason supplied in a MaxRetryError."""

    GENERIC_ERROR = "too many error responses"
    SPECIFIC_ERROR = "too many {status_code} error responses"


class SecurityWarning(HTTPWarning):
    """Warned when performing security reducing actions"""

    pass


class SubjectAltNameWarning(SecurityWarning):
    """Warned when connecting to a host with a certificate missing a SAN."""

    pass


class InsecureRequestWarning(SecurityWarning):
    """Warned when making an unverified HTTPS request."""

    pass


class SystemTimeWarning(SecurityWarning):
    """Warned when system time is suspected to be wrong"""

    pass


class InsecurePlatformWarning(SecurityWarning):
    """Warned when certain TLS/SSL configuration is not available on a platform."""

    pass


class SNIMissingWarning(HTTPWarning):
    """Warned when making a HTTPS request without SNI available."""

    pass


class DependencyWarning(HTTPWarning):
    """
    Warned when an attempt is made to import a module with missing optional
    dependencies.
    """

    pass


class ResponseNotChunked(ProtocolError, ValueError):
    """Response needs to be chunked in order to read it as chunks."""

    pass


class BodyNotHttplibCompatible(HTTPError):
    """
    Body should be :class:`http.client.HTTPResponse` like
    (have an fp attribute which returns raw chunks) for read_chunked().
    """

    pass


class IncompleteRead(HTTPError, httplib_IncompleteRead):
    """
    Response length doesn't match expected Content-Length

    Subclass of :class:`http.client.IncompleteRead` to allow int value
    for ``partial`` to avoid creating large objects on streamed reads.
    """

    def __init__(self, partial, expected):
        super(IncompleteRead, self).__init__(partial, expected)

    def __repr__(self):
        return "IncompleteRead(%i bytes read, %i more expected)" % (
            self.partial,
            self.expected,
        )


class InvalidChunkLength(HTTPError, httplib_IncompleteRead):
    """Invalid chunk length in a chunked response."""

    def __init__(self, response, length):
        super(InvalidChunkLength, self).__init__(
            response.tell(), response.length_remaining
        )
        self.response = response
        self.length = length

    def __repr__(self):
        return "InvalidChunkLength(got length %r, %i bytes read)" % (
            self.length,
            self.partial,
        )


class InvalidHeader(HTTPError):
    """The header provided was somehow invalid."""

    pass


class ProxySchemeUnknown(AssertionError, URLSchemeUnknown):
    """ProxyManager does not support the supplied scheme"""

    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.

    def __init__(self, scheme):
        # 'localhost' is here because our URL parser parses
        # localhost:8080 -> scheme=localhost, remove if we fix this.
        if scheme == "localhost":
            scheme = None
        if scheme is None:
            message = "Proxy URL had no scheme, should start with http:// or https://"
        else:
            message = (
                "Proxy URL had unsupported scheme %s, should use http:// or https://"
                % scheme
            )
        super(ProxySchemeUnknown, self).__init__(message)


class ProxySchemeUnsupported(ValueError):
    """Fetching HTTPS resources through HTTPS proxies is unsupported"""

    pass


class HeaderParsingError(HTTPError):
    """Raised by assert_header_parsing, but we convert it to a log.warning statement."""

    def __init__(self, defects, unparsed_data):
        message = "%s, unparsed data: %r" % (defects or "Unknown", unparsed_data)
        super(HeaderParsingError, self).__init__(message)


class UnrewindableBodyError(HTTPError):
    """urllib3 encountered an error when trying to rewind a body"""

    pass


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/__init__.py
# ========================================================
"""
Python HTTP library with thread-safe connection pooling, file post support, user friendly, and more
"""
from __future__ import absolute_import

# Set default logging handler to avoid "No handler found" warnings.
import logging
import warnings
from logging import NullHandler

from . import exceptions
from ._version import __version__
from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url
from .filepost import encode_multipart_formdata
from .poolmanager import PoolManager, ProxyManager, proxy_from_url
from .response import HTTPResponse
from .util.request import make_headers
from .util.retry import Retry
from .util.timeout import Timeout
from .util.url import get_host

# === NOTE TO REPACKAGERS AND VENDORS ===
# Please delete this block, this logic is only
# for urllib3 being distributed via PyPI.
# See: https://github.com/urllib3/urllib3/issues/2680
try:
    import urllib3_secure_extra  # type: ignore # noqa: F401
except ImportError:
    pass
else:
    warnings.warn(
        "'urllib3[secure]' extra is deprecated and will be removed "
        "in a future release of urllib3 2.x. Read more in this issue: "
        "https://github.com/urllib3/urllib3/issues/2680",
        category=DeprecationWarning,
        stacklevel=2,
    )

__author__ = "Andrey Petrov (andrey.petrov@shazow.net)"
__license__ = "MIT"
__version__ = __version__

__all__ = (
    "HTTPConnectionPool",
    "HTTPSConnectionPool",
    "PoolManager",
    "ProxyManager",
    "HTTPResponse",
    "Retry",
    "Timeout",
    "add_stderr_logger",
    "connection_from_url",
    "disable_warnings",
    "encode_multipart_formdata",
    "get_host",
    "make_headers",
    "proxy_from_url",
)

logging.getLogger(__name__).addHandler(NullHandler())


def add_stderr_logger(level=logging.DEBUG):
    """
    Helper for quickly adding a StreamHandler to the logger. Useful for
    debugging.

    Returns the handler after adding it.
    """
    # This method needs to be in this __init__.py to get the __name__ correct
    # even if urllib3 is vendored within another package.
    logger = logging.getLogger(__name__)
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
    logger.addHandler(handler)
    logger.setLevel(level)
    logger.debug("Added a stderr logging handler to logger: %s", __name__)
    return handler


# ... Clean up.
del NullHandler


# All warning filters *must* be appended unless you're really certain that they
# shouldn't be: otherwise, it's very hard for users to use most Python
# mechanisms to silence them.
# SecurityWarning's always go off by default.
warnings.simplefilter("always", exceptions.SecurityWarning, append=True)
# SubjectAltNameWarning's should go off once per host
warnings.simplefilter("default", exceptions.SubjectAltNameWarning, append=True)
# InsecurePlatformWarning's don't vary between requests, so we keep it default.
warnings.simplefilter("default", exceptions.InsecurePlatformWarning, append=True)
# SNIMissingWarnings should go off only once.
warnings.simplefilter("default", exceptions.SNIMissingWarning, append=True)


def disable_warnings(category=exceptions.HTTPWarning):
    """
    Helper for quickly disabling all urllib3 warnings.
    """
    warnings.simplefilter("ignore", category)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_appengine_environ.py
# ========================================================
"""
This module provides means to detect the App Engine environment.
"""

import os


def is_appengine():
    return is_local_appengine() or is_prod_appengine()


def is_appengine_sandbox():
    """Reports if the app is running in the first generation sandbox.

    The second generation runtimes are technically still in a sandbox, but it
    is much less restrictive, so generally you shouldn't need to check for it.
    see https://cloud.google.com/appengine/docs/standard/runtimes
    """
    return is_appengine() and os.environ["APPENGINE_RUNTIME"] == "python27"


def is_local_appengine():
    return "APPENGINE_RUNTIME" in os.environ and os.environ.get(
        "SERVER_SOFTWARE", ""
    ).startswith("Development/")


def is_prod_appengine():
    return "APPENGINE_RUNTIME" in os.environ and os.environ.get(
        "SERVER_SOFTWARE", ""
    ).startswith("Google App Engine/")


def is_prod_appengine_mvms():
    """Deprecated."""
    return False


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/socks.py
# ========================================================
# -*- coding: utf-8 -*-
"""
This module contains provisional support for SOCKS proxies from within
urllib3. This module supports SOCKS4, SOCKS4A (an extension of SOCKS4), and
SOCKS5. To enable its functionality, either install PySocks or install this
module with the ``socks`` extra.

The SOCKS implementation supports the full range of urllib3 features. It also
supports the following SOCKS features:

- SOCKS4A (``proxy_url='socks4a://...``)
- SOCKS4 (``proxy_url='socks4://...``)
- SOCKS5 with remote DNS (``proxy_url='socks5h://...``)
- SOCKS5 with local DNS (``proxy_url='socks5://...``)
- Usernames and passwords for the SOCKS proxy

.. note::
   It is recommended to use ``socks5h://`` or ``socks4a://`` schemes in
   your ``proxy_url`` to ensure that DNS resolution is done from the remote
   server instead of client-side when connecting to a domain name.

SOCKS4 supports IPv4 and domain names with the SOCKS4A extension. SOCKS5
supports IPv4, IPv6, and domain names.

When connecting to a SOCKS4 proxy the ``username`` portion of the ``proxy_url``
will be sent as the ``userid`` section of the SOCKS request:

.. code-block:: python

    proxy_url="socks4a://<userid>@proxy-host"

When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
of the ``proxy_url`` will be sent as the username/password to authenticate
with the proxy:

.. code-block:: python

    proxy_url="socks5h://<username>:<password>@proxy-host"

"""
from __future__ import absolute_import

try:
    import socks
except ImportError:
    import warnings

    from ..exceptions import DependencyWarning

    warnings.warn(
        (
            "SOCKS support in urllib3 requires the installation of optional "
            "dependencies: specifically, PySocks.  For more information, see "
            "https://urllib3.readthedocs.io/en/1.26.x/contrib.html#socks-proxies"
        ),
        DependencyWarning,
    )
    raise

from socket import error as SocketError
from socket import timeout as SocketTimeout

from ..connection import HTTPConnection, HTTPSConnection
from ..connectionpool import HTTPConnectionPool, HTTPSConnectionPool
from ..exceptions import ConnectTimeoutError, NewConnectionError
from ..poolmanager import PoolManager
from ..util.url import parse_url

try:
    import ssl
except ImportError:
    ssl = None


class SOCKSConnection(HTTPConnection):
    """
    A plain-text HTTP connection that connects via a SOCKS proxy.
    """

    def __init__(self, *args, **kwargs):
        self._socks_options = kwargs.pop("_socks_options")
        super(SOCKSConnection, self).__init__(*args, **kwargs)

    def _new_conn(self):
        """
        Establish a new connection via the SOCKS proxy.
        """
        extra_kw = {}
        if self.source_address:
            extra_kw["source_address"] = self.source_address

        if self.socket_options:
            extra_kw["socket_options"] = self.socket_options

        try:
            conn = socks.create_connection(
                (self.host, self.port),
                proxy_type=self._socks_options["socks_version"],
                proxy_addr=self._socks_options["proxy_host"],
                proxy_port=self._socks_options["proxy_port"],
                proxy_username=self._socks_options["username"],
                proxy_password=self._socks_options["password"],
                proxy_rdns=self._socks_options["rdns"],
                timeout=self.timeout,
                **extra_kw
            )

        except SocketTimeout:
            raise ConnectTimeoutError(
                self,
                "Connection to %s timed out. (connect timeout=%s)"
                % (self.host, self.timeout),
            )

        except socks.ProxyError as e:
            # This is fragile as hell, but it seems to be the only way to raise
            # useful errors here.
            if e.socket_err:
                error = e.socket_err
                if isinstance(error, SocketTimeout):
                    raise ConnectTimeoutError(
                        self,
                        "Connection to %s timed out. (connect timeout=%s)"
                        % (self.host, self.timeout),
                    )
                else:
                    raise NewConnectionError(
                        self, "Failed to establish a new connection: %s" % error
                    )
            else:
                raise NewConnectionError(
                    self, "Failed to establish a new connection: %s" % e
                )

        except SocketError as e:  # Defensive: PySocks should catch all these.
            raise NewConnectionError(
                self, "Failed to establish a new connection: %s" % e
            )

        return conn


# We don't need to duplicate the Verified/Unverified distinction from
# urllib3/connection.py here because the HTTPSConnection will already have been
# correctly set to either the Verified or Unverified form by that module. This
# means the SOCKSHTTPSConnection will automatically be the correct type.
class SOCKSHTTPSConnection(SOCKSConnection, HTTPSConnection):
    pass


class SOCKSHTTPConnectionPool(HTTPConnectionPool):
    ConnectionCls = SOCKSConnection


class SOCKSHTTPSConnectionPool(HTTPSConnectionPool):
    ConnectionCls = SOCKSHTTPSConnection


class SOCKSProxyManager(PoolManager):
    """
    A version of the urllib3 ProxyManager that routes connections via the
    defined SOCKS proxy.
    """

    pool_classes_by_scheme = {
        "http": SOCKSHTTPConnectionPool,
        "https": SOCKSHTTPSConnectionPool,
    }

    def __init__(
        self,
        proxy_url,
        username=None,
        password=None,
        num_pools=10,
        headers=None,
        **connection_pool_kw
    ):
        parsed = parse_url(proxy_url)

        if username is None and password is None and parsed.auth is not None:
            split = parsed.auth.split(":")
            if len(split) == 2:
                username, password = split
        if parsed.scheme == "socks5":
            socks_version = socks.PROXY_TYPE_SOCKS5
            rdns = False
        elif parsed.scheme == "socks5h":
            socks_version = socks.PROXY_TYPE_SOCKS5
            rdns = True
        elif parsed.scheme == "socks4":
            socks_version = socks.PROXY_TYPE_SOCKS4
            rdns = False
        elif parsed.scheme == "socks4a":
            socks_version = socks.PROXY_TYPE_SOCKS4
            rdns = True
        else:
            raise ValueError("Unable to determine SOCKS version from %s" % proxy_url)

        self.proxy_url = proxy_url

        socks_options = {
            "socks_version": socks_version,
            "proxy_host": parsed.host,
            "proxy_port": parsed.port,
            "username": username,
            "password": password,
            "rdns": rdns,
        }
        connection_pool_kw["_socks_options"] = socks_options

        super(SOCKSProxyManager, self).__init__(
            num_pools, headers, **connection_pool_kw
        )

        self.pool_classes_by_scheme = SOCKSProxyManager.pool_classes_by_scheme


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/__init__.py
# ========================================================


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/ntlmpool.py
# ========================================================
"""
NTLM authenticating pool, contributed by erikcederstran

Issue #10, see: http://code.google.com/p/urllib3/issues/detail?id=10
"""
from __future__ import absolute_import

import warnings
from logging import getLogger

from ntlm import ntlm

from .. import HTTPSConnectionPool
from ..packages.six.moves.http_client import HTTPSConnection

warnings.warn(
    "The 'urllib3.contrib.ntlmpool' module is deprecated and will be removed "
    "in urllib3 v2.0 release, urllib3 is not able to support it properly due "
    "to reasons listed in issue: https://github.com/urllib3/urllib3/issues/2282. "
    "If you are a user of this module please comment in the mentioned issue.",
    DeprecationWarning,
)

log = getLogger(__name__)


class NTLMConnectionPool(HTTPSConnectionPool):
    """
    Implements an NTLM authentication version of an urllib3 connection pool
    """

    scheme = "https"

    def __init__(self, user, pw, authurl, *args, **kwargs):
        """
        authurl is a random URL on the server that is protected by NTLM.
        user is the Windows user, probably in the DOMAIN\\username format.
        pw is the password for the user.
        """
        super(NTLMConnectionPool, self).__init__(*args, **kwargs)
        self.authurl = authurl
        self.rawuser = user
        user_parts = user.split("\\", 1)
        self.domain = user_parts[0].upper()
        self.user = user_parts[1]
        self.pw = pw

    def _new_conn(self):
        # Performs the NTLM handshake that secures the connection. The socket
        # must be kept open while requests are performed.
        self.num_connections += 1
        log.debug(
            "Starting NTLM HTTPS connection no. %d: https://%s%s",
            self.num_connections,
            self.host,
            self.authurl,
        )

        headers = {"Connection": "Keep-Alive"}
        req_header = "Authorization"
        resp_header = "www-authenticate"

        conn = HTTPSConnection(host=self.host, port=self.port)

        # Send negotiation message
        headers[req_header] = "NTLM %s" % ntlm.create_NTLM_NEGOTIATE_MESSAGE(
            self.rawuser
        )
        log.debug("Request headers: %s", headers)
        conn.request("GET", self.authurl, None, headers)
        res = conn.getresponse()
        reshdr = dict(res.headers)
        log.debug("Response status: %s %s", res.status, res.reason)
        log.debug("Response headers: %s", reshdr)
        log.debug("Response data: %s [...]", res.read(100))

        # Remove the reference to the socket, so that it can not be closed by
        # the response object (we want to keep the socket open)
        res.fp = None

        # Server should respond with a challenge message
        auth_header_values = reshdr[resp_header].split(", ")
        auth_header_value = None
        for s in auth_header_values:
            if s[:5] == "NTLM ":
                auth_header_value = s[5:]
        if auth_header_value is None:
            raise Exception(
                "Unexpected %s response header: %s" % (resp_header, reshdr[resp_header])
            )

        # Send authentication message
        ServerChallenge, NegotiateFlags = ntlm.parse_NTLM_CHALLENGE_MESSAGE(
            auth_header_value
        )
        auth_msg = ntlm.create_NTLM_AUTHENTICATE_MESSAGE(
            ServerChallenge, self.user, self.domain, self.pw, NegotiateFlags
        )
        headers[req_header] = "NTLM %s" % auth_msg
        log.debug("Request headers: %s", headers)
        conn.request("GET", self.authurl, None, headers)
        res = conn.getresponse()
        log.debug("Response status: %s %s", res.status, res.reason)
        log.debug("Response headers: %s", dict(res.headers))
        log.debug("Response data: %s [...]", res.read()[:100])
        if res.status != 200:
            if res.status == 401:
                raise Exception("Server rejected request: wrong username or password")
            raise Exception("Wrong server response: %s %s" % (res.status, res.reason))

        res.fp = None
        log.debug("Connection established")
        return conn

    def urlopen(
        self,
        method,
        url,
        body=None,
        headers=None,
        retries=3,
        redirect=True,
        assert_same_host=True,
    ):
        if headers is None:
            headers = {}
        headers["Connection"] = "Keep-Alive"
        return super(NTLMConnectionPool, self).urlopen(
            method, url, body, headers, retries, redirect, assert_same_host
        )


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/appengine.py
# ========================================================
"""
This module provides a pool manager that uses Google App Engine's
`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.

Example usage::

    from pip._vendor.urllib3 import PoolManager
    from pip._vendor.urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox

    if is_appengine_sandbox():
        # AppEngineManager uses AppEngine's URLFetch API behind the scenes
        http = AppEngineManager()
    else:
        # PoolManager uses a socket-level API behind the scenes
        http = PoolManager()

    r = http.request('GET', 'https://google.com/')

There are `limitations <https://cloud.google.com/appengine/docs/python/\
urlfetch/#Python_Quotas_and_limits>`_ to the URLFetch service and it may not be
the best choice for your application. There are three options for using
urllib3 on Google App Engine:

1. You can use :class:`AppEngineManager` with URLFetch. URLFetch is
   cost-effective in many circumstances as long as your usage is within the
   limitations.
2. You can use a normal :class:`~urllib3.PoolManager` by enabling sockets.
   Sockets also have `limitations and restrictions
   <https://cloud.google.com/appengine/docs/python/sockets/\
   #limitations-and-restrictions>`_ and have a lower free quota than URLFetch.
   To use sockets, be sure to specify the following in your ``app.yaml``::

        env_variables:
            GAE_USE_SOCKETS_HTTPLIB : 'true'

3. If you are using `App Engine Flexible
<https://cloud.google.com/appengine/docs/flexible/>`_, you can use the standard
:class:`PoolManager` without any configuration or special environment variables.
"""

from __future__ import absolute_import

import io
import logging
import warnings

from ..exceptions import (
    HTTPError,
    HTTPWarning,
    MaxRetryError,
    ProtocolError,
    SSLError,
    TimeoutError,
)
from ..packages.six.moves.urllib.parse import urljoin
from ..request import RequestMethods
from ..response import HTTPResponse
from ..util.retry import Retry
from ..util.timeout import Timeout
from . import _appengine_environ

try:
    from google.appengine.api import urlfetch
except ImportError:
    urlfetch = None


log = logging.getLogger(__name__)


class AppEnginePlatformWarning(HTTPWarning):
    pass


class AppEnginePlatformError(HTTPError):
    pass


class AppEngineManager(RequestMethods):
    """
    Connection manager for Google App Engine sandbox applications.

    This manager uses the URLFetch service directly instead of using the
    emulated httplib, and is subject to URLFetch limitations as described in
    the App Engine documentation `here
    <https://cloud.google.com/appengine/docs/python/urlfetch>`_.

    Notably it will raise an :class:`AppEnginePlatformError` if:
        * URLFetch is not available.
        * If you attempt to use this on App Engine Flexible, as full socket
          support is available.
        * If a request size is more than 10 megabytes.
        * If a response size is more than 32 megabytes.
        * If you use an unsupported request method such as OPTIONS.

    Beyond those cases, it will raise normal urllib3 errors.
    """

    def __init__(
        self,
        headers=None,
        retries=None,
        validate_certificate=True,
        urlfetch_retries=True,
    ):
        if not urlfetch:
            raise AppEnginePlatformError(
                "URLFetch is not available in this environment."
            )

        warnings.warn(
            "urllib3 is using URLFetch on Google App Engine sandbox instead "
            "of sockets. To use sockets directly instead of URLFetch see "
            "https://urllib3.readthedocs.io/en/1.26.x/reference/urllib3.contrib.html.",
            AppEnginePlatformWarning,
        )

        RequestMethods.__init__(self, headers)
        self.validate_certificate = validate_certificate
        self.urlfetch_retries = urlfetch_retries

        self.retries = retries or Retry.DEFAULT

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Return False to re-raise any potential exceptions
        return False

    def urlopen(
        self,
        method,
        url,
        body=None,
        headers=None,
        retries=None,
        redirect=True,
        timeout=Timeout.DEFAULT_TIMEOUT,
        **response_kw
    ):

        retries = self._get_retries(retries, redirect)

        try:
            follow_redirects = redirect and retries.redirect != 0 and retries.total
            response = urlfetch.fetch(
                url,
                payload=body,
                method=method,
                headers=headers or {},
                allow_truncated=False,
                follow_redirects=self.urlfetch_retries and follow_redirects,
                deadline=self._get_absolute_timeout(timeout),
                validate_certificate=self.validate_certificate,
            )
        except urlfetch.DeadlineExceededError as e:
            raise TimeoutError(self, e)

        except urlfetch.InvalidURLError as e:
            if "too large" in str(e):
                raise AppEnginePlatformError(
                    "URLFetch request too large, URLFetch only "
                    "supports requests up to 10mb in size.",
                    e,
                )
            raise ProtocolError(e)

        except urlfetch.DownloadError as e:
            if "Too many redirects" in str(e):
                raise MaxRetryError(self, url, reason=e)
            raise ProtocolError(e)

        except urlfetch.ResponseTooLargeError as e:
            raise AppEnginePlatformError(
                "URLFetch response too large, URLFetch only supports"
                "responses up to 32mb in size.",
                e,
            )

        except urlfetch.SSLCertificateError as e:
            raise SSLError(e)

        except urlfetch.InvalidMethodError as e:
            raise AppEnginePlatformError(
                "URLFetch does not support method: %s" % method, e
            )

        http_response = self._urlfetch_response_to_http_response(
            response, retries=retries, **response_kw
        )

        # Handle redirect?
        redirect_location = redirect and http_response.get_redirect_location()
        if redirect_location:
            # Check for redirect response
            if self.urlfetch_retries and retries.raise_on_redirect:
                raise MaxRetryError(self, url, "too many redirects")
            else:
                if http_response.status == 303:
                    method = "GET"

                try:
                    retries = retries.increment(
                        method, url, response=http_response, _pool=self
                    )
                except MaxRetryError:
                    if retries.raise_on_redirect:
                        raise MaxRetryError(self, url, "too many redirects")
                    return http_response

                retries.sleep_for_retry(http_response)
                log.debug("Redirecting %s -> %s", url, redirect_location)
                redirect_url = urljoin(url, redirect_location)
                return self.urlopen(
                    method,
                    redirect_url,
                    body,
                    headers,
                    retries=retries,
                    redirect=redirect,
                    timeout=timeout,
                    **response_kw
                )

        # Check if we should retry the HTTP response.
        has_retry_after = bool(http_response.headers.get("Retry-After"))
        if retries.is_retry(method, http_response.status, has_retry_after):
            retries = retries.increment(method, url, response=http_response, _pool=self)
            log.debug("Retry: %s", url)
            retries.sleep(http_response)
            return self.urlopen(
                method,
                url,
                body=body,
                headers=headers,
                retries=retries,
                redirect=redirect,
                timeout=timeout,
                **response_kw
            )

        return http_response

    def _urlfetch_response_to_http_response(self, urlfetch_resp, **response_kw):

        if is_prod_appengine():
            # Production GAE handles deflate encoding automatically, but does
            # not remove the encoding header.
            content_encoding = urlfetch_resp.headers.get("content-encoding")

            if content_encoding == "deflate":
                del urlfetch_resp.headers["content-encoding"]

        transfer_encoding = urlfetch_resp.headers.get("transfer-encoding")
        # We have a full response's content,
        # so let's make sure we don't report ourselves as chunked data.
        if transfer_encoding == "chunked":
            encodings = transfer_encoding.split(",")
            encodings.remove("chunked")
            urlfetch_resp.headers["transfer-encoding"] = ",".join(encodings)

        original_response = HTTPResponse(
            # In order for decoding to work, we must present the content as
            # a file-like object.
            body=io.BytesIO(urlfetch_resp.content),
            msg=urlfetch_resp.header_msg,
            headers=urlfetch_resp.headers,
            status=urlfetch_resp.status_code,
            **response_kw
        )

        return HTTPResponse(
            body=io.BytesIO(urlfetch_resp.content),
            headers=urlfetch_resp.headers,
            status=urlfetch_resp.status_code,
            original_response=original_response,
            **response_kw
        )

    def _get_absolute_timeout(self, timeout):
        if timeout is Timeout.DEFAULT_TIMEOUT:
            return None  # Defer to URLFetch's default.
        if isinstance(timeout, Timeout):
            if timeout._read is not None or timeout._connect is not None:
                warnings.warn(
                    "URLFetch does not support granular timeout settings, "
                    "reverting to total or default URLFetch timeout.",
                    AppEnginePlatformWarning,
                )
            return timeout.total
        return timeout

    def _get_retries(self, retries, redirect):
        if not isinstance(retries, Retry):
            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)

        if retries.connect or retries.read or retries.redirect:
            warnings.warn(
                "URLFetch only supports total retries and does not "
                "recognize connect, read, or redirect retry parameters.",
                AppEnginePlatformWarning,
            )

        return retries


# Alias methods from _appengine_environ to maintain public API interface.

is_appengine = _appengine_environ.is_appengine
is_appengine_sandbox = _appengine_environ.is_appengine_sandbox
is_local_appengine = _appengine_environ.is_local_appengine
is_prod_appengine = _appengine_environ.is_prod_appengine
is_prod_appengine_mvms = _appengine_environ.is_prod_appengine_mvms


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/securetransport.py
# ========================================================
"""
SecureTranport support for urllib3 via ctypes.

This makes platform-native TLS available to urllib3 users on macOS without the
use of a compiler. This is an important feature because the Python Package
Index is moving to become a TLSv1.2-or-higher server, and the default OpenSSL
that ships with macOS is not capable of doing TLSv1.2. The only way to resolve
this is to give macOS users an alternative solution to the problem, and that
solution is to use SecureTransport.

We use ctypes here because this solution must not require a compiler. That's
because pip is not allowed to require a compiler either.

This is not intended to be a seriously long-term solution to this problem.
The hope is that PEP 543 will eventually solve this issue for us, at which
point we can retire this contrib module. But in the short term, we need to
solve the impending tire fire that is Python on Mac without this kind of
contrib module. So...here we are.

To use this module, simply import and inject it::

    import pip._vendor.urllib3.contrib.securetransport as securetransport
    securetransport.inject_into_urllib3()

Happy TLSing!

This code is a bastardised version of the code found in Will Bond's oscrypto
library. An enormous debt is owed to him for blazing this trail for us. For
that reason, this code should be considered to be covered both by urllib3's
license and by oscrypto's:

.. code-block::

    Copyright (c) 2015-2016 Will Bond <will@wbond.net>

    Permission is hereby granted, free of charge, to any person obtaining a
    copy of this software and associated documentation files (the "Software"),
    to deal in the Software without restriction, including without limitation
    the rights to use, copy, modify, merge, publish, distribute, sublicense,
    and/or sell copies of the Software, and to permit persons to whom the
    Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in
    all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.
"""
from __future__ import absolute_import

import contextlib
import ctypes
import errno
import os.path
import shutil
import socket
import ssl
import struct
import threading
import weakref

from pip._vendor import six

from .. import util
from ..util.ssl_ import PROTOCOL_TLS_CLIENT
from ._securetransport.bindings import CoreFoundation, Security, SecurityConst
from ._securetransport.low_level import (
    _assert_no_error,
    _build_tls_unknown_ca_alert,
    _cert_array_from_pem,
    _create_cfstring_array,
    _load_client_cert_chain,
    _temporary_keychain,
)

try:  # Platform-specific: Python 2
    from socket import _fileobject
except ImportError:  # Platform-specific: Python 3
    _fileobject = None
    from ..packages.backports.makefile import backport_makefile

__all__ = ["inject_into_urllib3", "extract_from_urllib3"]

# SNI always works
HAS_SNI = True

orig_util_HAS_SNI = util.HAS_SNI
orig_util_SSLContext = util.ssl_.SSLContext

# This dictionary is used by the read callback to obtain a handle to the
# calling wrapped socket. This is a pretty silly approach, but for now it'll
# do. I feel like I should be able to smuggle a handle to the wrapped socket
# directly in the SSLConnectionRef, but for now this approach will work I
# guess.
#
# We need to lock around this structure for inserts, but we don't do it for
# reads/writes in the callbacks. The reasoning here goes as follows:
#
#    1. It is not possible to call into the callbacks before the dictionary is
#       populated, so once in the callback the id must be in the dictionary.
#    2. The callbacks don't mutate the dictionary, they only read from it, and
#       so cannot conflict with any of the insertions.
#
# This is good: if we had to lock in the callbacks we'd drastically slow down
# the performance of this code.
_connection_refs = weakref.WeakValueDictionary()
_connection_ref_lock = threading.Lock()

# Limit writes to 16kB. This is OpenSSL's limit, but we'll cargo-cult it over
# for no better reason than we need *a* limit, and this one is right there.
SSL_WRITE_BLOCKSIZE = 16384

# This is our equivalent of util.ssl_.DEFAULT_CIPHERS, but expanded out to
# individual cipher suites. We need to do this because this is how
# SecureTransport wants them.
CIPHER_SUITES = [
    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,
    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
    SecurityConst.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,
    SecurityConst.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
    SecurityConst.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,
    SecurityConst.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,
    SecurityConst.TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,
    SecurityConst.TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,
    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,
    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,
    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,
    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,
    SecurityConst.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,
    SecurityConst.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,
    SecurityConst.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,
    SecurityConst.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,
    SecurityConst.TLS_DHE_RSA_WITH_AES_256_CBC_SHA256,
    SecurityConst.TLS_DHE_RSA_WITH_AES_256_CBC_SHA,
    SecurityConst.TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,
    SecurityConst.TLS_DHE_RSA_WITH_AES_128_CBC_SHA,
    SecurityConst.TLS_AES_256_GCM_SHA384,
    SecurityConst.TLS_AES_128_GCM_SHA256,
    SecurityConst.TLS_RSA_WITH_AES_256_GCM_SHA384,
    SecurityConst.TLS_RSA_WITH_AES_128_GCM_SHA256,
    SecurityConst.TLS_AES_128_CCM_8_SHA256,
    SecurityConst.TLS_AES_128_CCM_SHA256,
    SecurityConst.TLS_RSA_WITH_AES_256_CBC_SHA256,
    SecurityConst.TLS_RSA_WITH_AES_128_CBC_SHA256,
    SecurityConst.TLS_RSA_WITH_AES_256_CBC_SHA,
    SecurityConst.TLS_RSA_WITH_AES_128_CBC_SHA,
]

# Basically this is simple: for PROTOCOL_SSLv23 we turn it into a low of
# TLSv1 and a high of TLSv1.2. For everything else, we pin to that version.
# TLSv1 to 1.2 are supported on macOS 10.8+
_protocol_to_min_max = {
    util.PROTOCOL_TLS: (SecurityConst.kTLSProtocol1, SecurityConst.kTLSProtocol12),
    PROTOCOL_TLS_CLIENT: (SecurityConst.kTLSProtocol1, SecurityConst.kTLSProtocol12),
}

if hasattr(ssl, "PROTOCOL_SSLv2"):
    _protocol_to_min_max[ssl.PROTOCOL_SSLv2] = (
        SecurityConst.kSSLProtocol2,
        SecurityConst.kSSLProtocol2,
    )
if hasattr(ssl, "PROTOCOL_SSLv3"):
    _protocol_to_min_max[ssl.PROTOCOL_SSLv3] = (
        SecurityConst.kSSLProtocol3,
        SecurityConst.kSSLProtocol3,
    )
if hasattr(ssl, "PROTOCOL_TLSv1"):
    _protocol_to_min_max[ssl.PROTOCOL_TLSv1] = (
        SecurityConst.kTLSProtocol1,
        SecurityConst.kTLSProtocol1,
    )
if hasattr(ssl, "PROTOCOL_TLSv1_1"):
    _protocol_to_min_max[ssl.PROTOCOL_TLSv1_1] = (
        SecurityConst.kTLSProtocol11,
        SecurityConst.kTLSProtocol11,
    )
if hasattr(ssl, "PROTOCOL_TLSv1_2"):
    _protocol_to_min_max[ssl.PROTOCOL_TLSv1_2] = (
        SecurityConst.kTLSProtocol12,
        SecurityConst.kTLSProtocol12,
    )


def inject_into_urllib3():
    """
    Monkey-patch urllib3 with SecureTransport-backed SSL-support.
    """
    util.SSLContext = SecureTransportContext
    util.ssl_.SSLContext = SecureTransportContext
    util.HAS_SNI = HAS_SNI
    util.ssl_.HAS_SNI = HAS_SNI
    util.IS_SECURETRANSPORT = True
    util.ssl_.IS_SECURETRANSPORT = True


def extract_from_urllib3():
    """
    Undo monkey-patching by :func:`inject_into_urllib3`.
    """
    util.SSLContext = orig_util_SSLContext
    util.ssl_.SSLContext = orig_util_SSLContext
    util.HAS_SNI = orig_util_HAS_SNI
    util.ssl_.HAS_SNI = orig_util_HAS_SNI
    util.IS_SECURETRANSPORT = False
    util.ssl_.IS_SECURETRANSPORT = False


def _read_callback(connection_id, data_buffer, data_length_pointer):
    """
    SecureTransport read callback. This is called by ST to request that data
    be returned from the socket.
    """
    wrapped_socket = None
    try:
        wrapped_socket = _connection_refs.get(connection_id)
        if wrapped_socket is None:
            return SecurityConst.errSSLInternal
        base_socket = wrapped_socket.socket

        requested_length = data_length_pointer[0]

        timeout = wrapped_socket.gettimeout()
        error = None
        read_count = 0

        try:
            while read_count < requested_length:
                if timeout is None or timeout >= 0:
                    if not util.wait_for_read(base_socket, timeout):
                        raise socket.error(errno.EAGAIN, "timed out")

                remaining = requested_length - read_count
                buffer = (ctypes.c_char * remaining).from_address(
                    data_buffer + read_count
                )
                chunk_size = base_socket.recv_into(buffer, remaining)
                read_count += chunk_size
                if not chunk_size:
                    if not read_count:
                        return SecurityConst.errSSLClosedGraceful
                    break
        except (socket.error) as e:
            error = e.errno

            if error is not None and error != errno.EAGAIN:
                data_length_pointer[0] = read_count
                if error == errno.ECONNRESET or error == errno.EPIPE:
                    return SecurityConst.errSSLClosedAbort
                raise

        data_length_pointer[0] = read_count

        if read_count != requested_length:
            return SecurityConst.errSSLWouldBlock

        return 0
    except Exception as e:
        if wrapped_socket is not None:
            wrapped_socket._exception = e
        return SecurityConst.errSSLInternal


def _write_callback(connection_id, data_buffer, data_length_pointer):
    """
    SecureTransport write callback. This is called by ST to request that data
    actually be sent on the network.
    """
    wrapped_socket = None
    try:
        wrapped_socket = _connection_refs.get(connection_id)
        if wrapped_socket is None:
            return SecurityConst.errSSLInternal
        base_socket = wrapped_socket.socket

        bytes_to_write = data_length_pointer[0]
        data = ctypes.string_at(data_buffer, bytes_to_write)

        timeout = wrapped_socket.gettimeout()
        error = None
        sent = 0

        try:
            while sent < bytes_to_write:
                if timeout is None or timeout >= 0:
                    if not util.wait_for_write(base_socket, timeout):
                        raise socket.error(errno.EAGAIN, "timed out")
                chunk_sent = base_socket.send(data)
                sent += chunk_sent

                # This has some needless copying here, but I'm not sure there's
                # much value in optimising this data path.
                data = data[chunk_sent:]
        except (socket.error) as e:
            error = e.errno

            if error is not None and error != errno.EAGAIN:
                data_length_pointer[0] = sent
                if error == errno.ECONNRESET or error == errno.EPIPE:
                    return SecurityConst.errSSLClosedAbort
                raise

        data_length_pointer[0] = sent

        if sent != bytes_to_write:
            return SecurityConst.errSSLWouldBlock

        return 0
    except Exception as e:
        if wrapped_socket is not None:
            wrapped_socket._exception = e
        return SecurityConst.errSSLInternal


# We need to keep these two objects references alive: if they get GC'd while
# in use then SecureTransport could attempt to call a function that is in freed
# memory. That would be...uh...bad. Yeah, that's the word. Bad.
_read_callback_pointer = Security.SSLReadFunc(_read_callback)
_write_callback_pointer = Security.SSLWriteFunc(_write_callback)


class WrappedSocket(object):
    """
    API-compatibility wrapper for Python's OpenSSL wrapped socket object.

    Note: _makefile_refs, _drop(), and _reuse() are needed for the garbage
    collector of PyPy.
    """

    def __init__(self, socket):
        self.socket = socket
        self.context = None
        self._makefile_refs = 0
        self._closed = False
        self._exception = None
        self._keychain = None
        self._keychain_dir = None
        self._client_cert_chain = None

        # We save off the previously-configured timeout and then set it to
        # zero. This is done because we use select and friends to handle the
        # timeouts, but if we leave the timeout set on the lower socket then
        # Python will "kindly" call select on that socket again for us. Avoid
        # that by forcing the timeout to zero.
        self._timeout = self.socket.gettimeout()
        self.socket.settimeout(0)

    @contextlib.contextmanager
    def _raise_on_error(self):
        """
        A context manager that can be used to wrap calls that do I/O from
        SecureTransport. If any of the I/O callbacks hit an exception, this
        context manager will correctly propagate the exception after the fact.
        This avoids silently swallowing those exceptions.

        It also correctly forces the socket closed.
        """
        self._exception = None

        # We explicitly don't catch around this yield because in the unlikely
        # event that an exception was hit in the block we don't want to swallow
        # it.
        yield
        if self._exception is not None:
            exception, self._exception = self._exception, None
            self.close()
            raise exception

    def _set_ciphers(self):
        """
        Sets up the allowed ciphers. By default this matches the set in
        util.ssl_.DEFAULT_CIPHERS, at least as supported by macOS. This is done
        custom and doesn't allow changing at this time, mostly because parsing
        OpenSSL cipher strings is going to be a freaking nightmare.
        """
        ciphers = (Security.SSLCipherSuite * len(CIPHER_SUITES))(*CIPHER_SUITES)
        result = Security.SSLSetEnabledCiphers(
            self.context, ciphers, len(CIPHER_SUITES)
        )
        _assert_no_error(result)

    def _set_alpn_protocols(self, protocols):
        """
        Sets up the ALPN protocols on the context.
        """
        if not protocols:
            return
        protocols_arr = _create_cfstring_array(protocols)
        try:
            result = Security.SSLSetALPNProtocols(self.context, protocols_arr)
            _assert_no_error(result)
        finally:
            CoreFoundation.CFRelease(protocols_arr)

    def _custom_validate(self, verify, trust_bundle):
        """
        Called when we have set custom validation. We do this in two cases:
        first, when cert validation is entirely disabled; and second, when
        using a custom trust DB.
        Raises an SSLError if the connection is not trusted.
        """
        # If we disabled cert validation, just say: cool.
        if not verify:
            return

        successes = (
            SecurityConst.kSecTrustResultUnspecified,
            SecurityConst.kSecTrustResultProceed,
        )
        try:
            trust_result = self._evaluate_trust(trust_bundle)
            if trust_result in successes:
                return
            reason = "error code: %d" % (trust_result,)
        except Exception as e:
            # Do not trust on error
            reason = "exception: %r" % (e,)

        # SecureTransport does not send an alert nor shuts down the connection.
        rec = _build_tls_unknown_ca_alert(self.version())
        self.socket.sendall(rec)
        # close the connection immediately
        # l_onoff = 1, activate linger
        # l_linger = 0, linger for 0 seoncds
        opts = struct.pack("ii", 1, 0)
        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_LINGER, opts)
        self.close()
        raise ssl.SSLError("certificate verify failed, %s" % reason)

    def _evaluate_trust(self, trust_bundle):
        # We want data in memory, so load it up.
        if os.path.isfile(trust_bundle):
            with open(trust_bundle, "rb") as f:
                trust_bundle = f.read()

        cert_array = None
        trust = Security.SecTrustRef()

        try:
            # Get a CFArray that contains the certs we want.
            cert_array = _cert_array_from_pem(trust_bundle)

            # Ok, now the hard part. We want to get the SecTrustRef that ST has
            # created for this connection, shove our CAs into it, tell ST to
            # ignore everything else it knows, and then ask if it can build a
            # chain. This is a buuuunch of code.
            result = Security.SSLCopyPeerTrust(self.context, ctypes.byref(trust))
            _assert_no_error(result)
            if not trust:
                raise ssl.SSLError("Failed to copy trust reference")

            result = Security.SecTrustSetAnchorCertificates(trust, cert_array)
            _assert_no_error(result)

            result = Security.SecTrustSetAnchorCertificatesOnly(trust, True)
            _assert_no_error(result)

            trust_result = Security.SecTrustResultType()
            result = Security.SecTrustEvaluate(trust, ctypes.byref(trust_result))
            _assert_no_error(result)
        finally:
            if trust:
                CoreFoundation.CFRelease(trust)

            if cert_array is not None:
                CoreFoundation.CFRelease(cert_array)

        return trust_result.value

    def handshake(
        self,
        server_hostname,
        verify,
        trust_bundle,
        min_version,
        max_version,
        client_cert,
        client_key,
        client_key_passphrase,
        alpn_protocols,
    ):
        """
        Actually performs the TLS handshake. This is run automatically by
        wrapped socket, and shouldn't be needed in user code.
        """
        # First, we do the initial bits of connection setup. We need to create
        # a context, set its I/O funcs, and set the connection reference.
        self.context = Security.SSLCreateContext(
            None, SecurityConst.kSSLClientSide, SecurityConst.kSSLStreamType
        )
        result = Security.SSLSetIOFuncs(
            self.context, _read_callback_pointer, _write_callback_pointer
        )
        _assert_no_error(result)

        # Here we need to compute the handle to use. We do this by taking the
        # id of self modulo 2**31 - 1. If this is already in the dictionary, we
        # just keep incrementing by one until we find a free space.
        with _connection_ref_lock:
            handle = id(self) % 2147483647
            while handle in _connection_refs:
                handle = (handle + 1) % 2147483647
            _connection_refs[handle] = self

        result = Security.SSLSetConnection(self.context, handle)
        _assert_no_error(result)

        # If we have a server hostname, we should set that too.
        if server_hostname:
            if not isinstance(server_hostname, bytes):
                server_hostname = server_hostname.encode("utf-8")

            result = Security.SSLSetPeerDomainName(
                self.context, server_hostname, len(server_hostname)
            )
            _assert_no_error(result)

        # Setup the ciphers.
        self._set_ciphers()

        # Setup the ALPN protocols.
        self._set_alpn_protocols(alpn_protocols)

        # Set the minimum and maximum TLS versions.
        result = Security.SSLSetProtocolVersionMin(self.context, min_version)
        _assert_no_error(result)

        result = Security.SSLSetProtocolVersionMax(self.context, max_version)
        _assert_no_error(result)

        # If there's a trust DB, we need to use it. We do that by telling
        # SecureTransport to break on server auth. We also do that if we don't
        # want to validate the certs at all: we just won't actually do any
        # authing in that case.
        if not verify or trust_bundle is not None:
            result = Security.SSLSetSessionOption(
                self.context, SecurityConst.kSSLSessionOptionBreakOnServerAuth, True
            )
            _assert_no_error(result)

        # If there's a client cert, we need to use it.
        if client_cert:
            self._keychain, self._keychain_dir = _temporary_keychain()
            self._client_cert_chain = _load_client_cert_chain(
                self._keychain, client_cert, client_key
            )
            result = Security.SSLSetCertificate(self.context, self._client_cert_chain)
            _assert_no_error(result)

        while True:
            with self._raise_on_error():
                result = Security.SSLHandshake(self.context)

                if result == SecurityConst.errSSLWouldBlock:
                    raise socket.timeout("handshake timed out")
                elif result == SecurityConst.errSSLServerAuthCompleted:
                    self._custom_validate(verify, trust_bundle)
                    continue
                else:
                    _assert_no_error(result)
                    break

    def fileno(self):
        return self.socket.fileno()

    # Copy-pasted from Python 3.5 source code
    def _decref_socketios(self):
        if self._makefile_refs > 0:
            self._makefile_refs -= 1
        if self._closed:
            self.close()

    def recv(self, bufsiz):
        buffer = ctypes.create_string_buffer(bufsiz)
        bytes_read = self.recv_into(buffer, bufsiz)
        data = buffer[:bytes_read]
        return data

    def recv_into(self, buffer, nbytes=None):
        # Read short on EOF.
        if self._closed:
            return 0

        if nbytes is None:
            nbytes = len(buffer)

        buffer = (ctypes.c_char * nbytes).from_buffer(buffer)
        processed_bytes = ctypes.c_size_t(0)

        with self._raise_on_error():
            result = Security.SSLRead(
                self.context, buffer, nbytes, ctypes.byref(processed_bytes)
            )

        # There are some result codes that we want to treat as "not always
        # errors". Specifically, those are errSSLWouldBlock,
        # errSSLClosedGraceful, and errSSLClosedNoNotify.
        if result == SecurityConst.errSSLWouldBlock:
            # If we didn't process any bytes, then this was just a time out.
            # However, we can get errSSLWouldBlock in situations when we *did*
            # read some data, and in those cases we should just read "short"
            # and return.
            if processed_bytes.value == 0:
                # Timed out, no data read.
                raise socket.timeout("recv timed out")
        elif result in (
            SecurityConst.errSSLClosedGraceful,
            SecurityConst.errSSLClosedNoNotify,
        ):
            # The remote peer has closed this connection. We should do so as
            # well. Note that we don't actually return here because in
            # principle this could actually be fired along with return data.
            # It's unlikely though.
            self.close()
        else:
            _assert_no_error(result)

        # Ok, we read and probably succeeded. We should return whatever data
        # was actually read.
        return processed_bytes.value

    def settimeout(self, timeout):
        self._timeout = timeout

    def gettimeout(self):
        return self._timeout

    def send(self, data):
        processed_bytes = ctypes.c_size_t(0)

        with self._raise_on_error():
            result = Security.SSLWrite(
                self.context, data, len(data), ctypes.byref(processed_bytes)
            )

        if result == SecurityConst.errSSLWouldBlock and processed_bytes.value == 0:
            # Timed out
            raise socket.timeout("send timed out")
        else:
            _assert_no_error(result)

        # We sent, and probably succeeded. Tell them how much we sent.
        return processed_bytes.value

    def sendall(self, data):
        total_sent = 0
        while total_sent < len(data):
            sent = self.send(data[total_sent : total_sent + SSL_WRITE_BLOCKSIZE])
            total_sent += sent

    def shutdown(self):
        with self._raise_on_error():
            Security.SSLClose(self.context)

    def close(self):
        # TODO: should I do clean shutdown here? Do I have to?
        if self._makefile_refs < 1:
            self._closed = True
            if self.context:
                CoreFoundation.CFRelease(self.context)
                self.context = None
            if self._client_cert_chain:
                CoreFoundation.CFRelease(self._client_cert_chain)
                self._client_cert_chain = None
            if self._keychain:
                Security.SecKeychainDelete(self._keychain)
                CoreFoundation.CFRelease(self._keychain)
                shutil.rmtree(self._keychain_dir)
                self._keychain = self._keychain_dir = None
            return self.socket.close()
        else:
            self._makefile_refs -= 1

    def getpeercert(self, binary_form=False):
        # Urgh, annoying.
        #
        # Here's how we do this:
        #
        # 1. Call SSLCopyPeerTrust to get hold of the trust object for this
        #    connection.
        # 2. Call SecTrustGetCertificateAtIndex for index 0 to get the leaf.
        # 3. To get the CN, call SecCertificateCopyCommonName and process that
        #    string so that it's of the appropriate type.
        # 4. To get the SAN, we need to do something a bit more complex:
        #    a. Call SecCertificateCopyValues to get the data, requesting
        #       kSecOIDSubjectAltName.
        #    b. Mess about with this dictionary to try to get the SANs out.
        #
        # This is gross. Really gross. It's going to be a few hundred LoC extra
        # just to repeat something that SecureTransport can *already do*. So my
        # operating assumption at this time is that what we want to do is
        # instead to just flag to urllib3 that it shouldn't do its own hostname
        # validation when using SecureTransport.
        if not binary_form:
            raise ValueError("SecureTransport only supports dumping binary certs")
        trust = Security.SecTrustRef()
        certdata = None
        der_bytes = None

        try:
            # Grab the trust store.
            result = Security.SSLCopyPeerTrust(self.context, ctypes.byref(trust))
            _assert_no_error(result)
            if not trust:
                # Probably we haven't done the handshake yet. No biggie.
                return None

            cert_count = Security.SecTrustGetCertificateCount(trust)
            if not cert_count:
                # Also a case that might happen if we haven't handshaked.
                # Handshook? Handshaken?
                return None

            leaf = Security.SecTrustGetCertificateAtIndex(trust, 0)
            assert leaf

            # Ok, now we want the DER bytes.
            certdata = Security.SecCertificateCopyData(leaf)
            assert certdata

            data_length = CoreFoundation.CFDataGetLength(certdata)
            data_buffer = CoreFoundation.CFDataGetBytePtr(certdata)
            der_bytes = ctypes.string_at(data_buffer, data_length)
        finally:
            if certdata:
                CoreFoundation.CFRelease(certdata)
            if trust:
                CoreFoundation.CFRelease(trust)

        return der_bytes

    def version(self):
        protocol = Security.SSLProtocol()
        result = Security.SSLGetNegotiatedProtocolVersion(
            self.context, ctypes.byref(protocol)
        )
        _assert_no_error(result)
        if protocol.value == SecurityConst.kTLSProtocol13:
            raise ssl.SSLError("SecureTransport does not support TLS 1.3")
        elif protocol.value == SecurityConst.kTLSProtocol12:
            return "TLSv1.2"
        elif protocol.value == SecurityConst.kTLSProtocol11:
            return "TLSv1.1"
        elif protocol.value == SecurityConst.kTLSProtocol1:
            return "TLSv1"
        elif protocol.value == SecurityConst.kSSLProtocol3:
            return "SSLv3"
        elif protocol.value == SecurityConst.kSSLProtocol2:
            return "SSLv2"
        else:
            raise ssl.SSLError("Unknown TLS version: %r" % protocol)

    def _reuse(self):
        self._makefile_refs += 1

    def _drop(self):
        if self._makefile_refs < 1:
            self.close()
        else:
            self._makefile_refs -= 1


if _fileobject:  # Platform-specific: Python 2

    def makefile(self, mode, bufsize=-1):
        self._makefile_refs += 1
        return _fileobject(self, mode, bufsize, close=True)

else:  # Platform-specific: Python 3

    def makefile(self, mode="r", buffering=None, *args, **kwargs):
        # We disable buffering with SecureTransport because it conflicts with
        # the buffering that ST does internally (see issue #1153 for more).
        buffering = 0
        return backport_makefile(self, mode, buffering, *args, **kwargs)


WrappedSocket.makefile = makefile


class SecureTransportContext(object):
    """
    I am a wrapper class for the SecureTransport library, to translate the
    interface of the standard library ``SSLContext`` object to calls into
    SecureTransport.
    """

    def __init__(self, protocol):
        self._min_version, self._max_version = _protocol_to_min_max[protocol]
        self._options = 0
        self._verify = False
        self._trust_bundle = None
        self._client_cert = None
        self._client_key = None
        self._client_key_passphrase = None
        self._alpn_protocols = None

    @property
    def check_hostname(self):
        """
        SecureTransport cannot have its hostname checking disabled. For more,
        see the comment on getpeercert() in this file.
        """
        return True

    @check_hostname.setter
    def check_hostname(self, value):
        """
        SecureTransport cannot have its hostname checking disabled. For more,
        see the comment on getpeercert() in this file.
        """
        pass

    @property
    def options(self):
        # TODO: Well, crap.
        #
        # So this is the bit of the code that is the most likely to cause us
        # trouble. Essentially we need to enumerate all of the SSL options that
        # users might want to use and try to see if we can sensibly translate
        # them, or whether we should just ignore them.
        return self._options

    @options.setter
    def options(self, value):
        # TODO: Update in line with above.
        self._options = value

    @property
    def verify_mode(self):
        return ssl.CERT_REQUIRED if self._verify else ssl.CERT_NONE

    @verify_mode.setter
    def verify_mode(self, value):
        self._verify = True if value == ssl.CERT_REQUIRED else False

    def set_default_verify_paths(self):
        # So, this has to do something a bit weird. Specifically, what it does
        # is nothing.
        #
        # This means that, if we had previously had load_verify_locations
        # called, this does not undo that. We need to do that because it turns
        # out that the rest of the urllib3 code will attempt to load the
        # default verify paths if it hasn't been told about any paths, even if
        # the context itself was sometime earlier. We resolve that by just
        # ignoring it.
        pass

    def load_default_certs(self):
        return self.set_default_verify_paths()

    def set_ciphers(self, ciphers):
        # For now, we just require the default cipher string.
        if ciphers != util.ssl_.DEFAULT_CIPHERS:
            raise ValueError("SecureTransport doesn't support custom cipher strings")

    def load_verify_locations(self, cafile=None, capath=None, cadata=None):
        # OK, we only really support cadata and cafile.
        if capath is not None:
            raise ValueError("SecureTransport does not support cert directories")

        # Raise if cafile does not exist.
        if cafile is not None:
            with open(cafile):
                pass

        self._trust_bundle = cafile or cadata

    def load_cert_chain(self, certfile, keyfile=None, password=None):
        self._client_cert = certfile
        self._client_key = keyfile
        self._client_cert_passphrase = password

    def set_alpn_protocols(self, protocols):
        """
        Sets the ALPN protocols that will later be set on the context.

        Raises a NotImplementedError if ALPN is not supported.
        """
        if not hasattr(Security, "SSLSetALPNProtocols"):
            raise NotImplementedError(
                "SecureTransport supports ALPN only in macOS 10.12+"
            )
        self._alpn_protocols = [six.ensure_binary(p) for p in protocols]

    def wrap_socket(
        self,
        sock,
        server_side=False,
        do_handshake_on_connect=True,
        suppress_ragged_eofs=True,
        server_hostname=None,
    ):
        # So, what do we do here? Firstly, we assert some properties. This is a
        # stripped down shim, so there is some functionality we don't support.
        # See PEP 543 for the real deal.
        assert not server_side
        assert do_handshake_on_connect
        assert suppress_ragged_eofs

        # Ok, we're good to go. Now we want to create the wrapped socket object
        # and store it in the appropriate place.
        wrapped_socket = WrappedSocket(sock)

        # Now we can handshake
        wrapped_socket.handshake(
            server_hostname,
            self._verify,
            self._trust_bundle,
            self._min_version,
            self._max_version,
            self._client_cert,
            self._client_key,
            self._client_key_passphrase,
            self._alpn_protocols,
        )
        return wrapped_socket


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/low_level.py
# ========================================================
"""
Low-level helpers for the SecureTransport bindings.

These are Python functions that are not directly related to the high-level APIs
but are necessary to get them to work. They include a whole bunch of low-level
CoreFoundation messing about and memory management. The concerns in this module
are almost entirely about trying to avoid memory leaks and providing
appropriate and useful assistance to the higher-level code.
"""
import base64
import ctypes
import itertools
import os
import re
import ssl
import struct
import tempfile

from .bindings import CFConst, CoreFoundation, Security

# This regular expression is used to grab PEM data out of a PEM bundle.
_PEM_CERTS_RE = re.compile(
    b"-----BEGIN CERTIFICATE-----\n(.*?)\n-----END CERTIFICATE-----", re.DOTALL
)


def _cf_data_from_bytes(bytestring):
    """
    Given a bytestring, create a CFData object from it. This CFData object must
    be CFReleased by the caller.
    """
    return CoreFoundation.CFDataCreate(
        CoreFoundation.kCFAllocatorDefault, bytestring, len(bytestring)
    )


def _cf_dictionary_from_tuples(tuples):
    """
    Given a list of Python tuples, create an associated CFDictionary.
    """
    dictionary_size = len(tuples)

    # We need to get the dictionary keys and values out in the same order.
    keys = (t[0] for t in tuples)
    values = (t[1] for t in tuples)
    cf_keys = (CoreFoundation.CFTypeRef * dictionary_size)(*keys)
    cf_values = (CoreFoundation.CFTypeRef * dictionary_size)(*values)

    return CoreFoundation.CFDictionaryCreate(
        CoreFoundation.kCFAllocatorDefault,
        cf_keys,
        cf_values,
        dictionary_size,
        CoreFoundation.kCFTypeDictionaryKeyCallBacks,
        CoreFoundation.kCFTypeDictionaryValueCallBacks,
    )


def _cfstr(py_bstr):
    """
    Given a Python binary data, create a CFString.
    The string must be CFReleased by the caller.
    """
    c_str = ctypes.c_char_p(py_bstr)
    cf_str = CoreFoundation.CFStringCreateWithCString(
        CoreFoundation.kCFAllocatorDefault,
        c_str,
        CFConst.kCFStringEncodingUTF8,
    )
    return cf_str


def _create_cfstring_array(lst):
    """
    Given a list of Python binary data, create an associated CFMutableArray.
    The array must be CFReleased by the caller.

    Raises an ssl.SSLError on failure.
    """
    cf_arr = None
    try:
        cf_arr = CoreFoundation.CFArrayCreateMutable(
            CoreFoundation.kCFAllocatorDefault,
            0,
            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
        )
        if not cf_arr:
            raise MemoryError("Unable to allocate memory!")
        for item in lst:
            cf_str = _cfstr(item)
            if not cf_str:
                raise MemoryError("Unable to allocate memory!")
            try:
                CoreFoundation.CFArrayAppendValue(cf_arr, cf_str)
            finally:
                CoreFoundation.CFRelease(cf_str)
    except BaseException as e:
        if cf_arr:
            CoreFoundation.CFRelease(cf_arr)
        raise ssl.SSLError("Unable to allocate array: %s" % (e,))
    return cf_arr


def _cf_string_to_unicode(value):
    """
    Creates a Unicode string from a CFString object. Used entirely for error
    reporting.

    Yes, it annoys me quite a lot that this function is this complex.
    """
    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))

    string = CoreFoundation.CFStringGetCStringPtr(
        value_as_void_p, CFConst.kCFStringEncodingUTF8
    )
    if string is None:
        buffer = ctypes.create_string_buffer(1024)
        result = CoreFoundation.CFStringGetCString(
            value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8
        )
        if not result:
            raise OSError("Error copying C string from CFStringRef")
        string = buffer.value
    if string is not None:
        string = string.decode("utf-8")
    return string


def _assert_no_error(error, exception_class=None):
    """
    Checks the return code and throws an exception if there is an error to
    report
    """
    if error == 0:
        return

    cf_error_string = Security.SecCopyErrorMessageString(error, None)
    output = _cf_string_to_unicode(cf_error_string)
    CoreFoundation.CFRelease(cf_error_string)

    if output is None or output == u"":
        output = u"OSStatus %s" % error

    if exception_class is None:
        exception_class = ssl.SSLError

    raise exception_class(output)


def _cert_array_from_pem(pem_bundle):
    """
    Given a bundle of certs in PEM format, turns them into a CFArray of certs
    that can be used to validate a cert chain.
    """
    # Normalize the PEM bundle's line endings.
    pem_bundle = pem_bundle.replace(b"\r\n", b"\n")

    der_certs = [
        base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)
    ]
    if not der_certs:
        raise ssl.SSLError("No root certificates specified")

    cert_array = CoreFoundation.CFArrayCreateMutable(
        CoreFoundation.kCFAllocatorDefault,
        0,
        ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
    )
    if not cert_array:
        raise ssl.SSLError("Unable to allocate memory!")

    try:
        for der_bytes in der_certs:
            certdata = _cf_data_from_bytes(der_bytes)
            if not certdata:
                raise ssl.SSLError("Unable to allocate memory!")
            cert = Security.SecCertificateCreateWithData(
                CoreFoundation.kCFAllocatorDefault, certdata
            )
            CoreFoundation.CFRelease(certdata)
            if not cert:
                raise ssl.SSLError("Unable to build cert object!")

            CoreFoundation.CFArrayAppendValue(cert_array, cert)
            CoreFoundation.CFRelease(cert)
    except Exception:
        # We need to free the array before the exception bubbles further.
        # We only want to do that if an error occurs: otherwise, the caller
        # should free.
        CoreFoundation.CFRelease(cert_array)
        raise

    return cert_array


def _is_cert(item):
    """
    Returns True if a given CFTypeRef is a certificate.
    """
    expected = Security.SecCertificateGetTypeID()
    return CoreFoundation.CFGetTypeID(item) == expected


def _is_identity(item):
    """
    Returns True if a given CFTypeRef is an identity.
    """
    expected = Security.SecIdentityGetTypeID()
    return CoreFoundation.CFGetTypeID(item) == expected


def _temporary_keychain():
    """
    This function creates a temporary Mac keychain that we can use to work with
    credentials. This keychain uses a one-time password and a temporary file to
    store the data. We expect to have one keychain per socket. The returned
    SecKeychainRef must be freed by the caller, including calling
    SecKeychainDelete.

    Returns a tuple of the SecKeychainRef and the path to the temporary
    directory that contains it.
    """
    # Unfortunately, SecKeychainCreate requires a path to a keychain. This
    # means we cannot use mkstemp to use a generic temporary file. Instead,
    # we're going to create a temporary directory and a filename to use there.
    # This filename will be 8 random bytes expanded into base64. We also need
    # some random bytes to password-protect the keychain we're creating, so we
    # ask for 40 random bytes.
    random_bytes = os.urandom(40)
    filename = base64.b16encode(random_bytes[:8]).decode("utf-8")
    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
    tempdirectory = tempfile.mkdtemp()

    keychain_path = os.path.join(tempdirectory, filename).encode("utf-8")

    # We now want to create the keychain itself.
    keychain = Security.SecKeychainRef()
    status = Security.SecKeychainCreate(
        keychain_path, len(password), password, False, None, ctypes.byref(keychain)
    )
    _assert_no_error(status)

    # Having created the keychain, we want to pass it off to the caller.
    return keychain, tempdirectory


def _load_items_from_file(keychain, path):
    """
    Given a single file, loads all the trust objects from it into arrays and
    the keychain.
    Returns a tuple of lists: the first list is a list of identities, the
    second a list of certs.
    """
    certificates = []
    identities = []
    result_array = None

    with open(path, "rb") as f:
        raw_filedata = f.read()

    try:
        filedata = CoreFoundation.CFDataCreate(
            CoreFoundation.kCFAllocatorDefault, raw_filedata, len(raw_filedata)
        )
        result_array = CoreFoundation.CFArrayRef()
        result = Security.SecItemImport(
            filedata,  # cert data
            None,  # Filename, leaving it out for now
            None,  # What the type of the file is, we don't care
            None,  # what's in the file, we don't care
            0,  # import flags
            None,  # key params, can include passphrase in the future
            keychain,  # The keychain to insert into
            ctypes.byref(result_array),  # Results
        )
        _assert_no_error(result)

        # A CFArray is not very useful to us as an intermediary
        # representation, so we are going to extract the objects we want
        # and then free the array. We don't need to keep hold of keys: the
        # keychain already has them!
        result_count = CoreFoundation.CFArrayGetCount(result_array)
        for index in range(result_count):
            item = CoreFoundation.CFArrayGetValueAtIndex(result_array, index)
            item = ctypes.cast(item, CoreFoundation.CFTypeRef)

            if _is_cert(item):
                CoreFoundation.CFRetain(item)
                certificates.append(item)
            elif _is_identity(item):
                CoreFoundation.CFRetain(item)
                identities.append(item)
    finally:
        if result_array:
            CoreFoundation.CFRelease(result_array)

        CoreFoundation.CFRelease(filedata)

    return (identities, certificates)


def _load_client_cert_chain(keychain, *paths):
    """
    Load certificates and maybe keys from a number of files. Has the end goal
    of returning a CFArray containing one SecIdentityRef, and then zero or more
    SecCertificateRef objects, suitable for use as a client certificate trust
    chain.
    """
    # Ok, the strategy.
    #
    # This relies on knowing that macOS will not give you a SecIdentityRef
    # unless you have imported a key into a keychain. This is a somewhat
    # artificial limitation of macOS (for example, it doesn't necessarily
    # affect iOS), but there is nothing inside Security.framework that lets you
    # get a SecIdentityRef without having a key in a keychain.
    #
    # So the policy here is we take all the files and iterate them in order.
    # Each one will use SecItemImport to have one or more objects loaded from
    # it. We will also point at a keychain that macOS can use to work with the
    # private key.
    #
    # Once we have all the objects, we'll check what we actually have. If we
    # already have a SecIdentityRef in hand, fab: we'll use that. Otherwise,
    # we'll take the first certificate (which we assume to be our leaf) and
    # ask the keychain to give us a SecIdentityRef with that cert's associated
    # key.
    #
    # We'll then return a CFArray containing the trust chain: one
    # SecIdentityRef and then zero-or-more SecCertificateRef objects. The
    # responsibility for freeing this CFArray will be with the caller. This
    # CFArray must remain alive for the entire connection, so in practice it
    # will be stored with a single SSLSocket, along with the reference to the
    # keychain.
    certificates = []
    identities = []

    # Filter out bad paths.
    paths = (path for path in paths if path)

    try:
        for file_path in paths:
            new_identities, new_certs = _load_items_from_file(keychain, file_path)
            identities.extend(new_identities)
            certificates.extend(new_certs)

        # Ok, we have everything. The question is: do we have an identity? If
        # not, we want to grab one from the first cert we have.
        if not identities:
            new_identity = Security.SecIdentityRef()
            status = Security.SecIdentityCreateWithCertificate(
                keychain, certificates[0], ctypes.byref(new_identity)
            )
            _assert_no_error(status)
            identities.append(new_identity)

            # We now want to release the original certificate, as we no longer
            # need it.
            CoreFoundation.CFRelease(certificates.pop(0))

        # We now need to build a new CFArray that holds the trust chain.
        trust_chain = CoreFoundation.CFArrayCreateMutable(
            CoreFoundation.kCFAllocatorDefault,
            0,
            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
        )
        for item in itertools.chain(identities, certificates):
            # ArrayAppendValue does a CFRetain on the item. That's fine,
            # because the finally block will release our other refs to them.
            CoreFoundation.CFArrayAppendValue(trust_chain, item)

        return trust_chain
    finally:
        for obj in itertools.chain(identities, certificates):
            CoreFoundation.CFRelease(obj)


TLS_PROTOCOL_VERSIONS = {
    "SSLv2": (0, 2),
    "SSLv3": (3, 0),
    "TLSv1": (3, 1),
    "TLSv1.1": (3, 2),
    "TLSv1.2": (3, 3),
}


def _build_tls_unknown_ca_alert(version):
    """
    Builds a TLS alert record for an unknown CA.
    """
    ver_maj, ver_min = TLS_PROTOCOL_VERSIONS[version]
    severity_fatal = 0x02
    description_unknown_ca = 0x30
    msg = struct.pack(">BB", severity_fatal, description_unknown_ca)
    msg_len = len(msg)
    record_type_alert = 0x15
    record = struct.pack(">BBBH", record_type_alert, ver_maj, ver_min, msg_len) + msg
    return record


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/__init__.py
# ========================================================


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/_securetransport/bindings.py
# ========================================================
"""
This module uses ctypes to bind a whole bunch of functions and constants from
SecureTransport. The goal here is to provide the low-level API to
SecureTransport. These are essentially the C-level functions and constants, and
they're pretty gross to work with.

This code is a bastardised version of the code found in Will Bond's oscrypto
library. An enormous debt is owed to him for blazing this trail for us. For
that reason, this code should be considered to be covered both by urllib3's
license and by oscrypto's:

    Copyright (c) 2015-2016 Will Bond <will@wbond.net>

    Permission is hereby granted, free of charge, to any person obtaining a
    copy of this software and associated documentation files (the "Software"),
    to deal in the Software without restriction, including without limitation
    the rights to use, copy, modify, merge, publish, distribute, sublicense,
    and/or sell copies of the Software, and to permit persons to whom the
    Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in
    all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.
"""
from __future__ import absolute_import

import platform
from ctypes import (
    CDLL,
    CFUNCTYPE,
    POINTER,
    c_bool,
    c_byte,
    c_char_p,
    c_int32,
    c_long,
    c_size_t,
    c_uint32,
    c_ulong,
    c_void_p,
)
from ctypes.util import find_library

from ...packages.six import raise_from

if platform.system() != "Darwin":
    raise ImportError("Only macOS is supported")

version = platform.mac_ver()[0]
version_info = tuple(map(int, version.split(".")))
if version_info < (10, 8):
    raise OSError(
        "Only OS X 10.8 and newer are supported, not %s.%s"
        % (version_info[0], version_info[1])
    )


def load_cdll(name, macos10_16_path):
    """Loads a CDLL by name, falling back to known path on 10.16+"""
    try:
        # Big Sur is technically 11 but we use 10.16 due to the Big Sur
        # beta being labeled as 10.16.
        if version_info >= (10, 16):
            path = macos10_16_path
        else:
            path = find_library(name)
        if not path:
            raise OSError  # Caught and reraised as 'ImportError'
        return CDLL(path, use_errno=True)
    except OSError:
        raise_from(ImportError("The library %s failed to load" % name), None)


Security = load_cdll(
    "Security", "/System/Library/Frameworks/Security.framework/Security"
)
CoreFoundation = load_cdll(
    "CoreFoundation",
    "/System/Library/Frameworks/CoreFoundation.framework/CoreFoundation",
)


Boolean = c_bool
CFIndex = c_long
CFStringEncoding = c_uint32
CFData = c_void_p
CFString = c_void_p
CFArray = c_void_p
CFMutableArray = c_void_p
CFDictionary = c_void_p
CFError = c_void_p
CFType = c_void_p
CFTypeID = c_ulong

CFTypeRef = POINTER(CFType)
CFAllocatorRef = c_void_p

OSStatus = c_int32

CFDataRef = POINTER(CFData)
CFStringRef = POINTER(CFString)
CFArrayRef = POINTER(CFArray)
CFMutableArrayRef = POINTER(CFMutableArray)
CFDictionaryRef = POINTER(CFDictionary)
CFArrayCallBacks = c_void_p
CFDictionaryKeyCallBacks = c_void_p
CFDictionaryValueCallBacks = c_void_p

SecCertificateRef = POINTER(c_void_p)
SecExternalFormat = c_uint32
SecExternalItemType = c_uint32
SecIdentityRef = POINTER(c_void_p)
SecItemImportExportFlags = c_uint32
SecItemImportExportKeyParameters = c_void_p
SecKeychainRef = POINTER(c_void_p)
SSLProtocol = c_uint32
SSLCipherSuite = c_uint32
SSLContextRef = POINTER(c_void_p)
SecTrustRef = POINTER(c_void_p)
SSLConnectionRef = c_uint32
SecTrustResultType = c_uint32
SecTrustOptionFlags = c_uint32
SSLProtocolSide = c_uint32
SSLConnectionType = c_uint32
SSLSessionOption = c_uint32


try:
    Security.SecItemImport.argtypes = [
        CFDataRef,
        CFStringRef,
        POINTER(SecExternalFormat),
        POINTER(SecExternalItemType),
        SecItemImportExportFlags,
        POINTER(SecItemImportExportKeyParameters),
        SecKeychainRef,
        POINTER(CFArrayRef),
    ]
    Security.SecItemImport.restype = OSStatus

    Security.SecCertificateGetTypeID.argtypes = []
    Security.SecCertificateGetTypeID.restype = CFTypeID

    Security.SecIdentityGetTypeID.argtypes = []
    Security.SecIdentityGetTypeID.restype = CFTypeID

    Security.SecKeyGetTypeID.argtypes = []
    Security.SecKeyGetTypeID.restype = CFTypeID

    Security.SecCertificateCreateWithData.argtypes = [CFAllocatorRef, CFDataRef]
    Security.SecCertificateCreateWithData.restype = SecCertificateRef

    Security.SecCertificateCopyData.argtypes = [SecCertificateRef]
    Security.SecCertificateCopyData.restype = CFDataRef

    Security.SecCopyErrorMessageString.argtypes = [OSStatus, c_void_p]
    Security.SecCopyErrorMessageString.restype = CFStringRef

    Security.SecIdentityCreateWithCertificate.argtypes = [
        CFTypeRef,
        SecCertificateRef,
        POINTER(SecIdentityRef),
    ]
    Security.SecIdentityCreateWithCertificate.restype = OSStatus

    Security.SecKeychainCreate.argtypes = [
        c_char_p,
        c_uint32,
        c_void_p,
        Boolean,
        c_void_p,
        POINTER(SecKeychainRef),
    ]
    Security.SecKeychainCreate.restype = OSStatus

    Security.SecKeychainDelete.argtypes = [SecKeychainRef]
    Security.SecKeychainDelete.restype = OSStatus

    Security.SecPKCS12Import.argtypes = [
        CFDataRef,
        CFDictionaryRef,
        POINTER(CFArrayRef),
    ]
    Security.SecPKCS12Import.restype = OSStatus

    SSLReadFunc = CFUNCTYPE(OSStatus, SSLConnectionRef, c_void_p, POINTER(c_size_t))
    SSLWriteFunc = CFUNCTYPE(
        OSStatus, SSLConnectionRef, POINTER(c_byte), POINTER(c_size_t)
    )

    Security.SSLSetIOFuncs.argtypes = [SSLContextRef, SSLReadFunc, SSLWriteFunc]
    Security.SSLSetIOFuncs.restype = OSStatus

    Security.SSLSetPeerID.argtypes = [SSLContextRef, c_char_p, c_size_t]
    Security.SSLSetPeerID.restype = OSStatus

    Security.SSLSetCertificate.argtypes = [SSLContextRef, CFArrayRef]
    Security.SSLSetCertificate.restype = OSStatus

    Security.SSLSetCertificateAuthorities.argtypes = [SSLContextRef, CFTypeRef, Boolean]
    Security.SSLSetCertificateAuthorities.restype = OSStatus

    Security.SSLSetConnection.argtypes = [SSLContextRef, SSLConnectionRef]
    Security.SSLSetConnection.restype = OSStatus

    Security.SSLSetPeerDomainName.argtypes = [SSLContextRef, c_char_p, c_size_t]
    Security.SSLSetPeerDomainName.restype = OSStatus

    Security.SSLHandshake.argtypes = [SSLContextRef]
    Security.SSLHandshake.restype = OSStatus

    Security.SSLRead.argtypes = [SSLContextRef, c_char_p, c_size_t, POINTER(c_size_t)]
    Security.SSLRead.restype = OSStatus

    Security.SSLWrite.argtypes = [SSLContextRef, c_char_p, c_size_t, POINTER(c_size_t)]
    Security.SSLWrite.restype = OSStatus

    Security.SSLClose.argtypes = [SSLContextRef]
    Security.SSLClose.restype = OSStatus

    Security.SSLGetNumberSupportedCiphers.argtypes = [SSLContextRef, POINTER(c_size_t)]
    Security.SSLGetNumberSupportedCiphers.restype = OSStatus

    Security.SSLGetSupportedCiphers.argtypes = [
        SSLContextRef,
        POINTER(SSLCipherSuite),
        POINTER(c_size_t),
    ]
    Security.SSLGetSupportedCiphers.restype = OSStatus

    Security.SSLSetEnabledCiphers.argtypes = [
        SSLContextRef,
        POINTER(SSLCipherSuite),
        c_size_t,
    ]
    Security.SSLSetEnabledCiphers.restype = OSStatus

    Security.SSLGetNumberEnabledCiphers.argtype = [SSLContextRef, POINTER(c_size_t)]
    Security.SSLGetNumberEnabledCiphers.restype = OSStatus

    Security.SSLGetEnabledCiphers.argtypes = [
        SSLContextRef,
        POINTER(SSLCipherSuite),
        POINTER(c_size_t),
    ]
    Security.SSLGetEnabledCiphers.restype = OSStatus

    Security.SSLGetNegotiatedCipher.argtypes = [SSLContextRef, POINTER(SSLCipherSuite)]
    Security.SSLGetNegotiatedCipher.restype = OSStatus

    Security.SSLGetNegotiatedProtocolVersion.argtypes = [
        SSLContextRef,
        POINTER(SSLProtocol),
    ]
    Security.SSLGetNegotiatedProtocolVersion.restype = OSStatus

    Security.SSLCopyPeerTrust.argtypes = [SSLContextRef, POINTER(SecTrustRef)]
    Security.SSLCopyPeerTrust.restype = OSStatus

    Security.SecTrustSetAnchorCertificates.argtypes = [SecTrustRef, CFArrayRef]
    Security.SecTrustSetAnchorCertificates.restype = OSStatus

    Security.SecTrustSetAnchorCertificatesOnly.argstypes = [SecTrustRef, Boolean]
    Security.SecTrustSetAnchorCertificatesOnly.restype = OSStatus

    Security.SecTrustEvaluate.argtypes = [SecTrustRef, POINTER(SecTrustResultType)]
    Security.SecTrustEvaluate.restype = OSStatus

    Security.SecTrustGetCertificateCount.argtypes = [SecTrustRef]
    Security.SecTrustGetCertificateCount.restype = CFIndex

    Security.SecTrustGetCertificateAtIndex.argtypes = [SecTrustRef, CFIndex]
    Security.SecTrustGetCertificateAtIndex.restype = SecCertificateRef

    Security.SSLCreateContext.argtypes = [
        CFAllocatorRef,
        SSLProtocolSide,
        SSLConnectionType,
    ]
    Security.SSLCreateContext.restype = SSLContextRef

    Security.SSLSetSessionOption.argtypes = [SSLContextRef, SSLSessionOption, Boolean]
    Security.SSLSetSessionOption.restype = OSStatus

    Security.SSLSetProtocolVersionMin.argtypes = [SSLContextRef, SSLProtocol]
    Security.SSLSetProtocolVersionMin.restype = OSStatus

    Security.SSLSetProtocolVersionMax.argtypes = [SSLContextRef, SSLProtocol]
    Security.SSLSetProtocolVersionMax.restype = OSStatus

    try:
        Security.SSLSetALPNProtocols.argtypes = [SSLContextRef, CFArrayRef]
        Security.SSLSetALPNProtocols.restype = OSStatus
    except AttributeError:
        # Supported only in 10.12+
        pass

    Security.SecCopyErrorMessageString.argtypes = [OSStatus, c_void_p]
    Security.SecCopyErrorMessageString.restype = CFStringRef

    Security.SSLReadFunc = SSLReadFunc
    Security.SSLWriteFunc = SSLWriteFunc
    Security.SSLContextRef = SSLContextRef
    Security.SSLProtocol = SSLProtocol
    Security.SSLCipherSuite = SSLCipherSuite
    Security.SecIdentityRef = SecIdentityRef
    Security.SecKeychainRef = SecKeychainRef
    Security.SecTrustRef = SecTrustRef
    Security.SecTrustResultType = SecTrustResultType
    Security.SecExternalFormat = SecExternalFormat
    Security.OSStatus = OSStatus

    Security.kSecImportExportPassphrase = CFStringRef.in_dll(
        Security, "kSecImportExportPassphrase"
    )
    Security.kSecImportItemIdentity = CFStringRef.in_dll(
        Security, "kSecImportItemIdentity"
    )

    # CoreFoundation time!
    CoreFoundation.CFRetain.argtypes = [CFTypeRef]
    CoreFoundation.CFRetain.restype = CFTypeRef

    CoreFoundation.CFRelease.argtypes = [CFTypeRef]
    CoreFoundation.CFRelease.restype = None

    CoreFoundation.CFGetTypeID.argtypes = [CFTypeRef]
    CoreFoundation.CFGetTypeID.restype = CFTypeID

    CoreFoundation.CFStringCreateWithCString.argtypes = [
        CFAllocatorRef,
        c_char_p,
        CFStringEncoding,
    ]
    CoreFoundation.CFStringCreateWithCString.restype = CFStringRef

    CoreFoundation.CFStringGetCStringPtr.argtypes = [CFStringRef, CFStringEncoding]
    CoreFoundation.CFStringGetCStringPtr.restype = c_char_p

    CoreFoundation.CFStringGetCString.argtypes = [
        CFStringRef,
        c_char_p,
        CFIndex,
        CFStringEncoding,
    ]
    CoreFoundation.CFStringGetCString.restype = c_bool

    CoreFoundation.CFDataCreate.argtypes = [CFAllocatorRef, c_char_p, CFIndex]
    CoreFoundation.CFDataCreate.restype = CFDataRef

    CoreFoundation.CFDataGetLength.argtypes = [CFDataRef]
    CoreFoundation.CFDataGetLength.restype = CFIndex

    CoreFoundation.CFDataGetBytePtr.argtypes = [CFDataRef]
    CoreFoundation.CFDataGetBytePtr.restype = c_void_p

    CoreFoundation.CFDictionaryCreate.argtypes = [
        CFAllocatorRef,
        POINTER(CFTypeRef),
        POINTER(CFTypeRef),
        CFIndex,
        CFDictionaryKeyCallBacks,
        CFDictionaryValueCallBacks,
    ]
    CoreFoundation.CFDictionaryCreate.restype = CFDictionaryRef

    CoreFoundation.CFDictionaryGetValue.argtypes = [CFDictionaryRef, CFTypeRef]
    CoreFoundation.CFDictionaryGetValue.restype = CFTypeRef

    CoreFoundation.CFArrayCreate.argtypes = [
        CFAllocatorRef,
        POINTER(CFTypeRef),
        CFIndex,
        CFArrayCallBacks,
    ]
    CoreFoundation.CFArrayCreate.restype = CFArrayRef

    CoreFoundation.CFArrayCreateMutable.argtypes = [
        CFAllocatorRef,
        CFIndex,
        CFArrayCallBacks,
    ]
    CoreFoundation.CFArrayCreateMutable.restype = CFMutableArrayRef

    CoreFoundation.CFArrayAppendValue.argtypes = [CFMutableArrayRef, c_void_p]
    CoreFoundation.CFArrayAppendValue.restype = None

    CoreFoundation.CFArrayGetCount.argtypes = [CFArrayRef]
    CoreFoundation.CFArrayGetCount.restype = CFIndex

    CoreFoundation.CFArrayGetValueAtIndex.argtypes = [CFArrayRef, CFIndex]
    CoreFoundation.CFArrayGetValueAtIndex.restype = c_void_p

    CoreFoundation.kCFAllocatorDefault = CFAllocatorRef.in_dll(
        CoreFoundation, "kCFAllocatorDefault"
    )
    CoreFoundation.kCFTypeArrayCallBacks = c_void_p.in_dll(
        CoreFoundation, "kCFTypeArrayCallBacks"
    )
    CoreFoundation.kCFTypeDictionaryKeyCallBacks = c_void_p.in_dll(
        CoreFoundation, "kCFTypeDictionaryKeyCallBacks"
    )
    CoreFoundation.kCFTypeDictionaryValueCallBacks = c_void_p.in_dll(
        CoreFoundation, "kCFTypeDictionaryValueCallBacks"
    )

    CoreFoundation.CFTypeRef = CFTypeRef
    CoreFoundation.CFArrayRef = CFArrayRef
    CoreFoundation.CFStringRef = CFStringRef
    CoreFoundation.CFDictionaryRef = CFDictionaryRef

except (AttributeError):
    raise ImportError("Error initializing ctypes")


class CFConst(object):
    """
    A class object that acts as essentially a namespace for CoreFoundation
    constants.
    """

    kCFStringEncodingUTF8 = CFStringEncoding(0x08000100)


class SecurityConst(object):
    """
    A class object that acts as essentially a namespace for Security constants.
    """

    kSSLSessionOptionBreakOnServerAuth = 0

    kSSLProtocol2 = 1
    kSSLProtocol3 = 2
    kTLSProtocol1 = 4
    kTLSProtocol11 = 7
    kTLSProtocol12 = 8
    # SecureTransport does not support TLS 1.3 even if there's a constant for it
    kTLSProtocol13 = 10
    kTLSProtocolMaxSupported = 999

    kSSLClientSide = 1
    kSSLStreamType = 0

    kSecFormatPEMSequence = 10

    kSecTrustResultInvalid = 0
    kSecTrustResultProceed = 1
    # This gap is present on purpose: this was kSecTrustResultConfirm, which
    # is deprecated.
    kSecTrustResultDeny = 3
    kSecTrustResultUnspecified = 4
    kSecTrustResultRecoverableTrustFailure = 5
    kSecTrustResultFatalTrustFailure = 6
    kSecTrustResultOtherError = 7

    errSSLProtocol = -9800
    errSSLWouldBlock = -9803
    errSSLClosedGraceful = -9805
    errSSLClosedNoNotify = -9816
    errSSLClosedAbort = -9806

    errSSLXCertChainInvalid = -9807
    errSSLCrypto = -9809
    errSSLInternal = -9810
    errSSLCertExpired = -9814
    errSSLCertNotYetValid = -9815
    errSSLUnknownRootCert = -9812
    errSSLNoRootCert = -9813
    errSSLHostNameMismatch = -9843
    errSSLPeerHandshakeFail = -9824
    errSSLPeerUserCancelled = -9839
    errSSLWeakPeerEphemeralDHKey = -9850
    errSSLServerAuthCompleted = -9841
    errSSLRecordOverflow = -9847

    errSecVerifyFailed = -67808
    errSecNoTrustSettings = -25263
    errSecItemNotFound = -25300
    errSecInvalidTrustSettings = -25262

    # Cipher suites. We only pick the ones our default cipher string allows.
    # Source: https://developer.apple.com/documentation/security/1550981-ssl_cipher_suite_values
    TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 = 0xC02C
    TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 = 0xC030
    TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 = 0xC02B
    TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 = 0xC02F
    TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 = 0xCCA9
    TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 = 0xCCA8
    TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 = 0x009F
    TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 = 0x009E
    TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384 = 0xC024
    TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384 = 0xC028
    TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA = 0xC00A
    TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA = 0xC014
    TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 = 0x006B
    TLS_DHE_RSA_WITH_AES_256_CBC_SHA = 0x0039
    TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 = 0xC023
    TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 = 0xC027
    TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA = 0xC009
    TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA = 0xC013
    TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 = 0x0067
    TLS_DHE_RSA_WITH_AES_128_CBC_SHA = 0x0033
    TLS_RSA_WITH_AES_256_GCM_SHA384 = 0x009D
    TLS_RSA_WITH_AES_128_GCM_SHA256 = 0x009C
    TLS_RSA_WITH_AES_256_CBC_SHA256 = 0x003D
    TLS_RSA_WITH_AES_128_CBC_SHA256 = 0x003C
    TLS_RSA_WITH_AES_256_CBC_SHA = 0x0035
    TLS_RSA_WITH_AES_128_CBC_SHA = 0x002F
    TLS_AES_128_GCM_SHA256 = 0x1301
    TLS_AES_256_GCM_SHA384 = 0x1302
    TLS_AES_128_CCM_8_SHA256 = 0x1305
    TLS_AES_128_CCM_SHA256 = 0x1304


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/contrib/pyopenssl.py
# ========================================================
"""
TLS with SNI_-support for Python 2. Follow these instructions if you would
like to verify TLS certificates in Python 2. Note, the default libraries do
*not* do certificate checking; you need to do additional work to validate
certificates yourself.

This needs the following packages installed:

* `pyOpenSSL`_ (tested with 16.0.0)
* `cryptography`_ (minimum 1.3.4, from pyopenssl)
* `idna`_ (minimum 2.0, from cryptography)

However, pyopenssl depends on cryptography, which depends on idna, so while we
use all three directly here we end up having relatively few packages required.

You can install them with the following command:

.. code-block:: bash

    $ python -m pip install pyopenssl cryptography idna

To activate certificate checking, call
:func:`~urllib3.contrib.pyopenssl.inject_into_urllib3` from your Python code
before you begin making HTTP requests. This can be done in a ``sitecustomize``
module, or at any other time before your application begins using ``urllib3``,
like this:

.. code-block:: python

    try:
        import pip._vendor.urllib3.contrib.pyopenssl as pyopenssl
        pyopenssl.inject_into_urllib3()
    except ImportError:
        pass

Now you can use :mod:`urllib3` as you normally would, and it will support SNI
when the required modules are installed.

Activating this module also has the positive side effect of disabling SSL/TLS
compression in Python 2 (see `CRIME attack`_).

.. _sni: https://en.wikipedia.org/wiki/Server_Name_Indication
.. _crime attack: https://en.wikipedia.org/wiki/CRIME_(security_exploit)
.. _pyopenssl: https://www.pyopenssl.org
.. _cryptography: https://cryptography.io
.. _idna: https://github.com/kjd/idna
"""
from __future__ import absolute_import

import OpenSSL.crypto
import OpenSSL.SSL
from cryptography import x509
from cryptography.hazmat.backends.openssl import backend as openssl_backend

try:
    from cryptography.x509 import UnsupportedExtension
except ImportError:
    # UnsupportedExtension is gone in cryptography >= 2.1.0
    class UnsupportedExtension(Exception):
        pass


from io import BytesIO
from socket import error as SocketError
from socket import timeout

try:  # Platform-specific: Python 2
    from socket import _fileobject
except ImportError:  # Platform-specific: Python 3
    _fileobject = None
    from ..packages.backports.makefile import backport_makefile

import logging
import ssl
import sys
import warnings

from .. import util
from ..packages import six
from ..util.ssl_ import PROTOCOL_TLS_CLIENT

warnings.warn(
    "'urllib3.contrib.pyopenssl' module is deprecated and will be removed "
    "in a future release of urllib3 2.x. Read more in this issue: "
    "https://github.com/urllib3/urllib3/issues/2680",
    category=DeprecationWarning,
    stacklevel=2,
)

__all__ = ["inject_into_urllib3", "extract_from_urllib3"]

# SNI always works.
HAS_SNI = True

# Map from urllib3 to PyOpenSSL compatible parameter-values.
_openssl_versions = {
    util.PROTOCOL_TLS: OpenSSL.SSL.SSLv23_METHOD,
    PROTOCOL_TLS_CLIENT: OpenSSL.SSL.SSLv23_METHOD,
    ssl.PROTOCOL_TLSv1: OpenSSL.SSL.TLSv1_METHOD,
}

if hasattr(ssl, "PROTOCOL_SSLv3") and hasattr(OpenSSL.SSL, "SSLv3_METHOD"):
    _openssl_versions[ssl.PROTOCOL_SSLv3] = OpenSSL.SSL.SSLv3_METHOD

if hasattr(ssl, "PROTOCOL_TLSv1_1") and hasattr(OpenSSL.SSL, "TLSv1_1_METHOD"):
    _openssl_versions[ssl.PROTOCOL_TLSv1_1] = OpenSSL.SSL.TLSv1_1_METHOD

if hasattr(ssl, "PROTOCOL_TLSv1_2") and hasattr(OpenSSL.SSL, "TLSv1_2_METHOD"):
    _openssl_versions[ssl.PROTOCOL_TLSv1_2] = OpenSSL.SSL.TLSv1_2_METHOD


_stdlib_to_openssl_verify = {
    ssl.CERT_NONE: OpenSSL.SSL.VERIFY_NONE,
    ssl.CERT_OPTIONAL: OpenSSL.SSL.VERIFY_PEER,
    ssl.CERT_REQUIRED: OpenSSL.SSL.VERIFY_PEER
    + OpenSSL.SSL.VERIFY_FAIL_IF_NO_PEER_CERT,
}
_openssl_to_stdlib_verify = dict((v, k) for k, v in _stdlib_to_openssl_verify.items())

# OpenSSL will only write 16K at a time
SSL_WRITE_BLOCKSIZE = 16384

orig_util_HAS_SNI = util.HAS_SNI
orig_util_SSLContext = util.ssl_.SSLContext


log = logging.getLogger(__name__)


def inject_into_urllib3():
    "Monkey-patch urllib3 with PyOpenSSL-backed SSL-support."

    _validate_dependencies_met()

    util.SSLContext = PyOpenSSLContext
    util.ssl_.SSLContext = PyOpenSSLContext
    util.HAS_SNI = HAS_SNI
    util.ssl_.HAS_SNI = HAS_SNI
    util.IS_PYOPENSSL = True
    util.ssl_.IS_PYOPENSSL = True


def extract_from_urllib3():
    "Undo monkey-patching by :func:`inject_into_urllib3`."

    util.SSLContext = orig_util_SSLContext
    util.ssl_.SSLContext = orig_util_SSLContext
    util.HAS_SNI = orig_util_HAS_SNI
    util.ssl_.HAS_SNI = orig_util_HAS_SNI
    util.IS_PYOPENSSL = False
    util.ssl_.IS_PYOPENSSL = False


def _validate_dependencies_met():
    """
    Verifies that PyOpenSSL's package-level dependencies have been met.
    Throws `ImportError` if they are not met.
    """
    # Method added in `cryptography==1.1`; not available in older versions
    from cryptography.x509.extensions import Extensions

    if getattr(Extensions, "get_extension_for_class", None) is None:
        raise ImportError(
            "'cryptography' module missing required functionality.  "
            "Try upgrading to v1.3.4 or newer."
        )

    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509
    # attribute is only present on those versions.
    from OpenSSL.crypto import X509

    x509 = X509()
    if getattr(x509, "_x509", None) is None:
        raise ImportError(
            "'pyOpenSSL' module missing required functionality. "
            "Try upgrading to v0.14 or newer."
        )


def _dnsname_to_stdlib(name):
    """
    Converts a dNSName SubjectAlternativeName field to the form used by the
    standard library on the given Python version.

    Cryptography produces a dNSName as a unicode string that was idna-decoded
    from ASCII bytes. We need to idna-encode that string to get it back, and
    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib
    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).

    If the name cannot be idna-encoded then we return None signalling that
    the name given should be skipped.
    """

    def idna_encode(name):
        """
        Borrowed wholesale from the Python Cryptography Project. It turns out
        that we can't just safely call `idna.encode`: it can explode for
        wildcard names. This avoids that problem.
        """
        from pip._vendor import idna

        try:
            for prefix in [u"*.", u"."]:
                if name.startswith(prefix):
                    name = name[len(prefix) :]
                    return prefix.encode("ascii") + idna.encode(name)
            return idna.encode(name)
        except idna.core.IDNAError:
            return None

    # Don't send IPv6 addresses through the IDNA encoder.
    if ":" in name:
        return name

    name = idna_encode(name)
    if name is None:
        return None
    elif sys.version_info >= (3, 0):
        name = name.decode("utf-8")
    return name


def get_subj_alt_name(peer_cert):
    """
    Given an PyOpenSSL certificate, provides all the subject alternative names.
    """
    # Pass the cert to cryptography, which has much better APIs for this.
    if hasattr(peer_cert, "to_cryptography"):
        cert = peer_cert.to_cryptography()
    else:
        der = OpenSSL.crypto.dump_certificate(OpenSSL.crypto.FILETYPE_ASN1, peer_cert)
        cert = x509.load_der_x509_certificate(der, openssl_backend)

    # We want to find the SAN extension. Ask Cryptography to locate it (it's
    # faster than looping in Python)
    try:
        ext = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName).value
    except x509.ExtensionNotFound:
        # No such extension, return the empty list.
        return []
    except (
        x509.DuplicateExtension,
        UnsupportedExtension,
        x509.UnsupportedGeneralNameType,
        UnicodeError,
    ) as e:
        # A problem has been found with the quality of the certificate. Assume
        # no SAN field is present.
        log.warning(
            "A problem was encountered with the certificate that prevented "
            "urllib3 from finding the SubjectAlternativeName field. This can "
            "affect certificate validation. The error was %s",
            e,
        )
        return []

    # We want to return dNSName and iPAddress fields. We need to cast the IPs
    # back to strings because the match_hostname function wants them as
    # strings.
    # Sadly the DNS names need to be idna encoded and then, on Python 3, UTF-8
    # decoded. This is pretty frustrating, but that's what the standard library
    # does with certificates, and so we need to attempt to do the same.
    # We also want to skip over names which cannot be idna encoded.
    names = [
        ("DNS", name)
        for name in map(_dnsname_to_stdlib, ext.get_values_for_type(x509.DNSName))
        if name is not None
    ]
    names.extend(
        ("IP Address", str(name)) for name in ext.get_values_for_type(x509.IPAddress)
    )

    return names


class WrappedSocket(object):
    """API-compatibility wrapper for Python OpenSSL's Connection-class.

    Note: _makefile_refs, _drop() and _reuse() are needed for the garbage
    collector of pypy.
    """

    def __init__(self, connection, socket, suppress_ragged_eofs=True):
        self.connection = connection
        self.socket = socket
        self.suppress_ragged_eofs = suppress_ragged_eofs
        self._makefile_refs = 0
        self._closed = False

    def fileno(self):
        return self.socket.fileno()

    # Copy-pasted from Python 3.5 source code
    def _decref_socketios(self):
        if self._makefile_refs > 0:
            self._makefile_refs -= 1
        if self._closed:
            self.close()

    def recv(self, *args, **kwargs):
        try:
            data = self.connection.recv(*args, **kwargs)
        except OpenSSL.SSL.SysCallError as e:
            if self.suppress_ragged_eofs and e.args == (-1, "Unexpected EOF"):
                return b""
            else:
                raise SocketError(str(e))
        except OpenSSL.SSL.ZeroReturnError:
            if self.connection.get_shutdown() == OpenSSL.SSL.RECEIVED_SHUTDOWN:
                return b""
            else:
                raise
        except OpenSSL.SSL.WantReadError:
            if not util.wait_for_read(self.socket, self.socket.gettimeout()):
                raise timeout("The read operation timed out")
            else:
                return self.recv(*args, **kwargs)

        # TLS 1.3 post-handshake authentication
        except OpenSSL.SSL.Error as e:
            raise ssl.SSLError("read error: %r" % e)
        else:
            return data

    def recv_into(self, *args, **kwargs):
        try:
            return self.connection.recv_into(*args, **kwargs)
        except OpenSSL.SSL.SysCallError as e:
            if self.suppress_ragged_eofs and e.args == (-1, "Unexpected EOF"):
                return 0
            else:
                raise SocketError(str(e))
        except OpenSSL.SSL.ZeroReturnError:
            if self.connection.get_shutdown() == OpenSSL.SSL.RECEIVED_SHUTDOWN:
                return 0
            else:
                raise
        except OpenSSL.SSL.WantReadError:
            if not util.wait_for_read(self.socket, self.socket.gettimeout()):
                raise timeout("The read operation timed out")
            else:
                return self.recv_into(*args, **kwargs)

        # TLS 1.3 post-handshake authentication
        except OpenSSL.SSL.Error as e:
            raise ssl.SSLError("read error: %r" % e)

    def settimeout(self, timeout):
        return self.socket.settimeout(timeout)

    def _send_until_done(self, data):
        while True:
            try:
                return self.connection.send(data)
            except OpenSSL.SSL.WantWriteError:
                if not util.wait_for_write(self.socket, self.socket.gettimeout()):
                    raise timeout()
                continue
            except OpenSSL.SSL.SysCallError as e:
                raise SocketError(str(e))

    def sendall(self, data):
        total_sent = 0
        while total_sent < len(data):
            sent = self._send_until_done(
                data[total_sent : total_sent + SSL_WRITE_BLOCKSIZE]
            )
            total_sent += sent

    def shutdown(self):
        # FIXME rethrow compatible exceptions should we ever use this
        self.connection.shutdown()

    def close(self):
        if self._makefile_refs < 1:
            try:
                self._closed = True
                return self.connection.close()
            except OpenSSL.SSL.Error:
                return
        else:
            self._makefile_refs -= 1

    def getpeercert(self, binary_form=False):
        x509 = self.connection.get_peer_certificate()

        if not x509:
            return x509

        if binary_form:
            return OpenSSL.crypto.dump_certificate(OpenSSL.crypto.FILETYPE_ASN1, x509)

        return {
            "subject": ((("commonName", x509.get_subject().CN),),),
            "subjectAltName": get_subj_alt_name(x509),
        }

    def version(self):
        return self.connection.get_protocol_version_name()

    def _reuse(self):
        self._makefile_refs += 1

    def _drop(self):
        if self._makefile_refs < 1:
            self.close()
        else:
            self._makefile_refs -= 1


if _fileobject:  # Platform-specific: Python 2

    def makefile(self, mode, bufsize=-1):
        self._makefile_refs += 1
        return _fileobject(self, mode, bufsize, close=True)

else:  # Platform-specific: Python 3
    makefile = backport_makefile

WrappedSocket.makefile = makefile


class PyOpenSSLContext(object):
    """
    I am a wrapper class for the PyOpenSSL ``Context`` object. I am responsible
    for translating the interface of the standard library ``SSLContext`` object
    to calls into PyOpenSSL.
    """

    def __init__(self, protocol):
        self.protocol = _openssl_versions[protocol]
        self._ctx = OpenSSL.SSL.Context(self.protocol)
        self._options = 0
        self.check_hostname = False

    @property
    def options(self):
        return self._options

    @options.setter
    def options(self, value):
        self._options = value
        self._ctx.set_options(value)

    @property
    def verify_mode(self):
        return _openssl_to_stdlib_verify[self._ctx.get_verify_mode()]

    @verify_mode.setter
    def verify_mode(self, value):
        self._ctx.set_verify(_stdlib_to_openssl_verify[value], _verify_callback)

    def set_default_verify_paths(self):
        self._ctx.set_default_verify_paths()

    def set_ciphers(self, ciphers):
        if isinstance(ciphers, six.text_type):
            ciphers = ciphers.encode("utf-8")
        self._ctx.set_cipher_list(ciphers)

    def load_verify_locations(self, cafile=None, capath=None, cadata=None):
        if cafile is not None:
            cafile = cafile.encode("utf-8")
        if capath is not None:
            capath = capath.encode("utf-8")
        try:
            self._ctx.load_verify_locations(cafile, capath)
            if cadata is not None:
                self._ctx.load_verify_locations(BytesIO(cadata))
        except OpenSSL.SSL.Error as e:
            raise ssl.SSLError("unable to load trusted certificates: %r" % e)

    def load_cert_chain(self, certfile, keyfile=None, password=None):
        self._ctx.use_certificate_chain_file(certfile)
        if password is not None:
            if not isinstance(password, six.binary_type):
                password = password.encode("utf-8")
            self._ctx.set_passwd_cb(lambda *_: password)
        self._ctx.use_privatekey_file(keyfile or certfile)

    def set_alpn_protocols(self, protocols):
        protocols = [six.ensure_binary(p) for p in protocols]
        return self._ctx.set_alpn_protos(protocols)

    def wrap_socket(
        self,
        sock,
        server_side=False,
        do_handshake_on_connect=True,
        suppress_ragged_eofs=True,
        server_hostname=None,
    ):
        cnx = OpenSSL.SSL.Connection(self._ctx, sock)

        if isinstance(server_hostname, six.text_type):  # Platform-specific: Python 3
            server_hostname = server_hostname.encode("utf-8")

        if server_hostname is not None:
            cnx.set_tlsext_host_name(server_hostname)

        cnx.set_connect_state()

        while True:
            try:
                cnx.do_handshake()
            except OpenSSL.SSL.WantReadError:
                if not util.wait_for_read(sock, sock.gettimeout()):
                    raise timeout("select timed out")
                continue
            except OpenSSL.SSL.Error as e:
                raise ssl.SSLError("bad handshake: %r" % e)
            break

        return WrappedSocket(cnx, sock)


def _verify_callback(cnx, x509, err_no, err_depth, return_code):
    return err_no == 0


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/_collections.py
# ========================================================
from __future__ import absolute_import

try:
    from collections.abc import Mapping, MutableMapping
except ImportError:
    from collections import Mapping, MutableMapping
try:
    from threading import RLock
except ImportError:  # Platform-specific: No threads available

    class RLock:
        def __enter__(self):
            pass

        def __exit__(self, exc_type, exc_value, traceback):
            pass


from collections import OrderedDict

from .exceptions import InvalidHeader
from .packages import six
from .packages.six import iterkeys, itervalues

__all__ = ["RecentlyUsedContainer", "HTTPHeaderDict"]


_Null = object()


class RecentlyUsedContainer(MutableMapping):
    """
    Provides a thread-safe dict-like container which maintains up to
    ``maxsize`` keys while throwing away the least-recently-used keys beyond
    ``maxsize``.

    :param maxsize:
        Maximum number of recent elements to retain.

    :param dispose_func:
        Every time an item is evicted from the container,
        ``dispose_func(value)`` is called.  Callback which will get called
    """

    ContainerCls = OrderedDict

    def __init__(self, maxsize=10, dispose_func=None):
        self._maxsize = maxsize
        self.dispose_func = dispose_func

        self._container = self.ContainerCls()
        self.lock = RLock()

    def __getitem__(self, key):
        # Re-insert the item, moving it to the end of the eviction line.
        with self.lock:
            item = self._container.pop(key)
            self._container[key] = item
            return item

    def __setitem__(self, key, value):
        evicted_value = _Null
        with self.lock:
            # Possibly evict the existing value of 'key'
            evicted_value = self._container.get(key, _Null)
            self._container[key] = value

            # If we didn't evict an existing value, we might have to evict the
            # least recently used item from the beginning of the container.
            if len(self._container) > self._maxsize:
                _key, evicted_value = self._container.popitem(last=False)

        if self.dispose_func and evicted_value is not _Null:
            self.dispose_func(evicted_value)

    def __delitem__(self, key):
        with self.lock:
            value = self._container.pop(key)

        if self.dispose_func:
            self.dispose_func(value)

    def __len__(self):
        with self.lock:
            return len(self._container)

    def __iter__(self):
        raise NotImplementedError(
            "Iteration over this class is unlikely to be threadsafe."
        )

    def clear(self):
        with self.lock:
            # Copy pointers to all values, then wipe the mapping
            values = list(itervalues(self._container))
            self._container.clear()

        if self.dispose_func:
            for value in values:
                self.dispose_func(value)

    def keys(self):
        with self.lock:
            return list(iterkeys(self._container))


class HTTPHeaderDict(MutableMapping):
    """
    :param headers:
        An iterable of field-value pairs. Must not contain multiple field names
        when compared case-insensitively.

    :param kwargs:
        Additional field-value pairs to pass in to ``dict.update``.

    A ``dict`` like container for storing HTTP Headers.

    Field names are stored and compared case-insensitively in compliance with
    RFC 7230. Iteration provides the first case-sensitive key seen for each
    case-insensitive pair.

    Using ``__setitem__`` syntax overwrites fields that compare equal
    case-insensitively in order to maintain ``dict``'s api. For fields that
    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``
    in a loop.

    If multiple fields that are equal case-insensitively are passed to the
    constructor or ``.update``, the behavior is undefined and some will be
    lost.

    >>> headers = HTTPHeaderDict()
    >>> headers.add('Set-Cookie', 'foo=bar')
    >>> headers.add('set-cookie', 'baz=quxx')
    >>> headers['content-length'] = '7'
    >>> headers['SET-cookie']
    'foo=bar, baz=quxx'
    >>> headers['Content-Length']
    '7'
    """

    def __init__(self, headers=None, **kwargs):
        super(HTTPHeaderDict, self).__init__()
        self._container = OrderedDict()
        if headers is not None:
            if isinstance(headers, HTTPHeaderDict):
                self._copy_from(headers)
            else:
                self.extend(headers)
        if kwargs:
            self.extend(kwargs)

    def __setitem__(self, key, val):
        self._container[key.lower()] = [key, val]
        return self._container[key.lower()]

    def __getitem__(self, key):
        val = self._container[key.lower()]
        return ", ".join(val[1:])

    def __delitem__(self, key):
        del self._container[key.lower()]

    def __contains__(self, key):
        return key.lower() in self._container

    def __eq__(self, other):
        if not isinstance(other, Mapping) and not hasattr(other, "keys"):
            return False
        if not isinstance(other, type(self)):
            other = type(self)(other)
        return dict((k.lower(), v) for k, v in self.itermerged()) == dict(
            (k.lower(), v) for k, v in other.itermerged()
        )

    def __ne__(self, other):
        return not self.__eq__(other)

    if six.PY2:  # Python 2
        iterkeys = MutableMapping.iterkeys
        itervalues = MutableMapping.itervalues

    __marker = object()

    def __len__(self):
        return len(self._container)

    def __iter__(self):
        # Only provide the originally cased names
        for vals in self._container.values():
            yield vals[0]

    def pop(self, key, default=__marker):
        """D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised.
        """
        # Using the MutableMapping function directly fails due to the private marker.
        # Using ordinary dict.pop would expose the internal structures.
        # So let's reinvent the wheel.
        try:
            value = self[key]
        except KeyError:
            if default is self.__marker:
                raise
            return default
        else:
            del self[key]
            return value

    def discard(self, key):
        try:
            del self[key]
        except KeyError:
            pass

    def add(self, key, val):
        """Adds a (name, value) pair, doesn't overwrite the value if it already
        exists.

        >>> headers = HTTPHeaderDict(foo='bar')
        >>> headers.add('Foo', 'baz')
        >>> headers['foo']
        'bar, baz'
        """
        key_lower = key.lower()
        new_vals = [key, val]
        # Keep the common case aka no item present as fast as possible
        vals = self._container.setdefault(key_lower, new_vals)
        if new_vals is not vals:
            vals.append(val)

    def extend(self, *args, **kwargs):
        """Generic import function for any type of header-like object.
        Adapted version of MutableMapping.update in order to insert items
        with self.add instead of self.__setitem__
        """
        if len(args) > 1:
            raise TypeError(
                "extend() takes at most 1 positional "
                "arguments ({0} given)".format(len(args))
            )
        other = args[0] if len(args) >= 1 else ()

        if isinstance(other, HTTPHeaderDict):
            for key, val in other.iteritems():
                self.add(key, val)
        elif isinstance(other, Mapping):
            for key in other:
                self.add(key, other[key])
        elif hasattr(other, "keys"):
            for key in other.keys():
                self.add(key, other[key])
        else:
            for key, value in other:
                self.add(key, value)

        for key, value in kwargs.items():
            self.add(key, value)

    def getlist(self, key, default=__marker):
        """Returns a list of all the values for the named field. Returns an
        empty list if the key doesn't exist."""
        try:
            vals = self._container[key.lower()]
        except KeyError:
            if default is self.__marker:
                return []
            return default
        else:
            return vals[1:]

    def _prepare_for_method_change(self):
        """
        Remove content-specific header fields before changing the request
        method to GET or HEAD according to RFC 9110, Section 15.4.
        """
        content_specific_headers = [
            "Content-Encoding",
            "Content-Language",
            "Content-Location",
            "Content-Type",
            "Content-Length",
            "Digest",
            "Last-Modified",
        ]
        for header in content_specific_headers:
            self.discard(header)
        return self

    # Backwards compatibility for httplib
    getheaders = getlist
    getallmatchingheaders = getlist
    iget = getlist

    # Backwards compatibility for http.cookiejar
    get_all = getlist

    def __repr__(self):
        return "%s(%s)" % (type(self).__name__, dict(self.itermerged()))

    def _copy_from(self, other):
        for key in other:
            val = other.getlist(key)
            if isinstance(val, list):
                # Don't need to convert tuples
                val = list(val)
            self._container[key.lower()] = [key] + val

    def copy(self):
        clone = type(self)()
        clone._copy_from(self)
        return clone

    def iteritems(self):
        """Iterate over all header lines, including duplicate ones."""
        for key in self:
            vals = self._container[key.lower()]
            for val in vals[1:]:
                yield vals[0], val

    def itermerged(self):
        """Iterate over all headers, merging duplicate ones together."""
        for key in self:
            val = self._container[key.lower()]
            yield val[0], ", ".join(val[1:])

    def items(self):
        return list(self.iteritems())

    @classmethod
    def from_httplib(cls, message):  # Python 2
        """Read headers from a Python 2 httplib message object."""
        # python2.7 does not expose a proper API for exporting multiheaders
        # efficiently. This function re-reads raw lines from the message
        # object and extracts the multiheaders properly.
        obs_fold_continued_leaders = (" ", "\t")
        headers = []

        for line in message.headers:
            if line.startswith(obs_fold_continued_leaders):
                if not headers:
                    # We received a header line that starts with OWS as described
                    # in RFC-7230 S3.2.4. This indicates a multiline header, but
                    # there exists no previous header to which we can attach it.
                    raise InvalidHeader(
                        "Header continuation with no previous header: %s" % line
                    )
                else:
                    key, value = headers[-1]
                    headers[-1] = (key, value + " " + line.strip())
                    continue

            key, value = line.split(":", 1)
            headers.append((key, value.strip()))

        return cls(headers)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/_version.py
# ========================================================
# This file is protected via CODEOWNERS
__version__ = "1.26.17"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py
# ========================================================
from __future__ import absolute_import

import io
import logging
import sys
import warnings
import zlib
from contextlib import contextmanager
from socket import error as SocketError
from socket import timeout as SocketTimeout

brotli = None

from . import util
from ._collections import HTTPHeaderDict
from .connection import BaseSSLError, HTTPException
from .exceptions import (
    BodyNotHttplibCompatible,
    DecodeError,
    HTTPError,
    IncompleteRead,
    InvalidChunkLength,
    InvalidHeader,
    ProtocolError,
    ReadTimeoutError,
    ResponseNotChunked,
    SSLError,
)
from .packages import six
from .util.response import is_fp_closed, is_response_to_head

log = logging.getLogger(__name__)


class DeflateDecoder(object):
    def __init__(self):
        self._first_try = True
        self._data = b""
        self._obj = zlib.decompressobj()

    def __getattr__(self, name):
        return getattr(self._obj, name)

    def decompress(self, data):
        if not data:
            return data

        if not self._first_try:
            return self._obj.decompress(data)

        self._data += data
        try:
            decompressed = self._obj.decompress(data)
            if decompressed:
                self._first_try = False
                self._data = None
            return decompressed
        except zlib.error:
            self._first_try = False
            self._obj = zlib.decompressobj(-zlib.MAX_WBITS)
            try:
                return self.decompress(self._data)
            finally:
                self._data = None


class GzipDecoderState(object):

    FIRST_MEMBER = 0
    OTHER_MEMBERS = 1
    SWALLOW_DATA = 2


class GzipDecoder(object):
    def __init__(self):
        self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
        self._state = GzipDecoderState.FIRST_MEMBER

    def __getattr__(self, name):
        return getattr(self._obj, name)

    def decompress(self, data):
        ret = bytearray()
        if self._state == GzipDecoderState.SWALLOW_DATA or not data:
            return bytes(ret)
        while True:
            try:
                ret += self._obj.decompress(data)
            except zlib.error:
                previous_state = self._state
                # Ignore data after the first error
                self._state = GzipDecoderState.SWALLOW_DATA
                if previous_state == GzipDecoderState.OTHER_MEMBERS:
                    # Allow trailing garbage acceptable in other gzip clients
                    return bytes(ret)
                raise
            data = self._obj.unused_data
            if not data:
                return bytes(ret)
            self._state = GzipDecoderState.OTHER_MEMBERS
            self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)


if brotli is not None:

    class BrotliDecoder(object):
        # Supports both 'brotlipy' and 'Brotli' packages
        # since they share an import name. The top branches
        # are for 'brotlipy' and bottom branches for 'Brotli'
        def __init__(self):
            self._obj = brotli.Decompressor()
            if hasattr(self._obj, "decompress"):
                self.decompress = self._obj.decompress
            else:
                self.decompress = self._obj.process

        def flush(self):
            if hasattr(self._obj, "flush"):
                return self._obj.flush()
            return b""


class MultiDecoder(object):
    """
    From RFC7231:
        If one or more encodings have been applied to a representation, the
        sender that applied the encodings MUST generate a Content-Encoding
        header field that lists the content codings in the order in which
        they were applied.
    """

    def __init__(self, modes):
        self._decoders = [_get_decoder(m.strip()) for m in modes.split(",")]

    def flush(self):
        return self._decoders[0].flush()

    def decompress(self, data):
        for d in reversed(self._decoders):
            data = d.decompress(data)
        return data


def _get_decoder(mode):
    if "," in mode:
        return MultiDecoder(mode)

    if mode == "gzip":
        return GzipDecoder()

    if brotli is not None and mode == "br":
        return BrotliDecoder()

    return DeflateDecoder()


class HTTPResponse(io.IOBase):
    """
    HTTP Response container.

    Backwards-compatible with :class:`http.client.HTTPResponse` but the response ``body`` is
    loaded and decoded on-demand when the ``data`` property is accessed.  This
    class is also compatible with the Python standard library's :mod:`io`
    module, and can hence be treated as a readable object in the context of that
    framework.

    Extra parameters for behaviour not present in :class:`http.client.HTTPResponse`:

    :param preload_content:
        If True, the response's body will be preloaded during construction.

    :param decode_content:
        If True, will attempt to decode the body based on the
        'content-encoding' header.

    :param original_response:
        When this HTTPResponse wrapper is generated from an :class:`http.client.HTTPResponse`
        object, it's convenient to include the original for debug purposes. It's
        otherwise unused.

    :param retries:
        The retries contains the last :class:`~urllib3.util.retry.Retry` that
        was used during the request.

    :param enforce_content_length:
        Enforce content length checking. Body returned by server must match
        value of Content-Length header, if present. Otherwise, raise error.
    """

    CONTENT_DECODERS = ["gzip", "deflate"]
    if brotli is not None:
        CONTENT_DECODERS += ["br"]
    REDIRECT_STATUSES = [301, 302, 303, 307, 308]

    def __init__(
        self,
        body="",
        headers=None,
        status=0,
        version=0,
        reason=None,
        strict=0,
        preload_content=True,
        decode_content=True,
        original_response=None,
        pool=None,
        connection=None,
        msg=None,
        retries=None,
        enforce_content_length=False,
        request_method=None,
        request_url=None,
        auto_close=True,
    ):

        if isinstance(headers, HTTPHeaderDict):
            self.headers = headers
        else:
            self.headers = HTTPHeaderDict(headers)
        self.status = status
        self.version = version
        self.reason = reason
        self.strict = strict
        self.decode_content = decode_content
        self.retries = retries
        self.enforce_content_length = enforce_content_length
        self.auto_close = auto_close

        self._decoder = None
        self._body = None
        self._fp = None
        self._original_response = original_response
        self._fp_bytes_read = 0
        self.msg = msg
        self._request_url = request_url

        if body and isinstance(body, (six.string_types, bytes)):
            self._body = body

        self._pool = pool
        self._connection = connection

        if hasattr(body, "read"):
            self._fp = body

        # Are we using the chunked-style of transfer encoding?
        self.chunked = False
        self.chunk_left = None
        tr_enc = self.headers.get("transfer-encoding", "").lower()
        # Don't incur the penalty of creating a list and then discarding it
        encodings = (enc.strip() for enc in tr_enc.split(","))
        if "chunked" in encodings:
            self.chunked = True

        # Determine length of response
        self.length_remaining = self._init_length(request_method)

        # If requested, preload the body.
        if preload_content and not self._body:
            self._body = self.read(decode_content=decode_content)

    def get_redirect_location(self):
        """
        Should we redirect and where to?

        :returns: Truthy redirect location string if we got a redirect status
            code and valid location. ``None`` if redirect status and no
            location. ``False`` if not a redirect status code.
        """
        if self.status in self.REDIRECT_STATUSES:
            return self.headers.get("location")

        return False

    def release_conn(self):
        if not self._pool or not self._connection:
            return

        self._pool._put_conn(self._connection)
        self._connection = None

    def drain_conn(self):
        """
        Read and discard any remaining HTTP response data in the response connection.

        Unread data in the HTTPResponse connection blocks the connection from being released back to the pool.
        """
        try:
            self.read()
        except (HTTPError, SocketError, BaseSSLError, HTTPException):
            pass

    @property
    def data(self):
        # For backwards-compat with earlier urllib3 0.4 and earlier.
        if self._body:
            return self._body

        if self._fp:
            return self.read(cache_content=True)

    @property
    def connection(self):
        return self._connection

    def isclosed(self):
        return is_fp_closed(self._fp)

    def tell(self):
        """
        Obtain the number of bytes pulled over the wire so far. May differ from
        the amount of content returned by :meth:``urllib3.response.HTTPResponse.read``
        if bytes are encoded on the wire (e.g, compressed).
        """
        return self._fp_bytes_read

    def _init_length(self, request_method):
        """
        Set initial length value for Response content if available.
        """
        length = self.headers.get("content-length")

        if length is not None:
            if self.chunked:
                # This Response will fail with an IncompleteRead if it can't be
                # received as chunked. This method falls back to attempt reading
                # the response before raising an exception.
                log.warning(
                    "Received response with both Content-Length and "
                    "Transfer-Encoding set. This is expressly forbidden "
                    "by RFC 7230 sec 3.3.2. Ignoring Content-Length and "
                    "attempting to process response as Transfer-Encoding: "
                    "chunked."
                )
                return None

            try:
                # RFC 7230 section 3.3.2 specifies multiple content lengths can
                # be sent in a single Content-Length header
                # (e.g. Content-Length: 42, 42). This line ensures the values
                # are all valid ints and that as long as the `set` length is 1,
                # all values are the same. Otherwise, the header is invalid.
                lengths = set([int(val) for val in length.split(",")])
                if len(lengths) > 1:
                    raise InvalidHeader(
                        "Content-Length contained multiple "
                        "unmatching values (%s)" % length
                    )
                length = lengths.pop()
            except ValueError:
                length = None
            else:
                if length < 0:
                    length = None

        # Convert status to int for comparison
        # In some cases, httplib returns a status of "_UNKNOWN"
        try:
            status = int(self.status)
        except ValueError:
            status = 0

        # Check for responses that shouldn't include a body
        if status in (204, 304) or 100 <= status < 200 or request_method == "HEAD":
            length = 0

        return length

    def _init_decoder(self):
        """
        Set-up the _decoder attribute if necessary.
        """
        # Note: content-encoding value should be case-insensitive, per RFC 7230
        # Section 3.2
        content_encoding = self.headers.get("content-encoding", "").lower()
        if self._decoder is None:
            if content_encoding in self.CONTENT_DECODERS:
                self._decoder = _get_decoder(content_encoding)
            elif "," in content_encoding:
                encodings = [
                    e.strip()
                    for e in content_encoding.split(",")
                    if e.strip() in self.CONTENT_DECODERS
                ]
                if len(encodings):
                    self._decoder = _get_decoder(content_encoding)

    DECODER_ERROR_CLASSES = (IOError, zlib.error)
    if brotli is not None:
        DECODER_ERROR_CLASSES += (brotli.error,)

    def _decode(self, data, decode_content, flush_decoder):
        """
        Decode the data passed in and potentially flush the decoder.
        """
        if not decode_content:
            return data

        try:
            if self._decoder:
                data = self._decoder.decompress(data)
        except self.DECODER_ERROR_CLASSES as e:
            content_encoding = self.headers.get("content-encoding", "").lower()
            raise DecodeError(
                "Received response with content-encoding: %s, but "
                "failed to decode it." % content_encoding,
                e,
            )
        if flush_decoder:
            data += self._flush_decoder()

        return data

    def _flush_decoder(self):
        """
        Flushes the decoder. Should only be called if the decoder is actually
        being used.
        """
        if self._decoder:
            buf = self._decoder.decompress(b"")
            return buf + self._decoder.flush()

        return b""

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.

        On exit, release the connection back to the pool.
        """
        clean_exit = False

        try:
            try:
                yield

            except SocketTimeout:
                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
                # there is yet no clean way to get at it from this context.
                raise ReadTimeoutError(self._pool, None, "Read timed out.")

            except BaseSSLError as e:
                # FIXME: Is there a better way to differentiate between SSLErrors?
                if "read operation timed out" not in str(e):
                    # SSL errors related to framing/MAC get wrapped and reraised here
                    raise SSLError(e)

                raise ReadTimeoutError(self._pool, None, "Read timed out.")

            except (HTTPException, SocketError) as e:
                # This includes IncompleteRead.
                raise ProtocolError("Connection broken: %r" % e, e)

            # If no exception is thrown, we should avoid cleaning up
            # unnecessarily.
            clean_exit = True
        finally:
            # If we didn't terminate cleanly, we need to throw away our
            # connection.
            if not clean_exit:
                # The response may not be closed but we're not going to use it
                # anymore so close it now to ensure that the connection is
                # released back to the pool.
                if self._original_response:
                    self._original_response.close()

                # Closing the response may not actually be sufficient to close
                # everything, so if we have a hold of the connection close that
                # too.
                if self._connection:
                    self._connection.close()

            # If we hold the original response but it's closed now, we should
            # return the connection back to the pool.
            if self._original_response and self._original_response.isclosed():
                self.release_conn()

    def _fp_read(self, amt):
        """
        Read a response with the thought that reading the number of bytes
        larger than can fit in a 32-bit int at a time via SSL in some
        known cases leads to an overflow error that has to be prevented
        if `amt` or `self.length_remaining` indicate that a problem may
        happen.

        The known cases:
          * 3.8 <= CPython < 3.9.7 because of a bug
            https://github.com/urllib3/urllib3/issues/2513#issuecomment-1152559900.
          * urllib3 injected with pyOpenSSL-backed SSL-support.
          * CPython < 3.10 only when `amt` does not fit 32-bit int.
        """
        assert self._fp
        c_int_max = 2 ** 31 - 1
        if (
            (
                (amt and amt > c_int_max)
                or (self.length_remaining and self.length_remaining > c_int_max)
            )
            and not util.IS_SECURETRANSPORT
            and (util.IS_PYOPENSSL or sys.version_info < (3, 10))
        ):
            buffer = io.BytesIO()
            # Besides `max_chunk_amt` being a maximum chunk size, it
            # affects memory overhead of reading a response by this
            # method in CPython.
            # `c_int_max` equal to 2 GiB - 1 byte is the actual maximum
            # chunk size that does not lead to an overflow error, but
            # 256 MiB is a compromise.
            max_chunk_amt = 2 ** 28
            while amt is None or amt != 0:
                if amt is not None:
                    chunk_amt = min(amt, max_chunk_amt)
                    amt -= chunk_amt
                else:
                    chunk_amt = max_chunk_amt
                data = self._fp.read(chunk_amt)
                if not data:
                    break
                buffer.write(data)
                del data  # to reduce peak memory usage by `max_chunk_amt`.
            return buffer.getvalue()
        else:
            # StringIO doesn't like amt=None
            return self._fp.read(amt) if amt is not None else self._fp.read()

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.

        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.

        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content

        if self._fp is None:
            return

        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)

        with self._error_catcher():
            data = self._fp_read(amt) if not fp_closed else b""
            if amt is None:
                flush_decoder = True
            else:
                cache_content = False
                if (
                    amt != 0 and not data
                ):  # Platform-specific: Buggy versions of Python.
                    # Close the connection when no data is returned
                    #
                    # This is redundant to what httplib/http.client _should_
                    # already do.  However, versions of python released before
                    # December 15, 2012 (http://bugs.python.org/issue16298) do
                    # not properly close the connection in all cases. There is
                    # no harm in redundantly calling close.
                    self._fp.close()
                    flush_decoder = True
                    if self.enforce_content_length and self.length_remaining not in (
                        0,
                        None,
                    ):
                        # This is an edge case that httplib failed to cover due
                        # to concerns of backward compatibility. We're
                        # addressing it here to make sure IncompleteRead is
                        # raised during streaming, so all calls with incorrect
                        # Content-Length are caught.
                        raise IncompleteRead(self._fp_bytes_read, self.length_remaining)

        if data:
            self._fp_bytes_read += len(data)
            if self.length_remaining is not None:
                self.length_remaining -= len(data)

            data = self._decode(data, decode_content, flush_decoder)

            if cache_content:
                self._body = data

        return data

    def stream(self, amt=2 ** 16, decode_content=None):
        """
        A generator wrapper for the read() method. A call will block until
        ``amt`` bytes have been read from the connection or until the
        connection is closed.

        :param amt:
            How much of the content to read. The generator will return up to
            much data per iteration, but may return less. This is particularly
            likely when using compressed data. However, the empty string will
            never be returned.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        """
        if self.chunked and self.supports_chunked_reads():
            for line in self.read_chunked(amt, decode_content=decode_content):
                yield line
        else:
            while not is_fp_closed(self._fp):
                data = self.read(amt=amt, decode_content=decode_content)

                if data:
                    yield data

    @classmethod
    def from_httplib(ResponseCls, r, **response_kw):
        """
        Given an :class:`http.client.HTTPResponse` instance ``r``, return a
        corresponding :class:`urllib3.response.HTTPResponse` object.

        Remaining parameters are passed to the HTTPResponse constructor, along
        with ``original_response=r``.
        """
        headers = r.msg

        if not isinstance(headers, HTTPHeaderDict):
            if six.PY2:
                # Python 2.7
                headers = HTTPHeaderDict.from_httplib(headers)
            else:
                headers = HTTPHeaderDict(headers.items())

        # HTTPResponse objects in Python 3 don't have a .strict attribute
        strict = getattr(r, "strict", 0)
        resp = ResponseCls(
            body=r,
            headers=headers,
            status=r.status,
            version=r.version,
            reason=r.reason,
            strict=strict,
            original_response=r,
            **response_kw
        )
        return resp

    # Backwards-compatibility methods for http.client.HTTPResponse
    def getheaders(self):
        warnings.warn(
            "HTTPResponse.getheaders() is deprecated and will be removed "
            "in urllib3 v2.1.0. Instead access HTTPResponse.headers directly.",
            category=DeprecationWarning,
            stacklevel=2,
        )
        return self.headers

    def getheader(self, name, default=None):
        warnings.warn(
            "HTTPResponse.getheader() is deprecated and will be removed "
            "in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).",
            category=DeprecationWarning,
            stacklevel=2,
        )
        return self.headers.get(name, default)

    # Backwards compatibility for http.cookiejar
    def info(self):
        return self.headers

    # Overrides from io.IOBase
    def close(self):
        if not self.closed:
            self._fp.close()

        if self._connection:
            self._connection.close()

        if not self.auto_close:
            io.IOBase.close(self)

    @property
    def closed(self):
        if not self.auto_close:
            return io.IOBase.closed.__get__(self)
        elif self._fp is None:
            return True
        elif hasattr(self._fp, "isclosed"):
            return self._fp.isclosed()
        elif hasattr(self._fp, "closed"):
            return self._fp.closed
        else:
            return True

    def fileno(self):
        if self._fp is None:
            raise IOError("HTTPResponse has no file to get a fileno from")
        elif hasattr(self._fp, "fileno"):
            return self._fp.fileno()
        else:
            raise IOError(
                "The file-like object this HTTPResponse is wrapped "
                "around has no file descriptor"
            )

    def flush(self):
        if (
            self._fp is not None
            and hasattr(self._fp, "flush")
            and not getattr(self._fp, "closed", False)
        ):
            return self._fp.flush()

    def readable(self):
        # This method is required for `io` module compatibility.
        return True

    def readinto(self, b):
        # This method is required for `io` module compatibility.
        temp = self.read(len(b))
        if len(temp) == 0:
            return 0
        else:
            b[: len(temp)] = temp
            return len(temp)

    def supports_chunked_reads(self):
        """
        Checks if the underlying file-like object looks like a
        :class:`http.client.HTTPResponse` object. We do this by testing for
        the fp attribute. If it is present we assume it returns raw chunks as
        processed by read_chunked().
        """
        return hasattr(self._fp, "fp")

    def _update_chunk_length(self):
        # First, we'll figure out length of a chunk and then
        # we'll try to read it from socket.
        if self.chunk_left is not None:
            return
        line = self._fp.fp.readline()
        line = line.split(b";", 1)[0]
        try:
            self.chunk_left = int(line, 16)
        except ValueError:
            # Invalid chunked protocol response, abort.
            self.close()
            raise InvalidChunkLength(self, line)

    def _handle_chunk(self, amt):
        returned_chunk = None
        if amt is None:
            chunk = self._fp._safe_read(self.chunk_left)
            returned_chunk = chunk
            self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
            self.chunk_left = None
        elif amt < self.chunk_left:
            value = self._fp._safe_read(amt)
            self.chunk_left = self.chunk_left - amt
            returned_chunk = value
        elif amt == self.chunk_left:
            value = self._fp._safe_read(amt)
            self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
            self.chunk_left = None
            returned_chunk = value
        else:  # amt > self.chunk_left
            returned_chunk = self._fp._safe_read(self.chunk_left)
            self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
            self.chunk_left = None
        return returned_chunk

    def read_chunked(self, amt=None, decode_content=None):
        """
        Similar to :meth:`HTTPResponse.read`, but with an additional
        parameter: ``decode_content``.

        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.

        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        """
        self._init_decoder()
        # FIXME: Rewrite this method and make it a class with a better structured logic.
        if not self.chunked:
            raise ResponseNotChunked(
                "Response is not chunked. "
                "Header 'transfer-encoding: chunked' is missing."
            )
        if not self.supports_chunked_reads():
            raise BodyNotHttplibCompatible(
                "Body should be http.client.HTTPResponse like. "
                "It should have have an fp attribute which returns raw chunks."
            )

        with self._error_catcher():
            # Don't bother reading the body of a HEAD request.
            if self._original_response and is_response_to_head(self._original_response):
                self._original_response.close()
                return

            # If a response is already read and closed
            # then return immediately.
            if self._fp.fp is None:
                return

            while True:
                self._update_chunk_length()
                if self.chunk_left == 0:
                    break
                chunk = self._handle_chunk(amt)
                decoded = self._decode(
                    chunk, decode_content=decode_content, flush_decoder=False
                )
                if decoded:
                    yield decoded

            if decode_content:
                # On CPython and PyPy, we should never need to flush the
                # decoder. However, on Jython we *might* need to, so
                # lets defensively do it anyway.
                decoded = self._flush_decoder()
                if decoded:  # Platform-specific: Jython.
                    yield decoded

            # Chunk content ends with \r\n: discard it.
            while True:
                line = self._fp.fp.readline()
                if not line:
                    # Some sites may not end with '\r\n'.
                    break
                if line == b"\r\n":
                    break

            # We read everything; close the "file".
            if self._original_response:
                self._original_response.close()

    def geturl(self):
        """
        Returns the URL that was the source of this response.
        If the request that generated this response redirected, this method
        will return the final redirect location.
        """
        if self.retries is not None and len(self.retries.history):
            return self.retries.history[-1].redirect_location
        else:
            return self._request_url

    def __iter__(self):
        buffer = []
        for chunk in self.stream(decode_content=True):
            if b"\n" in chunk:
                chunk = chunk.split(b"\n")
                yield b"".join(buffer) + chunk[0] + b"\n"
                for x in chunk[1:-1]:
                    yield x + b"\n"
                if chunk[-1]:
                    buffer = [chunk[-1]]
                else:
                    buffer = []
            else:
                buffer.append(chunk)
        if buffer:
            yield b"".join(buffer)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/fields.py
# ========================================================
from __future__ import absolute_import

import email.utils
import mimetypes
import re

from .packages import six


def guess_content_type(filename, default="application/octet-stream"):
    """
    Guess the "Content-Type" of a file.

    :param filename:
        The filename to guess the "Content-Type" of using :mod:`mimetypes`.
    :param default:
        If no "Content-Type" can be guessed, default to `default`.
    """
    if filename:
        return mimetypes.guess_type(filename)[0] or default
    return default


def format_header_param_rfc2231(name, value):
    """
    Helper function to format and quote a single header parameter using the
    strategy defined in RFC 2231.

    Particularly useful for header parameters which might contain
    non-ASCII values, like file names. This follows
    `RFC 2388 Section 4.4 <https://tools.ietf.org/html/rfc2388#section-4.4>`_.

    :param name:
        The name of the parameter, a string expected to be ASCII only.
    :param value:
        The value of the parameter, provided as ``bytes`` or `str``.
    :ret:
        An RFC-2231-formatted unicode string.
    """
    if isinstance(value, six.binary_type):
        value = value.decode("utf-8")

    if not any(ch in value for ch in '"\\\r\n'):
        result = u'%s="%s"' % (name, value)
        try:
            result.encode("ascii")
        except (UnicodeEncodeError, UnicodeDecodeError):
            pass
        else:
            return result

    if six.PY2:  # Python 2:
        value = value.encode("utf-8")

    # encode_rfc2231 accepts an encoded string and returns an ascii-encoded
    # string in Python 2 but accepts and returns unicode strings in Python 3
    value = email.utils.encode_rfc2231(value, "utf-8")
    value = "%s*=%s" % (name, value)

    if six.PY2:  # Python 2:
        value = value.decode("utf-8")

    return value


_HTML5_REPLACEMENTS = {
    u"\u0022": u"%22",
    # Replace "\" with "\\".
    u"\u005C": u"\u005C\u005C",
}

# All control characters from 0x00 to 0x1F *except* 0x1B.
_HTML5_REPLACEMENTS.update(
    {
        six.unichr(cc): u"%{:02X}".format(cc)
        for cc in range(0x00, 0x1F + 1)
        if cc not in (0x1B,)
    }
)


def _replace_multiple(value, needles_and_replacements):
    def replacer(match):
        return needles_and_replacements[match.group(0)]

    pattern = re.compile(
        r"|".join([re.escape(needle) for needle in needles_and_replacements.keys()])
    )

    result = pattern.sub(replacer, value)

    return result


def format_header_param_html5(name, value):
    """
    Helper function to format and quote a single header parameter using the
    HTML5 strategy.

    Particularly useful for header parameters which might contain
    non-ASCII values, like file names. This follows the `HTML5 Working Draft
    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.

    .. _HTML5 Working Draft Section 4.10.22.7:
        https://w3c.github.io/html/sec-forms.html#multipart-form-data

    :param name:
        The name of the parameter, a string expected to be ASCII only.
    :param value:
        The value of the parameter, provided as ``bytes`` or `str``.
    :ret:
        A unicode string, stripped of troublesome characters.
    """
    if isinstance(value, six.binary_type):
        value = value.decode("utf-8")

    value = _replace_multiple(value, _HTML5_REPLACEMENTS)

    return u'%s="%s"' % (name, value)


# For backwards-compatibility.
format_header_param = format_header_param_html5


class RequestField(object):
    """
    A data container for request body parameters.

    :param name:
        The name of this request field. Must be unicode.
    :param data:
        The data/value body.
    :param filename:
        An optional filename of the request field. Must be unicode.
    :param headers:
        An optional dict-like object of headers to initially use for the field.
    :param header_formatter:
        An optional callable that is used to encode and format the headers. By
        default, this is :func:`format_header_param_html5`.
    """

    def __init__(
        self,
        name,
        data,
        filename=None,
        headers=None,
        header_formatter=format_header_param_html5,
    ):
        self._name = name
        self._filename = filename
        self.data = data
        self.headers = {}
        if headers:
            self.headers = dict(headers)
        self.header_formatter = header_formatter

    @classmethod
    def from_tuples(cls, fieldname, value, header_formatter=format_header_param_html5):
        """
        A :class:`~urllib3.fields.RequestField` factory from old-style tuple parameters.

        Supports constructing :class:`~urllib3.fields.RequestField` from
        parameter of key/value strings AND key/filetuple. A filetuple is a
        (filename, data, MIME type) tuple where the MIME type is optional.
        For example::

            'foo': 'bar',
            'fakefile': ('foofile.txt', 'contents of foofile'),
            'realfile': ('barfile.txt', open('realfile').read()),
            'typedfile': ('bazfile.bin', open('bazfile').read(), 'image/jpeg'),
            'nonamefile': 'contents of nonamefile field',

        Field names and filenames must be unicode.
        """
        if isinstance(value, tuple):
            if len(value) == 3:
                filename, data, content_type = value
            else:
                filename, data = value
                content_type = guess_content_type(filename)
        else:
            filename = None
            content_type = None
            data = value

        request_param = cls(
            fieldname, data, filename=filename, header_formatter=header_formatter
        )
        request_param.make_multipart(content_type=content_type)

        return request_param

    def _render_part(self, name, value):
        """
        Overridable helper function to format a single header parameter. By
        default, this calls ``self.header_formatter``.

        :param name:
            The name of the parameter, a string expected to be ASCII only.
        :param value:
            The value of the parameter, provided as a unicode string.
        """

        return self.header_formatter(name, value)

    def _render_parts(self, header_parts):
        """
        Helper function to format and quote a single header.

        Useful for single headers that are composed of multiple items. E.g.,
        'Content-Disposition' fields.

        :param header_parts:
            A sequence of (k, v) tuples or a :class:`dict` of (k, v) to format
            as `k1="v1"; k2="v2"; ...`.
        """
        parts = []
        iterable = header_parts
        if isinstance(header_parts, dict):
            iterable = header_parts.items()

        for name, value in iterable:
            if value is not None:
                parts.append(self._render_part(name, value))

        return u"; ".join(parts)

    def render_headers(self):
        """
        Renders the headers for this request field.
        """
        lines = []

        sort_keys = ["Content-Disposition", "Content-Type", "Content-Location"]
        for sort_key in sort_keys:
            if self.headers.get(sort_key, False):
                lines.append(u"%s: %s" % (sort_key, self.headers[sort_key]))

        for header_name, header_value in self.headers.items():
            if header_name not in sort_keys:
                if header_value:
                    lines.append(u"%s: %s" % (header_name, header_value))

        lines.append(u"\r\n")
        return u"\r\n".join(lines)

    def make_multipart(
        self, content_disposition=None, content_type=None, content_location=None
    ):
        """
        Makes this request field into a multipart request field.

        This method overrides "Content-Disposition", "Content-Type" and
        "Content-Location" headers to the request parameter.

        :param content_type:
            The 'Content-Type' of the request body.
        :param content_location:
            The 'Content-Location' of the request body.

        """
        self.headers["Content-Disposition"] = content_disposition or u"form-data"
        self.headers["Content-Disposition"] += u"; ".join(
            [
                u"",
                self._render_parts(
                    ((u"name", self._name), (u"filename", self._filename))
                ),
            ]
        )
        self.headers["Content-Type"] = content_type
        self.headers["Content-Location"] = content_location


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/connectionpool.py
# ========================================================
from __future__ import absolute_import

import errno
import logging
import re
import socket
import sys
import warnings
from socket import error as SocketError
from socket import timeout as SocketTimeout

from ._collections import HTTPHeaderDict
from .connection import (
    BaseSSLError,
    BrokenPipeError,
    DummyConnection,
    HTTPConnection,
    HTTPException,
    HTTPSConnection,
    VerifiedHTTPSConnection,
    port_by_scheme,
)
from .exceptions import (
    ClosedPoolError,
    EmptyPoolError,
    HeaderParsingError,
    HostChangedError,
    InsecureRequestWarning,
    LocationValueError,
    MaxRetryError,
    NewConnectionError,
    ProtocolError,
    ProxyError,
    ReadTimeoutError,
    SSLError,
    TimeoutError,
)
from .packages import six
from .packages.six.moves import queue
from .request import RequestMethods
from .response import HTTPResponse
from .util.connection import is_connection_dropped
from .util.proxy import connection_requires_http_tunnel
from .util.queue import LifoQueue
from .util.request import set_file_position
from .util.response import assert_header_parsing
from .util.retry import Retry
from .util.ssl_match_hostname import CertificateError
from .util.timeout import Timeout
from .util.url import Url, _encode_target
from .util.url import _normalize_host as normalize_host
from .util.url import get_host, parse_url

try:  # Platform-specific: Python 3
    import weakref

    weakref_finalize = weakref.finalize
except AttributeError:  # Platform-specific: Python 2
    from .packages.backports.weakref_finalize import weakref_finalize

xrange = six.moves.xrange

log = logging.getLogger(__name__)

_Default = object()


# Pool objects
class ConnectionPool(object):
    """
    Base class for all connection pools, such as
    :class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.

    .. note::
       ConnectionPool.urlopen() does not normalize or percent-encode target URIs
       which is useful if your target server doesn't support percent-encoded
       target URIs.
    """

    scheme = None
    QueueCls = LifoQueue

    def __init__(self, host, port=None):
        if not host:
            raise LocationValueError("No host specified.")

        self.host = _normalize_host(host, scheme=self.scheme)
        self._proxy_host = host.lower()
        self.port = port

    def __str__(self):
        return "%s(host=%r, port=%r)" % (type(self).__name__, self.host, self.port)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
        # Return False to re-raise any potential exceptions
        return False

    def close(self):
        """
        Close all pooled connections and disable the pool.
        """
        pass


# This is taken from http://hg.python.org/cpython/file/7aaba721ebc0/Lib/socket.py#l252
_blocking_errnos = {errno.EAGAIN, errno.EWOULDBLOCK}


class HTTPConnectionPool(ConnectionPool, RequestMethods):
    """
    Thread-safe connection pool for one host.

    :param host:
        Host used for this HTTP Connection (e.g. "localhost"), passed into
        :class:`http.client.HTTPConnection`.

    :param port:
        Port used for this HTTP Connection (None is equivalent to 80), passed
        into :class:`http.client.HTTPConnection`.

    :param strict:
        Causes BadStatusLine to be raised if the status line can't be parsed
        as a valid HTTP/1.0 or 1.1 status line, passed into
        :class:`http.client.HTTPConnection`.

        .. note::
           Only works in Python 2. This parameter is ignored in Python 3.

    :param timeout:
        Socket timeout in seconds for each individual connection. This can
        be a float or integer, which sets the timeout for the HTTP request,
        or an instance of :class:`urllib3.util.Timeout` which gives you more
        fine-grained control over request timeouts. After the constructor has
        been parsed, this is always a `urllib3.util.Timeout` object.

    :param maxsize:
        Number of connections to save that can be reused. More than 1 is useful
        in multithreaded situations. If ``block`` is set to False, more
        connections will be created but they will not be saved once they've
        been used.

    :param block:
        If set to True, no more than ``maxsize`` connections will be used at
        a time. When no free connections are available, the call will block
        until a connection has been released. This is a useful side effect for
        particular multithreaded situations where one does not want to use more
        than maxsize connections per host to prevent flooding.

    :param headers:
        Headers to include with all requests, unless other headers are given
        explicitly.

    :param retries:
        Retry configuration to use by default with requests in this pool.

    :param _proxy:
        Parsed proxy URL, should not be used directly, instead, see
        :class:`urllib3.ProxyManager`

    :param _proxy_headers:
        A dictionary with proxy headers, should not be used directly,
        instead, see :class:`urllib3.ProxyManager`

    :param \\**conn_kw:
        Additional parameters are used to create fresh :class:`urllib3.connection.HTTPConnection`,
        :class:`urllib3.connection.HTTPSConnection` instances.
    """

    scheme = "http"
    ConnectionCls = HTTPConnection
    ResponseCls = HTTPResponse

    def __init__(
        self,
        host,
        port=None,
        strict=False,
        timeout=Timeout.DEFAULT_TIMEOUT,
        maxsize=1,
        block=False,
        headers=None,
        retries=None,
        _proxy=None,
        _proxy_headers=None,
        _proxy_config=None,
        **conn_kw
    ):
        ConnectionPool.__init__(self, host, port)
        RequestMethods.__init__(self, headers)

        self.strict = strict

        if not isinstance(timeout, Timeout):
            timeout = Timeout.from_float(timeout)

        if retries is None:
            retries = Retry.DEFAULT

        self.timeout = timeout
        self.retries = retries

        self.pool = self.QueueCls(maxsize)
        self.block = block

        self.proxy = _proxy
        self.proxy_headers = _proxy_headers or {}
        self.proxy_config = _proxy_config

        # Fill the queue up so that doing get() on it will block properly
        for _ in xrange(maxsize):
            self.pool.put(None)

        # These are mostly for testing and debugging purposes.
        self.num_connections = 0
        self.num_requests = 0
        self.conn_kw = conn_kw

        if self.proxy:
            # Enable Nagle's algorithm for proxies, to avoid packet fragmentation.
            # We cannot know if the user has added default socket options, so we cannot replace the
            # list.
            self.conn_kw.setdefault("socket_options", [])

            self.conn_kw["proxy"] = self.proxy
            self.conn_kw["proxy_config"] = self.proxy_config

        # Do not pass 'self' as callback to 'finalize'.
        # Then the 'finalize' would keep an endless living (leak) to self.
        # By just passing a reference to the pool allows the garbage collector
        # to free self if nobody else has a reference to it.
        pool = self.pool

        # Close all the HTTPConnections in the pool before the
        # HTTPConnectionPool object is garbage collected.
        weakref_finalize(self, _close_pool_connections, pool)

    def _new_conn(self):
        """
        Return a fresh :class:`HTTPConnection`.
        """
        self.num_connections += 1
        log.debug(
            "Starting new HTTP connection (%d): %s:%s",
            self.num_connections,
            self.host,
            self.port or "80",
        )

        conn = self.ConnectionCls(
            host=self.host,
            port=self.port,
            timeout=self.timeout.connect_timeout,
            strict=self.strict,
            **self.conn_kw
        )
        return conn

    def _get_conn(self, timeout=None):
        """
        Get a connection. Will return a pooled connection if one is available.

        If no connections are available and :prop:`.block` is ``False``, then a
        fresh connection is returned.

        :param timeout:
            Seconds to wait before giving up and raising
            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and
            :prop:`.block` is ``True``.
        """
        conn = None
        try:
            conn = self.pool.get(block=self.block, timeout=timeout)

        except AttributeError:  # self.pool is None
            raise ClosedPoolError(self, "Pool is closed.")

        except queue.Empty:
            if self.block:
                raise EmptyPoolError(
                    self,
                    "Pool reached maximum size and no more connections are allowed.",
                )
            pass  # Oh well, we'll create a new connection then

        # If this is a persistent connection, check if it got disconnected
        if conn and is_connection_dropped(conn):
            log.debug("Resetting dropped connection: %s", self.host)
            conn.close()
            if getattr(conn, "auto_open", 1) == 0:
                # This is a proxied connection that has been mutated by
                # http.client._tunnel() and cannot be reused (since it would
                # attempt to bypass the proxy)
                conn = None

        return conn or self._new_conn()

    def _put_conn(self, conn):
        """
        Put a connection back into the pool.

        :param conn:
            Connection object for the current host and port as returned by
            :meth:`._new_conn` or :meth:`._get_conn`.

        If the pool is already full, the connection is closed and discarded
        because we exceeded maxsize. If connections are discarded frequently,
        then maxsize should be increased.

        If the pool is closed, then the connection will be closed and discarded.
        """
        try:
            self.pool.put(conn, block=False)
            return  # Everything is dandy, done.
        except AttributeError:
            # self.pool is None.
            pass
        except queue.Full:
            # This should never happen if self.block == True
            log.warning(
                "Connection pool is full, discarding connection: %s. Connection pool size: %s",
                self.host,
                self.pool.qsize(),
            )
        # Connection never got put back into the pool, close it.
        if conn:
            conn.close()

    def _validate_conn(self, conn):
        """
        Called right before a request is made, after the socket is created.
        """
        pass

    def _prepare_proxy(self, conn):
        # Nothing to do for HTTP connections.
        pass

    def _get_timeout(self, timeout):
        """Helper that always returns a :class:`urllib3.util.Timeout`"""
        if timeout is _Default:
            return self.timeout.clone()

        if isinstance(timeout, Timeout):
            return timeout.clone()
        else:
            # User passed us an int/float. This is for backwards compatibility,
            # can be removed later
            return Timeout.from_float(timeout)

    def _raise_timeout(self, err, url, timeout_value):
        """Is the error actually a timeout? Will raise a ReadTimeout or pass"""

        if isinstance(err, SocketTimeout):
            raise ReadTimeoutError(
                self, url, "Read timed out. (read timeout=%s)" % timeout_value
            )

        # See the above comment about EAGAIN in Python 3. In Python 2 we have
        # to specifically catch it and throw the timeout error
        if hasattr(err, "errno") and err.errno in _blocking_errnos:
            raise ReadTimeoutError(
                self, url, "Read timed out. (read timeout=%s)" % timeout_value
            )

        # Catch possible read timeouts thrown as SSL errors. If not the
        # case, rethrow the original. We need to do this because of:
        # http://bugs.python.org/issue10272
        if "timed out" in str(err) or "did not complete (read)" in str(
            err
        ):  # Python < 2.7.4
            raise ReadTimeoutError(
                self, url, "Read timed out. (read timeout=%s)" % timeout_value
            )

    def _make_request(
        self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw
    ):
        """
        Perform a request on a given urllib connection object taken from our
        pool.

        :param conn:
            a connection from one of our connection pools

        :param timeout:
            Socket timeout in seconds for the request. This can be a
            float or integer, which will set the same timeout value for
            the socket connect and the socket read, or an instance of
            :class:`urllib3.util.Timeout`, which gives you more fine-grained
            control over your timeouts.
        """
        self.num_requests += 1

        timeout_obj = self._get_timeout(timeout)
        timeout_obj.start_connect()
        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)

        # Trigger any extra validation we need to do.
        try:
            self._validate_conn(conn)
        except (SocketTimeout, BaseSSLError) as e:
            # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.
            self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
            raise

        # conn.request() calls http.client.*.request, not the method in
        # urllib3.request. It also calls makefile (recv) on the socket.
        try:
            if chunked:
                conn.request_chunked(method, url, **httplib_request_kw)
            else:
                conn.request(method, url, **httplib_request_kw)

        # We are swallowing BrokenPipeError (errno.EPIPE) since the server is
        # legitimately able to close the connection after sending a valid response.
        # With this behaviour, the received response is still readable.
        except BrokenPipeError:
            # Python 3
            pass
        except IOError as e:
            # Python 2 and macOS/Linux
            # EPIPE and ESHUTDOWN are BrokenPipeError on Python 2, and EPROTOTYPE is needed on macOS
            # https://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/
            if e.errno not in {
                errno.EPIPE,
                errno.ESHUTDOWN,
                errno.EPROTOTYPE,
            }:
                raise

        # Reset the timeout for the recv() on the socket
        read_timeout = timeout_obj.read_timeout

        # App Engine doesn't have a sock attr
        if getattr(conn, "sock", None):
            # In Python 3 socket.py will catch EAGAIN and return None when you
            # try and read into the file pointer created by http.client, which
            # instead raises a BadStatusLine exception. Instead of catching
            # the exception and assuming all BadStatusLine exceptions are read
            # timeouts, check for a zero timeout before making the request.
            if read_timeout == 0:
                raise ReadTimeoutError(
                    self, url, "Read timed out. (read timeout=%s)" % read_timeout
                )
            if read_timeout is Timeout.DEFAULT_TIMEOUT:
                conn.sock.settimeout(socket.getdefaulttimeout())
            else:  # None or a value
                conn.sock.settimeout(read_timeout)

        # Receive the response from the server
        try:
            try:
                # Python 2.7, use buffering of HTTP responses
                httplib_response = conn.getresponse(buffering=True)
            except TypeError:
                # Python 3
                try:
                    httplib_response = conn.getresponse()
                except BaseException as e:
                    # Remove the TypeError from the exception chain in
                    # Python 3 (including for exceptions like SystemExit).
                    # Otherwise it looks like a bug in the code.
                    six.raise_from(e, None)
        except (SocketTimeout, BaseSSLError, SocketError) as e:
            self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
            raise

        # AppEngine doesn't have a version attr.
        http_version = getattr(conn, "_http_vsn_str", "HTTP/?")
        log.debug(
            '%s://%s:%s "%s %s %s" %s %s',
            self.scheme,
            self.host,
            self.port,
            method,
            url,
            http_version,
            httplib_response.status,
            httplib_response.length,
        )

        try:
            assert_header_parsing(httplib_response.msg)
        except (HeaderParsingError, TypeError) as hpe:  # Platform-specific: Python 3
            log.warning(
                "Failed to parse headers (url=%s): %s",
                self._absolute_url(url),
                hpe,
                exc_info=True,
            )

        return httplib_response

    def _absolute_url(self, path):
        return Url(scheme=self.scheme, host=self.host, port=self.port, path=path).url

    def close(self):
        """
        Close all pooled connections and disable the pool.
        """
        if self.pool is None:
            return
        # Disable access to the pool
        old_pool, self.pool = self.pool, None

        # Close all the HTTPConnections in the pool.
        _close_pool_connections(old_pool)

    def is_same_host(self, url):
        """
        Check if the given ``url`` is a member of the same host as this
        connection pool.
        """
        if url.startswith("/"):
            return True

        # TODO: Add optional support for socket.gethostbyname checking.
        scheme, host, port = get_host(url)
        if host is not None:
            host = _normalize_host(host, scheme=scheme)

        # Use explicit default port for comparison when none is given
        if self.port and not port:
            port = port_by_scheme.get(scheme)
        elif not self.port and port == port_by_scheme.get(scheme):
            port = None

        return (scheme, host, port) == (self.scheme, self.host, self.port)

    def urlopen(
        self,
        method,
        url,
        body=None,
        headers=None,
        retries=None,
        redirect=True,
        assert_same_host=True,
        timeout=_Default,
        pool_timeout=None,
        release_conn=None,
        chunked=False,
        body_pos=None,
        **response_kw
    ):
        """
        Get a connection from the pool and perform an HTTP request. This is the
        lowest level call for making a request, so you'll need to specify all
        the raw details.

        .. note::

           More commonly, it's appropriate to use a convenience method provided
           by :class:`.RequestMethods`, such as :meth:`request`.

        .. note::

           `release_conn` will only behave as expected if
           `preload_content=False` because we want to make
           `preload_content=False` the default behaviour someday soon without
           breaking backwards compatibility.

        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)

        :param url:
            The URL to perform the request on.

        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.

        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.

        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.

            Pass ``None`` to retry until you receive a response. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.

            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.

        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.

        :param redirect:
            If True, automatically handle redirects (status codes 301, 302,
            303, 307, 308). Each redirect counts as a retry. Disabling retries
            will disable redirect, too.

        :param assert_same_host:
            If ``True``, will make sure that the host of the pool requests is
            consistent else will raise HostChangedError. When ``False``, you can
            use the pool on an HTTP proxy and request foreign hosts.

        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.

        :param pool_timeout:
            If set and the pool is set to block=True, then this method will
            block for ``pool_timeout`` seconds and raise EmptyPoolError if no
            connection is available within the time period.

        :param release_conn:
            If False, then the urlopen call will not release the connection
            back into the pool once a response is received (but will release if
            you read the entire contents of the response such as when
            `preload_content=True`). This is useful if you're not preloading
            the response's content immediately. You will need to call
            ``r.release_conn()`` on the response ``r`` to return the connection
            back into the pool. If None, it takes the value of
            ``response_kw.get('preload_content', True)``.

        :param chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.

        :param int body_pos:
            Position to seek to in file-like body in the event of a retry or
            redirect. Typically this won't need to be set because urllib3 will
            auto-populate the value when needed.

        :param \\**response_kw:
            Additional parameters are passed to
            :meth:`urllib3.response.HTTPResponse.from_httplib`
        """

        parsed_url = parse_url(url)
        destination_scheme = parsed_url.scheme

        if headers is None:
            headers = self.headers

        if not isinstance(retries, Retry):
            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)

        if release_conn is None:
            release_conn = response_kw.get("preload_content", True)

        # Check host
        if assert_same_host and not self.is_same_host(url):
            raise HostChangedError(self, url, retries)

        # Ensure that the URL we're connecting to is properly encoded
        if url.startswith("/"):
            url = six.ensure_str(_encode_target(url))
        else:
            url = six.ensure_str(parsed_url.url)

        conn = None

        # Track whether `conn` needs to be released before
        # returning/raising/recursing. Update this variable if necessary, and
        # leave `release_conn` constant throughout the function. That way, if
        # the function recurses, the original value of `release_conn` will be
        # passed down into the recursive call, and its value will be respected.
        #
        # See issue #651 [1] for details.
        #
        # [1] <https://github.com/urllib3/urllib3/issues/651>
        release_this_conn = release_conn

        http_tunnel_required = connection_requires_http_tunnel(
            self.proxy, self.proxy_config, destination_scheme
        )

        # Merge the proxy headers. Only done when not using HTTP CONNECT. We
        # have to copy the headers dict so we can safely change it without those
        # changes being reflected in anyone else's copy.
        if not http_tunnel_required:
            headers = headers.copy()
            headers.update(self.proxy_headers)

        # Must keep the exception bound to a separate variable or else Python 3
        # complains about UnboundLocalError.
        err = None

        # Keep track of whether we cleanly exited the except block. This
        # ensures we do proper cleanup in finally.
        clean_exit = False

        # Rewind body position, if needed. Record current position
        # for future rewinds in the event of a redirect/retry.
        body_pos = set_file_position(body, body_pos)

        try:
            # Request a connection from the queue.
            timeout_obj = self._get_timeout(timeout)
            conn = self._get_conn(timeout=pool_timeout)

            conn.timeout = timeout_obj.connect_timeout

            is_new_proxy_conn = self.proxy is not None and not getattr(
                conn, "sock", None
            )
            if is_new_proxy_conn and http_tunnel_required:
                self._prepare_proxy(conn)

            # Make the request on the httplib connection object.
            httplib_response = self._make_request(
                conn,
                method,
                url,
                timeout=timeout_obj,
                body=body,
                headers=headers,
                chunked=chunked,
            )

            # If we're going to release the connection in ``finally:``, then
            # the response doesn't need to know about the connection. Otherwise
            # it will also try to release it and we'll have a double-release
            # mess.
            response_conn = conn if not release_conn else None

            # Pass method to Response for length checking
            response_kw["request_method"] = method

            # Import httplib's response into our own wrapper object
            response = self.ResponseCls.from_httplib(
                httplib_response,
                pool=self,
                connection=response_conn,
                retries=retries,
                **response_kw
            )

            # Everything went great!
            clean_exit = True

        except EmptyPoolError:
            # Didn't get a connection from the pool, no need to clean up
            clean_exit = True
            release_this_conn = False
            raise

        except (
            TimeoutError,
            HTTPException,
            SocketError,
            ProtocolError,
            BaseSSLError,
            SSLError,
            CertificateError,
        ) as e:
            # Discard the connection for these exceptions. It will be
            # replaced during the next _get_conn() call.
            clean_exit = False

            def _is_ssl_error_message_from_http_proxy(ssl_error):
                # We're trying to detect the message 'WRONG_VERSION_NUMBER' but
                # SSLErrors are kinda all over the place when it comes to the message,
                # so we try to cover our bases here!
                message = " ".join(re.split("[^a-z]", str(ssl_error).lower()))
                return (
                    "wrong version number" in message or "unknown protocol" in message
                )

            # Try to detect a common user error with proxies which is to
            # set an HTTP proxy to be HTTPS when it should be 'http://'
            # (ie {'http': 'http://proxy', 'https': 'https://proxy'})
            # Instead we add a nice error message and point to a URL.
            if (
                isinstance(e, BaseSSLError)
                and self.proxy
                and _is_ssl_error_message_from_http_proxy(e)
                and conn.proxy
                and conn.proxy.scheme == "https"
            ):
                e = ProxyError(
                    "Your proxy appears to only use HTTP and not HTTPS, "
                    "try changing your proxy URL to be HTTP. See: "
                    "https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
                    "#https-proxy-error-http-proxy",
                    SSLError(e),
                )
            elif isinstance(e, (BaseSSLError, CertificateError)):
                e = SSLError(e)
            elif isinstance(e, (SocketError, NewConnectionError)) and self.proxy:
                e = ProxyError("Cannot connect to proxy.", e)
            elif isinstance(e, (SocketError, HTTPException)):
                e = ProtocolError("Connection aborted.", e)

            retries = retries.increment(
                method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
            )
            retries.sleep()

            # Keep track of the error for the retry warning.
            err = e

        finally:
            if not clean_exit:
                # We hit some kind of exception, handled or otherwise. We need
                # to throw the connection away unless explicitly told not to.
                # Close the connection, set the variable to None, and make sure
                # we put the None back in the pool to avoid leaking it.
                conn = conn and conn.close()
                release_this_conn = True

            if release_this_conn:
                # Put the connection back to be reused. If the connection is
                # expired then it will be None, which will get replaced with a
                # fresh connection during _get_conn.
                self._put_conn(conn)

        if not conn:
            # Try again
            log.warning(
                "Retrying (%r) after connection broken by '%r': %s", retries, err, url
            )
            return self.urlopen(
                method,
                url,
                body,
                headers,
                retries,
                redirect,
                assert_same_host,
                timeout=timeout,
                pool_timeout=pool_timeout,
                release_conn=release_conn,
                chunked=chunked,
                body_pos=body_pos,
                **response_kw
            )

        # Handle redirect?
        redirect_location = redirect and response.get_redirect_location()
        if redirect_location:
            if response.status == 303:
                # Change the method according to RFC 9110, Section 15.4.4.
                method = "GET"
                # And lose the body not to transfer anything sensitive.
                body = None
                headers = HTTPHeaderDict(headers)._prepare_for_method_change()

            try:
                retries = retries.increment(method, url, response=response, _pool=self)
            except MaxRetryError:
                if retries.raise_on_redirect:
                    response.drain_conn()
                    raise
                return response

            response.drain_conn()
            retries.sleep_for_retry(response)
            log.debug("Redirecting %s -> %s", url, redirect_location)
            return self.urlopen(
                method,
                redirect_location,
                body,
                headers,
                retries=retries,
                redirect=redirect,
                assert_same_host=assert_same_host,
                timeout=timeout,
                pool_timeout=pool_timeout,
                release_conn=release_conn,
                chunked=chunked,
                body_pos=body_pos,
                **response_kw
            )

        # Check if we should retry the HTTP response.
        has_retry_after = bool(response.headers.get("Retry-After"))
        if retries.is_retry(method, response.status, has_retry_after):
            try:
                retries = retries.increment(method, url, response=response, _pool=self)
            except MaxRetryError:
                if retries.raise_on_status:
                    response.drain_conn()
                    raise
                return response

            response.drain_conn()
            retries.sleep(response)
            log.debug("Retry: %s", url)
            return self.urlopen(
                method,
                url,
                body,
                headers,
                retries=retries,
                redirect=redirect,
                assert_same_host=assert_same_host,
                timeout=timeout,
                pool_timeout=pool_timeout,
                release_conn=release_conn,
                chunked=chunked,
                body_pos=body_pos,
                **response_kw
            )

        return response


class HTTPSConnectionPool(HTTPConnectionPool):
    """
    Same as :class:`.HTTPConnectionPool`, but HTTPS.

    :class:`.HTTPSConnection` uses one of ``assert_fingerprint``,
    ``assert_hostname`` and ``host`` in this order to verify connections.
    If ``assert_hostname`` is False, no verification is done.

    The ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs``,
    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
    is available and are fed into :meth:`urllib3.util.ssl_wrap_socket` to upgrade
    the connection socket into an SSL socket.
    """

    scheme = "https"
    ConnectionCls = HTTPSConnection

    def __init__(
        self,
        host,
        port=None,
        strict=False,
        timeout=Timeout.DEFAULT_TIMEOUT,
        maxsize=1,
        block=False,
        headers=None,
        retries=None,
        _proxy=None,
        _proxy_headers=None,
        key_file=None,
        cert_file=None,
        cert_reqs=None,
        key_password=None,
        ca_certs=None,
        ssl_version=None,
        assert_hostname=None,
        assert_fingerprint=None,
        ca_cert_dir=None,
        **conn_kw
    ):

        HTTPConnectionPool.__init__(
            self,
            host,
            port,
            strict,
            timeout,
            maxsize,
            block,
            headers,
            retries,
            _proxy,
            _proxy_headers,
            **conn_kw
        )

        self.key_file = key_file
        self.cert_file = cert_file
        self.cert_reqs = cert_reqs
        self.key_password = key_password
        self.ca_certs = ca_certs
        self.ca_cert_dir = ca_cert_dir
        self.ssl_version = ssl_version
        self.assert_hostname = assert_hostname
        self.assert_fingerprint = assert_fingerprint

    def _prepare_conn(self, conn):
        """
        Prepare the ``connection`` for :meth:`urllib3.util.ssl_wrap_socket`
        and establish the tunnel if proxy is used.
        """

        if isinstance(conn, VerifiedHTTPSConnection):
            conn.set_cert(
                key_file=self.key_file,
                key_password=self.key_password,
                cert_file=self.cert_file,
                cert_reqs=self.cert_reqs,
                ca_certs=self.ca_certs,
                ca_cert_dir=self.ca_cert_dir,
                assert_hostname=self.assert_hostname,
                assert_fingerprint=self.assert_fingerprint,
            )
            conn.ssl_version = self.ssl_version
        return conn

    def _prepare_proxy(self, conn):
        """
        Establishes a tunnel connection through HTTP CONNECT.

        Tunnel connection is established early because otherwise httplib would
        improperly set Host: header to proxy's IP:port.
        """

        conn.set_tunnel(self._proxy_host, self.port, self.proxy_headers)

        if self.proxy.scheme == "https":
            conn.tls_in_tls_required = True

        conn.connect()

    def _new_conn(self):
        """
        Return a fresh :class:`http.client.HTTPSConnection`.
        """
        self.num_connections += 1
        log.debug(
            "Starting new HTTPS connection (%d): %s:%s",
            self.num_connections,
            self.host,
            self.port or "443",
        )

        if not self.ConnectionCls or self.ConnectionCls is DummyConnection:
            raise SSLError(
                "Can't connect to HTTPS URL because the SSL module is not available."
            )

        actual_host = self.host
        actual_port = self.port
        if self.proxy is not None:
            actual_host = self.proxy.host
            actual_port = self.proxy.port

        conn = self.ConnectionCls(
            host=actual_host,
            port=actual_port,
            timeout=self.timeout.connect_timeout,
            strict=self.strict,
            cert_file=self.cert_file,
            key_file=self.key_file,
            key_password=self.key_password,
            **self.conn_kw
        )

        return self._prepare_conn(conn)

    def _validate_conn(self, conn):
        """
        Called right before a request is made, after the socket is created.
        """
        super(HTTPSConnectionPool, self)._validate_conn(conn)

        # Force connect early to allow us to validate the connection.
        if not getattr(conn, "sock", None):  # AppEngine might not have  `.sock`
            conn.connect()

        if not conn.is_verified:
            warnings.warn(
                (
                    "Unverified HTTPS request is being made to host '%s'. "
                    "Adding certificate verification is strongly advised. See: "
                    "https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
                    "#ssl-warnings" % conn.host
                ),
                InsecureRequestWarning,
            )

        if getattr(conn, "proxy_is_verified", None) is False:
            warnings.warn(
                (
                    "Unverified HTTPS connection done to an HTTPS proxy. "
                    "Adding certificate verification is strongly advised. See: "
                    "https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
                    "#ssl-warnings"
                ),
                InsecureRequestWarning,
            )


def connection_from_url(url, **kw):
    """
    Given a url, return an :class:`.ConnectionPool` instance of its host.

    This is a shortcut for not having to parse out the scheme, host, and port
    of the url before creating an :class:`.ConnectionPool` instance.

    :param url:
        Absolute URL string that must include the scheme. Port is optional.

    :param \\**kw:
        Passes additional parameters to the constructor of the appropriate
        :class:`.ConnectionPool`. Useful for specifying things like
        timeout, maxsize, headers, etc.

    Example::

        >>> conn = connection_from_url('http://google.com/')
        >>> r = conn.request('GET', '/')
    """
    scheme, host, port = get_host(url)
    port = port or port_by_scheme.get(scheme, 80)
    if scheme == "https":
        return HTTPSConnectionPool(host, port=port, **kw)
    else:
        return HTTPConnectionPool(host, port=port, **kw)


def _normalize_host(host, scheme):
    """
    Normalize hosts for comparisons and use with sockets.
    """

    host = normalize_host(host, scheme)

    # httplib doesn't like it when we include brackets in IPv6 addresses
    # Specifically, if we include brackets but also pass the port then
    # httplib crazily doubles up the square brackets on the Host header.
    # Instead, we need to make sure we never pass ``None`` as the port.
    # However, for backward compatibility reasons we can't actually
    # *assert* that.  See http://bugs.python.org/issue28539
    if host.startswith("[") and host.endswith("]"):
        host = host[1:-1]
    return host


def _close_pool_connections(pool):
    """Drains a queue of connections and closes each one."""
    try:
        while True:
            conn = pool.get(block=False)
            if conn:
                conn.close()
    except queue.Empty:
        pass  # Done.


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/request.py
# ========================================================
from __future__ import absolute_import

import sys

from .filepost import encode_multipart_formdata
from .packages import six
from .packages.six.moves.urllib.parse import urlencode

__all__ = ["RequestMethods"]


class RequestMethods(object):
    """
    Convenience mixin for classes who implement a :meth:`urlopen` method, such
    as :class:`urllib3.HTTPConnectionPool` and
    :class:`urllib3.PoolManager`.

    Provides behavior for making common types of HTTP request methods and
    decides which type of request field encoding to use.

    Specifically,

    :meth:`.request_encode_url` is for sending requests whose fields are
    encoded in the URL (such as GET, HEAD, DELETE).

    :meth:`.request_encode_body` is for sending requests whose fields are
    encoded in the *body* of the request using multipart or www-form-urlencoded
    (such as for POST, PUT, PATCH).

    :meth:`.request` is for making any kind of request, it will look up the
    appropriate encoding format and use one of the above two methods to make
    the request.

    Initializer parameters:

    :param headers:
        Headers to include with all requests, unless other headers are given
        explicitly.
    """

    _encode_url_methods = {"DELETE", "GET", "HEAD", "OPTIONS"}

    def __init__(self, headers=None):
        self.headers = headers or {}

    def urlopen(
        self,
        method,
        url,
        body=None,
        headers=None,
        encode_multipart=True,
        multipart_boundary=None,
        **kw
    ):  # Abstract
        raise NotImplementedError(
            "Classes extending RequestMethods must implement "
            "their own ``urlopen`` method."
        )

    def request(self, method, url, fields=None, headers=None, **urlopen_kw):
        """
        Make a request using :meth:`urlopen` with the appropriate encoding of
        ``fields`` based on the ``method`` used.

        This is a convenience method that requires the least amount of manual
        effort. It can be used in most situations, while still having the
        option to drop down to more specific methods when necessary, such as
        :meth:`request_encode_url`, :meth:`request_encode_body`,
        or even the lowest level :meth:`urlopen`.
        """
        method = method.upper()

        urlopen_kw["request_url"] = url

        if method in self._encode_url_methods:
            return self.request_encode_url(
                method, url, fields=fields, headers=headers, **urlopen_kw
            )
        else:
            return self.request_encode_body(
                method, url, fields=fields, headers=headers, **urlopen_kw
            )

    def request_encode_url(self, method, url, fields=None, headers=None, **urlopen_kw):
        """
        Make a request using :meth:`urlopen` with the ``fields`` encoded in
        the url. This is useful for request methods like GET, HEAD, DELETE, etc.
        """
        if headers is None:
            headers = self.headers

        extra_kw = {"headers": headers}
        extra_kw.update(urlopen_kw)

        if fields:
            url += "?" + urlencode(fields)

        return self.urlopen(method, url, **extra_kw)

    def request_encode_body(
        self,
        method,
        url,
        fields=None,
        headers=None,
        encode_multipart=True,
        multipart_boundary=None,
        **urlopen_kw
    ):
        """
        Make a request using :meth:`urlopen` with the ``fields`` encoded in
        the body. This is useful for request methods like POST, PUT, PATCH, etc.

        When ``encode_multipart=True`` (default), then
        :func:`urllib3.encode_multipart_formdata` is used to encode
        the payload with the appropriate content type. Otherwise
        :func:`urllib.parse.urlencode` is used with the
        'application/x-www-form-urlencoded' content type.

        Multipart encoding must be used when posting files, and it's reasonably
        safe to use it in other times too. However, it may break request
        signing, such as with OAuth.

        Supports an optional ``fields`` parameter of key/value strings AND
        key/filetuple. A filetuple is a (filename, data, MIME type) tuple where
        the MIME type is optional. For example::

            fields = {
                'foo': 'bar',
                'fakefile': ('foofile.txt', 'contents of foofile'),
                'realfile': ('barfile.txt', open('realfile').read()),
                'typedfile': ('bazfile.bin', open('bazfile').read(),
                              'image/jpeg'),
                'nonamefile': 'contents of nonamefile field',
            }

        When uploading a file, providing a filename (the first parameter of the
        tuple) is optional but recommended to best mimic behavior of browsers.

        Note that if ``headers`` are supplied, the 'Content-Type' header will
        be overwritten because it depends on the dynamic random boundary string
        which is used to compose the body of the request. The random boundary
        string can be explicitly set with the ``multipart_boundary`` parameter.
        """
        if headers is None:
            headers = self.headers

        extra_kw = {"headers": {}}

        if fields:
            if "body" in urlopen_kw:
                raise TypeError(
                    "request got values for both 'fields' and 'body', can only specify one."
                )

            if encode_multipart:
                body, content_type = encode_multipart_formdata(
                    fields, boundary=multipart_boundary
                )
            else:
                body, content_type = (
                    urlencode(fields),
                    "application/x-www-form-urlencoded",
                )

            extra_kw["body"] = body
            extra_kw["headers"] = {"Content-Type": content_type}

        extra_kw["headers"].update(headers)
        extra_kw.update(urlopen_kw)

        return self.urlopen(method, url, **extra_kw)


if not six.PY2:

    class RequestModule(sys.modules[__name__].__class__):
        def __call__(self, *args, **kwargs):
            """
            If user tries to call this module directly urllib3 v2.x style raise an error to the user
            suggesting they may need urllib3 v2
            """
            raise TypeError(
                "'module' object is not callable\n"
                "urllib3.request() method is not supported in this release, "
                "upgrade to urllib3 v2 to use it\n"
                "see https://urllib3.readthedocs.io/en/stable/v2-migration-guide.html"
            )

    sys.modules[__name__].__class__ = RequestModule


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/urllib3/filepost.py
# ========================================================
from __future__ import absolute_import

import binascii
import codecs
import os
from io import BytesIO

from .fields import RequestField
from .packages import six
from .packages.six import b

writer = codecs.lookup("utf-8")[3]


def choose_boundary():
    """
    Our embarrassingly-simple replacement for mimetools.choose_boundary.
    """
    boundary = binascii.hexlify(os.urandom(16))
    if not six.PY2:
        boundary = boundary.decode("ascii")
    return boundary


def iter_field_objects(fields):
    """
    Iterate over fields.

    Supports list of (k, v) tuples and dicts, and lists of
    :class:`~urllib3.fields.RequestField`.

    """
    if isinstance(fields, dict):
        i = six.iteritems(fields)
    else:
        i = iter(fields)

    for field in i:
        if isinstance(field, RequestField):
            yield field
        else:
            yield RequestField.from_tuples(*field)


def iter_fields(fields):
    """
    .. deprecated:: 1.6

    Iterate over fields.

    The addition of :class:`~urllib3.fields.RequestField` makes this function
    obsolete. Instead, use :func:`iter_field_objects`, which returns
    :class:`~urllib3.fields.RequestField` objects.

    Supports list of (k, v) tuples and dicts.
    """
    if isinstance(fields, dict):
        return ((k, v) for k, v in six.iteritems(fields))

    return ((k, v) for k, v in fields)


def encode_multipart_formdata(fields, boundary=None):
    """
    Encode a dictionary of ``fields`` using the multipart/form-data MIME format.

    :param fields:
        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).

    :param boundary:
        If not specified, then a random boundary will be generated using
        :func:`urllib3.filepost.choose_boundary`.
    """
    body = BytesIO()
    if boundary is None:
        boundary = choose_boundary()

    for field in iter_field_objects(fields):
        body.write(b("--%s\r\n" % (boundary)))

        writer(body).write(field.render_headers())
        data = field.data

        if isinstance(data, int):
            data = str(data)  # Backwards compatibility

        if isinstance(data, six.text_type):
            writer(body).write(data)
        else:
            body.write(data)

        body.write(b"\r\n")

    body.write(b("--%s--\r\n" % (boundary)))

    content_type = str("multipart/form-data; boundary=%s" % boundary)

    return body.getvalue(), content_type


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/certifi/__init__.py
# ========================================================
from .core import contents, where

__all__ = ["contents", "where"]
__version__ = "2023.07.22"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/certifi/__main__.py
# ========================================================
import argparse

from pip._vendor.certifi import contents, where

parser = argparse.ArgumentParser()
parser.add_argument("-c", "--contents", action="store_true")
args = parser.parse_args()

if args.contents:
    print(contents())
else:
    print(where())


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/certifi/core.py
# ========================================================
"""
certifi.py
~~~~~~~~~~

This module returns the installation location of cacert.pem or its contents.
"""
import sys

DEBIAN_CA_CERTS_PATH = '/etc/ssl/certs/ca-certificates.crt'

if sys.version_info >= (3, 11):

    from importlib.resources import as_file, files

    _CACERT_CTX = None
    _CACERT_PATH = None

    def where() -> str:
        # This is slightly terrible, but we want to delay extracting the file
        # in cases where we're inside of a zipimport situation until someone
        # actually calls where(), but we don't want to re-extract the file
        # on every call of where(), so we'll do it once then store it in a
        # global variable.
        global _CACERT_CTX
        global _CACERT_PATH
        if _CACERT_PATH is None:
            # This is slightly janky, the importlib.resources API wants you to
            # manage the cleanup of this file, so it doesn't actually return a
            # path, it returns a context manager that will give you the path
            # when you enter it and will do any cleanup when you leave it. In
            # the common case of not needing a temporary file, it will just
            # return the file system location and the __exit__() is a no-op.
            #
            # We also have to hold onto the actual context manager, because
            # it will do the cleanup whenever it gets garbage collected, so
            # we will also store that at the global level as well.
            _CACERT_CTX = as_file(files("pip._vendor.certifi").joinpath("cacert.pem"))
            _CACERT_PATH = str(_CACERT_CTX.__enter__())

        return _CACERT_PATH

    def contents() -> str:
        return files("pip._vendor.certifi").joinpath("cacert.pem").read_text(encoding="ascii")

elif sys.version_info >= (3, 7):

    from importlib.resources import path as get_path, read_text

    _CACERT_CTX = None
    _CACERT_PATH = None

    def where() -> str:
        # This is slightly terrible, but we want to delay extracting the
        # file in cases where we're inside of a zipimport situation until
        # someone actually calls where(), but we don't want to re-extract
        # the file on every call of where(), so we'll do it once then store
        # it in a global variable.
        global _CACERT_CTX
        global _CACERT_PATH
        if _CACERT_PATH is None:
            # This is slightly janky, the importlib.resources API wants you
            # to manage the cleanup of this file, so it doesn't actually
            # return a path, it returns a context manager that will give
            # you the path when you enter it and will do any cleanup when
            # you leave it. In the common case of not needing a temporary
            # file, it will just return the file system location and the
            # __exit__() is a no-op.
            #
            # We also have to hold onto the actual context manager, because
            # it will do the cleanup whenever it gets garbage collected, so
            # we will also store that at the global level as well.
            _CACERT_CTX = get_path("pip._vendor.certifi", "cacert.pem")
            _CACERT_PATH = str(_CACERT_CTX.__enter__())

        return _CACERT_PATH

    def contents() -> str:
        return read_text("pip._vendor.certifi", "cacert.pem", encoding="ascii")

else:
    import os
    import types
    from typing import Union

    Package = Union[types.ModuleType, str]
    Resource = Union[str, "os.PathLike"]

    # This fallback will work for Python versions prior to 3.7 that lack the
    # importlib.resources module but relies on the existing `where` function
    # so won't address issues with environments like PyOxidizer that don't set
    # __file__ on modules.
    def read_text(
        package: Package,
        resource: Resource,
        encoding: str = 'utf-8',
        errors: str = 'strict'
    ) -> str:
        with open(where(), encoding=encoding) as data:
            return data.read()

    # If we don't have importlib.resources, then we will just do the old logic
    # of assuming we're on the filesystem and munge the path directly.
    def where() -> str:
        f = os.path.dirname(__file__)

        return os.path.join(f, "cacert.pem")

    def contents() -> str:
        return read_text("pip._vendor.certifi", "cacert.pem", encoding="ascii")


# Debian: Use system CA certs:
def where() -> str:
    return DEBIAN_CA_CERTS_PATH


def contents() -> str:
    with open(where(), "r", encoding="ascii") as data:
        return data.read()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/plugin.py
# ========================================================
"""
    pygments.plugin
    ~~~~~~~~~~~~~~~

    Pygments plugin interface. By default, this tries to use
    ``importlib.metadata``, which is in the Python standard
    library since Python 3.8, or its ``importlib_metadata``
    backport for earlier versions of Python. It falls back on
    ``pkg_resources`` if not found. Finally, if ``pkg_resources``
    is not found either, no plugins are loaded at all.

    lexer plugins::

        [pygments.lexers]
        yourlexer = yourmodule:YourLexer

    formatter plugins::

        [pygments.formatters]
        yourformatter = yourformatter:YourFormatter
        /.ext = yourformatter:YourFormatter

    As you can see, you can define extensions for the formatter
    with a leading slash.

    syntax plugins::

        [pygments.styles]
        yourstyle = yourstyle:YourStyle

    filter plugin::

        [pygments.filter]
        yourfilter = yourfilter:YourFilter


    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

LEXER_ENTRY_POINT = 'pygments.lexers'
FORMATTER_ENTRY_POINT = 'pygments.formatters'
STYLE_ENTRY_POINT = 'pygments.styles'
FILTER_ENTRY_POINT = 'pygments.filters'


def iter_entry_points(group_name):
    try:
        from importlib.metadata import entry_points
    except ImportError:
        try:
            from importlib_metadata import entry_points
        except ImportError:
            try:
                from pip._vendor.pkg_resources import iter_entry_points
            except (ImportError, OSError):
                return []
            else:
                return iter_entry_points(group_name)
    groups = entry_points()
    if hasattr(groups, 'select'):
        # New interface in Python 3.10 and newer versions of the
        # importlib_metadata backport.
        return groups.select(group=group_name)
    else:
        # Older interface, deprecated in Python 3.10 and recent
        # importlib_metadata, but we need it in Python 3.8 and 3.9.
        return groups.get(group_name, [])


def find_plugin_lexers():
    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):
        yield entrypoint.load()


def find_plugin_formatters():
    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()


def find_plugin_styles():
    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()


def find_plugin_filters():
    for entrypoint in iter_entry_points(FILTER_ENTRY_POINT):
        yield entrypoint.name, entrypoint.load()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/lexer.py
# ========================================================
"""
    pygments.lexer
    ~~~~~~~~~~~~~~

    Base lexer classes.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
import sys
import time

from pip._vendor.pygments.filter import apply_filters, Filter
from pip._vendor.pygments.filters import get_filter_by_name
from pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType
from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \
    make_analysator, Future, guess_decode
from pip._vendor.pygments.regexopt import regex_opt

__all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLexer',
           'LexerContext', 'include', 'inherit', 'bygroups', 'using', 'this',
           'default', 'words', 'line_re']

line_re = re.compile('.*?\n')

_encoding_map = [(b'\xef\xbb\xbf', 'utf-8'),
                 (b'\xff\xfe\0\0', 'utf-32'),
                 (b'\0\0\xfe\xff', 'utf-32be'),
                 (b'\xff\xfe', 'utf-16'),
                 (b'\xfe\xff', 'utf-16be')]

_default_analyse = staticmethod(lambda x: 0.0)


class LexerMeta(type):
    """
    This metaclass automagically converts ``analyse_text`` methods into
    static methods which always return float values.
    """

    def __new__(mcs, name, bases, d):
        if 'analyse_text' in d:
            d['analyse_text'] = make_analysator(d['analyse_text'])
        return type.__new__(mcs, name, bases, d)


class Lexer(metaclass=LexerMeta):
    """
    Lexer for a specific language.

    See also :doc:`lexerdevelopment`, a high-level guide to writing
    lexers.

    Lexer classes have attributes used for choosing the most appropriate
    lexer based on various criteria.

    .. autoattribute:: name
       :no-value:
    .. autoattribute:: aliases
       :no-value:
    .. autoattribute:: filenames
       :no-value:
    .. autoattribute:: alias_filenames
    .. autoattribute:: mimetypes
       :no-value:
    .. autoattribute:: priority

    Lexers included in Pygments should have an additional attribute:

    .. autoattribute:: url
       :no-value:

    You can pass options to the constructor. The basic options recognized
    by all lexers and processed by the base `Lexer` class are:

    ``stripnl``
        Strip leading and trailing newlines from the input (default: True).
    ``stripall``
        Strip all leading and trailing whitespace from the input
        (default: False).
    ``ensurenl``
        Make sure that the input ends with a newline (default: True).  This
        is required for some lexers that consume input linewise.

        .. versionadded:: 1.3

    ``tabsize``
        If given and greater than 0, expand tabs in the input (default: 0).
    ``encoding``
        If given, must be an encoding name. This encoding will be used to
        convert the input string to Unicode, if it is not already a Unicode
        string (default: ``'guess'``, which uses a simple UTF-8 / Locale /
        Latin1 detection.  Can also be ``'chardet'`` to use the chardet
        library, if it is installed.
    ``inencoding``
        Overrides the ``encoding`` if given.
    """

    #: Full name of the lexer, in human-readable form
    name = None

    #: A list of short, unique identifiers that can be used to look
    #: up the lexer from a list, e.g., using `get_lexer_by_name()`.
    aliases = []

    #: A list of `fnmatch` patterns that match filenames which contain
    #: content for this lexer. The patterns in this list should be unique among
    #: all lexers.
    filenames = []

    #: A list of `fnmatch` patterns that match filenames which may or may not
    #: contain content for this lexer. This list is used by the
    #: :func:`.guess_lexer_for_filename()` function, to determine which lexers
    #: are then included in guessing the correct one. That means that
    #: e.g. every lexer for HTML and a template language should include
    #: ``\*.html`` in this list.
    alias_filenames = []

    #: A list of MIME types for content that can be lexed with this lexer.
    mimetypes = []

    #: Priority, should multiple lexers match and no content is provided
    priority = 0

    #: URL of the language specification/definition. Used in the Pygments
    #: documentation.
    url = None

    def __init__(self, **options):
        """
        This constructor takes arbitrary options as keyword arguments.
        Every subclass must first process its own options and then call
        the `Lexer` constructor, since it processes the basic
        options like `stripnl`.

        An example looks like this:

        .. sourcecode:: python

           def __init__(self, **options):
               self.compress = options.get('compress', '')
               Lexer.__init__(self, **options)

        As these options must all be specifiable as strings (due to the
        command line usage), there are various utility functions
        available to help with that, see `Utilities`_.
        """
        self.options = options
        self.stripnl = get_bool_opt(options, 'stripnl', True)
        self.stripall = get_bool_opt(options, 'stripall', False)
        self.ensurenl = get_bool_opt(options, 'ensurenl', True)
        self.tabsize = get_int_opt(options, 'tabsize', 0)
        self.encoding = options.get('encoding', 'guess')
        self.encoding = options.get('inencoding') or self.encoding
        self.filters = []
        for filter_ in get_list_opt(options, 'filters', ()):
            self.add_filter(filter_)

    def __repr__(self):
        if self.options:
            return '<pygments.lexers.%s with %r>' % (self.__class__.__name__,
                                                     self.options)
        else:
            return '<pygments.lexers.%s>' % self.__class__.__name__

    def add_filter(self, filter_, **options):
        """
        Add a new stream filter to this lexer.
        """
        if not isinstance(filter_, Filter):
            filter_ = get_filter_by_name(filter_, **options)
        self.filters.append(filter_)

    def analyse_text(text):
        """
        A static method which is called for lexer guessing.

        It should analyse the text and return a float in the range
        from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
        will not be selected as the most probable one, if it returns
        ``1.0``, it will be selected immediately.  This is used by
        `guess_lexer`.

        The `LexerMeta` metaclass automatically wraps this function so
        that it works like a static method (no ``self`` or ``cls``
        parameter) and the return value is automatically converted to
        `float`. If the return value is an object that is boolean `False`
        it's the same as if the return values was ``0.0``.
        """

    def get_tokens(self, text, unfiltered=False):
        """
        This method is the basic interface of a lexer. It is called by
        the `highlight()` function. It must process the text and return an
        iterable of ``(tokentype, value)`` pairs from `text`.

        Normally, you don't need to override this method. The default
        implementation processes the options recognized by all lexers
        (`stripnl`, `stripall` and so on), and then yields all tokens
        from `get_tokens_unprocessed()`, with the ``index`` dropped.

        If `unfiltered` is set to `True`, the filtering mechanism is
        bypassed even if filters are defined.
        """
        if not isinstance(text, str):
            if self.encoding == 'guess':
                text, _ = guess_decode(text)
            elif self.encoding == 'chardet':
                try:
                    from pip._vendor import chardet
                except ImportError as e:
                    raise ImportError('To enable chardet encoding guessing, '
                                      'please install the chardet library '
                                      'from http://chardet.feedparser.org/') from e
                # check for BOM first
                decoded = None
                for bom, encoding in _encoding_map:
                    if text.startswith(bom):
                        decoded = text[len(bom):].decode(encoding, 'replace')
                        break
                # no BOM found, so use chardet
                if decoded is None:
                    enc = chardet.detect(text[:1024])  # Guess using first 1KB
                    decoded = text.decode(enc.get('encoding') or 'utf-8',
                                          'replace')
                text = decoded
            else:
                text = text.decode(self.encoding)
                if text.startswith('\ufeff'):
                    text = text[len('\ufeff'):]
        else:
            if text.startswith('\ufeff'):
                text = text[len('\ufeff'):]

        # text now *is* a unicode string
        text = text.replace('\r\n', '\n')
        text = text.replace('\r', '\n')
        if self.stripall:
            text = text.strip()
        elif self.stripnl:
            text = text.strip('\n')
        if self.tabsize > 0:
            text = text.expandtabs(self.tabsize)
        if self.ensurenl and not text.endswith('\n'):
            text += '\n'

        def streamer():
            for _, t, v in self.get_tokens_unprocessed(text):
                yield t, v
        stream = streamer()
        if not unfiltered:
            stream = apply_filters(stream, self.filters, self)
        return stream

    def get_tokens_unprocessed(self, text):
        """
        This method should process the text and return an iterable of
        ``(index, tokentype, value)`` tuples where ``index`` is the starting
        position of the token within the input text.

        It must be overridden by subclasses. It is recommended to
        implement it as a generator to maximize effectiveness.
        """
        raise NotImplementedError


class DelegatingLexer(Lexer):
    """
    This lexer takes two lexer as arguments. A root lexer and
    a language lexer. First everything is scanned using the language
    lexer, afterwards all ``Other`` tokens are lexed using the root
    lexer.

    The lexers from the ``template`` lexer package use this base lexer.
    """

    def __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):
        self.root_lexer = _root_lexer(**options)
        self.language_lexer = _language_lexer(**options)
        self.needle = _needle
        Lexer.__init__(self, **options)

    def get_tokens_unprocessed(self, text):
        buffered = ''
        insertions = []
        lng_buffer = []
        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
            if t is self.needle:
                if lng_buffer:
                    insertions.append((len(buffered), lng_buffer))
                    lng_buffer = []
                buffered += v
            else:
                lng_buffer.append((i, t, v))
        if lng_buffer:
            insertions.append((len(buffered), lng_buffer))
        return do_insertions(insertions,
                             self.root_lexer.get_tokens_unprocessed(buffered))


# ------------------------------------------------------------------------------
# RegexLexer and ExtendedRegexLexer
#


class include(str):  # pylint: disable=invalid-name
    """
    Indicates that a state should include rules from another state.
    """
    pass


class _inherit:
    """
    Indicates the a state should inherit from its superclass.
    """
    def __repr__(self):
        return 'inherit'

inherit = _inherit()  # pylint: disable=invalid-name


class combined(tuple):  # pylint: disable=invalid-name
    """
    Indicates a state combined from multiple states.
    """

    def __new__(cls, *args):
        return tuple.__new__(cls, args)

    def __init__(self, *args):
        # tuple.__init__ doesn't do anything
        pass


class _PseudoMatch:
    """
    A pseudo match object constructed from a string.
    """

    def __init__(self, start, text):
        self._text = text
        self._start = start

    def start(self, arg=None):
        return self._start

    def end(self, arg=None):
        return self._start + len(self._text)

    def group(self, arg=None):
        if arg:
            raise IndexError('No such group')
        return self._text

    def groups(self):
        return (self._text,)

    def groupdict(self):
        return {}


def bygroups(*args):
    """
    Callback that yields multiple actions for each group in the match.
    """
    def callback(lexer, match, ctx=None):
        for i, action in enumerate(args):
            if action is None:
                continue
            elif type(action) is _TokenType:
                data = match.group(i + 1)
                if data:
                    yield match.start(i + 1), action, data
            else:
                data = match.group(i + 1)
                if data is not None:
                    if ctx:
                        ctx.pos = match.start(i + 1)
                    for item in action(lexer,
                                       _PseudoMatch(match.start(i + 1), data), ctx):
                        if item:
                            yield item
        if ctx:
            ctx.pos = match.end()
    return callback


class _This:
    """
    Special singleton used for indicating the caller class.
    Used by ``using``.
    """

this = _This()


def using(_other, **kwargs):
    """
    Callback that processes the match with a different lexer.

    The keyword arguments are forwarded to the lexer, except `state` which
    is handled separately.

    `state` specifies the state that the new lexer will start in, and can
    be an enumerable such as ('root', 'inline', 'string') or a simple
    string which is assumed to be on top of the root state.

    Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.
    """
    gt_kwargs = {}
    if 'state' in kwargs:
        s = kwargs.pop('state')
        if isinstance(s, (list, tuple)):
            gt_kwargs['stack'] = s
        else:
            gt_kwargs['stack'] = ('root', s)

    if _other is this:
        def callback(lexer, match, ctx=None):
            # if keyword arguments are given the callback
            # function has to create a new lexer instance
            if kwargs:
                # XXX: cache that somehow
                kwargs.update(lexer.options)
                lx = lexer.__class__(**kwargs)
            else:
                lx = lexer
            s = match.start()
            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
                yield i + s, t, v
            if ctx:
                ctx.pos = match.end()
    else:
        def callback(lexer, match, ctx=None):
            # XXX: cache that somehow
            kwargs.update(lexer.options)
            lx = _other(**kwargs)

            s = match.start()
            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
                yield i + s, t, v
            if ctx:
                ctx.pos = match.end()
    return callback


class default:
    """
    Indicates a state or state action (e.g. #pop) to apply.
    For example default('#pop') is equivalent to ('', Token, '#pop')
    Note that state tuples may be used as well.

    .. versionadded:: 2.0
    """
    def __init__(self, state):
        self.state = state


class words(Future):
    """
    Indicates a list of literal words that is transformed into an optimized
    regex that matches any of the words.

    .. versionadded:: 2.0
    """
    def __init__(self, words, prefix='', suffix=''):
        self.words = words
        self.prefix = prefix
        self.suffix = suffix

    def get(self):
        return regex_opt(self.words, prefix=self.prefix, suffix=self.suffix)


class RegexLexerMeta(LexerMeta):
    """
    Metaclass for RegexLexer, creates the self._tokens attribute from
    self.tokens on the first instantiation.
    """

    def _process_regex(cls, regex, rflags, state):
        """Preprocess the regular expression component of a token definition."""
        if isinstance(regex, Future):
            regex = regex.get()
        return re.compile(regex, rflags).match

    def _process_token(cls, token):
        """Preprocess the token component of a token definition."""
        assert type(token) is _TokenType or callable(token), \
            'token type must be simple type or callable, not %r' % (token,)
        return token

    def _process_new_state(cls, new_state, unprocessed, processed):
        """Preprocess the state transition action of a token definition."""
        if isinstance(new_state, str):
            # an existing state
            if new_state == '#pop':
                return -1
            elif new_state in unprocessed:
                return (new_state,)
            elif new_state == '#push':
                return new_state
            elif new_state[:5] == '#pop:':
                return -int(new_state[5:])
            else:
                assert False, 'unknown new state %r' % new_state
        elif isinstance(new_state, combined):
            # combine a new state from existing ones
            tmp_state = '_tmp_%d' % cls._tmpname
            cls._tmpname += 1
            itokens = []
            for istate in new_state:
                assert istate != new_state, 'circular state ref %r' % istate
                itokens.extend(cls._process_state(unprocessed,
                                                  processed, istate))
            processed[tmp_state] = itokens
            return (tmp_state,)
        elif isinstance(new_state, tuple):
            # push more than one state
            for istate in new_state:
                assert (istate in unprocessed or
                        istate in ('#pop', '#push')), \
                    'unknown new state ' + istate
            return new_state
        else:
            assert False, 'unknown new state def %r' % new_state

    def _process_state(cls, unprocessed, processed, state):
        """Preprocess a single state definition."""
        assert type(state) is str, "wrong state name %r" % state
        assert state[0] != '#', "invalid state name %r" % state
        if state in processed:
            return processed[state]
        tokens = processed[state] = []
        rflags = cls.flags
        for tdef in unprocessed[state]:
            if isinstance(tdef, include):
                # it's a state reference
                assert tdef != state, "circular state reference %r" % state
                tokens.extend(cls._process_state(unprocessed, processed,
                                                 str(tdef)))
                continue
            if isinstance(tdef, _inherit):
                # should be processed already, but may not in the case of:
                # 1. the state has no counterpart in any parent
                # 2. the state includes more than one 'inherit'
                continue
            if isinstance(tdef, default):
                new_state = cls._process_new_state(tdef.state, unprocessed, processed)
                tokens.append((re.compile('').match, None, new_state))
                continue

            assert type(tdef) is tuple, "wrong rule def %r" % tdef

            try:
                rex = cls._process_regex(tdef[0], rflags, state)
            except Exception as err:
                raise ValueError("uncompilable regex %r in state %r of %r: %s" %
                                 (tdef[0], state, cls, err)) from err

            token = cls._process_token(tdef[1])

            if len(tdef) == 2:
                new_state = None
            else:
                new_state = cls._process_new_state(tdef[2],
                                                   unprocessed, processed)

            tokens.append((rex, token, new_state))
        return tokens

    def process_tokendef(cls, name, tokendefs=None):
        """Preprocess a dictionary of token definitions."""
        processed = cls._all_tokens[name] = {}
        tokendefs = tokendefs or cls.tokens[name]
        for state in list(tokendefs):
            cls._process_state(tokendefs, processed, state)
        return processed

    def get_tokendefs(cls):
        """
        Merge tokens from superclasses in MRO order, returning a single tokendef
        dictionary.

        Any state that is not defined by a subclass will be inherited
        automatically.  States that *are* defined by subclasses will, by
        default, override that state in the superclass.  If a subclass wishes to
        inherit definitions from a superclass, it can use the special value
        "inherit", which will cause the superclass' state definition to be
        included at that point in the state.
        """
        tokens = {}
        inheritable = {}
        for c in cls.__mro__:
            toks = c.__dict__.get('tokens', {})

            for state, items in toks.items():
                curitems = tokens.get(state)
                if curitems is None:
                    # N.b. because this is assigned by reference, sufficiently
                    # deep hierarchies are processed incrementally (e.g. for
                    # A(B), B(C), C(RegexLexer), B will be premodified so X(B)
                    # will not see any inherits in B).
                    tokens[state] = items
                    try:
                        inherit_ndx = items.index(inherit)
                    except ValueError:
                        continue
                    inheritable[state] = inherit_ndx
                    continue

                inherit_ndx = inheritable.pop(state, None)
                if inherit_ndx is None:
                    continue

                # Replace the "inherit" value with the items
                curitems[inherit_ndx:inherit_ndx+1] = items
                try:
                    # N.b. this is the index in items (that is, the superclass
                    # copy), so offset required when storing below.
                    new_inh_ndx = items.index(inherit)
                except ValueError:
                    pass
                else:
                    inheritable[state] = inherit_ndx + new_inh_ndx

        return tokens

    def __call__(cls, *args, **kwds):
        """Instantiate cls after preprocessing its token definitions."""
        if '_tokens' not in cls.__dict__:
            cls._all_tokens = {}
            cls._tmpname = 0
            if hasattr(cls, 'token_variants') and cls.token_variants:
                # don't process yet
                pass
            else:
                cls._tokens = cls.process_tokendef('', cls.get_tokendefs())

        return type.__call__(cls, *args, **kwds)


class RegexLexer(Lexer, metaclass=RegexLexerMeta):
    """
    Base for simple stateful regular expression-based lexers.
    Simplifies the lexing process so that you need only
    provide a list of states and regular expressions.
    """

    #: Flags for compiling the regular expressions.
    #: Defaults to MULTILINE.
    flags = re.MULTILINE

    #: At all time there is a stack of states. Initially, the stack contains
    #: a single state 'root'. The top of the stack is called "the current state".
    #:
    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
    #:
    #: ``new_state`` can be omitted to signify no state transition.
    #: If ``new_state`` is a string, it is pushed on the stack. This ensure
    #: the new current state is ``new_state``.
    #: If ``new_state`` is a tuple of strings, all of those strings are pushed
    #: on the stack and the current state will be the last element of the list.
    #: ``new_state`` can also be ``combined('state1', 'state2', ...)``
    #: to signify a new, anonymous state combined from the rules of two
    #: or more existing ones.
    #: Furthermore, it can be '#pop' to signify going back one step in
    #: the state stack, or '#push' to push the current state on the stack
    #: again. Note that if you push while in a combined state, the combined
    #: state itself is pushed, and not only the state in which the rule is
    #: defined.
    #:
    #: The tuple can also be replaced with ``include('state')``, in which
    #: case the rules from the state named by the string are included in the
    #: current one.
    tokens = {}

    def get_tokens_unprocessed(self, text, stack=('root',)):
        """
        Split ``text`` into (tokentype, text) pairs.

        ``stack`` is the initial stack (default: ``['root']``)
        """
        pos = 0
        tokendefs = self._tokens
        statestack = list(stack)
        statetokens = tokendefs[statestack[-1]]
        while 1:
            for rexmatch, action, new_state in statetokens:
                m = rexmatch(text, pos)
                if m:
                    if action is not None:
                        if type(action) is _TokenType:
                            yield pos, action, m.group()
                        else:
                            yield from action(self, m)
                    pos = m.end()
                    if new_state is not None:
                        # state transition
                        if isinstance(new_state, tuple):
                            for state in new_state:
                                if state == '#pop':
                                    if len(statestack) > 1:
                                        statestack.pop()
                                elif state == '#push':
                                    statestack.append(statestack[-1])
                                else:
                                    statestack.append(state)
                        elif isinstance(new_state, int):
                            # pop, but keep at least one state on the stack
                            # (random code leading to unexpected pops should
                            # not allow exceptions)
                            if abs(new_state) >= len(statestack):
                                del statestack[1:]
                            else:
                                del statestack[new_state:]
                        elif new_state == '#push':
                            statestack.append(statestack[-1])
                        else:
                            assert False, "wrong state def: %r" % new_state
                        statetokens = tokendefs[statestack[-1]]
                    break
            else:
                # We are here only if all state tokens have been considered
                # and there was not a match on any of them.
                try:
                    if text[pos] == '\n':
                        # at EOL, reset state to "root"
                        statestack = ['root']
                        statetokens = tokendefs['root']
                        yield pos, Whitespace, '\n'
                        pos += 1
                        continue
                    yield pos, Error, text[pos]
                    pos += 1
                except IndexError:
                    break


class LexerContext:
    """
    A helper object that holds lexer position data.
    """

    def __init__(self, text, pos, stack=None, end=None):
        self.text = text
        self.pos = pos
        self.end = end or len(text)  # end=0 not supported ;-)
        self.stack = stack or ['root']

    def __repr__(self):
        return 'LexerContext(%r, %r, %r)' % (
            self.text, self.pos, self.stack)


class ExtendedRegexLexer(RegexLexer):
    """
    A RegexLexer that uses a context object to store its state.
    """

    def get_tokens_unprocessed(self, text=None, context=None):
        """
        Split ``text`` into (tokentype, text) pairs.
        If ``context`` is given, use this lexer context instead.
        """
        tokendefs = self._tokens
        if not context:
            ctx = LexerContext(text, 0)
            statetokens = tokendefs['root']
        else:
            ctx = context
            statetokens = tokendefs[ctx.stack[-1]]
            text = ctx.text
        while 1:
            for rexmatch, action, new_state in statetokens:
                m = rexmatch(text, ctx.pos, ctx.end)
                if m:
                    if action is not None:
                        if type(action) is _TokenType:
                            yield ctx.pos, action, m.group()
                            ctx.pos = m.end()
                        else:
                            yield from action(self, m, ctx)
                            if not new_state:
                                # altered the state stack?
                                statetokens = tokendefs[ctx.stack[-1]]
                    # CAUTION: callback must set ctx.pos!
                    if new_state is not None:
                        # state transition
                        if isinstance(new_state, tuple):
                            for state in new_state:
                                if state == '#pop':
                                    if len(ctx.stack) > 1:
                                        ctx.stack.pop()
                                elif state == '#push':
                                    ctx.stack.append(ctx.stack[-1])
                                else:
                                    ctx.stack.append(state)
                        elif isinstance(new_state, int):
                            # see RegexLexer for why this check is made
                            if abs(new_state) >= len(ctx.stack):
                                del ctx.stack[1:]
                            else:
                                del ctx.stack[new_state:]
                        elif new_state == '#push':
                            ctx.stack.append(ctx.stack[-1])
                        else:
                            assert False, "wrong state def: %r" % new_state
                        statetokens = tokendefs[ctx.stack[-1]]
                    break
            else:
                try:
                    if ctx.pos >= ctx.end:
                        break
                    if text[ctx.pos] == '\n':
                        # at EOL, reset state to "root"
                        ctx.stack = ['root']
                        statetokens = tokendefs['root']
                        yield ctx.pos, Text, '\n'
                        ctx.pos += 1
                        continue
                    yield ctx.pos, Error, text[ctx.pos]
                    ctx.pos += 1
                except IndexError:
                    break


def do_insertions(insertions, tokens):
    """
    Helper for lexers which must combine the results of several
    sublexers.

    ``insertions`` is a list of ``(index, itokens)`` pairs.
    Each ``itokens`` iterable should be inserted at position
    ``index`` into the token stream given by the ``tokens``
    argument.

    The result is a combined token stream.

    TODO: clean up the code here.
    """
    insertions = iter(insertions)
    try:
        index, itokens = next(insertions)
    except StopIteration:
        # no insertions
        yield from tokens
        return

    realpos = None
    insleft = True

    # iterate over the token stream where we want to insert
    # the tokens from the insertion list.
    for i, t, v in tokens:
        # first iteration. store the position of first item
        if realpos is None:
            realpos = i
        oldi = 0
        while insleft and i + len(v) >= index:
            tmpval = v[oldi:index - i]
            if tmpval:
                yield realpos, t, tmpval
                realpos += len(tmpval)
            for it_index, it_token, it_value in itokens:
                yield realpos, it_token, it_value
                realpos += len(it_value)
            oldi = index - i
            try:
                index, itokens = next(insertions)
            except StopIteration:
                insleft = False
                break  # not strictly necessary
        if oldi < len(v):
            yield realpos, t, v[oldi:]
            realpos += len(v) - oldi

    # leftover tokens
    while insleft:
        # no normal tokens, set realpos to zero
        realpos = realpos or 0
        for p, t, v in itokens:
            yield realpos, t, v
            realpos += len(v)
        try:
            index, itokens = next(insertions)
        except StopIteration:
            insleft = False
            break  # not strictly necessary


class ProfilingRegexLexerMeta(RegexLexerMeta):
    """Metaclass for ProfilingRegexLexer, collects regex timing info."""

    def _process_regex(cls, regex, rflags, state):
        if isinstance(regex, words):
            rex = regex_opt(regex.words, prefix=regex.prefix,
                            suffix=regex.suffix)
        else:
            rex = regex
        compiled = re.compile(rex, rflags)

        def match_func(text, pos, endpos=sys.maxsize):
            info = cls._prof_data[-1].setdefault((state, rex), [0, 0.0])
            t0 = time.time()
            res = compiled.match(text, pos, endpos)
            t1 = time.time()
            info[0] += 1
            info[1] += t1 - t0
            return res
        return match_func


class ProfilingRegexLexer(RegexLexer, metaclass=ProfilingRegexLexerMeta):
    """Drop-in replacement for RegexLexer that does profiling of its regexes."""

    _prof_data = []
    _prof_sort_index = 4  # defaults to time per call

    def get_tokens_unprocessed(self, text, stack=('root',)):
        # this needs to be a stack, since using(this) will produce nested calls
        self.__class__._prof_data.append({})
        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)
        rawdata = self.__class__._prof_data.pop()
        data = sorted(((s, repr(r).strip('u\'').replace('\\\\', '\\')[:65],
                        n, 1000 * t, 1000 * t / n)
                       for ((s, r), (n, t)) in rawdata.items()),
                      key=lambda x: x[self._prof_sort_index],
                      reverse=True)
        sum_total = sum(x[3] for x in data)

        print()
        print('Profiling result for %s lexing %d chars in %.3f ms' %
              (self.__class__.__name__, len(text), sum_total))
        print('=' * 110)
        print('%-20s %-64s ncalls  tottime  percall' % ('state', 'regex'))
        print('-' * 110)
        for d in data:
            print('%-20s %-65s %5d %8.4f %8.4f' % d)
        print('=' * 110)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/cmdline.py
# ========================================================
"""
    pygments.cmdline
    ~~~~~~~~~~~~~~~~

    Command line interface.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import os
import sys
import shutil
import argparse
from textwrap import dedent

from pip._vendor.pygments import __version__, highlight
from pip._vendor.pygments.util import ClassNotFound, OptionError, docstring_headline, \
    guess_decode, guess_decode_from_terminal, terminal_encoding, \
    UnclosingTextIOWrapper
from pip._vendor.pygments.lexers import get_all_lexers, get_lexer_by_name, guess_lexer, \
    load_lexer_from_file, get_lexer_for_filename, find_lexer_class_for_filename
from pip._vendor.pygments.lexers.special import TextLexer
from pip._vendor.pygments.formatters.latex import LatexEmbeddedLexer, LatexFormatter
from pip._vendor.pygments.formatters import get_all_formatters, get_formatter_by_name, \
    load_formatter_from_file, get_formatter_for_filename, find_formatter_class
from pip._vendor.pygments.formatters.terminal import TerminalFormatter
from pip._vendor.pygments.formatters.terminal256 import Terminal256Formatter, TerminalTrueColorFormatter
from pip._vendor.pygments.filters import get_all_filters, find_filter_class
from pip._vendor.pygments.styles import get_all_styles, get_style_by_name


def _parse_options(o_strs):
    opts = {}
    if not o_strs:
        return opts
    for o_str in o_strs:
        if not o_str.strip():
            continue
        o_args = o_str.split(',')
        for o_arg in o_args:
            o_arg = o_arg.strip()
            try:
                o_key, o_val = o_arg.split('=', 1)
                o_key = o_key.strip()
                o_val = o_val.strip()
            except ValueError:
                opts[o_arg] = True
            else:
                opts[o_key] = o_val
    return opts


def _parse_filters(f_strs):
    filters = []
    if not f_strs:
        return filters
    for f_str in f_strs:
        if ':' in f_str:
            fname, fopts = f_str.split(':', 1)
            filters.append((fname, _parse_options([fopts])))
        else:
            filters.append((f_str, {}))
    return filters


def _print_help(what, name):
    try:
        if what == 'lexer':
            cls = get_lexer_by_name(name)
            print("Help on the %s lexer:" % cls.name)
            print(dedent(cls.__doc__))
        elif what == 'formatter':
            cls = find_formatter_class(name)
            print("Help on the %s formatter:" % cls.name)
            print(dedent(cls.__doc__))
        elif what == 'filter':
            cls = find_filter_class(name)
            print("Help on the %s filter:" % name)
            print(dedent(cls.__doc__))
        return 0
    except (AttributeError, ValueError):
        print("%s not found!" % what, file=sys.stderr)
        return 1


def _print_list(what):
    if what == 'lexer':
        print()
        print("Lexers:")
        print("~~~~~~~")

        info = []
        for fullname, names, exts, _ in get_all_lexers():
            tup = (', '.join(names)+':', fullname,
                   exts and '(filenames ' + ', '.join(exts) + ')' or '')
            info.append(tup)
        info.sort()
        for i in info:
            print(('* %s\n    %s %s') % i)

    elif what == 'formatter':
        print()
        print("Formatters:")
        print("~~~~~~~~~~~")

        info = []
        for cls in get_all_formatters():
            doc = docstring_headline(cls)
            tup = (', '.join(cls.aliases) + ':', doc, cls.filenames and
                   '(filenames ' + ', '.join(cls.filenames) + ')' or '')
            info.append(tup)
        info.sort()
        for i in info:
            print(('* %s\n    %s %s') % i)

    elif what == 'filter':
        print()
        print("Filters:")
        print("~~~~~~~~")

        for name in get_all_filters():
            cls = find_filter_class(name)
            print("* " + name + ':')
            print("    %s" % docstring_headline(cls))

    elif what == 'style':
        print()
        print("Styles:")
        print("~~~~~~~")

        for name in get_all_styles():
            cls = get_style_by_name(name)
            print("* " + name + ':')
            print("    %s" % docstring_headline(cls))


def _print_list_as_json(requested_items):
    import json
    result = {}
    if 'lexer' in requested_items:
        info = {}
        for fullname, names, filenames, mimetypes in get_all_lexers():
            info[fullname] = {
                'aliases': names,
                'filenames': filenames,
                'mimetypes': mimetypes
            }
        result['lexers'] = info

    if 'formatter' in requested_items:
        info = {}
        for cls in get_all_formatters():
            doc = docstring_headline(cls)
            info[cls.name] = {
                'aliases': cls.aliases,
                'filenames': cls.filenames,
                'doc': doc
            }
        result['formatters'] = info

    if 'filter' in requested_items:
        info = {}
        for name in get_all_filters():
            cls = find_filter_class(name)
            info[name] = {
                'doc': docstring_headline(cls)
            }
        result['filters'] = info

    if 'style' in requested_items:
        info = {}
        for name in get_all_styles():
            cls = get_style_by_name(name)
            info[name] = {
                'doc': docstring_headline(cls)
            }
        result['styles'] = info

    json.dump(result, sys.stdout)

def main_inner(parser, argns):
    if argns.help:
        parser.print_help()
        return 0

    if argns.V:
        print('Pygments version %s, (c) 2006-2023 by Georg Brandl, Matthus '
              'Chajdas and contributors.' % __version__)
        return 0

    def is_only_option(opt):
        return not any(v for (k, v) in vars(argns).items() if k != opt)

    # handle ``pygmentize -L``
    if argns.L is not None:
        arg_set = set()
        for k, v in vars(argns).items():
            if v:
                arg_set.add(k)

        arg_set.discard('L')
        arg_set.discard('json')

        if arg_set:
            parser.print_help(sys.stderr)
            return 2

        # print version
        if not argns.json:
            main(['', '-V'])
        allowed_types = {'lexer', 'formatter', 'filter', 'style'}
        largs = [arg.rstrip('s') for arg in argns.L]
        if any(arg not in allowed_types for arg in largs):
            parser.print_help(sys.stderr)
            return 0
        if not largs:
            largs = allowed_types
        if not argns.json:
            for arg in largs:
                _print_list(arg)
        else:
            _print_list_as_json(largs)
        return 0

    # handle ``pygmentize -H``
    if argns.H:
        if not is_only_option('H'):
            parser.print_help(sys.stderr)
            return 2
        what, name = argns.H
        if what not in ('lexer', 'formatter', 'filter'):
            parser.print_help(sys.stderr)
            return 2
        return _print_help(what, name)

    # parse -O options
    parsed_opts = _parse_options(argns.O or [])

    # parse -P options
    for p_opt in argns.P or []:
        try:
            name, value = p_opt.split('=', 1)
        except ValueError:
            parsed_opts[p_opt] = True
        else:
            parsed_opts[name] = value

    # encodings
    inencoding = parsed_opts.get('inencoding', parsed_opts.get('encoding'))
    outencoding = parsed_opts.get('outencoding', parsed_opts.get('encoding'))

    # handle ``pygmentize -N``
    if argns.N:
        lexer = find_lexer_class_for_filename(argns.N)
        if lexer is None:
            lexer = TextLexer

        print(lexer.aliases[0])
        return 0

    # handle ``pygmentize -C``
    if argns.C:
        inp = sys.stdin.buffer.read()
        try:
            lexer = guess_lexer(inp, inencoding=inencoding)
        except ClassNotFound:
            lexer = TextLexer

        print(lexer.aliases[0])
        return 0

    # handle ``pygmentize -S``
    S_opt = argns.S
    a_opt = argns.a
    if S_opt is not None:
        f_opt = argns.f
        if not f_opt:
            parser.print_help(sys.stderr)
            return 2
        if argns.l or argns.INPUTFILE:
            parser.print_help(sys.stderr)
            return 2

        try:
            parsed_opts['style'] = S_opt
            fmter = get_formatter_by_name(f_opt, **parsed_opts)
        except ClassNotFound as err:
            print(err, file=sys.stderr)
            return 1

        print(fmter.get_style_defs(a_opt or ''))
        return 0

    # if no -S is given, -a is not allowed
    if argns.a is not None:
        parser.print_help(sys.stderr)
        return 2

    # parse -F options
    F_opts = _parse_filters(argns.F or [])

    # -x: allow custom (eXternal) lexers and formatters
    allow_custom_lexer_formatter = bool(argns.x)

    # select lexer
    lexer = None

    # given by name?
    lexername = argns.l
    if lexername:
        # custom lexer, located relative to user's cwd
        if allow_custom_lexer_formatter and '.py' in lexername:
            try:
                filename = None
                name = None
                if ':' in lexername:
                    filename, name = lexername.rsplit(':', 1)

                    if '.py' in name:
                        # This can happen on Windows: If the lexername is
                        # C:\lexer.py -- return to normal load path in that case
                        name = None

                if filename and name:
                    lexer = load_lexer_from_file(filename, name,
                                                 **parsed_opts)
                else:
                    lexer = load_lexer_from_file(lexername, **parsed_opts)
            except ClassNotFound as err:
                print('Error:', err, file=sys.stderr)
                return 1
        else:
            try:
                lexer = get_lexer_by_name(lexername, **parsed_opts)
            except (OptionError, ClassNotFound) as err:
                print('Error:', err, file=sys.stderr)
                return 1

    # read input code
    code = None

    if argns.INPUTFILE:
        if argns.s:
            print('Error: -s option not usable when input file specified',
                  file=sys.stderr)
            return 2

        infn = argns.INPUTFILE
        try:
            with open(infn, 'rb') as infp:
                code = infp.read()
        except Exception as err:
            print('Error: cannot read infile:', err, file=sys.stderr)
            return 1
        if not inencoding:
            code, inencoding = guess_decode(code)

        # do we have to guess the lexer?
        if not lexer:
            try:
                lexer = get_lexer_for_filename(infn, code, **parsed_opts)
            except ClassNotFound as err:
                if argns.g:
                    try:
                        lexer = guess_lexer(code, **parsed_opts)
                    except ClassNotFound:
                        lexer = TextLexer(**parsed_opts)
                else:
                    print('Error:', err, file=sys.stderr)
                    return 1
            except OptionError as err:
                print('Error:', err, file=sys.stderr)
                return 1

    elif not argns.s:  # treat stdin as full file (-s support is later)
        # read code from terminal, always in binary mode since we want to
        # decode ourselves and be tolerant with it
        code = sys.stdin.buffer.read()  # use .buffer to get a binary stream
        if not inencoding:
            code, inencoding = guess_decode_from_terminal(code, sys.stdin)
            # else the lexer will do the decoding
        if not lexer:
            try:
                lexer = guess_lexer(code, **parsed_opts)
            except ClassNotFound:
                lexer = TextLexer(**parsed_opts)

    else:  # -s option needs a lexer with -l
        if not lexer:
            print('Error: when using -s a lexer has to be selected with -l',
                  file=sys.stderr)
            return 2

    # process filters
    for fname, fopts in F_opts:
        try:
            lexer.add_filter(fname, **fopts)
        except ClassNotFound as err:
            print('Error:', err, file=sys.stderr)
            return 1

    # select formatter
    outfn = argns.o
    fmter = argns.f
    if fmter:
        # custom formatter, located relative to user's cwd
        if allow_custom_lexer_formatter and '.py' in fmter:
            try:
                filename = None
                name = None
                if ':' in fmter:
                    # Same logic as above for custom lexer
                    filename, name = fmter.rsplit(':', 1)

                    if '.py' in name:
                        name = None

                if filename and name:
                    fmter = load_formatter_from_file(filename, name,
                                                     **parsed_opts)
                else:
                    fmter = load_formatter_from_file(fmter, **parsed_opts)
            except ClassNotFound as err:
                print('Error:', err, file=sys.stderr)
                return 1
        else:
            try:
                fmter = get_formatter_by_name(fmter, **parsed_opts)
            except (OptionError, ClassNotFound) as err:
                print('Error:', err, file=sys.stderr)
                return 1

    if outfn:
        if not fmter:
            try:
                fmter = get_formatter_for_filename(outfn, **parsed_opts)
            except (OptionError, ClassNotFound) as err:
                print('Error:', err, file=sys.stderr)
                return 1
        try:
            outfile = open(outfn, 'wb')
        except Exception as err:
            print('Error: cannot open outfile:', err, file=sys.stderr)
            return 1
    else:
        if not fmter:
            if os.environ.get('COLORTERM','') in ('truecolor', '24bit'):
                fmter = TerminalTrueColorFormatter(**parsed_opts)
            elif '256' in os.environ.get('TERM', ''):
                fmter = Terminal256Formatter(**parsed_opts)
            else:
                fmter = TerminalFormatter(**parsed_opts)
        outfile = sys.stdout.buffer

    # determine output encoding if not explicitly selected
    if not outencoding:
        if outfn:
            # output file? use lexer encoding for now (can still be None)
            fmter.encoding = inencoding
        else:
            # else use terminal encoding
            fmter.encoding = terminal_encoding(sys.stdout)

    # provide coloring under Windows, if possible
    if not outfn and sys.platform in ('win32', 'cygwin') and \
       fmter.name in ('Terminal', 'Terminal256'):  # pragma: no cover
        # unfortunately colorama doesn't support binary streams on Py3
        outfile = UnclosingTextIOWrapper(outfile, encoding=fmter.encoding)
        fmter.encoding = None
        try:
            import pip._vendor.colorama.initialise as colorama_initialise
        except ImportError:
            pass
        else:
            outfile = colorama_initialise.wrap_stream(
                outfile, convert=None, strip=None, autoreset=False, wrap=True)

    # When using the LaTeX formatter and the option `escapeinside` is
    # specified, we need a special lexer which collects escaped text
    # before running the chosen language lexer.
    escapeinside = parsed_opts.get('escapeinside', '')
    if len(escapeinside) == 2 and isinstance(fmter, LatexFormatter):
        left = escapeinside[0]
        right = escapeinside[1]
        lexer = LatexEmbeddedLexer(left, right, lexer)

    # ... and do it!
    if not argns.s:
        # process whole input as per normal...
        try:
            highlight(code, lexer, fmter, outfile)
        finally:
            if outfn:
                outfile.close()
        return 0
    else:
        # line by line processing of stdin (eg: for 'tail -f')...
        try:
            while 1:
                line = sys.stdin.buffer.readline()
                if not line:
                    break
                if not inencoding:
                    line = guess_decode_from_terminal(line, sys.stdin)[0]
                highlight(line, lexer, fmter, outfile)
                if hasattr(outfile, 'flush'):
                    outfile.flush()
            return 0
        except KeyboardInterrupt:  # pragma: no cover
            return 0
        finally:
            if outfn:
                outfile.close()


class HelpFormatter(argparse.HelpFormatter):
    def __init__(self, prog, indent_increment=2, max_help_position=16, width=None):
        if width is None:
            try:
                width = shutil.get_terminal_size().columns - 2
            except Exception:
                pass
        argparse.HelpFormatter.__init__(self, prog, indent_increment,
                                        max_help_position, width)


def main(args=sys.argv):
    """
    Main command line entry point.
    """
    desc = "Highlight an input file and write the result to an output file."
    parser = argparse.ArgumentParser(description=desc, add_help=False,
                                     formatter_class=HelpFormatter)

    operation = parser.add_argument_group('Main operation')
    lexersel = operation.add_mutually_exclusive_group()
    lexersel.add_argument(
        '-l', metavar='LEXER',
        help='Specify the lexer to use.  (Query names with -L.)  If not '
        'given and -g is not present, the lexer is guessed from the filename.')
    lexersel.add_argument(
        '-g', action='store_true',
        help='Guess the lexer from the file contents, or pass through '
        'as plain text if nothing can be guessed.')
    operation.add_argument(
        '-F', metavar='FILTER[:options]', action='append',
        help='Add a filter to the token stream.  (Query names with -L.) '
        'Filter options are given after a colon if necessary.')
    operation.add_argument(
        '-f', metavar='FORMATTER',
        help='Specify the formatter to use.  (Query names with -L.) '
        'If not given, the formatter is guessed from the output filename, '
        'and defaults to the terminal formatter if the output is to the '
        'terminal or an unknown file extension.')
    operation.add_argument(
        '-O', metavar='OPTION=value[,OPTION=value,...]', action='append',
        help='Give options to the lexer and formatter as a comma-separated '
        'list of key-value pairs. '
        'Example: `-O bg=light,python=cool`.')
    operation.add_argument(
        '-P', metavar='OPTION=value', action='append',
        help='Give a single option to the lexer and formatter - with this '
        'you can pass options whose value contains commas and equal signs. '
        'Example: `-P "heading=Pygments, the Python highlighter"`.')
    operation.add_argument(
        '-o', metavar='OUTPUTFILE',
        help='Where to write the output.  Defaults to standard output.')

    operation.add_argument(
        'INPUTFILE', nargs='?',
        help='Where to read the input.  Defaults to standard input.')

    flags = parser.add_argument_group('Operation flags')
    flags.add_argument(
        '-v', action='store_true',
        help='Print a detailed traceback on unhandled exceptions, which '
        'is useful for debugging and bug reports.')
    flags.add_argument(
        '-s', action='store_true',
        help='Process lines one at a time until EOF, rather than waiting to '
        'process the entire file.  This only works for stdin, only for lexers '
        'with no line-spanning constructs, and is intended for streaming '
        'input such as you get from `tail -f`. '
        'Example usage: `tail -f sql.log | pygmentize -s -l sql`.')
    flags.add_argument(
        '-x', action='store_true',
        help='Allow custom lexers and formatters to be loaded from a .py file '
        'relative to the current working directory. For example, '
        '`-l ./customlexer.py -x`. By default, this option expects a file '
        'with a class named CustomLexer or CustomFormatter; you can also '
        'specify your own class name with a colon (`-l ./lexer.py:MyLexer`). '
        'Users should be very careful not to use this option with untrusted '
        'files, because it will import and run them.')
    flags.add_argument('--json', help='Output as JSON. This can '
        'be only used in conjunction with -L.',
        default=False,
        action='store_true')

    special_modes_group = parser.add_argument_group(
        'Special modes - do not do any highlighting')
    special_modes = special_modes_group.add_mutually_exclusive_group()
    special_modes.add_argument(
        '-S', metavar='STYLE -f formatter',
        help='Print style definitions for STYLE for a formatter '
        'given with -f. The argument given by -a is formatter '
        'dependent.')
    special_modes.add_argument(
        '-L', nargs='*', metavar='WHAT',
        help='List lexers, formatters, styles or filters -- '
        'give additional arguments for the thing(s) you want to list '
        '(e.g. "styles"), or omit them to list everything.')
    special_modes.add_argument(
        '-N', metavar='FILENAME',
        help='Guess and print out a lexer name based solely on the given '
        'filename. Does not take input or highlight anything. If no specific '
        'lexer can be determined, "text" is printed.')
    special_modes.add_argument(
        '-C', action='store_true',
        help='Like -N, but print out a lexer name based solely on '
        'a given content from standard input.')
    special_modes.add_argument(
        '-H', action='store', nargs=2, metavar=('NAME', 'TYPE'),
        help='Print detailed help for the object <name> of type <type>, '
        'where <type> is one of "lexer", "formatter" or "filter".')
    special_modes.add_argument(
        '-V', action='store_true',
        help='Print the package version.')
    special_modes.add_argument(
        '-h', '--help', action='store_true',
        help='Print this help.')
    special_modes_group.add_argument(
        '-a', metavar='ARG',
        help='Formatter-specific additional argument for the -S (print '
        'style sheet) mode.')

    argns = parser.parse_args(args[1:])

    try:
        return main_inner(parser, argns)
    except BrokenPipeError:
        # someone closed our stdout, e.g. by quitting a pager.
        return 0
    except Exception:
        if argns.v:
            print(file=sys.stderr)
            print('*' * 65, file=sys.stderr)
            print('An unhandled exception occurred while highlighting.',
                  file=sys.stderr)
            print('Please report the whole traceback to the issue tracker at',
                  file=sys.stderr)
            print('<https://github.com/pygments/pygments/issues>.',
                  file=sys.stderr)
            print('*' * 65, file=sys.stderr)
            print(file=sys.stderr)
            raise
        import traceback
        info = traceback.format_exception(*sys.exc_info())
        msg = info[-1].strip()
        if len(info) >= 3:
            # extract relevant file and position info
            msg += '\n   (f%s)' % info[-2].split('\n')[0].strip()[1:]
        print(file=sys.stderr)
        print('*** Error while highlighting:', file=sys.stderr)
        print(msg, file=sys.stderr)
        print('*** If this is a bug you want to report, please rerun with -v.',
              file=sys.stderr)
        return 1


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/scanner.py
# ========================================================
"""
    pygments.scanner
    ~~~~~~~~~~~~~~~~

    This library implements a regex based scanner. Some languages
    like Pascal are easy to parse but have some keywords that
    depend on the context. Because of this it's impossible to lex
    that just by using a regular expression lexer like the
    `RegexLexer`.

    Have a look at the `DelphiLexer` to get an idea of how to use
    this scanner.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""
import re


class EndOfText(RuntimeError):
    """
    Raise if end of text is reached and the user
    tried to call a match function.
    """


class Scanner:
    """
    Simple scanner

    All method patterns are regular expression strings (not
    compiled expressions!)
    """

    def __init__(self, text, flags=0):
        """
        :param text:    The text which should be scanned
        :param flags:   default regular expression flags
        """
        self.data = text
        self.data_length = len(text)
        self.start_pos = 0
        self.pos = 0
        self.flags = flags
        self.last = None
        self.match = None
        self._re_cache = {}

    def eos(self):
        """`True` if the scanner reached the end of text."""
        return self.pos >= self.data_length
    eos = property(eos, eos.__doc__)

    def check(self, pattern):
        """
        Apply `pattern` on the current position and return
        the match object. (Doesn't touch pos). Use this for
        lookahead.
        """
        if self.eos:
            raise EndOfText()
        if pattern not in self._re_cache:
            self._re_cache[pattern] = re.compile(pattern, self.flags)
        return self._re_cache[pattern].match(self.data, self.pos)

    def test(self, pattern):
        """Apply a pattern on the current position and check
        if it patches. Doesn't touch pos.
        """
        return self.check(pattern) is not None

    def scan(self, pattern):
        """
        Scan the text for the given pattern and update pos/match
        and related fields. The return value is a boolean that
        indicates if the pattern matched. The matched value is
        stored on the instance as ``match``, the last value is
        stored as ``last``. ``start_pos`` is the position of the
        pointer before the pattern was matched, ``pos`` is the
        end position.
        """
        if self.eos:
            raise EndOfText()
        if pattern not in self._re_cache:
            self._re_cache[pattern] = re.compile(pattern, self.flags)
        self.last = self.match
        m = self._re_cache[pattern].match(self.data, self.pos)
        if m is None:
            return False
        self.start_pos = m.start()
        self.pos = m.end()
        self.match = m.group()
        return True

    def get_char(self):
        """Scan exactly one char."""
        self.scan('.')

    def __repr__(self):
        return '<%s %d/%d>' % (
            self.__class__.__name__,
            self.pos,
            self.data_length
        )


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/__init__.py
# ========================================================
"""
    Pygments
    ~~~~~~~~

    Pygments is a syntax highlighting package written in Python.

    It is a generic syntax highlighter for general use in all kinds of software
    such as forum systems, wikis or other applications that need to prettify
    source code. Highlights are:

    * a wide range of common languages and markup formats is supported
    * special attention is paid to details, increasing quality by a fair amount
    * support for new languages and formats are added easily
    * a number of output formats, presently HTML, LaTeX, RTF, SVG, all image
      formats that PIL supports, and ANSI sequences
    * it is usable as a command-line tool and as a library
    * ... and it highlights even Brainfuck!

    The `Pygments master branch`_ is installable with ``easy_install Pygments==dev``.

    .. _Pygments master branch:
       https://github.com/pygments/pygments/archive/master.zip#egg=Pygments-dev

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""
from io import StringIO, BytesIO

__version__ = '2.15.1'
__docformat__ = 'restructuredtext'

__all__ = ['lex', 'format', 'highlight']


def lex(code, lexer):
    """
    Lex `code` with the `lexer` (must be a `Lexer` instance)
    and return an iterable of tokens. Currently, this only calls
    `lexer.get_tokens()`.
    """
    try:
        return lexer.get_tokens(code)
    except TypeError:
        # Heuristic to catch a common mistake.
        from pip._vendor.pygments.lexer import RegexLexer
        if isinstance(lexer, type) and issubclass(lexer, RegexLexer):
            raise TypeError('lex() argument must be a lexer instance, '
                            'not a class')
        raise


def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
    """
    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
    (a `Formatter` instance).

    If ``outfile`` is given and a valid file object (an object with a
    ``write`` method), the result will be written to it, otherwise it
    is returned as a string.
    """
    try:
        if not outfile:
            realoutfile = getattr(formatter, 'encoding', None) and BytesIO() or StringIO()
            formatter.format(tokens, realoutfile)
            return realoutfile.getvalue()
        else:
            formatter.format(tokens, outfile)
    except TypeError:
        # Heuristic to catch a common mistake.
        from pip._vendor.pygments.formatter import Formatter
        if isinstance(formatter, type) and issubclass(formatter, Formatter):
            raise TypeError('format() argument must be a formatter instance, '
                            'not a class')
        raise


def highlight(code, lexer, formatter, outfile=None):
    """
    This is the most high-level highlighting function. It combines `lex` and
    `format` in one function.
    """
    return format(lex(code, lexer), formatter, outfile)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/sphinxext.py
# ========================================================
"""
    pygments.sphinxext
    ~~~~~~~~~~~~~~~~~~

    Sphinx extension to generate automatic documentation of lexers,
    formatters and filters.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import sys

from docutils import nodes
from docutils.statemachine import ViewList
from docutils.parsers.rst import Directive
from sphinx.util.nodes import nested_parse_with_titles


MODULEDOC = '''
.. module:: %s

%s
%s
'''

LEXERDOC = '''
.. class:: %s

    :Short names: %s
    :Filenames:   %s
    :MIME types:  %s

    %s

'''

FMTERDOC = '''
.. class:: %s

    :Short names: %s
    :Filenames: %s

    %s

'''

FILTERDOC = '''
.. class:: %s

    :Name: %s

    %s

'''


class PygmentsDoc(Directive):
    """
    A directive to collect all lexers/formatters/filters and generate
    autoclass directives for them.
    """
    has_content = False
    required_arguments = 1
    optional_arguments = 0
    final_argument_whitespace = False
    option_spec = {}

    def run(self):
        self.filenames = set()
        if self.arguments[0] == 'lexers':
            out = self.document_lexers()
        elif self.arguments[0] == 'formatters':
            out = self.document_formatters()
        elif self.arguments[0] == 'filters':
            out = self.document_filters()
        elif self.arguments[0] == 'lexers_overview':
            out = self.document_lexers_overview()
        else:
            raise Exception('invalid argument for "pygmentsdoc" directive')
        node = nodes.compound()
        vl = ViewList(out.split('\n'), source='')
        nested_parse_with_titles(self.state, vl, node)
        for fn in self.filenames:
            self.state.document.settings.record_dependencies.add(fn)
        return node.children

    def document_lexers_overview(self):
        """Generate a tabular overview of all lexers.

        The columns are the lexer name, the extensions handled by this lexer
        (or "None"), the aliases and a link to the lexer class."""
        from pip._vendor.pygments.lexers._mapping import LEXERS
        from pip._vendor.pygments.lexers import find_lexer_class
        out = []

        table = []

        def format_link(name, url):
            if url:
                return f'`{name} <{url}>`_'
            return name

        for classname, data in sorted(LEXERS.items(), key=lambda x: x[1][1].lower()):
            lexer_cls = find_lexer_class(data[1])
            extensions = lexer_cls.filenames + lexer_cls.alias_filenames

            table.append({
                'name': format_link(data[1], lexer_cls.url),
                'extensions': ', '.join(extensions).replace('*', '\\*').replace('_', '\\') or 'None',
                'aliases': ', '.join(data[2]),
                'class': f'{data[0]}.{classname}'
            })

        column_names = ['name', 'extensions', 'aliases', 'class']
        column_lengths = [max([len(row[column]) for row in table if row[column]])
                          for column in column_names]

        def write_row(*columns):
            """Format a table row"""
            out = []
            for l, c in zip(column_lengths, columns):
                if c:
                    out.append(c.ljust(l))
                else:
                    out.append(' '*l)

            return ' '.join(out)

        def write_seperator():
            """Write a table separator row"""
            sep = ['='*c for c in column_lengths]
            return write_row(*sep)

        out.append(write_seperator())
        out.append(write_row('Name', 'Extension(s)', 'Short name(s)', 'Lexer class'))
        out.append(write_seperator())
        for row in table:
            out.append(write_row(
                row['name'],
                row['extensions'],
                row['aliases'],
                f':class:`~{row["class"]}`'))
        out.append(write_seperator())

        return '\n'.join(out)

    def document_lexers(self):
        from pip._vendor.pygments.lexers._mapping import LEXERS
        out = []
        modules = {}
        moduledocstrings = {}
        for classname, data in sorted(LEXERS.items(), key=lambda x: x[0]):
            module = data[0]
            mod = __import__(module, None, None, [classname])
            self.filenames.add(mod.__file__)
            cls = getattr(mod, classname)
            if not cls.__doc__:
                print("Warning: %s does not have a docstring." % classname)
            docstring = cls.__doc__
            if isinstance(docstring, bytes):
                docstring = docstring.decode('utf8')
            modules.setdefault(module, []).append((
                classname,
                ', '.join(data[2]) or 'None',
                ', '.join(data[3]).replace('*', '\\*').replace('_', '\\') or 'None',
                ', '.join(data[4]) or 'None',
                docstring))
            if module not in moduledocstrings:
                moddoc = mod.__doc__
                if isinstance(moddoc, bytes):
                    moddoc = moddoc.decode('utf8')
                moduledocstrings[module] = moddoc

        for module, lexers in sorted(modules.items(), key=lambda x: x[0]):
            if moduledocstrings[module] is None:
                raise Exception("Missing docstring for %s" % (module,))
            heading = moduledocstrings[module].splitlines()[4].strip().rstrip('.')
            out.append(MODULEDOC % (module, heading, '-'*len(heading)))
            for data in lexers:
                out.append(LEXERDOC % data)

        return ''.join(out)

    def document_formatters(self):
        from pip._vendor.pygments.formatters import FORMATTERS

        out = []
        for classname, data in sorted(FORMATTERS.items(), key=lambda x: x[0]):
            module = data[0]
            mod = __import__(module, None, None, [classname])
            self.filenames.add(mod.__file__)
            cls = getattr(mod, classname)
            docstring = cls.__doc__
            if isinstance(docstring, bytes):
                docstring = docstring.decode('utf8')
            heading = cls.__name__
            out.append(FMTERDOC % (heading, ', '.join(data[2]) or 'None',
                                   ', '.join(data[3]).replace('*', '\\*') or 'None',
                                   docstring))
        return ''.join(out)

    def document_filters(self):
        from pip._vendor.pygments.filters import FILTERS

        out = []
        for name, cls in FILTERS.items():
            self.filenames.add(sys.modules[cls.__module__].__file__)
            docstring = cls.__doc__
            if isinstance(docstring, bytes):
                docstring = docstring.decode('utf8')
            out.append(FILTERDOC % (cls.__name__, name, docstring))
        return ''.join(out)


def setup(app):
    app.add_directive('pygmentsdoc', PygmentsDoc)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/__main__.py
# ========================================================
"""
    pygments.__main__
    ~~~~~~~~~~~~~~~~~

    Main entry point for ``python -m pygments``.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import sys
from pip._vendor.pygments.cmdline import main

try:
    sys.exit(main(sys.argv))
except KeyboardInterrupt:
    sys.exit(1)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/token.py
# ========================================================
"""
    pygments.token
    ~~~~~~~~~~~~~~

    Basic token types and the standard tokens.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""


class _TokenType(tuple):
    parent = None

    def split(self):
        buf = []
        node = self
        while node is not None:
            buf.append(node)
            node = node.parent
        buf.reverse()
        return buf

    def __init__(self, *args):
        # no need to call super.__init__
        self.subtypes = set()

    def __contains__(self, val):
        return self is val or (
            type(val) is self.__class__ and
            val[:len(self)] == self
        )

    def __getattr__(self, val):
        if not val or not val[0].isupper():
            return tuple.__getattribute__(self, val)
        new = _TokenType(self + (val,))
        setattr(self, val, new)
        self.subtypes.add(new)
        new.parent = self
        return new

    def __repr__(self):
        return 'Token' + (self and '.' or '') + '.'.join(self)

    def __copy__(self):
        # These instances are supposed to be singletons
        return self

    def __deepcopy__(self, memo):
        # These instances are supposed to be singletons
        return self


Token = _TokenType()

# Special token types
Text = Token.Text
Whitespace = Text.Whitespace
Escape = Token.Escape
Error = Token.Error
# Text that doesn't belong to this lexer (e.g. HTML in PHP)
Other = Token.Other

# Common token types for source code
Keyword = Token.Keyword
Name = Token.Name
Literal = Token.Literal
String = Literal.String
Number = Literal.Number
Punctuation = Token.Punctuation
Operator = Token.Operator
Comment = Token.Comment

# Generic types for non-source code
Generic = Token.Generic

# String and some others are not direct children of Token.
# alias them:
Token.Token = Token
Token.String = String
Token.Number = Number


def is_token_subtype(ttype, other):
    """
    Return True if ``ttype`` is a subtype of ``other``.

    exists for backwards compatibility. use ``ttype in other`` now.
    """
    return ttype in other


def string_to_tokentype(s):
    """
    Convert a string into a token type::

        >>> string_to_token('String.Double')
        Token.Literal.String.Double
        >>> string_to_token('Token.Literal.Number')
        Token.Literal.Number
        >>> string_to_token('')
        Token

    Tokens that are already tokens are returned unchanged:

        >>> string_to_token(String)
        Token.Literal.String
    """
    if isinstance(s, _TokenType):
        return s
    if not s:
        return Token
    node = Token
    for item in s.split('.'):
        node = getattr(node, item)
    return node


# Map standard token types to short names, used in CSS class naming.
# If you add a new item, please be sure to run this file to perform
# a consistency check for duplicate values.
STANDARD_TYPES = {
    Token:                         '',

    Text:                          '',
    Whitespace:                    'w',
    Escape:                        'esc',
    Error:                         'err',
    Other:                         'x',

    Keyword:                       'k',
    Keyword.Constant:              'kc',
    Keyword.Declaration:           'kd',
    Keyword.Namespace:             'kn',
    Keyword.Pseudo:                'kp',
    Keyword.Reserved:              'kr',
    Keyword.Type:                  'kt',

    Name:                          'n',
    Name.Attribute:                'na',
    Name.Builtin:                  'nb',
    Name.Builtin.Pseudo:           'bp',
    Name.Class:                    'nc',
    Name.Constant:                 'no',
    Name.Decorator:                'nd',
    Name.Entity:                   'ni',
    Name.Exception:                'ne',
    Name.Function:                 'nf',
    Name.Function.Magic:           'fm',
    Name.Property:                 'py',
    Name.Label:                    'nl',
    Name.Namespace:                'nn',
    Name.Other:                    'nx',
    Name.Tag:                      'nt',
    Name.Variable:                 'nv',
    Name.Variable.Class:           'vc',
    Name.Variable.Global:          'vg',
    Name.Variable.Instance:        'vi',
    Name.Variable.Magic:           'vm',

    Literal:                       'l',
    Literal.Date:                  'ld',

    String:                        's',
    String.Affix:                  'sa',
    String.Backtick:               'sb',
    String.Char:                   'sc',
    String.Delimiter:              'dl',
    String.Doc:                    'sd',
    String.Double:                 's2',
    String.Escape:                 'se',
    String.Heredoc:                'sh',
    String.Interpol:               'si',
    String.Other:                  'sx',
    String.Regex:                  'sr',
    String.Single:                 's1',
    String.Symbol:                 'ss',

    Number:                        'm',
    Number.Bin:                    'mb',
    Number.Float:                  'mf',
    Number.Hex:                    'mh',
    Number.Integer:                'mi',
    Number.Integer.Long:           'il',
    Number.Oct:                    'mo',

    Operator:                      'o',
    Operator.Word:                 'ow',

    Punctuation:                   'p',
    Punctuation.Marker:            'pm',

    Comment:                       'c',
    Comment.Hashbang:              'ch',
    Comment.Multiline:             'cm',
    Comment.Preproc:               'cp',
    Comment.PreprocFile:           'cpf',
    Comment.Single:                'c1',
    Comment.Special:               'cs',

    Generic:                       'g',
    Generic.Deleted:               'gd',
    Generic.Emph:                  'ge',
    Generic.Error:                 'gr',
    Generic.Heading:               'gh',
    Generic.Inserted:              'gi',
    Generic.Output:                'go',
    Generic.Prompt:                'gp',
    Generic.Strong:                'gs',
    Generic.Subheading:            'gu',
    Generic.Traceback:             'gt',
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/style.py
# ========================================================
"""
    pygments.style
    ~~~~~~~~~~~~~~

    Basic style object.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.token import Token, STANDARD_TYPES

# Default mapping of ansixxx to RGB colors.
_ansimap = {
    # dark
    'ansiblack': '000000',
    'ansired': '7f0000',
    'ansigreen': '007f00',
    'ansiyellow': '7f7fe0',
    'ansiblue': '00007f',
    'ansimagenta': '7f007f',
    'ansicyan': '007f7f',
    'ansigray': 'e5e5e5',
    # normal
    'ansibrightblack': '555555',
    'ansibrightred': 'ff0000',
    'ansibrightgreen': '00ff00',
    'ansibrightyellow': 'ffff00',
    'ansibrightblue': '0000ff',
    'ansibrightmagenta': 'ff00ff',
    'ansibrightcyan': '00ffff',
    'ansiwhite': 'ffffff',
}
# mapping of deprecated #ansixxx colors to new color names
_deprecated_ansicolors = {
    # dark
    '#ansiblack': 'ansiblack',
    '#ansidarkred': 'ansired',
    '#ansidarkgreen': 'ansigreen',
    '#ansibrown': 'ansiyellow',
    '#ansidarkblue': 'ansiblue',
    '#ansipurple': 'ansimagenta',
    '#ansiteal': 'ansicyan',
    '#ansilightgray': 'ansigray',
    # normal
    '#ansidarkgray': 'ansibrightblack',
    '#ansired': 'ansibrightred',
    '#ansigreen': 'ansibrightgreen',
    '#ansiyellow': 'ansibrightyellow',
    '#ansiblue': 'ansibrightblue',
    '#ansifuchsia': 'ansibrightmagenta',
    '#ansiturquoise': 'ansibrightcyan',
    '#ansiwhite': 'ansiwhite',
}
ansicolors = set(_ansimap)


class StyleMeta(type):

    def __new__(mcs, name, bases, dct):
        obj = type.__new__(mcs, name, bases, dct)
        for token in STANDARD_TYPES:
            if token not in obj.styles:
                obj.styles[token] = ''

        def colorformat(text):
            if text in ansicolors:
                return text
            if text[0:1] == '#':
                col = text[1:]
                if len(col) == 6:
                    return col
                elif len(col) == 3:
                    return col[0] * 2 + col[1] * 2 + col[2] * 2
            elif text == '':
                return ''
            elif text.startswith('var') or text.startswith('calc'):
                return text
            assert False, "wrong color format %r" % text

        _styles = obj._styles = {}

        for ttype in obj.styles:
            for token in ttype.split():
                if token in _styles:
                    continue
                ndef = _styles.get(token.parent, None)
                styledefs = obj.styles.get(token, '').split()
                if not ndef or token is None:
                    ndef = ['', 0, 0, 0, '', '', 0, 0, 0]
                elif 'noinherit' in styledefs and token is not Token:
                    ndef = _styles[Token][:]
                else:
                    ndef = ndef[:]
                _styles[token] = ndef
                for styledef in obj.styles.get(token, '').split():
                    if styledef == 'noinherit':
                        pass
                    elif styledef == 'bold':
                        ndef[1] = 1
                    elif styledef == 'nobold':
                        ndef[1] = 0
                    elif styledef == 'italic':
                        ndef[2] = 1
                    elif styledef == 'noitalic':
                        ndef[2] = 0
                    elif styledef == 'underline':
                        ndef[3] = 1
                    elif styledef == 'nounderline':
                        ndef[3] = 0
                    elif styledef[:3] == 'bg:':
                        ndef[4] = colorformat(styledef[3:])
                    elif styledef[:7] == 'border:':
                        ndef[5] = colorformat(styledef[7:])
                    elif styledef == 'roman':
                        ndef[6] = 1
                    elif styledef == 'sans':
                        ndef[7] = 1
                    elif styledef == 'mono':
                        ndef[8] = 1
                    else:
                        ndef[0] = colorformat(styledef)

        return obj

    def style_for_token(cls, token):
        t = cls._styles[token]
        ansicolor = bgansicolor = None
        color = t[0]
        if color in _deprecated_ansicolors:
            color = _deprecated_ansicolors[color]
        if color in ansicolors:
            ansicolor = color
            color = _ansimap[color]
        bgcolor = t[4]
        if bgcolor in _deprecated_ansicolors:
            bgcolor = _deprecated_ansicolors[bgcolor]
        if bgcolor in ansicolors:
            bgansicolor = bgcolor
            bgcolor = _ansimap[bgcolor]

        return {
            'color':        color or None,
            'bold':         bool(t[1]),
            'italic':       bool(t[2]),
            'underline':    bool(t[3]),
            'bgcolor':      bgcolor or None,
            'border':       t[5] or None,
            'roman':        bool(t[6]) or None,
            'sans':         bool(t[7]) or None,
            'mono':         bool(t[8]) or None,
            'ansicolor':    ansicolor,
            'bgansicolor':  bgansicolor,
        }

    def list_styles(cls):
        return list(cls)

    def styles_token(cls, ttype):
        return ttype in cls._styles

    def __iter__(cls):
        for token in cls._styles:
            yield token, cls.style_for_token(token)

    def __len__(cls):
        return len(cls._styles)


class Style(metaclass=StyleMeta):

    #: overall background color (``None`` means transparent)
    background_color = '#ffffff'

    #: highlight background color
    highlight_color = '#ffffcc'

    #: line number font color
    line_number_color = 'inherit'

    #: line number background color
    line_number_background_color = 'transparent'

    #: special line number font color
    line_number_special_color = '#000000'

    #: special line number background color
    line_number_special_background_color = '#ffffc0'

    #: Style definitions for individual token types.
    styles = {}

    # Attribute for lexers defined within Pygments. If set
    # to True, the style is not shown in the style gallery
    # on the website. This is intended for language-specific
    # styles.
    web_style_gallery_exclude = False


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/unistring.py
# ========================================================
"""
    pygments.unistring
    ~~~~~~~~~~~~~~~~~~

    Strings of all Unicode characters of a certain category.
    Used for matching in Unicode-aware languages. Run to regenerate.

    Inspired by chartypes_create.py from the MoinMoin project.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

Cc = '\x00-\x1f\x7f-\x9f'

Cf = '\xad\u0600-\u0605\u061c\u06dd\u070f\u08e2\u180e\u200b-\u200f\u202a-\u202e\u2060-\u2064\u2066-\u206f\ufeff\ufff9-\ufffb\U000110bd\U000110cd\U0001bca0-\U0001bca3\U0001d173-\U0001d17a\U000e0001\U000e0020-\U000e007f'

Cn = '\u0378-\u0379\u0380-\u0383\u038b\u038d\u03a2\u0530\u0557-\u0558\u058b-\u058c\u0590\u05c8-\u05cf\u05eb-\u05ee\u05f5-\u05ff\u061d\u070e\u074b-\u074c\u07b2-\u07bf\u07fb-\u07fc\u082e-\u082f\u083f\u085c-\u085d\u085f\u086b-\u089f\u08b5\u08be-\u08d2\u0984\u098d-\u098e\u0991-\u0992\u09a9\u09b1\u09b3-\u09b5\u09ba-\u09bb\u09c5-\u09c6\u09c9-\u09ca\u09cf-\u09d6\u09d8-\u09db\u09de\u09e4-\u09e5\u09ff-\u0a00\u0a04\u0a0b-\u0a0e\u0a11-\u0a12\u0a29\u0a31\u0a34\u0a37\u0a3a-\u0a3b\u0a3d\u0a43-\u0a46\u0a49-\u0a4a\u0a4e-\u0a50\u0a52-\u0a58\u0a5d\u0a5f-\u0a65\u0a77-\u0a80\u0a84\u0a8e\u0a92\u0aa9\u0ab1\u0ab4\u0aba-\u0abb\u0ac6\u0aca\u0ace-\u0acf\u0ad1-\u0adf\u0ae4-\u0ae5\u0af2-\u0af8\u0b00\u0b04\u0b0d-\u0b0e\u0b11-\u0b12\u0b29\u0b31\u0b34\u0b3a-\u0b3b\u0b45-\u0b46\u0b49-\u0b4a\u0b4e-\u0b55\u0b58-\u0b5b\u0b5e\u0b64-\u0b65\u0b78-\u0b81\u0b84\u0b8b-\u0b8d\u0b91\u0b96-\u0b98\u0b9b\u0b9d\u0ba0-\u0ba2\u0ba5-\u0ba7\u0bab-\u0bad\u0bba-\u0bbd\u0bc3-\u0bc5\u0bc9\u0bce-\u0bcf\u0bd1-\u0bd6\u0bd8-\u0be5\u0bfb-\u0bff\u0c0d\u0c11\u0c29\u0c3a-\u0c3c\u0c45\u0c49\u0c4e-\u0c54\u0c57\u0c5b-\u0c5f\u0c64-\u0c65\u0c70-\u0c77\u0c8d\u0c91\u0ca9\u0cb4\u0cba-\u0cbb\u0cc5\u0cc9\u0cce-\u0cd4\u0cd7-\u0cdd\u0cdf\u0ce4-\u0ce5\u0cf0\u0cf3-\u0cff\u0d04\u0d0d\u0d11\u0d45\u0d49\u0d50-\u0d53\u0d64-\u0d65\u0d80-\u0d81\u0d84\u0d97-\u0d99\u0db2\u0dbc\u0dbe-\u0dbf\u0dc7-\u0dc9\u0dcb-\u0dce\u0dd5\u0dd7\u0de0-\u0de5\u0df0-\u0df1\u0df5-\u0e00\u0e3b-\u0e3e\u0e5c-\u0e80\u0e83\u0e85-\u0e86\u0e89\u0e8b-\u0e8c\u0e8e-\u0e93\u0e98\u0ea0\u0ea4\u0ea6\u0ea8-\u0ea9\u0eac\u0eba\u0ebe-\u0ebf\u0ec5\u0ec7\u0ece-\u0ecf\u0eda-\u0edb\u0ee0-\u0eff\u0f48\u0f6d-\u0f70\u0f98\u0fbd\u0fcd\u0fdb-\u0fff\u10c6\u10c8-\u10cc\u10ce-\u10cf\u1249\u124e-\u124f\u1257\u1259\u125e-\u125f\u1289\u128e-\u128f\u12b1\u12b6-\u12b7\u12bf\u12c1\u12c6-\u12c7\u12d7\u1311\u1316-\u1317\u135b-\u135c\u137d-\u137f\u139a-\u139f\u13f6-\u13f7\u13fe-\u13ff\u169d-\u169f\u16f9-\u16ff\u170d\u1715-\u171f\u1737-\u173f\u1754-\u175f\u176d\u1771\u1774-\u177f\u17de-\u17df\u17ea-\u17ef\u17fa-\u17ff\u180f\u181a-\u181f\u1879-\u187f\u18ab-\u18af\u18f6-\u18ff\u191f\u192c-\u192f\u193c-\u193f\u1941-\u1943\u196e-\u196f\u1975-\u197f\u19ac-\u19af\u19ca-\u19cf\u19db-\u19dd\u1a1c-\u1a1d\u1a5f\u1a7d-\u1a7e\u1a8a-\u1a8f\u1a9a-\u1a9f\u1aae-\u1aaf\u1abf-\u1aff\u1b4c-\u1b4f\u1b7d-\u1b7f\u1bf4-\u1bfb\u1c38-\u1c3a\u1c4a-\u1c4c\u1c89-\u1c8f\u1cbb-\u1cbc\u1cc8-\u1ccf\u1cfa-\u1cff\u1dfa\u1f16-\u1f17\u1f1e-\u1f1f\u1f46-\u1f47\u1f4e-\u1f4f\u1f58\u1f5a\u1f5c\u1f5e\u1f7e-\u1f7f\u1fb5\u1fc5\u1fd4-\u1fd5\u1fdc\u1ff0-\u1ff1\u1ff5\u1fff\u2065\u2072-\u2073\u208f\u209d-\u209f\u20c0-\u20cf\u20f1-\u20ff\u218c-\u218f\u2427-\u243f\u244b-\u245f\u2b74-\u2b75\u2b96-\u2b97\u2bc9\u2bff\u2c2f\u2c5f\u2cf4-\u2cf8\u2d26\u2d28-\u2d2c\u2d2e-\u2d2f\u2d68-\u2d6e\u2d71-\u2d7e\u2d97-\u2d9f\u2da7\u2daf\u2db7\u2dbf\u2dc7\u2dcf\u2dd7\u2ddf\u2e4f-\u2e7f\u2e9a\u2ef4-\u2eff\u2fd6-\u2fef\u2ffc-\u2fff\u3040\u3097-\u3098\u3100-\u3104\u3130\u318f\u31bb-\u31bf\u31e4-\u31ef\u321f\u32ff\u4db6-\u4dbf\u9ff0-\u9fff\ua48d-\ua48f\ua4c7-\ua4cf\ua62c-\ua63f\ua6f8-\ua6ff\ua7ba-\ua7f6\ua82c-\ua82f\ua83a-\ua83f\ua878-\ua87f\ua8c6-\ua8cd\ua8da-\ua8df\ua954-\ua95e\ua97d-\ua97f\ua9ce\ua9da-\ua9dd\ua9ff\uaa37-\uaa3f\uaa4e-\uaa4f\uaa5a-\uaa5b\uaac3-\uaada\uaaf7-\uab00\uab07-\uab08\uab0f-\uab10\uab17-\uab1f\uab27\uab2f\uab66-\uab6f\uabee-\uabef\uabfa-\uabff\ud7a4-\ud7af\ud7c7-\ud7ca\ud7fc-\ud7ff\ufa6e-\ufa6f\ufada-\ufaff\ufb07-\ufb12\ufb18-\ufb1c\ufb37\ufb3d\ufb3f\ufb42\ufb45\ufbc2-\ufbd2\ufd40-\ufd4f\ufd90-\ufd91\ufdc8-\ufdef\ufdfe-\ufdff\ufe1a-\ufe1f\ufe53\ufe67\ufe6c-\ufe6f\ufe75\ufefd-\ufefe\uff00\uffbf-\uffc1\uffc8-\uffc9\uffd0-\uffd1\uffd8-\uffd9\uffdd-\uffdf\uffe7\uffef-\ufff8\ufffe-\uffff\U0001000c\U00010027\U0001003b\U0001003e\U0001004e-\U0001004f\U0001005e-\U0001007f\U000100fb-\U000100ff\U00010103-\U00010106\U00010134-\U00010136\U0001018f\U0001019c-\U0001019f\U000101a1-\U000101cf\U000101fe-\U0001027f\U0001029d-\U0001029f\U000102d1-\U000102df\U000102fc-\U000102ff\U00010324-\U0001032c\U0001034b-\U0001034f\U0001037b-\U0001037f\U0001039e\U000103c4-\U000103c7\U000103d6-\U000103ff\U0001049e-\U0001049f\U000104aa-\U000104af\U000104d4-\U000104d7\U000104fc-\U000104ff\U00010528-\U0001052f\U00010564-\U0001056e\U00010570-\U000105ff\U00010737-\U0001073f\U00010756-\U0001075f\U00010768-\U000107ff\U00010806-\U00010807\U00010809\U00010836\U00010839-\U0001083b\U0001083d-\U0001083e\U00010856\U0001089f-\U000108a6\U000108b0-\U000108df\U000108f3\U000108f6-\U000108fa\U0001091c-\U0001091e\U0001093a-\U0001093e\U00010940-\U0001097f\U000109b8-\U000109bb\U000109d0-\U000109d1\U00010a04\U00010a07-\U00010a0b\U00010a14\U00010a18\U00010a36-\U00010a37\U00010a3b-\U00010a3e\U00010a49-\U00010a4f\U00010a59-\U00010a5f\U00010aa0-\U00010abf\U00010ae7-\U00010aea\U00010af7-\U00010aff\U00010b36-\U00010b38\U00010b56-\U00010b57\U00010b73-\U00010b77\U00010b92-\U00010b98\U00010b9d-\U00010ba8\U00010bb0-\U00010bff\U00010c49-\U00010c7f\U00010cb3-\U00010cbf\U00010cf3-\U00010cf9\U00010d28-\U00010d2f\U00010d3a-\U00010e5f\U00010e7f-\U00010eff\U00010f28-\U00010f2f\U00010f5a-\U00010fff\U0001104e-\U00011051\U00011070-\U0001107e\U000110c2-\U000110cc\U000110ce-\U000110cf\U000110e9-\U000110ef\U000110fa-\U000110ff\U00011135\U00011147-\U0001114f\U00011177-\U0001117f\U000111ce-\U000111cf\U000111e0\U000111f5-\U000111ff\U00011212\U0001123f-\U0001127f\U00011287\U00011289\U0001128e\U0001129e\U000112aa-\U000112af\U000112eb-\U000112ef\U000112fa-\U000112ff\U00011304\U0001130d-\U0001130e\U00011311-\U00011312\U00011329\U00011331\U00011334\U0001133a\U00011345-\U00011346\U00011349-\U0001134a\U0001134e-\U0001134f\U00011351-\U00011356\U00011358-\U0001135c\U00011364-\U00011365\U0001136d-\U0001136f\U00011375-\U000113ff\U0001145a\U0001145c\U0001145f-\U0001147f\U000114c8-\U000114cf\U000114da-\U0001157f\U000115b6-\U000115b7\U000115de-\U000115ff\U00011645-\U0001164f\U0001165a-\U0001165f\U0001166d-\U0001167f\U000116b8-\U000116bf\U000116ca-\U000116ff\U0001171b-\U0001171c\U0001172c-\U0001172f\U00011740-\U000117ff\U0001183c-\U0001189f\U000118f3-\U000118fe\U00011900-\U000119ff\U00011a48-\U00011a4f\U00011a84-\U00011a85\U00011aa3-\U00011abf\U00011af9-\U00011bff\U00011c09\U00011c37\U00011c46-\U00011c4f\U00011c6d-\U00011c6f\U00011c90-\U00011c91\U00011ca8\U00011cb7-\U00011cff\U00011d07\U00011d0a\U00011d37-\U00011d39\U00011d3b\U00011d3e\U00011d48-\U00011d4f\U00011d5a-\U00011d5f\U00011d66\U00011d69\U00011d8f\U00011d92\U00011d99-\U00011d9f\U00011daa-\U00011edf\U00011ef9-\U00011fff\U0001239a-\U000123ff\U0001246f\U00012475-\U0001247f\U00012544-\U00012fff\U0001342f-\U000143ff\U00014647-\U000167ff\U00016a39-\U00016a3f\U00016a5f\U00016a6a-\U00016a6d\U00016a70-\U00016acf\U00016aee-\U00016aef\U00016af6-\U00016aff\U00016b46-\U00016b4f\U00016b5a\U00016b62\U00016b78-\U00016b7c\U00016b90-\U00016e3f\U00016e9b-\U00016eff\U00016f45-\U00016f4f\U00016f7f-\U00016f8e\U00016fa0-\U00016fdf\U00016fe2-\U00016fff\U000187f2-\U000187ff\U00018af3-\U0001afff\U0001b11f-\U0001b16f\U0001b2fc-\U0001bbff\U0001bc6b-\U0001bc6f\U0001bc7d-\U0001bc7f\U0001bc89-\U0001bc8f\U0001bc9a-\U0001bc9b\U0001bca4-\U0001cfff\U0001d0f6-\U0001d0ff\U0001d127-\U0001d128\U0001d1e9-\U0001d1ff\U0001d246-\U0001d2df\U0001d2f4-\U0001d2ff\U0001d357-\U0001d35f\U0001d379-\U0001d3ff\U0001d455\U0001d49d\U0001d4a0-\U0001d4a1\U0001d4a3-\U0001d4a4\U0001d4a7-\U0001d4a8\U0001d4ad\U0001d4ba\U0001d4bc\U0001d4c4\U0001d506\U0001d50b-\U0001d50c\U0001d515\U0001d51d\U0001d53a\U0001d53f\U0001d545\U0001d547-\U0001d549\U0001d551\U0001d6a6-\U0001d6a7\U0001d7cc-\U0001d7cd\U0001da8c-\U0001da9a\U0001daa0\U0001dab0-\U0001dfff\U0001e007\U0001e019-\U0001e01a\U0001e022\U0001e025\U0001e02b-\U0001e7ff\U0001e8c5-\U0001e8c6\U0001e8d7-\U0001e8ff\U0001e94b-\U0001e94f\U0001e95a-\U0001e95d\U0001e960-\U0001ec70\U0001ecb5-\U0001edff\U0001ee04\U0001ee20\U0001ee23\U0001ee25-\U0001ee26\U0001ee28\U0001ee33\U0001ee38\U0001ee3a\U0001ee3c-\U0001ee41\U0001ee43-\U0001ee46\U0001ee48\U0001ee4a\U0001ee4c\U0001ee50\U0001ee53\U0001ee55-\U0001ee56\U0001ee58\U0001ee5a\U0001ee5c\U0001ee5e\U0001ee60\U0001ee63\U0001ee65-\U0001ee66\U0001ee6b\U0001ee73\U0001ee78\U0001ee7d\U0001ee7f\U0001ee8a\U0001ee9c-\U0001eea0\U0001eea4\U0001eeaa\U0001eebc-\U0001eeef\U0001eef2-\U0001efff\U0001f02c-\U0001f02f\U0001f094-\U0001f09f\U0001f0af-\U0001f0b0\U0001f0c0\U0001f0d0\U0001f0f6-\U0001f0ff\U0001f10d-\U0001f10f\U0001f16c-\U0001f16f\U0001f1ad-\U0001f1e5\U0001f203-\U0001f20f\U0001f23c-\U0001f23f\U0001f249-\U0001f24f\U0001f252-\U0001f25f\U0001f266-\U0001f2ff\U0001f6d5-\U0001f6df\U0001f6ed-\U0001f6ef\U0001f6fa-\U0001f6ff\U0001f774-\U0001f77f\U0001f7d9-\U0001f7ff\U0001f80c-\U0001f80f\U0001f848-\U0001f84f\U0001f85a-\U0001f85f\U0001f888-\U0001f88f\U0001f8ae-\U0001f8ff\U0001f90c-\U0001f90f\U0001f93f\U0001f971-\U0001f972\U0001f977-\U0001f979\U0001f97b\U0001f9a3-\U0001f9af\U0001f9ba-\U0001f9bf\U0001f9c3-\U0001f9cf\U0001fa00-\U0001fa5f\U0001fa6e-\U0001ffff\U0002a6d7-\U0002a6ff\U0002b735-\U0002b73f\U0002b81e-\U0002b81f\U0002cea2-\U0002ceaf\U0002ebe1-\U0002f7ff\U0002fa1e-\U000e0000\U000e0002-\U000e001f\U000e0080-\U000e00ff\U000e01f0-\U000effff\U000ffffe-\U000fffff\U0010fffe-\U0010ffff'

Co = '\ue000-\uf8ff\U000f0000-\U000ffffd\U00100000-\U0010fffd'

Cs = '\ud800-\udbff\\\udc00\udc01-\udfff'

Ll = 'a-z\xb5\xdf-\xf6\xf8-\xff\u0101\u0103\u0105\u0107\u0109\u010b\u010d\u010f\u0111\u0113\u0115\u0117\u0119\u011b\u011d\u011f\u0121\u0123\u0125\u0127\u0129\u012b\u012d\u012f\u0131\u0133\u0135\u0137-\u0138\u013a\u013c\u013e\u0140\u0142\u0144\u0146\u0148-\u0149\u014b\u014d\u014f\u0151\u0153\u0155\u0157\u0159\u015b\u015d\u015f\u0161\u0163\u0165\u0167\u0169\u016b\u016d\u016f\u0171\u0173\u0175\u0177\u017a\u017c\u017e-\u0180\u0183\u0185\u0188\u018c-\u018d\u0192\u0195\u0199-\u019b\u019e\u01a1\u01a3\u01a5\u01a8\u01aa-\u01ab\u01ad\u01b0\u01b4\u01b6\u01b9-\u01ba\u01bd-\u01bf\u01c6\u01c9\u01cc\u01ce\u01d0\u01d2\u01d4\u01d6\u01d8\u01da\u01dc-\u01dd\u01df\u01e1\u01e3\u01e5\u01e7\u01e9\u01eb\u01ed\u01ef-\u01f0\u01f3\u01f5\u01f9\u01fb\u01fd\u01ff\u0201\u0203\u0205\u0207\u0209\u020b\u020d\u020f\u0211\u0213\u0215\u0217\u0219\u021b\u021d\u021f\u0221\u0223\u0225\u0227\u0229\u022b\u022d\u022f\u0231\u0233-\u0239\u023c\u023f-\u0240\u0242\u0247\u0249\u024b\u024d\u024f-\u0293\u0295-\u02af\u0371\u0373\u0377\u037b-\u037d\u0390\u03ac-\u03ce\u03d0-\u03d1\u03d5-\u03d7\u03d9\u03db\u03dd\u03df\u03e1\u03e3\u03e5\u03e7\u03e9\u03eb\u03ed\u03ef-\u03f3\u03f5\u03f8\u03fb-\u03fc\u0430-\u045f\u0461\u0463\u0465\u0467\u0469\u046b\u046d\u046f\u0471\u0473\u0475\u0477\u0479\u047b\u047d\u047f\u0481\u048b\u048d\u048f\u0491\u0493\u0495\u0497\u0499\u049b\u049d\u049f\u04a1\u04a3\u04a5\u04a7\u04a9\u04ab\u04ad\u04af\u04b1\u04b3\u04b5\u04b7\u04b9\u04bb\u04bd\u04bf\u04c2\u04c4\u04c6\u04c8\u04ca\u04cc\u04ce-\u04cf\u04d1\u04d3\u04d5\u04d7\u04d9\u04db\u04dd\u04df\u04e1\u04e3\u04e5\u04e7\u04e9\u04eb\u04ed\u04ef\u04f1\u04f3\u04f5\u04f7\u04f9\u04fb\u04fd\u04ff\u0501\u0503\u0505\u0507\u0509\u050b\u050d\u050f\u0511\u0513\u0515\u0517\u0519\u051b\u051d\u051f\u0521\u0523\u0525\u0527\u0529\u052b\u052d\u052f\u0560-\u0588\u10d0-\u10fa\u10fd-\u10ff\u13f8-\u13fd\u1c80-\u1c88\u1d00-\u1d2b\u1d6b-\u1d77\u1d79-\u1d9a\u1e01\u1e03\u1e05\u1e07\u1e09\u1e0b\u1e0d\u1e0f\u1e11\u1e13\u1e15\u1e17\u1e19\u1e1b\u1e1d\u1e1f\u1e21\u1e23\u1e25\u1e27\u1e29\u1e2b\u1e2d\u1e2f\u1e31\u1e33\u1e35\u1e37\u1e39\u1e3b\u1e3d\u1e3f\u1e41\u1e43\u1e45\u1e47\u1e49\u1e4b\u1e4d\u1e4f\u1e51\u1e53\u1e55\u1e57\u1e59\u1e5b\u1e5d\u1e5f\u1e61\u1e63\u1e65\u1e67\u1e69\u1e6b\u1e6d\u1e6f\u1e71\u1e73\u1e75\u1e77\u1e79\u1e7b\u1e7d\u1e7f\u1e81\u1e83\u1e85\u1e87\u1e89\u1e8b\u1e8d\u1e8f\u1e91\u1e93\u1e95-\u1e9d\u1e9f\u1ea1\u1ea3\u1ea5\u1ea7\u1ea9\u1eab\u1ead\u1eaf\u1eb1\u1eb3\u1eb5\u1eb7\u1eb9\u1ebb\u1ebd\u1ebf\u1ec1\u1ec3\u1ec5\u1ec7\u1ec9\u1ecb\u1ecd\u1ecf\u1ed1\u1ed3\u1ed5\u1ed7\u1ed9\u1edb\u1edd\u1edf\u1ee1\u1ee3\u1ee5\u1ee7\u1ee9\u1eeb\u1eed\u1eef\u1ef1\u1ef3\u1ef5\u1ef7\u1ef9\u1efb\u1efd\u1eff-\u1f07\u1f10-\u1f15\u1f20-\u1f27\u1f30-\u1f37\u1f40-\u1f45\u1f50-\u1f57\u1f60-\u1f67\u1f70-\u1f7d\u1f80-\u1f87\u1f90-\u1f97\u1fa0-\u1fa7\u1fb0-\u1fb4\u1fb6-\u1fb7\u1fbe\u1fc2-\u1fc4\u1fc6-\u1fc7\u1fd0-\u1fd3\u1fd6-\u1fd7\u1fe0-\u1fe7\u1ff2-\u1ff4\u1ff6-\u1ff7\u210a\u210e-\u210f\u2113\u212f\u2134\u2139\u213c-\u213d\u2146-\u2149\u214e\u2184\u2c30-\u2c5e\u2c61\u2c65-\u2c66\u2c68\u2c6a\u2c6c\u2c71\u2c73-\u2c74\u2c76-\u2c7b\u2c81\u2c83\u2c85\u2c87\u2c89\u2c8b\u2c8d\u2c8f\u2c91\u2c93\u2c95\u2c97\u2c99\u2c9b\u2c9d\u2c9f\u2ca1\u2ca3\u2ca5\u2ca7\u2ca9\u2cab\u2cad\u2caf\u2cb1\u2cb3\u2cb5\u2cb7\u2cb9\u2cbb\u2cbd\u2cbf\u2cc1\u2cc3\u2cc5\u2cc7\u2cc9\u2ccb\u2ccd\u2ccf\u2cd1\u2cd3\u2cd5\u2cd7\u2cd9\u2cdb\u2cdd\u2cdf\u2ce1\u2ce3-\u2ce4\u2cec\u2cee\u2cf3\u2d00-\u2d25\u2d27\u2d2d\ua641\ua643\ua645\ua647\ua649\ua64b\ua64d\ua64f\ua651\ua653\ua655\ua657\ua659\ua65b\ua65d\ua65f\ua661\ua663\ua665\ua667\ua669\ua66b\ua66d\ua681\ua683\ua685\ua687\ua689\ua68b\ua68d\ua68f\ua691\ua693\ua695\ua697\ua699\ua69b\ua723\ua725\ua727\ua729\ua72b\ua72d\ua72f-\ua731\ua733\ua735\ua737\ua739\ua73b\ua73d\ua73f\ua741\ua743\ua745\ua747\ua749\ua74b\ua74d\ua74f\ua751\ua753\ua755\ua757\ua759\ua75b\ua75d\ua75f\ua761\ua763\ua765\ua767\ua769\ua76b\ua76d\ua76f\ua771-\ua778\ua77a\ua77c\ua77f\ua781\ua783\ua785\ua787\ua78c\ua78e\ua791\ua793-\ua795\ua797\ua799\ua79b\ua79d\ua79f\ua7a1\ua7a3\ua7a5\ua7a7\ua7a9\ua7af\ua7b5\ua7b7\ua7b9\ua7fa\uab30-\uab5a\uab60-\uab65\uab70-\uabbf\ufb00-\ufb06\ufb13-\ufb17\uff41-\uff5a\U00010428-\U0001044f\U000104d8-\U000104fb\U00010cc0-\U00010cf2\U000118c0-\U000118df\U00016e60-\U00016e7f\U0001d41a-\U0001d433\U0001d44e-\U0001d454\U0001d456-\U0001d467\U0001d482-\U0001d49b\U0001d4b6-\U0001d4b9\U0001d4bb\U0001d4bd-\U0001d4c3\U0001d4c5-\U0001d4cf\U0001d4ea-\U0001d503\U0001d51e-\U0001d537\U0001d552-\U0001d56b\U0001d586-\U0001d59f\U0001d5ba-\U0001d5d3\U0001d5ee-\U0001d607\U0001d622-\U0001d63b\U0001d656-\U0001d66f\U0001d68a-\U0001d6a5\U0001d6c2-\U0001d6da\U0001d6dc-\U0001d6e1\U0001d6fc-\U0001d714\U0001d716-\U0001d71b\U0001d736-\U0001d74e\U0001d750-\U0001d755\U0001d770-\U0001d788\U0001d78a-\U0001d78f\U0001d7aa-\U0001d7c2\U0001d7c4-\U0001d7c9\U0001d7cb\U0001e922-\U0001e943'

Lm = '\u02b0-\u02c1\u02c6-\u02d1\u02e0-\u02e4\u02ec\u02ee\u0374\u037a\u0559\u0640\u06e5-\u06e6\u07f4-\u07f5\u07fa\u081a\u0824\u0828\u0971\u0e46\u0ec6\u10fc\u17d7\u1843\u1aa7\u1c78-\u1c7d\u1d2c-\u1d6a\u1d78\u1d9b-\u1dbf\u2071\u207f\u2090-\u209c\u2c7c-\u2c7d\u2d6f\u2e2f\u3005\u3031-\u3035\u303b\u309d-\u309e\u30fc-\u30fe\ua015\ua4f8-\ua4fd\ua60c\ua67f\ua69c-\ua69d\ua717-\ua71f\ua770\ua788\ua7f8-\ua7f9\ua9cf\ua9e6\uaa70\uaadd\uaaf3-\uaaf4\uab5c-\uab5f\uff70\uff9e-\uff9f\U00016b40-\U00016b43\U00016f93-\U00016f9f\U00016fe0-\U00016fe1'

Lo = '\xaa\xba\u01bb\u01c0-\u01c3\u0294\u05d0-\u05ea\u05ef-\u05f2\u0620-\u063f\u0641-\u064a\u066e-\u066f\u0671-\u06d3\u06d5\u06ee-\u06ef\u06fa-\u06fc\u06ff\u0710\u0712-\u072f\u074d-\u07a5\u07b1\u07ca-\u07ea\u0800-\u0815\u0840-\u0858\u0860-\u086a\u08a0-\u08b4\u08b6-\u08bd\u0904-\u0939\u093d\u0950\u0958-\u0961\u0972-\u0980\u0985-\u098c\u098f-\u0990\u0993-\u09a8\u09aa-\u09b0\u09b2\u09b6-\u09b9\u09bd\u09ce\u09dc-\u09dd\u09df-\u09e1\u09f0-\u09f1\u09fc\u0a05-\u0a0a\u0a0f-\u0a10\u0a13-\u0a28\u0a2a-\u0a30\u0a32-\u0a33\u0a35-\u0a36\u0a38-\u0a39\u0a59-\u0a5c\u0a5e\u0a72-\u0a74\u0a85-\u0a8d\u0a8f-\u0a91\u0a93-\u0aa8\u0aaa-\u0ab0\u0ab2-\u0ab3\u0ab5-\u0ab9\u0abd\u0ad0\u0ae0-\u0ae1\u0af9\u0b05-\u0b0c\u0b0f-\u0b10\u0b13-\u0b28\u0b2a-\u0b30\u0b32-\u0b33\u0b35-\u0b39\u0b3d\u0b5c-\u0b5d\u0b5f-\u0b61\u0b71\u0b83\u0b85-\u0b8a\u0b8e-\u0b90\u0b92-\u0b95\u0b99-\u0b9a\u0b9c\u0b9e-\u0b9f\u0ba3-\u0ba4\u0ba8-\u0baa\u0bae-\u0bb9\u0bd0\u0c05-\u0c0c\u0c0e-\u0c10\u0c12-\u0c28\u0c2a-\u0c39\u0c3d\u0c58-\u0c5a\u0c60-\u0c61\u0c80\u0c85-\u0c8c\u0c8e-\u0c90\u0c92-\u0ca8\u0caa-\u0cb3\u0cb5-\u0cb9\u0cbd\u0cde\u0ce0-\u0ce1\u0cf1-\u0cf2\u0d05-\u0d0c\u0d0e-\u0d10\u0d12-\u0d3a\u0d3d\u0d4e\u0d54-\u0d56\u0d5f-\u0d61\u0d7a-\u0d7f\u0d85-\u0d96\u0d9a-\u0db1\u0db3-\u0dbb\u0dbd\u0dc0-\u0dc6\u0e01-\u0e30\u0e32-\u0e33\u0e40-\u0e45\u0e81-\u0e82\u0e84\u0e87-\u0e88\u0e8a\u0e8d\u0e94-\u0e97\u0e99-\u0e9f\u0ea1-\u0ea3\u0ea5\u0ea7\u0eaa-\u0eab\u0ead-\u0eb0\u0eb2-\u0eb3\u0ebd\u0ec0-\u0ec4\u0edc-\u0edf\u0f00\u0f40-\u0f47\u0f49-\u0f6c\u0f88-\u0f8c\u1000-\u102a\u103f\u1050-\u1055\u105a-\u105d\u1061\u1065-\u1066\u106e-\u1070\u1075-\u1081\u108e\u1100-\u1248\u124a-\u124d\u1250-\u1256\u1258\u125a-\u125d\u1260-\u1288\u128a-\u128d\u1290-\u12b0\u12b2-\u12b5\u12b8-\u12be\u12c0\u12c2-\u12c5\u12c8-\u12d6\u12d8-\u1310\u1312-\u1315\u1318-\u135a\u1380-\u138f\u1401-\u166c\u166f-\u167f\u1681-\u169a\u16a0-\u16ea\u16f1-\u16f8\u1700-\u170c\u170e-\u1711\u1720-\u1731\u1740-\u1751\u1760-\u176c\u176e-\u1770\u1780-\u17b3\u17dc\u1820-\u1842\u1844-\u1878\u1880-\u1884\u1887-\u18a8\u18aa\u18b0-\u18f5\u1900-\u191e\u1950-\u196d\u1970-\u1974\u1980-\u19ab\u19b0-\u19c9\u1a00-\u1a16\u1a20-\u1a54\u1b05-\u1b33\u1b45-\u1b4b\u1b83-\u1ba0\u1bae-\u1baf\u1bba-\u1be5\u1c00-\u1c23\u1c4d-\u1c4f\u1c5a-\u1c77\u1ce9-\u1cec\u1cee-\u1cf1\u1cf5-\u1cf6\u2135-\u2138\u2d30-\u2d67\u2d80-\u2d96\u2da0-\u2da6\u2da8-\u2dae\u2db0-\u2db6\u2db8-\u2dbe\u2dc0-\u2dc6\u2dc8-\u2dce\u2dd0-\u2dd6\u2dd8-\u2dde\u3006\u303c\u3041-\u3096\u309f\u30a1-\u30fa\u30ff\u3105-\u312f\u3131-\u318e\u31a0-\u31ba\u31f0-\u31ff\u3400-\u4db5\u4e00-\u9fef\ua000-\ua014\ua016-\ua48c\ua4d0-\ua4f7\ua500-\ua60b\ua610-\ua61f\ua62a-\ua62b\ua66e\ua6a0-\ua6e5\ua78f\ua7f7\ua7fb-\ua801\ua803-\ua805\ua807-\ua80a\ua80c-\ua822\ua840-\ua873\ua882-\ua8b3\ua8f2-\ua8f7\ua8fb\ua8fd-\ua8fe\ua90a-\ua925\ua930-\ua946\ua960-\ua97c\ua984-\ua9b2\ua9e0-\ua9e4\ua9e7-\ua9ef\ua9fa-\ua9fe\uaa00-\uaa28\uaa40-\uaa42\uaa44-\uaa4b\uaa60-\uaa6f\uaa71-\uaa76\uaa7a\uaa7e-\uaaaf\uaab1\uaab5-\uaab6\uaab9-\uaabd\uaac0\uaac2\uaadb-\uaadc\uaae0-\uaaea\uaaf2\uab01-\uab06\uab09-\uab0e\uab11-\uab16\uab20-\uab26\uab28-\uab2e\uabc0-\uabe2\uac00-\ud7a3\ud7b0-\ud7c6\ud7cb-\ud7fb\uf900-\ufa6d\ufa70-\ufad9\ufb1d\ufb1f-\ufb28\ufb2a-\ufb36\ufb38-\ufb3c\ufb3e\ufb40-\ufb41\ufb43-\ufb44\ufb46-\ufbb1\ufbd3-\ufd3d\ufd50-\ufd8f\ufd92-\ufdc7\ufdf0-\ufdfb\ufe70-\ufe74\ufe76-\ufefc\uff66-\uff6f\uff71-\uff9d\uffa0-\uffbe\uffc2-\uffc7\uffca-\uffcf\uffd2-\uffd7\uffda-\uffdc\U00010000-\U0001000b\U0001000d-\U00010026\U00010028-\U0001003a\U0001003c-\U0001003d\U0001003f-\U0001004d\U00010050-\U0001005d\U00010080-\U000100fa\U00010280-\U0001029c\U000102a0-\U000102d0\U00010300-\U0001031f\U0001032d-\U00010340\U00010342-\U00010349\U00010350-\U00010375\U00010380-\U0001039d\U000103a0-\U000103c3\U000103c8-\U000103cf\U00010450-\U0001049d\U00010500-\U00010527\U00010530-\U00010563\U00010600-\U00010736\U00010740-\U00010755\U00010760-\U00010767\U00010800-\U00010805\U00010808\U0001080a-\U00010835\U00010837-\U00010838\U0001083c\U0001083f-\U00010855\U00010860-\U00010876\U00010880-\U0001089e\U000108e0-\U000108f2\U000108f4-\U000108f5\U00010900-\U00010915\U00010920-\U00010939\U00010980-\U000109b7\U000109be-\U000109bf\U00010a00\U00010a10-\U00010a13\U00010a15-\U00010a17\U00010a19-\U00010a35\U00010a60-\U00010a7c\U00010a80-\U00010a9c\U00010ac0-\U00010ac7\U00010ac9-\U00010ae4\U00010b00-\U00010b35\U00010b40-\U00010b55\U00010b60-\U00010b72\U00010b80-\U00010b91\U00010c00-\U00010c48\U00010d00-\U00010d23\U00010f00-\U00010f1c\U00010f27\U00010f30-\U00010f45\U00011003-\U00011037\U00011083-\U000110af\U000110d0-\U000110e8\U00011103-\U00011126\U00011144\U00011150-\U00011172\U00011176\U00011183-\U000111b2\U000111c1-\U000111c4\U000111da\U000111dc\U00011200-\U00011211\U00011213-\U0001122b\U00011280-\U00011286\U00011288\U0001128a-\U0001128d\U0001128f-\U0001129d\U0001129f-\U000112a8\U000112b0-\U000112de\U00011305-\U0001130c\U0001130f-\U00011310\U00011313-\U00011328\U0001132a-\U00011330\U00011332-\U00011333\U00011335-\U00011339\U0001133d\U00011350\U0001135d-\U00011361\U00011400-\U00011434\U00011447-\U0001144a\U00011480-\U000114af\U000114c4-\U000114c5\U000114c7\U00011580-\U000115ae\U000115d8-\U000115db\U00011600-\U0001162f\U00011644\U00011680-\U000116aa\U00011700-\U0001171a\U00011800-\U0001182b\U000118ff\U00011a00\U00011a0b-\U00011a32\U00011a3a\U00011a50\U00011a5c-\U00011a83\U00011a86-\U00011a89\U00011a9d\U00011ac0-\U00011af8\U00011c00-\U00011c08\U00011c0a-\U00011c2e\U00011c40\U00011c72-\U00011c8f\U00011d00-\U00011d06\U00011d08-\U00011d09\U00011d0b-\U00011d30\U00011d46\U00011d60-\U00011d65\U00011d67-\U00011d68\U00011d6a-\U00011d89\U00011d98\U00011ee0-\U00011ef2\U00012000-\U00012399\U00012480-\U00012543\U00013000-\U0001342e\U00014400-\U00014646\U00016800-\U00016a38\U00016a40-\U00016a5e\U00016ad0-\U00016aed\U00016b00-\U00016b2f\U00016b63-\U00016b77\U00016b7d-\U00016b8f\U00016f00-\U00016f44\U00016f50\U00017000-\U000187f1\U00018800-\U00018af2\U0001b000-\U0001b11e\U0001b170-\U0001b2fb\U0001bc00-\U0001bc6a\U0001bc70-\U0001bc7c\U0001bc80-\U0001bc88\U0001bc90-\U0001bc99\U0001e800-\U0001e8c4\U0001ee00-\U0001ee03\U0001ee05-\U0001ee1f\U0001ee21-\U0001ee22\U0001ee24\U0001ee27\U0001ee29-\U0001ee32\U0001ee34-\U0001ee37\U0001ee39\U0001ee3b\U0001ee42\U0001ee47\U0001ee49\U0001ee4b\U0001ee4d-\U0001ee4f\U0001ee51-\U0001ee52\U0001ee54\U0001ee57\U0001ee59\U0001ee5b\U0001ee5d\U0001ee5f\U0001ee61-\U0001ee62\U0001ee64\U0001ee67-\U0001ee6a\U0001ee6c-\U0001ee72\U0001ee74-\U0001ee77\U0001ee79-\U0001ee7c\U0001ee7e\U0001ee80-\U0001ee89\U0001ee8b-\U0001ee9b\U0001eea1-\U0001eea3\U0001eea5-\U0001eea9\U0001eeab-\U0001eebb\U00020000-\U0002a6d6\U0002a700-\U0002b734\U0002b740-\U0002b81d\U0002b820-\U0002cea1\U0002ceb0-\U0002ebe0\U0002f800-\U0002fa1d'

Lt = '\u01c5\u01c8\u01cb\u01f2\u1f88-\u1f8f\u1f98-\u1f9f\u1fa8-\u1faf\u1fbc\u1fcc\u1ffc'

Lu = 'A-Z\xc0-\xd6\xd8-\xde\u0100\u0102\u0104\u0106\u0108\u010a\u010c\u010e\u0110\u0112\u0114\u0116\u0118\u011a\u011c\u011e\u0120\u0122\u0124\u0126\u0128\u012a\u012c\u012e\u0130\u0132\u0134\u0136\u0139\u013b\u013d\u013f\u0141\u0143\u0145\u0147\u014a\u014c\u014e\u0150\u0152\u0154\u0156\u0158\u015a\u015c\u015e\u0160\u0162\u0164\u0166\u0168\u016a\u016c\u016e\u0170\u0172\u0174\u0176\u0178-\u0179\u017b\u017d\u0181-\u0182\u0184\u0186-\u0187\u0189-\u018b\u018e-\u0191\u0193-\u0194\u0196-\u0198\u019c-\u019d\u019f-\u01a0\u01a2\u01a4\u01a6-\u01a7\u01a9\u01ac\u01ae-\u01af\u01b1-\u01b3\u01b5\u01b7-\u01b8\u01bc\u01c4\u01c7\u01ca\u01cd\u01cf\u01d1\u01d3\u01d5\u01d7\u01d9\u01db\u01de\u01e0\u01e2\u01e4\u01e6\u01e8\u01ea\u01ec\u01ee\u01f1\u01f4\u01f6-\u01f8\u01fa\u01fc\u01fe\u0200\u0202\u0204\u0206\u0208\u020a\u020c\u020e\u0210\u0212\u0214\u0216\u0218\u021a\u021c\u021e\u0220\u0222\u0224\u0226\u0228\u022a\u022c\u022e\u0230\u0232\u023a-\u023b\u023d-\u023e\u0241\u0243-\u0246\u0248\u024a\u024c\u024e\u0370\u0372\u0376\u037f\u0386\u0388-\u038a\u038c\u038e-\u038f\u0391-\u03a1\u03a3-\u03ab\u03cf\u03d2-\u03d4\u03d8\u03da\u03dc\u03de\u03e0\u03e2\u03e4\u03e6\u03e8\u03ea\u03ec\u03ee\u03f4\u03f7\u03f9-\u03fa\u03fd-\u042f\u0460\u0462\u0464\u0466\u0468\u046a\u046c\u046e\u0470\u0472\u0474\u0476\u0478\u047a\u047c\u047e\u0480\u048a\u048c\u048e\u0490\u0492\u0494\u0496\u0498\u049a\u049c\u049e\u04a0\u04a2\u04a4\u04a6\u04a8\u04aa\u04ac\u04ae\u04b0\u04b2\u04b4\u04b6\u04b8\u04ba\u04bc\u04be\u04c0-\u04c1\u04c3\u04c5\u04c7\u04c9\u04cb\u04cd\u04d0\u04d2\u04d4\u04d6\u04d8\u04da\u04dc\u04de\u04e0\u04e2\u04e4\u04e6\u04e8\u04ea\u04ec\u04ee\u04f0\u04f2\u04f4\u04f6\u04f8\u04fa\u04fc\u04fe\u0500\u0502\u0504\u0506\u0508\u050a\u050c\u050e\u0510\u0512\u0514\u0516\u0518\u051a\u051c\u051e\u0520\u0522\u0524\u0526\u0528\u052a\u052c\u052e\u0531-\u0556\u10a0-\u10c5\u10c7\u10cd\u13a0-\u13f5\u1c90-\u1cba\u1cbd-\u1cbf\u1e00\u1e02\u1e04\u1e06\u1e08\u1e0a\u1e0c\u1e0e\u1e10\u1e12\u1e14\u1e16\u1e18\u1e1a\u1e1c\u1e1e\u1e20\u1e22\u1e24\u1e26\u1e28\u1e2a\u1e2c\u1e2e\u1e30\u1e32\u1e34\u1e36\u1e38\u1e3a\u1e3c\u1e3e\u1e40\u1e42\u1e44\u1e46\u1e48\u1e4a\u1e4c\u1e4e\u1e50\u1e52\u1e54\u1e56\u1e58\u1e5a\u1e5c\u1e5e\u1e60\u1e62\u1e64\u1e66\u1e68\u1e6a\u1e6c\u1e6e\u1e70\u1e72\u1e74\u1e76\u1e78\u1e7a\u1e7c\u1e7e\u1e80\u1e82\u1e84\u1e86\u1e88\u1e8a\u1e8c\u1e8e\u1e90\u1e92\u1e94\u1e9e\u1ea0\u1ea2\u1ea4\u1ea6\u1ea8\u1eaa\u1eac\u1eae\u1eb0\u1eb2\u1eb4\u1eb6\u1eb8\u1eba\u1ebc\u1ebe\u1ec0\u1ec2\u1ec4\u1ec6\u1ec8\u1eca\u1ecc\u1ece\u1ed0\u1ed2\u1ed4\u1ed6\u1ed8\u1eda\u1edc\u1ede\u1ee0\u1ee2\u1ee4\u1ee6\u1ee8\u1eea\u1eec\u1eee\u1ef0\u1ef2\u1ef4\u1ef6\u1ef8\u1efa\u1efc\u1efe\u1f08-\u1f0f\u1f18-\u1f1d\u1f28-\u1f2f\u1f38-\u1f3f\u1f48-\u1f4d\u1f59\u1f5b\u1f5d\u1f5f\u1f68-\u1f6f\u1fb8-\u1fbb\u1fc8-\u1fcb\u1fd8-\u1fdb\u1fe8-\u1fec\u1ff8-\u1ffb\u2102\u2107\u210b-\u210d\u2110-\u2112\u2115\u2119-\u211d\u2124\u2126\u2128\u212a-\u212d\u2130-\u2133\u213e-\u213f\u2145\u2183\u2c00-\u2c2e\u2c60\u2c62-\u2c64\u2c67\u2c69\u2c6b\u2c6d-\u2c70\u2c72\u2c75\u2c7e-\u2c80\u2c82\u2c84\u2c86\u2c88\u2c8a\u2c8c\u2c8e\u2c90\u2c92\u2c94\u2c96\u2c98\u2c9a\u2c9c\u2c9e\u2ca0\u2ca2\u2ca4\u2ca6\u2ca8\u2caa\u2cac\u2cae\u2cb0\u2cb2\u2cb4\u2cb6\u2cb8\u2cba\u2cbc\u2cbe\u2cc0\u2cc2\u2cc4\u2cc6\u2cc8\u2cca\u2ccc\u2cce\u2cd0\u2cd2\u2cd4\u2cd6\u2cd8\u2cda\u2cdc\u2cde\u2ce0\u2ce2\u2ceb\u2ced\u2cf2\ua640\ua642\ua644\ua646\ua648\ua64a\ua64c\ua64e\ua650\ua652\ua654\ua656\ua658\ua65a\ua65c\ua65e\ua660\ua662\ua664\ua666\ua668\ua66a\ua66c\ua680\ua682\ua684\ua686\ua688\ua68a\ua68c\ua68e\ua690\ua692\ua694\ua696\ua698\ua69a\ua722\ua724\ua726\ua728\ua72a\ua72c\ua72e\ua732\ua734\ua736\ua738\ua73a\ua73c\ua73e\ua740\ua742\ua744\ua746\ua748\ua74a\ua74c\ua74e\ua750\ua752\ua754\ua756\ua758\ua75a\ua75c\ua75e\ua760\ua762\ua764\ua766\ua768\ua76a\ua76c\ua76e\ua779\ua77b\ua77d-\ua77e\ua780\ua782\ua784\ua786\ua78b\ua78d\ua790\ua792\ua796\ua798\ua79a\ua79c\ua79e\ua7a0\ua7a2\ua7a4\ua7a6\ua7a8\ua7aa-\ua7ae\ua7b0-\ua7b4\ua7b6\ua7b8\uff21-\uff3a\U00010400-\U00010427\U000104b0-\U000104d3\U00010c80-\U00010cb2\U000118a0-\U000118bf\U00016e40-\U00016e5f\U0001d400-\U0001d419\U0001d434-\U0001d44d\U0001d468-\U0001d481\U0001d49c\U0001d49e-\U0001d49f\U0001d4a2\U0001d4a5-\U0001d4a6\U0001d4a9-\U0001d4ac\U0001d4ae-\U0001d4b5\U0001d4d0-\U0001d4e9\U0001d504-\U0001d505\U0001d507-\U0001d50a\U0001d50d-\U0001d514\U0001d516-\U0001d51c\U0001d538-\U0001d539\U0001d53b-\U0001d53e\U0001d540-\U0001d544\U0001d546\U0001d54a-\U0001d550\U0001d56c-\U0001d585\U0001d5a0-\U0001d5b9\U0001d5d4-\U0001d5ed\U0001d608-\U0001d621\U0001d63c-\U0001d655\U0001d670-\U0001d689\U0001d6a8-\U0001d6c0\U0001d6e2-\U0001d6fa\U0001d71c-\U0001d734\U0001d756-\U0001d76e\U0001d790-\U0001d7a8\U0001d7ca\U0001e900-\U0001e921'

Mc = '\u0903\u093b\u093e-\u0940\u0949-\u094c\u094e-\u094f\u0982-\u0983\u09be-\u09c0\u09c7-\u09c8\u09cb-\u09cc\u09d7\u0a03\u0a3e-\u0a40\u0a83\u0abe-\u0ac0\u0ac9\u0acb-\u0acc\u0b02-\u0b03\u0b3e\u0b40\u0b47-\u0b48\u0b4b-\u0b4c\u0b57\u0bbe-\u0bbf\u0bc1-\u0bc2\u0bc6-\u0bc8\u0bca-\u0bcc\u0bd7\u0c01-\u0c03\u0c41-\u0c44\u0c82-\u0c83\u0cbe\u0cc0-\u0cc4\u0cc7-\u0cc8\u0cca-\u0ccb\u0cd5-\u0cd6\u0d02-\u0d03\u0d3e-\u0d40\u0d46-\u0d48\u0d4a-\u0d4c\u0d57\u0d82-\u0d83\u0dcf-\u0dd1\u0dd8-\u0ddf\u0df2-\u0df3\u0f3e-\u0f3f\u0f7f\u102b-\u102c\u1031\u1038\u103b-\u103c\u1056-\u1057\u1062-\u1064\u1067-\u106d\u1083-\u1084\u1087-\u108c\u108f\u109a-\u109c\u17b6\u17be-\u17c5\u17c7-\u17c8\u1923-\u1926\u1929-\u192b\u1930-\u1931\u1933-\u1938\u1a19-\u1a1a\u1a55\u1a57\u1a61\u1a63-\u1a64\u1a6d-\u1a72\u1b04\u1b35\u1b3b\u1b3d-\u1b41\u1b43-\u1b44\u1b82\u1ba1\u1ba6-\u1ba7\u1baa\u1be7\u1bea-\u1bec\u1bee\u1bf2-\u1bf3\u1c24-\u1c2b\u1c34-\u1c35\u1ce1\u1cf2-\u1cf3\u1cf7\u302e-\u302f\ua823-\ua824\ua827\ua880-\ua881\ua8b4-\ua8c3\ua952-\ua953\ua983\ua9b4-\ua9b5\ua9ba-\ua9bb\ua9bd-\ua9c0\uaa2f-\uaa30\uaa33-\uaa34\uaa4d\uaa7b\uaa7d\uaaeb\uaaee-\uaaef\uaaf5\uabe3-\uabe4\uabe6-\uabe7\uabe9-\uabea\uabec\U00011000\U00011002\U00011082\U000110b0-\U000110b2\U000110b7-\U000110b8\U0001112c\U00011145-\U00011146\U00011182\U000111b3-\U000111b5\U000111bf-\U000111c0\U0001122c-\U0001122e\U00011232-\U00011233\U00011235\U000112e0-\U000112e2\U00011302-\U00011303\U0001133e-\U0001133f\U00011341-\U00011344\U00011347-\U00011348\U0001134b-\U0001134d\U00011357\U00011362-\U00011363\U00011435-\U00011437\U00011440-\U00011441\U00011445\U000114b0-\U000114b2\U000114b9\U000114bb-\U000114be\U000114c1\U000115af-\U000115b1\U000115b8-\U000115bb\U000115be\U00011630-\U00011632\U0001163b-\U0001163c\U0001163e\U000116ac\U000116ae-\U000116af\U000116b6\U00011720-\U00011721\U00011726\U0001182c-\U0001182e\U00011838\U00011a39\U00011a57-\U00011a58\U00011a97\U00011c2f\U00011c3e\U00011ca9\U00011cb1\U00011cb4\U00011d8a-\U00011d8e\U00011d93-\U00011d94\U00011d96\U00011ef5-\U00011ef6\U00016f51-\U00016f7e\U0001d165-\U0001d166\U0001d16d-\U0001d172'

Me = '\u0488-\u0489\u1abe\u20dd-\u20e0\u20e2-\u20e4\ua670-\ua672'

Mn = '\u0300-\u036f\u0483-\u0487\u0591-\u05bd\u05bf\u05c1-\u05c2\u05c4-\u05c5\u05c7\u0610-\u061a\u064b-\u065f\u0670\u06d6-\u06dc\u06df-\u06e4\u06e7-\u06e8\u06ea-\u06ed\u0711\u0730-\u074a\u07a6-\u07b0\u07eb-\u07f3\u07fd\u0816-\u0819\u081b-\u0823\u0825-\u0827\u0829-\u082d\u0859-\u085b\u08d3-\u08e1\u08e3-\u0902\u093a\u093c\u0941-\u0948\u094d\u0951-\u0957\u0962-\u0963\u0981\u09bc\u09c1-\u09c4\u09cd\u09e2-\u09e3\u09fe\u0a01-\u0a02\u0a3c\u0a41-\u0a42\u0a47-\u0a48\u0a4b-\u0a4d\u0a51\u0a70-\u0a71\u0a75\u0a81-\u0a82\u0abc\u0ac1-\u0ac5\u0ac7-\u0ac8\u0acd\u0ae2-\u0ae3\u0afa-\u0aff\u0b01\u0b3c\u0b3f\u0b41-\u0b44\u0b4d\u0b56\u0b62-\u0b63\u0b82\u0bc0\u0bcd\u0c00\u0c04\u0c3e-\u0c40\u0c46-\u0c48\u0c4a-\u0c4d\u0c55-\u0c56\u0c62-\u0c63\u0c81\u0cbc\u0cbf\u0cc6\u0ccc-\u0ccd\u0ce2-\u0ce3\u0d00-\u0d01\u0d3b-\u0d3c\u0d41-\u0d44\u0d4d\u0d62-\u0d63\u0dca\u0dd2-\u0dd4\u0dd6\u0e31\u0e34-\u0e3a\u0e47-\u0e4e\u0eb1\u0eb4-\u0eb9\u0ebb-\u0ebc\u0ec8-\u0ecd\u0f18-\u0f19\u0f35\u0f37\u0f39\u0f71-\u0f7e\u0f80-\u0f84\u0f86-\u0f87\u0f8d-\u0f97\u0f99-\u0fbc\u0fc6\u102d-\u1030\u1032-\u1037\u1039-\u103a\u103d-\u103e\u1058-\u1059\u105e-\u1060\u1071-\u1074\u1082\u1085-\u1086\u108d\u109d\u135d-\u135f\u1712-\u1714\u1732-\u1734\u1752-\u1753\u1772-\u1773\u17b4-\u17b5\u17b7-\u17bd\u17c6\u17c9-\u17d3\u17dd\u180b-\u180d\u1885-\u1886\u18a9\u1920-\u1922\u1927-\u1928\u1932\u1939-\u193b\u1a17-\u1a18\u1a1b\u1a56\u1a58-\u1a5e\u1a60\u1a62\u1a65-\u1a6c\u1a73-\u1a7c\u1a7f\u1ab0-\u1abd\u1b00-\u1b03\u1b34\u1b36-\u1b3a\u1b3c\u1b42\u1b6b-\u1b73\u1b80-\u1b81\u1ba2-\u1ba5\u1ba8-\u1ba9\u1bab-\u1bad\u1be6\u1be8-\u1be9\u1bed\u1bef-\u1bf1\u1c2c-\u1c33\u1c36-\u1c37\u1cd0-\u1cd2\u1cd4-\u1ce0\u1ce2-\u1ce8\u1ced\u1cf4\u1cf8-\u1cf9\u1dc0-\u1df9\u1dfb-\u1dff\u20d0-\u20dc\u20e1\u20e5-\u20f0\u2cef-\u2cf1\u2d7f\u2de0-\u2dff\u302a-\u302d\u3099-\u309a\ua66f\ua674-\ua67d\ua69e-\ua69f\ua6f0-\ua6f1\ua802\ua806\ua80b\ua825-\ua826\ua8c4-\ua8c5\ua8e0-\ua8f1\ua8ff\ua926-\ua92d\ua947-\ua951\ua980-\ua982\ua9b3\ua9b6-\ua9b9\ua9bc\ua9e5\uaa29-\uaa2e\uaa31-\uaa32\uaa35-\uaa36\uaa43\uaa4c\uaa7c\uaab0\uaab2-\uaab4\uaab7-\uaab8\uaabe-\uaabf\uaac1\uaaec-\uaaed\uaaf6\uabe5\uabe8\uabed\ufb1e\ufe00-\ufe0f\ufe20-\ufe2f\U000101fd\U000102e0\U00010376-\U0001037a\U00010a01-\U00010a03\U00010a05-\U00010a06\U00010a0c-\U00010a0f\U00010a38-\U00010a3a\U00010a3f\U00010ae5-\U00010ae6\U00010d24-\U00010d27\U00010f46-\U00010f50\U00011001\U00011038-\U00011046\U0001107f-\U00011081\U000110b3-\U000110b6\U000110b9-\U000110ba\U00011100-\U00011102\U00011127-\U0001112b\U0001112d-\U00011134\U00011173\U00011180-\U00011181\U000111b6-\U000111be\U000111c9-\U000111cc\U0001122f-\U00011231\U00011234\U00011236-\U00011237\U0001123e\U000112df\U000112e3-\U000112ea\U00011300-\U00011301\U0001133b-\U0001133c\U00011340\U00011366-\U0001136c\U00011370-\U00011374\U00011438-\U0001143f\U00011442-\U00011444\U00011446\U0001145e\U000114b3-\U000114b8\U000114ba\U000114bf-\U000114c0\U000114c2-\U000114c3\U000115b2-\U000115b5\U000115bc-\U000115bd\U000115bf-\U000115c0\U000115dc-\U000115dd\U00011633-\U0001163a\U0001163d\U0001163f-\U00011640\U000116ab\U000116ad\U000116b0-\U000116b5\U000116b7\U0001171d-\U0001171f\U00011722-\U00011725\U00011727-\U0001172b\U0001182f-\U00011837\U00011839-\U0001183a\U00011a01-\U00011a0a\U00011a33-\U00011a38\U00011a3b-\U00011a3e\U00011a47\U00011a51-\U00011a56\U00011a59-\U00011a5b\U00011a8a-\U00011a96\U00011a98-\U00011a99\U00011c30-\U00011c36\U00011c38-\U00011c3d\U00011c3f\U00011c92-\U00011ca7\U00011caa-\U00011cb0\U00011cb2-\U00011cb3\U00011cb5-\U00011cb6\U00011d31-\U00011d36\U00011d3a\U00011d3c-\U00011d3d\U00011d3f-\U00011d45\U00011d47\U00011d90-\U00011d91\U00011d95\U00011d97\U00011ef3-\U00011ef4\U00016af0-\U00016af4\U00016b30-\U00016b36\U00016f8f-\U00016f92\U0001bc9d-\U0001bc9e\U0001d167-\U0001d169\U0001d17b-\U0001d182\U0001d185-\U0001d18b\U0001d1aa-\U0001d1ad\U0001d242-\U0001d244\U0001da00-\U0001da36\U0001da3b-\U0001da6c\U0001da75\U0001da84\U0001da9b-\U0001da9f\U0001daa1-\U0001daaf\U0001e000-\U0001e006\U0001e008-\U0001e018\U0001e01b-\U0001e021\U0001e023-\U0001e024\U0001e026-\U0001e02a\U0001e8d0-\U0001e8d6\U0001e944-\U0001e94a\U000e0100-\U000e01ef'

Nd = '0-9\u0660-\u0669\u06f0-\u06f9\u07c0-\u07c9\u0966-\u096f\u09e6-\u09ef\u0a66-\u0a6f\u0ae6-\u0aef\u0b66-\u0b6f\u0be6-\u0bef\u0c66-\u0c6f\u0ce6-\u0cef\u0d66-\u0d6f\u0de6-\u0def\u0e50-\u0e59\u0ed0-\u0ed9\u0f20-\u0f29\u1040-\u1049\u1090-\u1099\u17e0-\u17e9\u1810-\u1819\u1946-\u194f\u19d0-\u19d9\u1a80-\u1a89\u1a90-\u1a99\u1b50-\u1b59\u1bb0-\u1bb9\u1c40-\u1c49\u1c50-\u1c59\ua620-\ua629\ua8d0-\ua8d9\ua900-\ua909\ua9d0-\ua9d9\ua9f0-\ua9f9\uaa50-\uaa59\uabf0-\uabf9\uff10-\uff19\U000104a0-\U000104a9\U00010d30-\U00010d39\U00011066-\U0001106f\U000110f0-\U000110f9\U00011136-\U0001113f\U000111d0-\U000111d9\U000112f0-\U000112f9\U00011450-\U00011459\U000114d0-\U000114d9\U00011650-\U00011659\U000116c0-\U000116c9\U00011730-\U00011739\U000118e0-\U000118e9\U00011c50-\U00011c59\U00011d50-\U00011d59\U00011da0-\U00011da9\U00016a60-\U00016a69\U00016b50-\U00016b59\U0001d7ce-\U0001d7ff\U0001e950-\U0001e959'

Nl = '\u16ee-\u16f0\u2160-\u2182\u2185-\u2188\u3007\u3021-\u3029\u3038-\u303a\ua6e6-\ua6ef\U00010140-\U00010174\U00010341\U0001034a\U000103d1-\U000103d5\U00012400-\U0001246e'

No = '\xb2-\xb3\xb9\xbc-\xbe\u09f4-\u09f9\u0b72-\u0b77\u0bf0-\u0bf2\u0c78-\u0c7e\u0d58-\u0d5e\u0d70-\u0d78\u0f2a-\u0f33\u1369-\u137c\u17f0-\u17f9\u19da\u2070\u2074-\u2079\u2080-\u2089\u2150-\u215f\u2189\u2460-\u249b\u24ea-\u24ff\u2776-\u2793\u2cfd\u3192-\u3195\u3220-\u3229\u3248-\u324f\u3251-\u325f\u3280-\u3289\u32b1-\u32bf\ua830-\ua835\U00010107-\U00010133\U00010175-\U00010178\U0001018a-\U0001018b\U000102e1-\U000102fb\U00010320-\U00010323\U00010858-\U0001085f\U00010879-\U0001087f\U000108a7-\U000108af\U000108fb-\U000108ff\U00010916-\U0001091b\U000109bc-\U000109bd\U000109c0-\U000109cf\U000109d2-\U000109ff\U00010a40-\U00010a48\U00010a7d-\U00010a7e\U00010a9d-\U00010a9f\U00010aeb-\U00010aef\U00010b58-\U00010b5f\U00010b78-\U00010b7f\U00010ba9-\U00010baf\U00010cfa-\U00010cff\U00010e60-\U00010e7e\U00010f1d-\U00010f26\U00010f51-\U00010f54\U00011052-\U00011065\U000111e1-\U000111f4\U0001173a-\U0001173b\U000118ea-\U000118f2\U00011c5a-\U00011c6c\U00016b5b-\U00016b61\U00016e80-\U00016e96\U0001d2e0-\U0001d2f3\U0001d360-\U0001d378\U0001e8c7-\U0001e8cf\U0001ec71-\U0001ecab\U0001ecad-\U0001ecaf\U0001ecb1-\U0001ecb4\U0001f100-\U0001f10c'

Pc = '_\u203f-\u2040\u2054\ufe33-\ufe34\ufe4d-\ufe4f\uff3f'

Pd = '\\-\u058a\u05be\u1400\u1806\u2010-\u2015\u2e17\u2e1a\u2e3a-\u2e3b\u2e40\u301c\u3030\u30a0\ufe31-\ufe32\ufe58\ufe63\uff0d'

Pe = ')\\]}\u0f3b\u0f3d\u169c\u2046\u207e\u208e\u2309\u230b\u232a\u2769\u276b\u276d\u276f\u2771\u2773\u2775\u27c6\u27e7\u27e9\u27eb\u27ed\u27ef\u2984\u2986\u2988\u298a\u298c\u298e\u2990\u2992\u2994\u2996\u2998\u29d9\u29db\u29fd\u2e23\u2e25\u2e27\u2e29\u3009\u300b\u300d\u300f\u3011\u3015\u3017\u3019\u301b\u301e-\u301f\ufd3e\ufe18\ufe36\ufe38\ufe3a\ufe3c\ufe3e\ufe40\ufe42\ufe44\ufe48\ufe5a\ufe5c\ufe5e\uff09\uff3d\uff5d\uff60\uff63'

Pf = '\xbb\u2019\u201d\u203a\u2e03\u2e05\u2e0a\u2e0d\u2e1d\u2e21'

Pi = '\xab\u2018\u201b-\u201c\u201f\u2039\u2e02\u2e04\u2e09\u2e0c\u2e1c\u2e20'

Po = "!-#%-'*,.-/:-;?-@\\\\\xa1\xa7\xb6-\xb7\xbf\u037e\u0387\u055a-\u055f\u0589\u05c0\u05c3\u05c6\u05f3-\u05f4\u0609-\u060a\u060c-\u060d\u061b\u061e-\u061f\u066a-\u066d\u06d4\u0700-\u070d\u07f7-\u07f9\u0830-\u083e\u085e\u0964-\u0965\u0970\u09fd\u0a76\u0af0\u0c84\u0df4\u0e4f\u0e5a-\u0e5b\u0f04-\u0f12\u0f14\u0f85\u0fd0-\u0fd4\u0fd9-\u0fda\u104a-\u104f\u10fb\u1360-\u1368\u166d-\u166e\u16eb-\u16ed\u1735-\u1736\u17d4-\u17d6\u17d8-\u17da\u1800-\u1805\u1807-\u180a\u1944-\u1945\u1a1e-\u1a1f\u1aa0-\u1aa6\u1aa8-\u1aad\u1b5a-\u1b60\u1bfc-\u1bff\u1c3b-\u1c3f\u1c7e-\u1c7f\u1cc0-\u1cc7\u1cd3\u2016-\u2017\u2020-\u2027\u2030-\u2038\u203b-\u203e\u2041-\u2043\u2047-\u2051\u2053\u2055-\u205e\u2cf9-\u2cfc\u2cfe-\u2cff\u2d70\u2e00-\u2e01\u2e06-\u2e08\u2e0b\u2e0e-\u2e16\u2e18-\u2e19\u2e1b\u2e1e-\u2e1f\u2e2a-\u2e2e\u2e30-\u2e39\u2e3c-\u2e3f\u2e41\u2e43-\u2e4e\u3001-\u3003\u303d\u30fb\ua4fe-\ua4ff\ua60d-\ua60f\ua673\ua67e\ua6f2-\ua6f7\ua874-\ua877\ua8ce-\ua8cf\ua8f8-\ua8fa\ua8fc\ua92e-\ua92f\ua95f\ua9c1-\ua9cd\ua9de-\ua9df\uaa5c-\uaa5f\uaade-\uaadf\uaaf0-\uaaf1\uabeb\ufe10-\ufe16\ufe19\ufe30\ufe45-\ufe46\ufe49-\ufe4c\ufe50-\ufe52\ufe54-\ufe57\ufe5f-\ufe61\ufe68\ufe6a-\ufe6b\uff01-\uff03\uff05-\uff07\uff0a\uff0c\uff0e-\uff0f\uff1a-\uff1b\uff1f-\uff20\uff3c\uff61\uff64-\uff65\U00010100-\U00010102\U0001039f\U000103d0\U0001056f\U00010857\U0001091f\U0001093f\U00010a50-\U00010a58\U00010a7f\U00010af0-\U00010af6\U00010b39-\U00010b3f\U00010b99-\U00010b9c\U00010f55-\U00010f59\U00011047-\U0001104d\U000110bb-\U000110bc\U000110be-\U000110c1\U00011140-\U00011143\U00011174-\U00011175\U000111c5-\U000111c8\U000111cd\U000111db\U000111dd-\U000111df\U00011238-\U0001123d\U000112a9\U0001144b-\U0001144f\U0001145b\U0001145d\U000114c6\U000115c1-\U000115d7\U00011641-\U00011643\U00011660-\U0001166c\U0001173c-\U0001173e\U0001183b\U00011a3f-\U00011a46\U00011a9a-\U00011a9c\U00011a9e-\U00011aa2\U00011c41-\U00011c45\U00011c70-\U00011c71\U00011ef7-\U00011ef8\U00012470-\U00012474\U00016a6e-\U00016a6f\U00016af5\U00016b37-\U00016b3b\U00016b44\U00016e97-\U00016e9a\U0001bc9f\U0001da87-\U0001da8b\U0001e95e-\U0001e95f"

Ps = '(\\[{\u0f3a\u0f3c\u169b\u201a\u201e\u2045\u207d\u208d\u2308\u230a\u2329\u2768\u276a\u276c\u276e\u2770\u2772\u2774\u27c5\u27e6\u27e8\u27ea\u27ec\u27ee\u2983\u2985\u2987\u2989\u298b\u298d\u298f\u2991\u2993\u2995\u2997\u29d8\u29da\u29fc\u2e22\u2e24\u2e26\u2e28\u2e42\u3008\u300a\u300c\u300e\u3010\u3014\u3016\u3018\u301a\u301d\ufd3f\ufe17\ufe35\ufe37\ufe39\ufe3b\ufe3d\ufe3f\ufe41\ufe43\ufe47\ufe59\ufe5b\ufe5d\uff08\uff3b\uff5b\uff5f\uff62'

Sc = '$\xa2-\xa5\u058f\u060b\u07fe-\u07ff\u09f2-\u09f3\u09fb\u0af1\u0bf9\u0e3f\u17db\u20a0-\u20bf\ua838\ufdfc\ufe69\uff04\uffe0-\uffe1\uffe5-\uffe6\U0001ecb0'

Sk = '\\^`\xa8\xaf\xb4\xb8\u02c2-\u02c5\u02d2-\u02df\u02e5-\u02eb\u02ed\u02ef-\u02ff\u0375\u0384-\u0385\u1fbd\u1fbf-\u1fc1\u1fcd-\u1fcf\u1fdd-\u1fdf\u1fed-\u1fef\u1ffd-\u1ffe\u309b-\u309c\ua700-\ua716\ua720-\ua721\ua789-\ua78a\uab5b\ufbb2-\ufbc1\uff3e\uff40\uffe3\U0001f3fb-\U0001f3ff'

Sm = '+<->|~\xac\xb1\xd7\xf7\u03f6\u0606-\u0608\u2044\u2052\u207a-\u207c\u208a-\u208c\u2118\u2140-\u2144\u214b\u2190-\u2194\u219a-\u219b\u21a0\u21a3\u21a6\u21ae\u21ce-\u21cf\u21d2\u21d4\u21f4-\u22ff\u2320-\u2321\u237c\u239b-\u23b3\u23dc-\u23e1\u25b7\u25c1\u25f8-\u25ff\u266f\u27c0-\u27c4\u27c7-\u27e5\u27f0-\u27ff\u2900-\u2982\u2999-\u29d7\u29dc-\u29fb\u29fe-\u2aff\u2b30-\u2b44\u2b47-\u2b4c\ufb29\ufe62\ufe64-\ufe66\uff0b\uff1c-\uff1e\uff5c\uff5e\uffe2\uffe9-\uffec\U0001d6c1\U0001d6db\U0001d6fb\U0001d715\U0001d735\U0001d74f\U0001d76f\U0001d789\U0001d7a9\U0001d7c3\U0001eef0-\U0001eef1'

So = '\xa6\xa9\xae\xb0\u0482\u058d-\u058e\u060e-\u060f\u06de\u06e9\u06fd-\u06fe\u07f6\u09fa\u0b70\u0bf3-\u0bf8\u0bfa\u0c7f\u0d4f\u0d79\u0f01-\u0f03\u0f13\u0f15-\u0f17\u0f1a-\u0f1f\u0f34\u0f36\u0f38\u0fbe-\u0fc5\u0fc7-\u0fcc\u0fce-\u0fcf\u0fd5-\u0fd8\u109e-\u109f\u1390-\u1399\u1940\u19de-\u19ff\u1b61-\u1b6a\u1b74-\u1b7c\u2100-\u2101\u2103-\u2106\u2108-\u2109\u2114\u2116-\u2117\u211e-\u2123\u2125\u2127\u2129\u212e\u213a-\u213b\u214a\u214c-\u214d\u214f\u218a-\u218b\u2195-\u2199\u219c-\u219f\u21a1-\u21a2\u21a4-\u21a5\u21a7-\u21ad\u21af-\u21cd\u21d0-\u21d1\u21d3\u21d5-\u21f3\u2300-\u2307\u230c-\u231f\u2322-\u2328\u232b-\u237b\u237d-\u239a\u23b4-\u23db\u23e2-\u2426\u2440-\u244a\u249c-\u24e9\u2500-\u25b6\u25b8-\u25c0\u25c2-\u25f7\u2600-\u266e\u2670-\u2767\u2794-\u27bf\u2800-\u28ff\u2b00-\u2b2f\u2b45-\u2b46\u2b4d-\u2b73\u2b76-\u2b95\u2b98-\u2bc8\u2bca-\u2bfe\u2ce5-\u2cea\u2e80-\u2e99\u2e9b-\u2ef3\u2f00-\u2fd5\u2ff0-\u2ffb\u3004\u3012-\u3013\u3020\u3036-\u3037\u303e-\u303f\u3190-\u3191\u3196-\u319f\u31c0-\u31e3\u3200-\u321e\u322a-\u3247\u3250\u3260-\u327f\u328a-\u32b0\u32c0-\u32fe\u3300-\u33ff\u4dc0-\u4dff\ua490-\ua4c6\ua828-\ua82b\ua836-\ua837\ua839\uaa77-\uaa79\ufdfd\uffe4\uffe8\uffed-\uffee\ufffc-\ufffd\U00010137-\U0001013f\U00010179-\U00010189\U0001018c-\U0001018e\U00010190-\U0001019b\U000101a0\U000101d0-\U000101fc\U00010877-\U00010878\U00010ac8\U0001173f\U00016b3c-\U00016b3f\U00016b45\U0001bc9c\U0001d000-\U0001d0f5\U0001d100-\U0001d126\U0001d129-\U0001d164\U0001d16a-\U0001d16c\U0001d183-\U0001d184\U0001d18c-\U0001d1a9\U0001d1ae-\U0001d1e8\U0001d200-\U0001d241\U0001d245\U0001d300-\U0001d356\U0001d800-\U0001d9ff\U0001da37-\U0001da3a\U0001da6d-\U0001da74\U0001da76-\U0001da83\U0001da85-\U0001da86\U0001ecac\U0001f000-\U0001f02b\U0001f030-\U0001f093\U0001f0a0-\U0001f0ae\U0001f0b1-\U0001f0bf\U0001f0c1-\U0001f0cf\U0001f0d1-\U0001f0f5\U0001f110-\U0001f16b\U0001f170-\U0001f1ac\U0001f1e6-\U0001f202\U0001f210-\U0001f23b\U0001f240-\U0001f248\U0001f250-\U0001f251\U0001f260-\U0001f265\U0001f300-\U0001f3fa\U0001f400-\U0001f6d4\U0001f6e0-\U0001f6ec\U0001f6f0-\U0001f6f9\U0001f700-\U0001f773\U0001f780-\U0001f7d8\U0001f800-\U0001f80b\U0001f810-\U0001f847\U0001f850-\U0001f859\U0001f860-\U0001f887\U0001f890-\U0001f8ad\U0001f900-\U0001f90b\U0001f910-\U0001f93e\U0001f940-\U0001f970\U0001f973-\U0001f976\U0001f97a\U0001f97c-\U0001f9a2\U0001f9b0-\U0001f9b9\U0001f9c0-\U0001f9c2\U0001f9d0-\U0001f9ff\U0001fa60-\U0001fa6d'

Zl = '\u2028'

Zp = '\u2029'

Zs = ' \xa0\u1680\u2000-\u200a\u202f\u205f\u3000'

xid_continue = '0-9A-Z_a-z\xaa\xb5\xb7\xba\xc0-\xd6\xd8-\xf6\xf8-\u02c1\u02c6-\u02d1\u02e0-\u02e4\u02ec\u02ee\u0300-\u0374\u0376-\u0377\u037b-\u037d\u037f\u0386-\u038a\u038c\u038e-\u03a1\u03a3-\u03f5\u03f7-\u0481\u0483-\u0487\u048a-\u052f\u0531-\u0556\u0559\u0560-\u0588\u0591-\u05bd\u05bf\u05c1-\u05c2\u05c4-\u05c5\u05c7\u05d0-\u05ea\u05ef-\u05f2\u0610-\u061a\u0620-\u0669\u066e-\u06d3\u06d5-\u06dc\u06df-\u06e8\u06ea-\u06fc\u06ff\u0710-\u074a\u074d-\u07b1\u07c0-\u07f5\u07fa\u07fd\u0800-\u082d\u0840-\u085b\u0860-\u086a\u08a0-\u08b4\u08b6-\u08bd\u08d3-\u08e1\u08e3-\u0963\u0966-\u096f\u0971-\u0983\u0985-\u098c\u098f-\u0990\u0993-\u09a8\u09aa-\u09b0\u09b2\u09b6-\u09b9\u09bc-\u09c4\u09c7-\u09c8\u09cb-\u09ce\u09d7\u09dc-\u09dd\u09df-\u09e3\u09e6-\u09f1\u09fc\u09fe\u0a01-\u0a03\u0a05-\u0a0a\u0a0f-\u0a10\u0a13-\u0a28\u0a2a-\u0a30\u0a32-\u0a33\u0a35-\u0a36\u0a38-\u0a39\u0a3c\u0a3e-\u0a42\u0a47-\u0a48\u0a4b-\u0a4d\u0a51\u0a59-\u0a5c\u0a5e\u0a66-\u0a75\u0a81-\u0a83\u0a85-\u0a8d\u0a8f-\u0a91\u0a93-\u0aa8\u0aaa-\u0ab0\u0ab2-\u0ab3\u0ab5-\u0ab9\u0abc-\u0ac5\u0ac7-\u0ac9\u0acb-\u0acd\u0ad0\u0ae0-\u0ae3\u0ae6-\u0aef\u0af9-\u0aff\u0b01-\u0b03\u0b05-\u0b0c\u0b0f-\u0b10\u0b13-\u0b28\u0b2a-\u0b30\u0b32-\u0b33\u0b35-\u0b39\u0b3c-\u0b44\u0b47-\u0b48\u0b4b-\u0b4d\u0b56-\u0b57\u0b5c-\u0b5d\u0b5f-\u0b63\u0b66-\u0b6f\u0b71\u0b82-\u0b83\u0b85-\u0b8a\u0b8e-\u0b90\u0b92-\u0b95\u0b99-\u0b9a\u0b9c\u0b9e-\u0b9f\u0ba3-\u0ba4\u0ba8-\u0baa\u0bae-\u0bb9\u0bbe-\u0bc2\u0bc6-\u0bc8\u0bca-\u0bcd\u0bd0\u0bd7\u0be6-\u0bef\u0c00-\u0c0c\u0c0e-\u0c10\u0c12-\u0c28\u0c2a-\u0c39\u0c3d-\u0c44\u0c46-\u0c48\u0c4a-\u0c4d\u0c55-\u0c56\u0c58-\u0c5a\u0c60-\u0c63\u0c66-\u0c6f\u0c80-\u0c83\u0c85-\u0c8c\u0c8e-\u0c90\u0c92-\u0ca8\u0caa-\u0cb3\u0cb5-\u0cb9\u0cbc-\u0cc4\u0cc6-\u0cc8\u0cca-\u0ccd\u0cd5-\u0cd6\u0cde\u0ce0-\u0ce3\u0ce6-\u0cef\u0cf1-\u0cf2\u0d00-\u0d03\u0d05-\u0d0c\u0d0e-\u0d10\u0d12-\u0d44\u0d46-\u0d48\u0d4a-\u0d4e\u0d54-\u0d57\u0d5f-\u0d63\u0d66-\u0d6f\u0d7a-\u0d7f\u0d82-\u0d83\u0d85-\u0d96\u0d9a-\u0db1\u0db3-\u0dbb\u0dbd\u0dc0-\u0dc6\u0dca\u0dcf-\u0dd4\u0dd6\u0dd8-\u0ddf\u0de6-\u0def\u0df2-\u0df3\u0e01-\u0e3a\u0e40-\u0e4e\u0e50-\u0e59\u0e81-\u0e82\u0e84\u0e87-\u0e88\u0e8a\u0e8d\u0e94-\u0e97\u0e99-\u0e9f\u0ea1-\u0ea3\u0ea5\u0ea7\u0eaa-\u0eab\u0ead-\u0eb9\u0ebb-\u0ebd\u0ec0-\u0ec4\u0ec6\u0ec8-\u0ecd\u0ed0-\u0ed9\u0edc-\u0edf\u0f00\u0f18-\u0f19\u0f20-\u0f29\u0f35\u0f37\u0f39\u0f3e-\u0f47\u0f49-\u0f6c\u0f71-\u0f84\u0f86-\u0f97\u0f99-\u0fbc\u0fc6\u1000-\u1049\u1050-\u109d\u10a0-\u10c5\u10c7\u10cd\u10d0-\u10fa\u10fc-\u1248\u124a-\u124d\u1250-\u1256\u1258\u125a-\u125d\u1260-\u1288\u128a-\u128d\u1290-\u12b0\u12b2-\u12b5\u12b8-\u12be\u12c0\u12c2-\u12c5\u12c8-\u12d6\u12d8-\u1310\u1312-\u1315\u1318-\u135a\u135d-\u135f\u1369-\u1371\u1380-\u138f\u13a0-\u13f5\u13f8-\u13fd\u1401-\u166c\u166f-\u167f\u1681-\u169a\u16a0-\u16ea\u16ee-\u16f8\u1700-\u170c\u170e-\u1714\u1720-\u1734\u1740-\u1753\u1760-\u176c\u176e-\u1770\u1772-\u1773\u1780-\u17d3\u17d7\u17dc-\u17dd\u17e0-\u17e9\u180b-\u180d\u1810-\u1819\u1820-\u1878\u1880-\u18aa\u18b0-\u18f5\u1900-\u191e\u1920-\u192b\u1930-\u193b\u1946-\u196d\u1970-\u1974\u1980-\u19ab\u19b0-\u19c9\u19d0-\u19da\u1a00-\u1a1b\u1a20-\u1a5e\u1a60-\u1a7c\u1a7f-\u1a89\u1a90-\u1a99\u1aa7\u1ab0-\u1abd\u1b00-\u1b4b\u1b50-\u1b59\u1b6b-\u1b73\u1b80-\u1bf3\u1c00-\u1c37\u1c40-\u1c49\u1c4d-\u1c7d\u1c80-\u1c88\u1c90-\u1cba\u1cbd-\u1cbf\u1cd0-\u1cd2\u1cd4-\u1cf9\u1d00-\u1df9\u1dfb-\u1f15\u1f18-\u1f1d\u1f20-\u1f45\u1f48-\u1f4d\u1f50-\u1f57\u1f59\u1f5b\u1f5d\u1f5f-\u1f7d\u1f80-\u1fb4\u1fb6-\u1fbc\u1fbe\u1fc2-\u1fc4\u1fc6-\u1fcc\u1fd0-\u1fd3\u1fd6-\u1fdb\u1fe0-\u1fec\u1ff2-\u1ff4\u1ff6-\u1ffc\u203f-\u2040\u2054\u2071\u207f\u2090-\u209c\u20d0-\u20dc\u20e1\u20e5-\u20f0\u2102\u2107\u210a-\u2113\u2115\u2118-\u211d\u2124\u2126\u2128\u212a-\u2139\u213c-\u213f\u2145-\u2149\u214e\u2160-\u2188\u2c00-\u2c2e\u2c30-\u2c5e\u2c60-\u2ce4\u2ceb-\u2cf3\u2d00-\u2d25\u2d27\u2d2d\u2d30-\u2d67\u2d6f\u2d7f-\u2d96\u2da0-\u2da6\u2da8-\u2dae\u2db0-\u2db6\u2db8-\u2dbe\u2dc0-\u2dc6\u2dc8-\u2dce\u2dd0-\u2dd6\u2dd8-\u2dde\u2de0-\u2dff\u3005-\u3007\u3021-\u302f\u3031-\u3035\u3038-\u303c\u3041-\u3096\u3099-\u309a\u309d-\u309f\u30a1-\u30fa\u30fc-\u30ff\u3105-\u312f\u3131-\u318e\u31a0-\u31ba\u31f0-\u31ff\u3400-\u4db5\u4e00-\u9fef\ua000-\ua48c\ua4d0-\ua4fd\ua500-\ua60c\ua610-\ua62b\ua640-\ua66f\ua674-\ua67d\ua67f-\ua6f1\ua717-\ua71f\ua722-\ua788\ua78b-\ua7b9\ua7f7-\ua827\ua840-\ua873\ua880-\ua8c5\ua8d0-\ua8d9\ua8e0-\ua8f7\ua8fb\ua8fd-\ua92d\ua930-\ua953\ua960-\ua97c\ua980-\ua9c0\ua9cf-\ua9d9\ua9e0-\ua9fe\uaa00-\uaa36\uaa40-\uaa4d\uaa50-\uaa59\uaa60-\uaa76\uaa7a-\uaac2\uaadb-\uaadd\uaae0-\uaaef\uaaf2-\uaaf6\uab01-\uab06\uab09-\uab0e\uab11-\uab16\uab20-\uab26\uab28-\uab2e\uab30-\uab5a\uab5c-\uab65\uab70-\uabea\uabec-\uabed\uabf0-\uabf9\uac00-\ud7a3\ud7b0-\ud7c6\ud7cb-\ud7fb\uf900-\ufa6d\ufa70-\ufad9\ufb00-\ufb06\ufb13-\ufb17\ufb1d-\ufb28\ufb2a-\ufb36\ufb38-\ufb3c\ufb3e\ufb40-\ufb41\ufb43-\ufb44\ufb46-\ufbb1\ufbd3-\ufc5d\ufc64-\ufd3d\ufd50-\ufd8f\ufd92-\ufdc7\ufdf0-\ufdf9\ufe00-\ufe0f\ufe20-\ufe2f\ufe33-\ufe34\ufe4d-\ufe4f\ufe71\ufe73\ufe77\ufe79\ufe7b\ufe7d\ufe7f-\ufefc\uff10-\uff19\uff21-\uff3a\uff3f\uff41-\uff5a\uff66-\uffbe\uffc2-\uffc7\uffca-\uffcf\uffd2-\uffd7\uffda-\uffdc\U00010000-\U0001000b\U0001000d-\U00010026\U00010028-\U0001003a\U0001003c-\U0001003d\U0001003f-\U0001004d\U00010050-\U0001005d\U00010080-\U000100fa\U00010140-\U00010174\U000101fd\U00010280-\U0001029c\U000102a0-\U000102d0\U000102e0\U00010300-\U0001031f\U0001032d-\U0001034a\U00010350-\U0001037a\U00010380-\U0001039d\U000103a0-\U000103c3\U000103c8-\U000103cf\U000103d1-\U000103d5\U00010400-\U0001049d\U000104a0-\U000104a9\U000104b0-\U000104d3\U000104d8-\U000104fb\U00010500-\U00010527\U00010530-\U00010563\U00010600-\U00010736\U00010740-\U00010755\U00010760-\U00010767\U00010800-\U00010805\U00010808\U0001080a-\U00010835\U00010837-\U00010838\U0001083c\U0001083f-\U00010855\U00010860-\U00010876\U00010880-\U0001089e\U000108e0-\U000108f2\U000108f4-\U000108f5\U00010900-\U00010915\U00010920-\U00010939\U00010980-\U000109b7\U000109be-\U000109bf\U00010a00-\U00010a03\U00010a05-\U00010a06\U00010a0c-\U00010a13\U00010a15-\U00010a17\U00010a19-\U00010a35\U00010a38-\U00010a3a\U00010a3f\U00010a60-\U00010a7c\U00010a80-\U00010a9c\U00010ac0-\U00010ac7\U00010ac9-\U00010ae6\U00010b00-\U00010b35\U00010b40-\U00010b55\U00010b60-\U00010b72\U00010b80-\U00010b91\U00010c00-\U00010c48\U00010c80-\U00010cb2\U00010cc0-\U00010cf2\U00010d00-\U00010d27\U00010d30-\U00010d39\U00010f00-\U00010f1c\U00010f27\U00010f30-\U00010f50\U00011000-\U00011046\U00011066-\U0001106f\U0001107f-\U000110ba\U000110d0-\U000110e8\U000110f0-\U000110f9\U00011100-\U00011134\U00011136-\U0001113f\U00011144-\U00011146\U00011150-\U00011173\U00011176\U00011180-\U000111c4\U000111c9-\U000111cc\U000111d0-\U000111da\U000111dc\U00011200-\U00011211\U00011213-\U00011237\U0001123e\U00011280-\U00011286\U00011288\U0001128a-\U0001128d\U0001128f-\U0001129d\U0001129f-\U000112a8\U000112b0-\U000112ea\U000112f0-\U000112f9\U00011300-\U00011303\U00011305-\U0001130c\U0001130f-\U00011310\U00011313-\U00011328\U0001132a-\U00011330\U00011332-\U00011333\U00011335-\U00011339\U0001133b-\U00011344\U00011347-\U00011348\U0001134b-\U0001134d\U00011350\U00011357\U0001135d-\U00011363\U00011366-\U0001136c\U00011370-\U00011374\U00011400-\U0001144a\U00011450-\U00011459\U0001145e\U00011480-\U000114c5\U000114c7\U000114d0-\U000114d9\U00011580-\U000115b5\U000115b8-\U000115c0\U000115d8-\U000115dd\U00011600-\U00011640\U00011644\U00011650-\U00011659\U00011680-\U000116b7\U000116c0-\U000116c9\U00011700-\U0001171a\U0001171d-\U0001172b\U00011730-\U00011739\U00011800-\U0001183a\U000118a0-\U000118e9\U000118ff\U00011a00-\U00011a3e\U00011a47\U00011a50-\U00011a83\U00011a86-\U00011a99\U00011a9d\U00011ac0-\U00011af8\U00011c00-\U00011c08\U00011c0a-\U00011c36\U00011c38-\U00011c40\U00011c50-\U00011c59\U00011c72-\U00011c8f\U00011c92-\U00011ca7\U00011ca9-\U00011cb6\U00011d00-\U00011d06\U00011d08-\U00011d09\U00011d0b-\U00011d36\U00011d3a\U00011d3c-\U00011d3d\U00011d3f-\U00011d47\U00011d50-\U00011d59\U00011d60-\U00011d65\U00011d67-\U00011d68\U00011d6a-\U00011d8e\U00011d90-\U00011d91\U00011d93-\U00011d98\U00011da0-\U00011da9\U00011ee0-\U00011ef6\U00012000-\U00012399\U00012400-\U0001246e\U00012480-\U00012543\U00013000-\U0001342e\U00014400-\U00014646\U00016800-\U00016a38\U00016a40-\U00016a5e\U00016a60-\U00016a69\U00016ad0-\U00016aed\U00016af0-\U00016af4\U00016b00-\U00016b36\U00016b40-\U00016b43\U00016b50-\U00016b59\U00016b63-\U00016b77\U00016b7d-\U00016b8f\U00016e40-\U00016e7f\U00016f00-\U00016f44\U00016f50-\U00016f7e\U00016f8f-\U00016f9f\U00016fe0-\U00016fe1\U00017000-\U000187f1\U00018800-\U00018af2\U0001b000-\U0001b11e\U0001b170-\U0001b2fb\U0001bc00-\U0001bc6a\U0001bc70-\U0001bc7c\U0001bc80-\U0001bc88\U0001bc90-\U0001bc99\U0001bc9d-\U0001bc9e\U0001d165-\U0001d169\U0001d16d-\U0001d172\U0001d17b-\U0001d182\U0001d185-\U0001d18b\U0001d1aa-\U0001d1ad\U0001d242-\U0001d244\U0001d400-\U0001d454\U0001d456-\U0001d49c\U0001d49e-\U0001d49f\U0001d4a2\U0001d4a5-\U0001d4a6\U0001d4a9-\U0001d4ac\U0001d4ae-\U0001d4b9\U0001d4bb\U0001d4bd-\U0001d4c3\U0001d4c5-\U0001d505\U0001d507-\U0001d50a\U0001d50d-\U0001d514\U0001d516-\U0001d51c\U0001d51e-\U0001d539\U0001d53b-\U0001d53e\U0001d540-\U0001d544\U0001d546\U0001d54a-\U0001d550\U0001d552-\U0001d6a5\U0001d6a8-\U0001d6c0\U0001d6c2-\U0001d6da\U0001d6dc-\U0001d6fa\U0001d6fc-\U0001d714\U0001d716-\U0001d734\U0001d736-\U0001d74e\U0001d750-\U0001d76e\U0001d770-\U0001d788\U0001d78a-\U0001d7a8\U0001d7aa-\U0001d7c2\U0001d7c4-\U0001d7cb\U0001d7ce-\U0001d7ff\U0001da00-\U0001da36\U0001da3b-\U0001da6c\U0001da75\U0001da84\U0001da9b-\U0001da9f\U0001daa1-\U0001daaf\U0001e000-\U0001e006\U0001e008-\U0001e018\U0001e01b-\U0001e021\U0001e023-\U0001e024\U0001e026-\U0001e02a\U0001e800-\U0001e8c4\U0001e8d0-\U0001e8d6\U0001e900-\U0001e94a\U0001e950-\U0001e959\U0001ee00-\U0001ee03\U0001ee05-\U0001ee1f\U0001ee21-\U0001ee22\U0001ee24\U0001ee27\U0001ee29-\U0001ee32\U0001ee34-\U0001ee37\U0001ee39\U0001ee3b\U0001ee42\U0001ee47\U0001ee49\U0001ee4b\U0001ee4d-\U0001ee4f\U0001ee51-\U0001ee52\U0001ee54\U0001ee57\U0001ee59\U0001ee5b\U0001ee5d\U0001ee5f\U0001ee61-\U0001ee62\U0001ee64\U0001ee67-\U0001ee6a\U0001ee6c-\U0001ee72\U0001ee74-\U0001ee77\U0001ee79-\U0001ee7c\U0001ee7e\U0001ee80-\U0001ee89\U0001ee8b-\U0001ee9b\U0001eea1-\U0001eea3\U0001eea5-\U0001eea9\U0001eeab-\U0001eebb\U00020000-\U0002a6d6\U0002a700-\U0002b734\U0002b740-\U0002b81d\U0002b820-\U0002cea1\U0002ceb0-\U0002ebe0\U0002f800-\U0002fa1d\U000e0100-\U000e01ef'

xid_start = 'A-Z_a-z\xaa\xb5\xba\xc0-\xd6\xd8-\xf6\xf8-\u02c1\u02c6-\u02d1\u02e0-\u02e4\u02ec\u02ee\u0370-\u0374\u0376-\u0377\u037b-\u037d\u037f\u0386\u0388-\u038a\u038c\u038e-\u03a1\u03a3-\u03f5\u03f7-\u0481\u048a-\u052f\u0531-\u0556\u0559\u0560-\u0588\u05d0-\u05ea\u05ef-\u05f2\u0620-\u064a\u066e-\u066f\u0671-\u06d3\u06d5\u06e5-\u06e6\u06ee-\u06ef\u06fa-\u06fc\u06ff\u0710\u0712-\u072f\u074d-\u07a5\u07b1\u07ca-\u07ea\u07f4-\u07f5\u07fa\u0800-\u0815\u081a\u0824\u0828\u0840-\u0858\u0860-\u086a\u08a0-\u08b4\u08b6-\u08bd\u0904-\u0939\u093d\u0950\u0958-\u0961\u0971-\u0980\u0985-\u098c\u098f-\u0990\u0993-\u09a8\u09aa-\u09b0\u09b2\u09b6-\u09b9\u09bd\u09ce\u09dc-\u09dd\u09df-\u09e1\u09f0-\u09f1\u09fc\u0a05-\u0a0a\u0a0f-\u0a10\u0a13-\u0a28\u0a2a-\u0a30\u0a32-\u0a33\u0a35-\u0a36\u0a38-\u0a39\u0a59-\u0a5c\u0a5e\u0a72-\u0a74\u0a85-\u0a8d\u0a8f-\u0a91\u0a93-\u0aa8\u0aaa-\u0ab0\u0ab2-\u0ab3\u0ab5-\u0ab9\u0abd\u0ad0\u0ae0-\u0ae1\u0af9\u0b05-\u0b0c\u0b0f-\u0b10\u0b13-\u0b28\u0b2a-\u0b30\u0b32-\u0b33\u0b35-\u0b39\u0b3d\u0b5c-\u0b5d\u0b5f-\u0b61\u0b71\u0b83\u0b85-\u0b8a\u0b8e-\u0b90\u0b92-\u0b95\u0b99-\u0b9a\u0b9c\u0b9e-\u0b9f\u0ba3-\u0ba4\u0ba8-\u0baa\u0bae-\u0bb9\u0bd0\u0c05-\u0c0c\u0c0e-\u0c10\u0c12-\u0c28\u0c2a-\u0c39\u0c3d\u0c58-\u0c5a\u0c60-\u0c61\u0c80\u0c85-\u0c8c\u0c8e-\u0c90\u0c92-\u0ca8\u0caa-\u0cb3\u0cb5-\u0cb9\u0cbd\u0cde\u0ce0-\u0ce1\u0cf1-\u0cf2\u0d05-\u0d0c\u0d0e-\u0d10\u0d12-\u0d3a\u0d3d\u0d4e\u0d54-\u0d56\u0d5f-\u0d61\u0d7a-\u0d7f\u0d85-\u0d96\u0d9a-\u0db1\u0db3-\u0dbb\u0dbd\u0dc0-\u0dc6\u0e01-\u0e30\u0e32\u0e40-\u0e46\u0e81-\u0e82\u0e84\u0e87-\u0e88\u0e8a\u0e8d\u0e94-\u0e97\u0e99-\u0e9f\u0ea1-\u0ea3\u0ea5\u0ea7\u0eaa-\u0eab\u0ead-\u0eb0\u0eb2\u0ebd\u0ec0-\u0ec4\u0ec6\u0edc-\u0edf\u0f00\u0f40-\u0f47\u0f49-\u0f6c\u0f88-\u0f8c\u1000-\u102a\u103f\u1050-\u1055\u105a-\u105d\u1061\u1065-\u1066\u106e-\u1070\u1075-\u1081\u108e\u10a0-\u10c5\u10c7\u10cd\u10d0-\u10fa\u10fc-\u1248\u124a-\u124d\u1250-\u1256\u1258\u125a-\u125d\u1260-\u1288\u128a-\u128d\u1290-\u12b0\u12b2-\u12b5\u12b8-\u12be\u12c0\u12c2-\u12c5\u12c8-\u12d6\u12d8-\u1310\u1312-\u1315\u1318-\u135a\u1380-\u138f\u13a0-\u13f5\u13f8-\u13fd\u1401-\u166c\u166f-\u167f\u1681-\u169a\u16a0-\u16ea\u16ee-\u16f8\u1700-\u170c\u170e-\u1711\u1720-\u1731\u1740-\u1751\u1760-\u176c\u176e-\u1770\u1780-\u17b3\u17d7\u17dc\u1820-\u1878\u1880-\u18a8\u18aa\u18b0-\u18f5\u1900-\u191e\u1950-\u196d\u1970-\u1974\u1980-\u19ab\u19b0-\u19c9\u1a00-\u1a16\u1a20-\u1a54\u1aa7\u1b05-\u1b33\u1b45-\u1b4b\u1b83-\u1ba0\u1bae-\u1baf\u1bba-\u1be5\u1c00-\u1c23\u1c4d-\u1c4f\u1c5a-\u1c7d\u1c80-\u1c88\u1c90-\u1cba\u1cbd-\u1cbf\u1ce9-\u1cec\u1cee-\u1cf1\u1cf5-\u1cf6\u1d00-\u1dbf\u1e00-\u1f15\u1f18-\u1f1d\u1f20-\u1f45\u1f48-\u1f4d\u1f50-\u1f57\u1f59\u1f5b\u1f5d\u1f5f-\u1f7d\u1f80-\u1fb4\u1fb6-\u1fbc\u1fbe\u1fc2-\u1fc4\u1fc6-\u1fcc\u1fd0-\u1fd3\u1fd6-\u1fdb\u1fe0-\u1fec\u1ff2-\u1ff4\u1ff6-\u1ffc\u2071\u207f\u2090-\u209c\u2102\u2107\u210a-\u2113\u2115\u2118-\u211d\u2124\u2126\u2128\u212a-\u2139\u213c-\u213f\u2145-\u2149\u214e\u2160-\u2188\u2c00-\u2c2e\u2c30-\u2c5e\u2c60-\u2ce4\u2ceb-\u2cee\u2cf2-\u2cf3\u2d00-\u2d25\u2d27\u2d2d\u2d30-\u2d67\u2d6f\u2d80-\u2d96\u2da0-\u2da6\u2da8-\u2dae\u2db0-\u2db6\u2db8-\u2dbe\u2dc0-\u2dc6\u2dc8-\u2dce\u2dd0-\u2dd6\u2dd8-\u2dde\u3005-\u3007\u3021-\u3029\u3031-\u3035\u3038-\u303c\u3041-\u3096\u309d-\u309f\u30a1-\u30fa\u30fc-\u30ff\u3105-\u312f\u3131-\u318e\u31a0-\u31ba\u31f0-\u31ff\u3400-\u4db5\u4e00-\u9fef\ua000-\ua48c\ua4d0-\ua4fd\ua500-\ua60c\ua610-\ua61f\ua62a-\ua62b\ua640-\ua66e\ua67f-\ua69d\ua6a0-\ua6ef\ua717-\ua71f\ua722-\ua788\ua78b-\ua7b9\ua7f7-\ua801\ua803-\ua805\ua807-\ua80a\ua80c-\ua822\ua840-\ua873\ua882-\ua8b3\ua8f2-\ua8f7\ua8fb\ua8fd-\ua8fe\ua90a-\ua925\ua930-\ua946\ua960-\ua97c\ua984-\ua9b2\ua9cf\ua9e0-\ua9e4\ua9e6-\ua9ef\ua9fa-\ua9fe\uaa00-\uaa28\uaa40-\uaa42\uaa44-\uaa4b\uaa60-\uaa76\uaa7a\uaa7e-\uaaaf\uaab1\uaab5-\uaab6\uaab9-\uaabd\uaac0\uaac2\uaadb-\uaadd\uaae0-\uaaea\uaaf2-\uaaf4\uab01-\uab06\uab09-\uab0e\uab11-\uab16\uab20-\uab26\uab28-\uab2e\uab30-\uab5a\uab5c-\uab65\uab70-\uabe2\uac00-\ud7a3\ud7b0-\ud7c6\ud7cb-\ud7fb\uf900-\ufa6d\ufa70-\ufad9\ufb00-\ufb06\ufb13-\ufb17\ufb1d\ufb1f-\ufb28\ufb2a-\ufb36\ufb38-\ufb3c\ufb3e\ufb40-\ufb41\ufb43-\ufb44\ufb46-\ufbb1\ufbd3-\ufc5d\ufc64-\ufd3d\ufd50-\ufd8f\ufd92-\ufdc7\ufdf0-\ufdf9\ufe71\ufe73\ufe77\ufe79\ufe7b\ufe7d\ufe7f-\ufefc\uff21-\uff3a\uff41-\uff5a\uff66-\uff9d\uffa0-\uffbe\uffc2-\uffc7\uffca-\uffcf\uffd2-\uffd7\uffda-\uffdc\U00010000-\U0001000b\U0001000d-\U00010026\U00010028-\U0001003a\U0001003c-\U0001003d\U0001003f-\U0001004d\U00010050-\U0001005d\U00010080-\U000100fa\U00010140-\U00010174\U00010280-\U0001029c\U000102a0-\U000102d0\U00010300-\U0001031f\U0001032d-\U0001034a\U00010350-\U00010375\U00010380-\U0001039d\U000103a0-\U000103c3\U000103c8-\U000103cf\U000103d1-\U000103d5\U00010400-\U0001049d\U000104b0-\U000104d3\U000104d8-\U000104fb\U00010500-\U00010527\U00010530-\U00010563\U00010600-\U00010736\U00010740-\U00010755\U00010760-\U00010767\U00010800-\U00010805\U00010808\U0001080a-\U00010835\U00010837-\U00010838\U0001083c\U0001083f-\U00010855\U00010860-\U00010876\U00010880-\U0001089e\U000108e0-\U000108f2\U000108f4-\U000108f5\U00010900-\U00010915\U00010920-\U00010939\U00010980-\U000109b7\U000109be-\U000109bf\U00010a00\U00010a10-\U00010a13\U00010a15-\U00010a17\U00010a19-\U00010a35\U00010a60-\U00010a7c\U00010a80-\U00010a9c\U00010ac0-\U00010ac7\U00010ac9-\U00010ae4\U00010b00-\U00010b35\U00010b40-\U00010b55\U00010b60-\U00010b72\U00010b80-\U00010b91\U00010c00-\U00010c48\U00010c80-\U00010cb2\U00010cc0-\U00010cf2\U00010d00-\U00010d23\U00010f00-\U00010f1c\U00010f27\U00010f30-\U00010f45\U00011003-\U00011037\U00011083-\U000110af\U000110d0-\U000110e8\U00011103-\U00011126\U00011144\U00011150-\U00011172\U00011176\U00011183-\U000111b2\U000111c1-\U000111c4\U000111da\U000111dc\U00011200-\U00011211\U00011213-\U0001122b\U00011280-\U00011286\U00011288\U0001128a-\U0001128d\U0001128f-\U0001129d\U0001129f-\U000112a8\U000112b0-\U000112de\U00011305-\U0001130c\U0001130f-\U00011310\U00011313-\U00011328\U0001132a-\U00011330\U00011332-\U00011333\U00011335-\U00011339\U0001133d\U00011350\U0001135d-\U00011361\U00011400-\U00011434\U00011447-\U0001144a\U00011480-\U000114af\U000114c4-\U000114c5\U000114c7\U00011580-\U000115ae\U000115d8-\U000115db\U00011600-\U0001162f\U00011644\U00011680-\U000116aa\U00011700-\U0001171a\U00011800-\U0001182b\U000118a0-\U000118df\U000118ff\U00011a00\U00011a0b-\U00011a32\U00011a3a\U00011a50\U00011a5c-\U00011a83\U00011a86-\U00011a89\U00011a9d\U00011ac0-\U00011af8\U00011c00-\U00011c08\U00011c0a-\U00011c2e\U00011c40\U00011c72-\U00011c8f\U00011d00-\U00011d06\U00011d08-\U00011d09\U00011d0b-\U00011d30\U00011d46\U00011d60-\U00011d65\U00011d67-\U00011d68\U00011d6a-\U00011d89\U00011d98\U00011ee0-\U00011ef2\U00012000-\U00012399\U00012400-\U0001246e\U00012480-\U00012543\U00013000-\U0001342e\U00014400-\U00014646\U00016800-\U00016a38\U00016a40-\U00016a5e\U00016ad0-\U00016aed\U00016b00-\U00016b2f\U00016b40-\U00016b43\U00016b63-\U00016b77\U00016b7d-\U00016b8f\U00016e40-\U00016e7f\U00016f00-\U00016f44\U00016f50\U00016f93-\U00016f9f\U00016fe0-\U00016fe1\U00017000-\U000187f1\U00018800-\U00018af2\U0001b000-\U0001b11e\U0001b170-\U0001b2fb\U0001bc00-\U0001bc6a\U0001bc70-\U0001bc7c\U0001bc80-\U0001bc88\U0001bc90-\U0001bc99\U0001d400-\U0001d454\U0001d456-\U0001d49c\U0001d49e-\U0001d49f\U0001d4a2\U0001d4a5-\U0001d4a6\U0001d4a9-\U0001d4ac\U0001d4ae-\U0001d4b9\U0001d4bb\U0001d4bd-\U0001d4c3\U0001d4c5-\U0001d505\U0001d507-\U0001d50a\U0001d50d-\U0001d514\U0001d516-\U0001d51c\U0001d51e-\U0001d539\U0001d53b-\U0001d53e\U0001d540-\U0001d544\U0001d546\U0001d54a-\U0001d550\U0001d552-\U0001d6a5\U0001d6a8-\U0001d6c0\U0001d6c2-\U0001d6da\U0001d6dc-\U0001d6fa\U0001d6fc-\U0001d714\U0001d716-\U0001d734\U0001d736-\U0001d74e\U0001d750-\U0001d76e\U0001d770-\U0001d788\U0001d78a-\U0001d7a8\U0001d7aa-\U0001d7c2\U0001d7c4-\U0001d7cb\U0001e800-\U0001e8c4\U0001e900-\U0001e943\U0001ee00-\U0001ee03\U0001ee05-\U0001ee1f\U0001ee21-\U0001ee22\U0001ee24\U0001ee27\U0001ee29-\U0001ee32\U0001ee34-\U0001ee37\U0001ee39\U0001ee3b\U0001ee42\U0001ee47\U0001ee49\U0001ee4b\U0001ee4d-\U0001ee4f\U0001ee51-\U0001ee52\U0001ee54\U0001ee57\U0001ee59\U0001ee5b\U0001ee5d\U0001ee5f\U0001ee61-\U0001ee62\U0001ee64\U0001ee67-\U0001ee6a\U0001ee6c-\U0001ee72\U0001ee74-\U0001ee77\U0001ee79-\U0001ee7c\U0001ee7e\U0001ee80-\U0001ee89\U0001ee8b-\U0001ee9b\U0001eea1-\U0001eea3\U0001eea5-\U0001eea9\U0001eeab-\U0001eebb\U00020000-\U0002a6d6\U0002a700-\U0002b734\U0002b740-\U0002b81d\U0002b820-\U0002cea1\U0002ceb0-\U0002ebe0\U0002f800-\U0002fa1d'

cats = ['Cc', 'Cf', 'Cn', 'Co', 'Cs', 'Ll', 'Lm', 'Lo', 'Lt', 'Lu', 'Mc', 'Me', 'Mn', 'Nd', 'Nl', 'No', 'Pc', 'Pd', 'Pe', 'Pf', 'Pi', 'Po', 'Ps', 'Sc', 'Sk', 'Sm', 'So', 'Zl', 'Zp', 'Zs']

# Generated from unidata 11.0.0

def combine(*args):
    return ''.join(globals()[cat] for cat in args)


def allexcept(*args):
    newcats = cats[:]
    for arg in args:
        newcats.remove(arg)
    return ''.join(globals()[cat] for cat in newcats)


def _handle_runs(char_list):  # pragma: no cover
    buf = []
    for c in char_list:
        if len(c) == 1:
            if buf and buf[-1][1] == chr(ord(c)-1):
                buf[-1] = (buf[-1][0], c)
            else:
                buf.append((c, c))
        else:
            buf.append((c, c))
    for a, b in buf:
        if a == b:
            yield a
        else:
            yield '%s-%s' % (a, b)


if __name__ == '__main__':  # pragma: no cover
    import unicodedata

    categories = {'xid_start': [], 'xid_continue': []}

    with open(__file__, encoding='utf-8') as fp:
        content = fp.read()

    header = content[:content.find('Cc =')]
    footer = content[content.find("def combine("):]

    for code in range(0x110000):
        c = chr(code)
        cat = unicodedata.category(c)
        if ord(c) == 0xdc00:
            # Hack to avoid combining this combining with the preceding high
            # surrogate, 0xdbff, when doing a repr.
            c = '\\' + c
        elif ord(c) in (0x2d, 0x5b, 0x5c, 0x5d, 0x5e):
            # Escape regex metachars.
            c = '\\' + c
        categories.setdefault(cat, []).append(c)
        # XID_START and XID_CONTINUE are special categories used for matching
        # identifiers in Python 3.
        if c.isidentifier():
            categories['xid_start'].append(c)
        if ('a' + c).isidentifier():
            categories['xid_continue'].append(c)

    with open(__file__, 'w', encoding='utf-8') as fp:
        fp.write(header)

        for cat in sorted(categories):
            val = ''.join(_handle_runs(categories[cat]))
            fp.write('%s = %a\n\n' % (cat, val))

        cats = sorted(categories)
        cats.remove('xid_start')
        cats.remove('xid_continue')
        fp.write('cats = %r\n\n' % cats)

        fp.write('# Generated from unidata %s\n\n' % (unicodedata.unidata_version,))

        fp.write(footer)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/console.py
# ========================================================
"""
    pygments.console
    ~~~~~~~~~~~~~~~~

    Format colored console output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

esc = "\x1b["

codes = {}
codes[""] = ""
codes["reset"] = esc + "39;49;00m"

codes["bold"] = esc + "01m"
codes["faint"] = esc + "02m"
codes["standout"] = esc + "03m"
codes["underline"] = esc + "04m"
codes["blink"] = esc + "05m"
codes["overline"] = esc + "06m"

dark_colors = ["black", "red", "green", "yellow", "blue",
               "magenta", "cyan", "gray"]
light_colors = ["brightblack", "brightred", "brightgreen", "brightyellow", "brightblue",
                "brightmagenta", "brightcyan", "white"]

x = 30
for d, l in zip(dark_colors, light_colors):
    codes[d] = esc + "%im" % x
    codes[l] = esc + "%im" % (60 + x)
    x += 1

del d, l, x

codes["white"] = codes["bold"]


def reset_color():
    return codes["reset"]


def colorize(color_key, text):
    return codes[color_key] + text + codes["reset"]


def ansiformat(attr, text):
    """
    Format ``text`` with a color and/or some attributes::

        color       normal color
        *color*     bold color
        _color_     underlined color
        +color+     blinking color
    """
    result = []
    if attr[:1] == attr[-1:] == '+':
        result.append(codes['blink'])
        attr = attr[1:-1]
    if attr[:1] == attr[-1:] == '*':
        result.append(codes['bold'])
        attr = attr[1:-1]
    if attr[:1] == attr[-1:] == '_':
        result.append(codes['underline'])
        attr = attr[1:-1]
    result.append(codes[attr])
    result.append(text)
    result.append(codes['reset'])
    return ''.join(result)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/modeline.py
# ========================================================
"""
    pygments.modeline
    ~~~~~~~~~~~~~~~~~

    A simple modeline parser (based on pymodeline).

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re

__all__ = ['get_filetype_from_buffer']


modeline_re = re.compile(r'''
    (?: vi | vim | ex ) (?: [<=>]? \d* )? :
    .* (?: ft | filetype | syn | syntax ) = ( [^:\s]+ )
''', re.VERBOSE)


def get_filetype_from_line(l):
    m = modeline_re.search(l)
    if m:
        return m.group(1)


def get_filetype_from_buffer(buf, max_lines=5):
    """
    Scan the buffer for modelines and return filetype if one is found.
    """
    lines = buf.splitlines()
    for l in lines[-1:-max_lines-1:-1]:
        ret = get_filetype_from_line(l)
        if ret:
            return ret
    for i in range(max_lines, -1, -1):
        if i < len(lines):
            ret = get_filetype_from_line(lines[i])
            if ret:
                return ret

    return None


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/filter.py
# ========================================================
"""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""


def apply_filters(stream, filters, lexer=None):
    """
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """
    return type(f.__name__, (FunctionFilter,), {
        '__module__': getattr(f, '__module__'),
        '__doc__': f.__doc__,
        'function': f,
    })


class Filter:
    """
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """
    function = None

    def __init__(self, **options):
        if not hasattr(self, 'function'):
            raise TypeError('%r used without bound function' %
                            self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/styles/__init__.py
# ========================================================
"""
    pygments.styles
    ~~~~~~~~~~~~~~~

    Contains built-in styles.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.plugin import find_plugin_styles
from pip._vendor.pygments.util import ClassNotFound

#: A dictionary of built-in styles, mapping style names to
#: ``'submodule::classname'`` strings.
STYLE_MAP = {
    'default':  'default::DefaultStyle',
    'emacs':    'emacs::EmacsStyle',
    'friendly': 'friendly::FriendlyStyle',
    'friendly_grayscale': 'friendly_grayscale::FriendlyGrayscaleStyle',
    'colorful': 'colorful::ColorfulStyle',
    'autumn':   'autumn::AutumnStyle',
    'murphy':   'murphy::MurphyStyle',
    'manni':    'manni::ManniStyle',
    'material': 'material::MaterialStyle',
    'monokai':  'monokai::MonokaiStyle',
    'perldoc':  'perldoc::PerldocStyle',
    'pastie':   'pastie::PastieStyle',
    'borland':  'borland::BorlandStyle',
    'trac':     'trac::TracStyle',
    'native':   'native::NativeStyle',
    'fruity':   'fruity::FruityStyle',
    'bw':       'bw::BlackWhiteStyle',
    'vim':      'vim::VimStyle',
    'vs':       'vs::VisualStudioStyle',
    'tango':    'tango::TangoStyle',
    'rrt':      'rrt::RrtStyle',
    'xcode':    'xcode::XcodeStyle',
    'igor':     'igor::IgorStyle',
    'paraiso-light': 'paraiso_light::ParaisoLightStyle',
    'paraiso-dark': 'paraiso_dark::ParaisoDarkStyle',
    'lovelace': 'lovelace::LovelaceStyle',
    'algol':    'algol::AlgolStyle',
    'algol_nu': 'algol_nu::Algol_NuStyle',
    'arduino':  'arduino::ArduinoStyle',
    'rainbow_dash': 'rainbow_dash::RainbowDashStyle',
    'abap':     'abap::AbapStyle',
    'solarized-dark': 'solarized::SolarizedDarkStyle',
    'solarized-light': 'solarized::SolarizedLightStyle',
    'sas':         'sas::SasStyle',
    'staroffice' : 'staroffice::StarofficeStyle',
    'stata':       'stata_light::StataLightStyle',
    'stata-light': 'stata_light::StataLightStyle',
    'stata-dark':  'stata_dark::StataDarkStyle',
    'inkpot':      'inkpot::InkPotStyle',
    'zenburn': 'zenburn::ZenburnStyle',
    'gruvbox-dark': 'gruvbox::GruvboxDarkStyle',
    'gruvbox-light': 'gruvbox::GruvboxLightStyle',
    'dracula': 'dracula::DraculaStyle',
    'one-dark': 'onedark::OneDarkStyle',
    'lilypond' : 'lilypond::LilyPondStyle',
    'nord': 'nord::NordStyle',
    'nord-darker': 'nord::NordDarkerStyle',
    'github-dark': 'gh_dark::GhDarkStyle'
}


def get_style_by_name(name):
    """
    Return a style class by its short name. The names of the builtin styles
    are listed in :data:`pygments.styles.STYLE_MAP`.

    Will raise :exc:`pygments.util.ClassNotFound` if no style of that name is
    found.
    """
    if name in STYLE_MAP:
        mod, cls = STYLE_MAP[name].split('::')
        builtin = "yes"
    else:
        for found_name, style in find_plugin_styles():
            if name == found_name:
                return style
        # perhaps it got dropped into our styles package
        builtin = ""
        mod = name
        cls = name.title() + "Style"

    try:
        mod = __import__('pygments.styles.' + mod, None, None, [cls])
    except ImportError:
        raise ClassNotFound("Could not find style module %r" % mod +
                         (builtin and ", though it should be builtin") + ".")
    try:
        return getattr(mod, cls)
    except AttributeError:
        raise ClassNotFound("Could not find style class %r in style module." % cls)


def get_all_styles():
    """Return a generator for all styles by name, both builtin and plugin."""
    yield from STYLE_MAP
    for name, _ in find_plugin_styles():
        yield name


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/_mapping.py
# ========================================================
# Automatically generated by scripts/gen_mapfiles.py.
# DO NOT EDIT BY HAND; run `tox -e mapfiles` instead.

LEXERS = {
    'ABAPLexer': ('pip._vendor.pygments.lexers.business', 'ABAP', ('abap',), ('*.abap', '*.ABAP'), ('text/x-abap',)),
    'AMDGPULexer': ('pip._vendor.pygments.lexers.amdgpu', 'AMDGPU', ('amdgpu',), ('*.isa',), ()),
    'APLLexer': ('pip._vendor.pygments.lexers.apl', 'APL', ('apl',), ('*.apl', '*.aplf', '*.aplo', '*.apln', '*.aplc', '*.apli', '*.dyalog'), ()),
    'AbnfLexer': ('pip._vendor.pygments.lexers.grammar_notation', 'ABNF', ('abnf',), ('*.abnf',), ('text/x-abnf',)),
    'ActionScript3Lexer': ('pip._vendor.pygments.lexers.actionscript', 'ActionScript 3', ('actionscript3', 'as3'), ('*.as',), ('application/x-actionscript3', 'text/x-actionscript3', 'text/actionscript3')),
    'ActionScriptLexer': ('pip._vendor.pygments.lexers.actionscript', 'ActionScript', ('actionscript', 'as'), ('*.as',), ('application/x-actionscript', 'text/x-actionscript', 'text/actionscript')),
    'AdaLexer': ('pip._vendor.pygments.lexers.ada', 'Ada', ('ada', 'ada95', 'ada2005'), ('*.adb', '*.ads', '*.ada'), ('text/x-ada',)),
    'AdlLexer': ('pip._vendor.pygments.lexers.archetype', 'ADL', ('adl',), ('*.adl', '*.adls', '*.adlf', '*.adlx'), ()),
    'AgdaLexer': ('pip._vendor.pygments.lexers.haskell', 'Agda', ('agda',), ('*.agda',), ('text/x-agda',)),
    'AheuiLexer': ('pip._vendor.pygments.lexers.esoteric', 'Aheui', ('aheui',), ('*.aheui',), ()),
    'AlloyLexer': ('pip._vendor.pygments.lexers.dsls', 'Alloy', ('alloy',), ('*.als',), ('text/x-alloy',)),
    'AmbientTalkLexer': ('pip._vendor.pygments.lexers.ambient', 'AmbientTalk', ('ambienttalk', 'ambienttalk/2', 'at'), ('*.at',), ('text/x-ambienttalk',)),
    'AmplLexer': ('pip._vendor.pygments.lexers.ampl', 'Ampl', ('ampl',), ('*.run',), ()),
    'Angular2HtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML + Angular2', ('html+ng2',), ('*.ng2',), ()),
    'Angular2Lexer': ('pip._vendor.pygments.lexers.templates', 'Angular2', ('ng2',), (), ()),
    'AntlrActionScriptLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With ActionScript Target', ('antlr-actionscript', 'antlr-as'), ('*.G', '*.g'), ()),
    'AntlrCSharpLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With C# Target', ('antlr-csharp', 'antlr-c#'), ('*.G', '*.g'), ()),
    'AntlrCppLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With CPP Target', ('antlr-cpp',), ('*.G', '*.g'), ()),
    'AntlrJavaLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With Java Target', ('antlr-java',), ('*.G', '*.g'), ()),
    'AntlrLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR', ('antlr',), (), ()),
    'AntlrObjectiveCLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With ObjectiveC Target', ('antlr-objc',), ('*.G', '*.g'), ()),
    'AntlrPerlLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With Perl Target', ('antlr-perl',), ('*.G', '*.g'), ()),
    'AntlrPythonLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With Python Target', ('antlr-python',), ('*.G', '*.g'), ()),
    'AntlrRubyLexer': ('pip._vendor.pygments.lexers.parsers', 'ANTLR With Ruby Target', ('antlr-ruby', 'antlr-rb'), ('*.G', '*.g'), ()),
    'ApacheConfLexer': ('pip._vendor.pygments.lexers.configs', 'ApacheConf', ('apacheconf', 'aconf', 'apache'), ('.htaccess', 'apache.conf', 'apache2.conf'), ('text/x-apacheconf',)),
    'AppleScriptLexer': ('pip._vendor.pygments.lexers.scripting', 'AppleScript', ('applescript',), ('*.applescript',), ()),
    'ArduinoLexer': ('pip._vendor.pygments.lexers.c_like', 'Arduino', ('arduino',), ('*.ino',), ('text/x-arduino',)),
    'ArrowLexer': ('pip._vendor.pygments.lexers.arrow', 'Arrow', ('arrow',), ('*.arw',), ()),
    'ArturoLexer': ('pip._vendor.pygments.lexers.arturo', 'Arturo', ('arturo', 'art'), ('*.art',), ()),
    'AscLexer': ('pip._vendor.pygments.lexers.asc', 'ASCII armored', ('asc', 'pem'), ('*.asc', '*.pem', 'id_dsa', 'id_ecdsa', 'id_ecdsa_sk', 'id_ed25519', 'id_ed25519_sk', 'id_rsa'), ('application/pgp-keys', 'application/pgp-encrypted', 'application/pgp-signature')),
    'AspectJLexer': ('pip._vendor.pygments.lexers.jvm', 'AspectJ', ('aspectj',), ('*.aj',), ('text/x-aspectj',)),
    'AsymptoteLexer': ('pip._vendor.pygments.lexers.graphics', 'Asymptote', ('asymptote', 'asy'), ('*.asy',), ('text/x-asymptote',)),
    'AugeasLexer': ('pip._vendor.pygments.lexers.configs', 'Augeas', ('augeas',), ('*.aug',), ()),
    'AutoItLexer': ('pip._vendor.pygments.lexers.automation', 'AutoIt', ('autoit',), ('*.au3',), ('text/x-autoit',)),
    'AutohotkeyLexer': ('pip._vendor.pygments.lexers.automation', 'autohotkey', ('autohotkey', 'ahk'), ('*.ahk', '*.ahkl'), ('text/x-autohotkey',)),
    'AwkLexer': ('pip._vendor.pygments.lexers.textedit', 'Awk', ('awk', 'gawk', 'mawk', 'nawk'), ('*.awk',), ('application/x-awk',)),
    'BBCBasicLexer': ('pip._vendor.pygments.lexers.basic', 'BBC Basic', ('bbcbasic',), ('*.bbc',), ()),
    'BBCodeLexer': ('pip._vendor.pygments.lexers.markup', 'BBCode', ('bbcode',), (), ('text/x-bbcode',)),
    'BCLexer': ('pip._vendor.pygments.lexers.algebra', 'BC', ('bc',), ('*.bc',), ()),
    'BSTLexer': ('pip._vendor.pygments.lexers.bibtex', 'BST', ('bst', 'bst-pybtex'), ('*.bst',), ()),
    'BareLexer': ('pip._vendor.pygments.lexers.bare', 'BARE', ('bare',), ('*.bare',), ()),
    'BaseMakefileLexer': ('pip._vendor.pygments.lexers.make', 'Base Makefile', ('basemake',), (), ()),
    'BashLexer': ('pip._vendor.pygments.lexers.shell', 'Bash', ('bash', 'sh', 'ksh', 'zsh', 'shell'), ('*.sh', '*.ksh', '*.bash', '*.ebuild', '*.eclass', '*.exheres-0', '*.exlib', '*.zsh', '.bashrc', 'bashrc', '.bash_*', 'bash_*', 'zshrc', '.zshrc', '.kshrc', 'kshrc', 'PKGBUILD'), ('application/x-sh', 'application/x-shellscript', 'text/x-shellscript')),
    'BashSessionLexer': ('pip._vendor.pygments.lexers.shell', 'Bash Session', ('console', 'shell-session'), ('*.sh-session', '*.shell-session'), ('application/x-shell-session', 'application/x-sh-session')),
    'BatchLexer': ('pip._vendor.pygments.lexers.shell', 'Batchfile', ('batch', 'bat', 'dosbatch', 'winbatch'), ('*.bat', '*.cmd'), ('application/x-dos-batch',)),
    'BddLexer': ('pip._vendor.pygments.lexers.bdd', 'Bdd', ('bdd',), ('*.feature',), ('text/x-bdd',)),
    'BefungeLexer': ('pip._vendor.pygments.lexers.esoteric', 'Befunge', ('befunge',), ('*.befunge',), ('application/x-befunge',)),
    'BerryLexer': ('pip._vendor.pygments.lexers.berry', 'Berry', ('berry', 'be'), ('*.be',), ('text/x-berry', 'application/x-berry')),
    'BibTeXLexer': ('pip._vendor.pygments.lexers.bibtex', 'BibTeX', ('bibtex', 'bib'), ('*.bib',), ('text/x-bibtex',)),
    'BlitzBasicLexer': ('pip._vendor.pygments.lexers.basic', 'BlitzBasic', ('blitzbasic', 'b3d', 'bplus'), ('*.bb', '*.decls'), ('text/x-bb',)),
    'BlitzMaxLexer': ('pip._vendor.pygments.lexers.basic', 'BlitzMax', ('blitzmax', 'bmax'), ('*.bmx',), ('text/x-bmx',)),
    'BnfLexer': ('pip._vendor.pygments.lexers.grammar_notation', 'BNF', ('bnf',), ('*.bnf',), ('text/x-bnf',)),
    'BoaLexer': ('pip._vendor.pygments.lexers.boa', 'Boa', ('boa',), ('*.boa',), ()),
    'BooLexer': ('pip._vendor.pygments.lexers.dotnet', 'Boo', ('boo',), ('*.boo',), ('text/x-boo',)),
    'BoogieLexer': ('pip._vendor.pygments.lexers.verification', 'Boogie', ('boogie',), ('*.bpl',), ()),
    'BrainfuckLexer': ('pip._vendor.pygments.lexers.esoteric', 'Brainfuck', ('brainfuck', 'bf'), ('*.bf', '*.b'), ('application/x-brainfuck',)),
    'BugsLexer': ('pip._vendor.pygments.lexers.modeling', 'BUGS', ('bugs', 'winbugs', 'openbugs'), ('*.bug',), ()),
    'CAmkESLexer': ('pip._vendor.pygments.lexers.esoteric', 'CAmkES', ('camkes', 'idl4'), ('*.camkes', '*.idl4'), ()),
    'CLexer': ('pip._vendor.pygments.lexers.c_cpp', 'C', ('c',), ('*.c', '*.h', '*.idc', '*.x[bp]m'), ('text/x-chdr', 'text/x-csrc', 'image/x-xbitmap', 'image/x-xpixmap')),
    'CMakeLexer': ('pip._vendor.pygments.lexers.make', 'CMake', ('cmake',), ('*.cmake', 'CMakeLists.txt'), ('text/x-cmake',)),
    'CObjdumpLexer': ('pip._vendor.pygments.lexers.asm', 'c-objdump', ('c-objdump',), ('*.c-objdump',), ('text/x-c-objdump',)),
    'CPSALexer': ('pip._vendor.pygments.lexers.lisp', 'CPSA', ('cpsa',), ('*.cpsa',), ()),
    'CSSUL4Lexer': ('pip._vendor.pygments.lexers.ul4', 'CSS+UL4', ('css+ul4',), ('*.cssul4',), ()),
    'CSharpAspxLexer': ('pip._vendor.pygments.lexers.dotnet', 'aspx-cs', ('aspx-cs',), ('*.aspx', '*.asax', '*.ascx', '*.ashx', '*.asmx', '*.axd'), ()),
    'CSharpLexer': ('pip._vendor.pygments.lexers.dotnet', 'C#', ('csharp', 'c#', 'cs'), ('*.cs',), ('text/x-csharp',)),
    'Ca65Lexer': ('pip._vendor.pygments.lexers.asm', 'ca65 assembler', ('ca65',), ('*.s',), ()),
    'CadlLexer': ('pip._vendor.pygments.lexers.archetype', 'cADL', ('cadl',), ('*.cadl',), ()),
    'CapDLLexer': ('pip._vendor.pygments.lexers.esoteric', 'CapDL', ('capdl',), ('*.cdl',), ()),
    'CapnProtoLexer': ('pip._vendor.pygments.lexers.capnproto', "Cap'n Proto", ('capnp',), ('*.capnp',), ()),
    'CarbonLexer': ('pip._vendor.pygments.lexers.carbon', 'Carbon', ('carbon',), ('*.carbon',), ('text/x-carbon',)),
    'CbmBasicV2Lexer': ('pip._vendor.pygments.lexers.basic', 'CBM BASIC V2', ('cbmbas',), ('*.bas',), ()),
    'CddlLexer': ('pip._vendor.pygments.lexers.cddl', 'CDDL', ('cddl',), ('*.cddl',), ('text/x-cddl',)),
    'CeylonLexer': ('pip._vendor.pygments.lexers.jvm', 'Ceylon', ('ceylon',), ('*.ceylon',), ('text/x-ceylon',)),
    'Cfengine3Lexer': ('pip._vendor.pygments.lexers.configs', 'CFEngine3', ('cfengine3', 'cf3'), ('*.cf',), ()),
    'ChaiscriptLexer': ('pip._vendor.pygments.lexers.scripting', 'ChaiScript', ('chaiscript', 'chai'), ('*.chai',), ('text/x-chaiscript', 'application/x-chaiscript')),
    'ChapelLexer': ('pip._vendor.pygments.lexers.chapel', 'Chapel', ('chapel', 'chpl'), ('*.chpl',), ()),
    'CharmciLexer': ('pip._vendor.pygments.lexers.c_like', 'Charmci', ('charmci',), ('*.ci',), ()),
    'CheetahHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Cheetah', ('html+cheetah', 'html+spitfire', 'htmlcheetah'), (), ('text/html+cheetah', 'text/html+spitfire')),
    'CheetahJavascriptLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Cheetah', ('javascript+cheetah', 'js+cheetah', 'javascript+spitfire', 'js+spitfire'), (), ('application/x-javascript+cheetah', 'text/x-javascript+cheetah', 'text/javascript+cheetah', 'application/x-javascript+spitfire', 'text/x-javascript+spitfire', 'text/javascript+spitfire')),
    'CheetahLexer': ('pip._vendor.pygments.lexers.templates', 'Cheetah', ('cheetah', 'spitfire'), ('*.tmpl', '*.spt'), ('application/x-cheetah', 'application/x-spitfire')),
    'CheetahXmlLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Cheetah', ('xml+cheetah', 'xml+spitfire'), (), ('application/xml+cheetah', 'application/xml+spitfire')),
    'CirruLexer': ('pip._vendor.pygments.lexers.webmisc', 'Cirru', ('cirru',), ('*.cirru',), ('text/x-cirru',)),
    'ClayLexer': ('pip._vendor.pygments.lexers.c_like', 'Clay', ('clay',), ('*.clay',), ('text/x-clay',)),
    'CleanLexer': ('pip._vendor.pygments.lexers.clean', 'Clean', ('clean',), ('*.icl', '*.dcl'), ()),
    'ClojureLexer': ('pip._vendor.pygments.lexers.jvm', 'Clojure', ('clojure', 'clj'), ('*.clj', '*.cljc'), ('text/x-clojure', 'application/x-clojure')),
    'ClojureScriptLexer': ('pip._vendor.pygments.lexers.jvm', 'ClojureScript', ('clojurescript', 'cljs'), ('*.cljs',), ('text/x-clojurescript', 'application/x-clojurescript')),
    'CobolFreeformatLexer': ('pip._vendor.pygments.lexers.business', 'COBOLFree', ('cobolfree',), ('*.cbl', '*.CBL'), ()),
    'CobolLexer': ('pip._vendor.pygments.lexers.business', 'COBOL', ('cobol',), ('*.cob', '*.COB', '*.cpy', '*.CPY'), ('text/x-cobol',)),
    'CoffeeScriptLexer': ('pip._vendor.pygments.lexers.javascript', 'CoffeeScript', ('coffeescript', 'coffee-script', 'coffee'), ('*.coffee',), ('text/coffeescript',)),
    'ColdfusionCFCLexer': ('pip._vendor.pygments.lexers.templates', 'Coldfusion CFC', ('cfc',), ('*.cfc',), ()),
    'ColdfusionHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'Coldfusion HTML', ('cfm',), ('*.cfm', '*.cfml'), ('application/x-coldfusion',)),
    'ColdfusionLexer': ('pip._vendor.pygments.lexers.templates', 'cfstatement', ('cfs',), (), ()),
    'Comal80Lexer': ('pip._vendor.pygments.lexers.comal', 'COMAL-80', ('comal', 'comal80'), ('*.cml', '*.comal'), ()),
    'CommonLispLexer': ('pip._vendor.pygments.lexers.lisp', 'Common Lisp', ('common-lisp', 'cl', 'lisp'), ('*.cl', '*.lisp'), ('text/x-common-lisp',)),
    'ComponentPascalLexer': ('pip._vendor.pygments.lexers.oberon', 'Component Pascal', ('componentpascal', 'cp'), ('*.cp', '*.cps'), ('text/x-component-pascal',)),
    'CoqLexer': ('pip._vendor.pygments.lexers.theorem', 'Coq', ('coq',), ('*.v',), ('text/x-coq',)),
    'CplintLexer': ('pip._vendor.pygments.lexers.cplint', 'cplint', ('cplint',), ('*.ecl', '*.prolog', '*.pro', '*.pl', '*.P', '*.lpad', '*.cpl'), ('text/x-cplint',)),
    'CppLexer': ('pip._vendor.pygments.lexers.c_cpp', 'C++', ('cpp', 'c++'), ('*.cpp', '*.hpp', '*.c++', '*.h++', '*.cc', '*.hh', '*.cxx', '*.hxx', '*.C', '*.H', '*.cp', '*.CPP', '*.tpp'), ('text/x-c++hdr', 'text/x-c++src')),
    'CppObjdumpLexer': ('pip._vendor.pygments.lexers.asm', 'cpp-objdump', ('cpp-objdump', 'c++-objdumb', 'cxx-objdump'), ('*.cpp-objdump', '*.c++-objdump', '*.cxx-objdump'), ('text/x-cpp-objdump',)),
    'CrmshLexer': ('pip._vendor.pygments.lexers.dsls', 'Crmsh', ('crmsh', 'pcmk'), ('*.crmsh', '*.pcmk'), ()),
    'CrocLexer': ('pip._vendor.pygments.lexers.d', 'Croc', ('croc',), ('*.croc',), ('text/x-crocsrc',)),
    'CryptolLexer': ('pip._vendor.pygments.lexers.haskell', 'Cryptol', ('cryptol', 'cry'), ('*.cry',), ('text/x-cryptol',)),
    'CrystalLexer': ('pip._vendor.pygments.lexers.crystal', 'Crystal', ('cr', 'crystal'), ('*.cr',), ('text/x-crystal',)),
    'CsoundDocumentLexer': ('pip._vendor.pygments.lexers.csound', 'Csound Document', ('csound-document', 'csound-csd'), ('*.csd',), ()),
    'CsoundOrchestraLexer': ('pip._vendor.pygments.lexers.csound', 'Csound Orchestra', ('csound', 'csound-orc'), ('*.orc', '*.udo'), ()),
    'CsoundScoreLexer': ('pip._vendor.pygments.lexers.csound', 'Csound Score', ('csound-score', 'csound-sco'), ('*.sco',), ()),
    'CssDjangoLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Django/Jinja', ('css+django', 'css+jinja'), ('*.css.j2', '*.css.jinja2'), ('text/css+django', 'text/css+jinja')),
    'CssErbLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Ruby', ('css+ruby', 'css+erb'), (), ('text/css+ruby',)),
    'CssGenshiLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Genshi Text', ('css+genshitext', 'css+genshi'), (), ('text/css+genshi',)),
    'CssLexer': ('pip._vendor.pygments.lexers.css', 'CSS', ('css',), ('*.css',), ('text/css',)),
    'CssPhpLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+PHP', ('css+php',), (), ('text/css+php',)),
    'CssSmartyLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Smarty', ('css+smarty',), (), ('text/css+smarty',)),
    'CudaLexer': ('pip._vendor.pygments.lexers.c_like', 'CUDA', ('cuda', 'cu'), ('*.cu', '*.cuh'), ('text/x-cuda',)),
    'CypherLexer': ('pip._vendor.pygments.lexers.graph', 'Cypher', ('cypher',), ('*.cyp', '*.cypher'), ()),
    'CythonLexer': ('pip._vendor.pygments.lexers.python', 'Cython', ('cython', 'pyx', 'pyrex'), ('*.pyx', '*.pxd', '*.pxi'), ('text/x-cython', 'application/x-cython')),
    'DLexer': ('pip._vendor.pygments.lexers.d', 'D', ('d',), ('*.d', '*.di'), ('text/x-dsrc',)),
    'DObjdumpLexer': ('pip._vendor.pygments.lexers.asm', 'd-objdump', ('d-objdump',), ('*.d-objdump',), ('text/x-d-objdump',)),
    'DarcsPatchLexer': ('pip._vendor.pygments.lexers.diff', 'Darcs Patch', ('dpatch',), ('*.dpatch', '*.darcspatch'), ()),
    'DartLexer': ('pip._vendor.pygments.lexers.javascript', 'Dart', ('dart',), ('*.dart',), ('text/x-dart',)),
    'Dasm16Lexer': ('pip._vendor.pygments.lexers.asm', 'DASM16', ('dasm16',), ('*.dasm16', '*.dasm'), ('text/x-dasm16',)),
    'DaxLexer': ('pip._vendor.pygments.lexers.dax', 'Dax', ('dax',), ('*.dax',), ()),
    'DebianControlLexer': ('pip._vendor.pygments.lexers.installers', 'Debian Control file', ('debcontrol', 'control'), ('control',), ()),
    'DelphiLexer': ('pip._vendor.pygments.lexers.pascal', 'Delphi', ('delphi', 'pas', 'pascal', 'objectpascal'), ('*.pas', '*.dpr'), ('text/x-pascal',)),
    'DevicetreeLexer': ('pip._vendor.pygments.lexers.devicetree', 'Devicetree', ('devicetree', 'dts'), ('*.dts', '*.dtsi'), ('text/x-c',)),
    'DgLexer': ('pip._vendor.pygments.lexers.python', 'dg', ('dg',), ('*.dg',), ('text/x-dg',)),
    'DiffLexer': ('pip._vendor.pygments.lexers.diff', 'Diff', ('diff', 'udiff'), ('*.diff', '*.patch'), ('text/x-diff', 'text/x-patch')),
    'DjangoLexer': ('pip._vendor.pygments.lexers.templates', 'Django/Jinja', ('django', 'jinja'), (), ('application/x-django-templating', 'application/x-jinja')),
    'DockerLexer': ('pip._vendor.pygments.lexers.configs', 'Docker', ('docker', 'dockerfile'), ('Dockerfile', '*.docker'), ('text/x-dockerfile-config',)),
    'DtdLexer': ('pip._vendor.pygments.lexers.html', 'DTD', ('dtd',), ('*.dtd',), ('application/xml-dtd',)),
    'DuelLexer': ('pip._vendor.pygments.lexers.webmisc', 'Duel', ('duel', 'jbst', 'jsonml+bst'), ('*.duel', '*.jbst'), ('text/x-duel', 'text/x-jbst')),
    'DylanConsoleLexer': ('pip._vendor.pygments.lexers.dylan', 'Dylan session', ('dylan-console', 'dylan-repl'), ('*.dylan-console',), ('text/x-dylan-console',)),
    'DylanLexer': ('pip._vendor.pygments.lexers.dylan', 'Dylan', ('dylan',), ('*.dylan', '*.dyl', '*.intr'), ('text/x-dylan',)),
    'DylanLidLexer': ('pip._vendor.pygments.lexers.dylan', 'DylanLID', ('dylan-lid', 'lid'), ('*.lid', '*.hdp'), ('text/x-dylan-lid',)),
    'ECLLexer': ('pip._vendor.pygments.lexers.ecl', 'ECL', ('ecl',), ('*.ecl',), ('application/x-ecl',)),
    'ECLexer': ('pip._vendor.pygments.lexers.c_like', 'eC', ('ec',), ('*.ec', '*.eh'), ('text/x-echdr', 'text/x-ecsrc')),
    'EarlGreyLexer': ('pip._vendor.pygments.lexers.javascript', 'Earl Grey', ('earl-grey', 'earlgrey', 'eg'), ('*.eg',), ('text/x-earl-grey',)),
    'EasytrieveLexer': ('pip._vendor.pygments.lexers.scripting', 'Easytrieve', ('easytrieve',), ('*.ezt', '*.mac'), ('text/x-easytrieve',)),
    'EbnfLexer': ('pip._vendor.pygments.lexers.parsers', 'EBNF', ('ebnf',), ('*.ebnf',), ('text/x-ebnf',)),
    'EiffelLexer': ('pip._vendor.pygments.lexers.eiffel', 'Eiffel', ('eiffel',), ('*.e',), ('text/x-eiffel',)),
    'ElixirConsoleLexer': ('pip._vendor.pygments.lexers.erlang', 'Elixir iex session', ('iex',), (), ('text/x-elixir-shellsession',)),
    'ElixirLexer': ('pip._vendor.pygments.lexers.erlang', 'Elixir', ('elixir', 'ex', 'exs'), ('*.ex', '*.eex', '*.exs', '*.leex'), ('text/x-elixir',)),
    'ElmLexer': ('pip._vendor.pygments.lexers.elm', 'Elm', ('elm',), ('*.elm',), ('text/x-elm',)),
    'ElpiLexer': ('pip._vendor.pygments.lexers.elpi', 'Elpi', ('elpi',), ('*.elpi',), ('text/x-elpi',)),
    'EmacsLispLexer': ('pip._vendor.pygments.lexers.lisp', 'EmacsLisp', ('emacs-lisp', 'elisp', 'emacs'), ('*.el',), ('text/x-elisp', 'application/x-elisp')),
    'EmailLexer': ('pip._vendor.pygments.lexers.email', 'E-mail', ('email', 'eml'), ('*.eml',), ('message/rfc822',)),
    'ErbLexer': ('pip._vendor.pygments.lexers.templates', 'ERB', ('erb',), (), ('application/x-ruby-templating',)),
    'ErlangLexer': ('pip._vendor.pygments.lexers.erlang', 'Erlang', ('erlang',), ('*.erl', '*.hrl', '*.es', '*.escript'), ('text/x-erlang',)),
    'ErlangShellLexer': ('pip._vendor.pygments.lexers.erlang', 'Erlang erl session', ('erl',), ('*.erl-sh',), ('text/x-erl-shellsession',)),
    'EvoqueHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Evoque', ('html+evoque',), ('*.html',), ('text/html+evoque',)),
    'EvoqueLexer': ('pip._vendor.pygments.lexers.templates', 'Evoque', ('evoque',), ('*.evoque',), ('application/x-evoque',)),
    'EvoqueXmlLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Evoque', ('xml+evoque',), ('*.xml',), ('application/xml+evoque',)),
    'ExeclineLexer': ('pip._vendor.pygments.lexers.shell', 'execline', ('execline',), ('*.exec',), ()),
    'EzhilLexer': ('pip._vendor.pygments.lexers.ezhil', 'Ezhil', ('ezhil',), ('*.n',), ('text/x-ezhil',)),
    'FSharpLexer': ('pip._vendor.pygments.lexers.dotnet', 'F#', ('fsharp', 'f#'), ('*.fs', '*.fsi', '*.fsx'), ('text/x-fsharp',)),
    'FStarLexer': ('pip._vendor.pygments.lexers.ml', 'FStar', ('fstar',), ('*.fst', '*.fsti'), ('text/x-fstar',)),
    'FactorLexer': ('pip._vendor.pygments.lexers.factor', 'Factor', ('factor',), ('*.factor',), ('text/x-factor',)),
    'FancyLexer': ('pip._vendor.pygments.lexers.ruby', 'Fancy', ('fancy', 'fy'), ('*.fy', '*.fancypack'), ('text/x-fancysrc',)),
    'FantomLexer': ('pip._vendor.pygments.lexers.fantom', 'Fantom', ('fan',), ('*.fan',), ('application/x-fantom',)),
    'FelixLexer': ('pip._vendor.pygments.lexers.felix', 'Felix', ('felix', 'flx'), ('*.flx', '*.flxh'), ('text/x-felix',)),
    'FennelLexer': ('pip._vendor.pygments.lexers.lisp', 'Fennel', ('fennel', 'fnl'), ('*.fnl',), ()),
    'FiftLexer': ('pip._vendor.pygments.lexers.fift', 'Fift', ('fift', 'fif'), ('*.fif',), ()),
    'FishShellLexer': ('pip._vendor.pygments.lexers.shell', 'Fish', ('fish', 'fishshell'), ('*.fish', '*.load'), ('application/x-fish',)),
    'FlatlineLexer': ('pip._vendor.pygments.lexers.dsls', 'Flatline', ('flatline',), (), ('text/x-flatline',)),
    'FloScriptLexer': ('pip._vendor.pygments.lexers.floscript', 'FloScript', ('floscript', 'flo'), ('*.flo',), ()),
    'ForthLexer': ('pip._vendor.pygments.lexers.forth', 'Forth', ('forth',), ('*.frt', '*.fs'), ('application/x-forth',)),
    'FortranFixedLexer': ('pip._vendor.pygments.lexers.fortran', 'FortranFixed', ('fortranfixed',), ('*.f', '*.F'), ()),
    'FortranLexer': ('pip._vendor.pygments.lexers.fortran', 'Fortran', ('fortran', 'f90'), ('*.f03', '*.f90', '*.F03', '*.F90'), ('text/x-fortran',)),
    'FoxProLexer': ('pip._vendor.pygments.lexers.foxpro', 'FoxPro', ('foxpro', 'vfp', 'clipper', 'xbase'), ('*.PRG', '*.prg'), ()),
    'FreeFemLexer': ('pip._vendor.pygments.lexers.freefem', 'Freefem', ('freefem',), ('*.edp',), ('text/x-freefem',)),
    'FuncLexer': ('pip._vendor.pygments.lexers.func', 'FunC', ('func', 'fc'), ('*.fc', '*.func'), ()),
    'FutharkLexer': ('pip._vendor.pygments.lexers.futhark', 'Futhark', ('futhark',), ('*.fut',), ('text/x-futhark',)),
    'GAPConsoleLexer': ('pip._vendor.pygments.lexers.algebra', 'GAP session', ('gap-console', 'gap-repl'), ('*.tst',), ()),
    'GAPLexer': ('pip._vendor.pygments.lexers.algebra', 'GAP', ('gap',), ('*.g', '*.gd', '*.gi', '*.gap'), ()),
    'GDScriptLexer': ('pip._vendor.pygments.lexers.gdscript', 'GDScript', ('gdscript', 'gd'), ('*.gd',), ('text/x-gdscript', 'application/x-gdscript')),
    'GLShaderLexer': ('pip._vendor.pygments.lexers.graphics', 'GLSL', ('glsl',), ('*.vert', '*.frag', '*.geo'), ('text/x-glslsrc',)),
    'GSQLLexer': ('pip._vendor.pygments.lexers.gsql', 'GSQL', ('gsql',), ('*.gsql',), ()),
    'GasLexer': ('pip._vendor.pygments.lexers.asm', 'GAS', ('gas', 'asm'), ('*.s', '*.S'), ('text/x-gas',)),
    'GcodeLexer': ('pip._vendor.pygments.lexers.gcodelexer', 'g-code', ('gcode',), ('*.gcode',), ()),
    'GenshiLexer': ('pip._vendor.pygments.lexers.templates', 'Genshi', ('genshi', 'kid', 'xml+genshi', 'xml+kid'), ('*.kid',), ('application/x-genshi', 'application/x-kid')),
    'GenshiTextLexer': ('pip._vendor.pygments.lexers.templates', 'Genshi Text', ('genshitext',), (), ('application/x-genshi-text', 'text/x-genshi')),
    'GettextLexer': ('pip._vendor.pygments.lexers.textfmts', 'Gettext Catalog', ('pot', 'po'), ('*.pot', '*.po'), ('application/x-gettext', 'text/x-gettext', 'text/gettext')),
    'GherkinLexer': ('pip._vendor.pygments.lexers.testing', 'Gherkin', ('gherkin', 'cucumber'), ('*.feature',), ('text/x-gherkin',)),
    'GnuplotLexer': ('pip._vendor.pygments.lexers.graphics', 'Gnuplot', ('gnuplot',), ('*.plot', '*.plt'), ('text/x-gnuplot',)),
    'GoLexer': ('pip._vendor.pygments.lexers.go', 'Go', ('go', 'golang'), ('*.go',), ('text/x-gosrc',)),
    'GoloLexer': ('pip._vendor.pygments.lexers.jvm', 'Golo', ('golo',), ('*.golo',), ()),
    'GoodDataCLLexer': ('pip._vendor.pygments.lexers.business', 'GoodData-CL', ('gooddata-cl',), ('*.gdc',), ('text/x-gooddata-cl',)),
    'GosuLexer': ('pip._vendor.pygments.lexers.jvm', 'Gosu', ('gosu',), ('*.gs', '*.gsx', '*.gsp', '*.vark'), ('text/x-gosu',)),
    'GosuTemplateLexer': ('pip._vendor.pygments.lexers.jvm', 'Gosu Template', ('gst',), ('*.gst',), ('text/x-gosu-template',)),
    'GraphvizLexer': ('pip._vendor.pygments.lexers.graphviz', 'Graphviz', ('graphviz', 'dot'), ('*.gv', '*.dot'), ('text/x-graphviz', 'text/vnd.graphviz')),
    'GroffLexer': ('pip._vendor.pygments.lexers.markup', 'Groff', ('groff', 'nroff', 'man'), ('*.[1-9]', '*.man', '*.1p', '*.3pm'), ('application/x-troff', 'text/troff')),
    'GroovyLexer': ('pip._vendor.pygments.lexers.jvm', 'Groovy', ('groovy',), ('*.groovy', '*.gradle'), ('text/x-groovy',)),
    'HLSLShaderLexer': ('pip._vendor.pygments.lexers.graphics', 'HLSL', ('hlsl',), ('*.hlsl', '*.hlsli'), ('text/x-hlsl',)),
    'HTMLUL4Lexer': ('pip._vendor.pygments.lexers.ul4', 'HTML+UL4', ('html+ul4',), ('*.htmlul4',), ()),
    'HamlLexer': ('pip._vendor.pygments.lexers.html', 'Haml', ('haml',), ('*.haml',), ('text/x-haml',)),
    'HandlebarsHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Handlebars', ('html+handlebars',), ('*.handlebars', '*.hbs'), ('text/html+handlebars', 'text/x-handlebars-template')),
    'HandlebarsLexer': ('pip._vendor.pygments.lexers.templates', 'Handlebars', ('handlebars',), (), ()),
    'HaskellLexer': ('pip._vendor.pygments.lexers.haskell', 'Haskell', ('haskell', 'hs'), ('*.hs',), ('text/x-haskell',)),
    'HaxeLexer': ('pip._vendor.pygments.lexers.haxe', 'Haxe', ('haxe', 'hxsl', 'hx'), ('*.hx', '*.hxsl'), ('text/haxe', 'text/x-haxe', 'text/x-hx')),
    'HexdumpLexer': ('pip._vendor.pygments.lexers.hexdump', 'Hexdump', ('hexdump',), (), ()),
    'HsailLexer': ('pip._vendor.pygments.lexers.asm', 'HSAIL', ('hsail', 'hsa'), ('*.hsail',), ('text/x-hsail',)),
    'HspecLexer': ('pip._vendor.pygments.lexers.haskell', 'Hspec', ('hspec',), ('*Spec.hs',), ()),
    'HtmlDjangoLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Django/Jinja', ('html+django', 'html+jinja', 'htmldjango'), ('*.html.j2', '*.htm.j2', '*.xhtml.j2', '*.html.jinja2', '*.htm.jinja2', '*.xhtml.jinja2'), ('text/html+django', 'text/html+jinja')),
    'HtmlGenshiLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Genshi', ('html+genshi', 'html+kid'), (), ('text/html+genshi',)),
    'HtmlLexer': ('pip._vendor.pygments.lexers.html', 'HTML', ('html',), ('*.html', '*.htm', '*.xhtml', '*.xslt'), ('text/html', 'application/xhtml+xml')),
    'HtmlPhpLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+PHP', ('html+php',), ('*.phtml',), ('application/x-php', 'application/x-httpd-php', 'application/x-httpd-php3', 'application/x-httpd-php4', 'application/x-httpd-php5')),
    'HtmlSmartyLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Smarty', ('html+smarty',), (), ('text/html+smarty',)),
    'HttpLexer': ('pip._vendor.pygments.lexers.textfmts', 'HTTP', ('http',), (), ()),
    'HxmlLexer': ('pip._vendor.pygments.lexers.haxe', 'Hxml', ('haxeml', 'hxml'), ('*.hxml',), ()),
    'HyLexer': ('pip._vendor.pygments.lexers.lisp', 'Hy', ('hylang',), ('*.hy',), ('text/x-hy', 'application/x-hy')),
    'HybrisLexer': ('pip._vendor.pygments.lexers.scripting', 'Hybris', ('hybris', 'hy'), ('*.hy', '*.hyb'), ('text/x-hybris', 'application/x-hybris')),
    'IDLLexer': ('pip._vendor.pygments.lexers.idl', 'IDL', ('idl',), ('*.pro',), ('text/idl',)),
    'IconLexer': ('pip._vendor.pygments.lexers.unicon', 'Icon', ('icon',), ('*.icon', '*.ICON'), ()),
    'IdrisLexer': ('pip._vendor.pygments.lexers.haskell', 'Idris', ('idris', 'idr'), ('*.idr',), ('text/x-idris',)),
    'IgorLexer': ('pip._vendor.pygments.lexers.igor', 'Igor', ('igor', 'igorpro'), ('*.ipf',), ('text/ipf',)),
    'Inform6Lexer': ('pip._vendor.pygments.lexers.int_fiction', 'Inform 6', ('inform6', 'i6'), ('*.inf',), ()),
    'Inform6TemplateLexer': ('pip._vendor.pygments.lexers.int_fiction', 'Inform 6 template', ('i6t',), ('*.i6t',), ()),
    'Inform7Lexer': ('pip._vendor.pygments.lexers.int_fiction', 'Inform 7', ('inform7', 'i7'), ('*.ni', '*.i7x'), ()),
    'IniLexer': ('pip._vendor.pygments.lexers.configs', 'INI', ('ini', 'cfg', 'dosini'), ('*.ini', '*.cfg', '*.inf', '.editorconfig', '*.service', '*.socket', '*.device', '*.mount', '*.automount', '*.swap', '*.target', '*.path', '*.timer', '*.slice', '*.scope'), ('text/x-ini', 'text/inf')),
    'IoLexer': ('pip._vendor.pygments.lexers.iolang', 'Io', ('io',), ('*.io',), ('text/x-iosrc',)),
    'IokeLexer': ('pip._vendor.pygments.lexers.jvm', 'Ioke', ('ioke', 'ik'), ('*.ik',), ('text/x-iokesrc',)),
    'IrcLogsLexer': ('pip._vendor.pygments.lexers.textfmts', 'IRC logs', ('irc',), ('*.weechatlog',), ('text/x-irclog',)),
    'IsabelleLexer': ('pip._vendor.pygments.lexers.theorem', 'Isabelle', ('isabelle',), ('*.thy',), ('text/x-isabelle',)),
    'JLexer': ('pip._vendor.pygments.lexers.j', 'J', ('j',), ('*.ijs',), ('text/x-j',)),
    'JMESPathLexer': ('pip._vendor.pygments.lexers.jmespath', 'JMESPath', ('jmespath', 'jp'), ('*.jp',), ()),
    'JSLTLexer': ('pip._vendor.pygments.lexers.jslt', 'JSLT', ('jslt',), ('*.jslt',), ('text/x-jslt',)),
    'JagsLexer': ('pip._vendor.pygments.lexers.modeling', 'JAGS', ('jags',), ('*.jag', '*.bug'), ()),
    'JasminLexer': ('pip._vendor.pygments.lexers.jvm', 'Jasmin', ('jasmin', 'jasminxt'), ('*.j',), ()),
    'JavaLexer': ('pip._vendor.pygments.lexers.jvm', 'Java', ('java',), ('*.java',), ('text/x-java',)),
    'JavascriptDjangoLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Django/Jinja', ('javascript+django', 'js+django', 'javascript+jinja', 'js+jinja'), ('*.js.j2', '*.js.jinja2'), ('application/x-javascript+django', 'application/x-javascript+jinja', 'text/x-javascript+django', 'text/x-javascript+jinja', 'text/javascript+django', 'text/javascript+jinja')),
    'JavascriptErbLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Ruby', ('javascript+ruby', 'js+ruby', 'javascript+erb', 'js+erb'), (), ('application/x-javascript+ruby', 'text/x-javascript+ruby', 'text/javascript+ruby')),
    'JavascriptGenshiLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Genshi Text', ('js+genshitext', 'js+genshi', 'javascript+genshitext', 'javascript+genshi'), (), ('application/x-javascript+genshi', 'text/x-javascript+genshi', 'text/javascript+genshi')),
    'JavascriptLexer': ('pip._vendor.pygments.lexers.javascript', 'JavaScript', ('javascript', 'js'), ('*.js', '*.jsm', '*.mjs', '*.cjs'), ('application/javascript', 'application/x-javascript', 'text/x-javascript', 'text/javascript')),
    'JavascriptPhpLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+PHP', ('javascript+php', 'js+php'), (), ('application/x-javascript+php', 'text/x-javascript+php', 'text/javascript+php')),
    'JavascriptSmartyLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Smarty', ('javascript+smarty', 'js+smarty'), (), ('application/x-javascript+smarty', 'text/x-javascript+smarty', 'text/javascript+smarty')),
    'JavascriptUL4Lexer': ('pip._vendor.pygments.lexers.ul4', 'Javascript+UL4', ('js+ul4',), ('*.jsul4',), ()),
    'JclLexer': ('pip._vendor.pygments.lexers.scripting', 'JCL', ('jcl',), ('*.jcl',), ('text/x-jcl',)),
    'JsgfLexer': ('pip._vendor.pygments.lexers.grammar_notation', 'JSGF', ('jsgf',), ('*.jsgf',), ('application/jsgf', 'application/x-jsgf', 'text/jsgf')),
    'JsonBareObjectLexer': ('pip._vendor.pygments.lexers.data', 'JSONBareObject', (), (), ()),
    'JsonLdLexer': ('pip._vendor.pygments.lexers.data', 'JSON-LD', ('jsonld', 'json-ld'), ('*.jsonld',), ('application/ld+json',)),
    'JsonLexer': ('pip._vendor.pygments.lexers.data', 'JSON', ('json', 'json-object'), ('*.json', 'Pipfile.lock'), ('application/json', 'application/json-object')),
    'JsonnetLexer': ('pip._vendor.pygments.lexers.jsonnet', 'Jsonnet', ('jsonnet',), ('*.jsonnet', '*.libsonnet'), ()),
    'JspLexer': ('pip._vendor.pygments.lexers.templates', 'Java Server Page', ('jsp',), ('*.jsp',), ('application/x-jsp',)),
    'JuliaConsoleLexer': ('pip._vendor.pygments.lexers.julia', 'Julia console', ('jlcon', 'julia-repl'), (), ()),
    'JuliaLexer': ('pip._vendor.pygments.lexers.julia', 'Julia', ('julia', 'jl'), ('*.jl',), ('text/x-julia', 'application/x-julia')),
    'JuttleLexer': ('pip._vendor.pygments.lexers.javascript', 'Juttle', ('juttle',), ('*.juttle',), ('application/juttle', 'application/x-juttle', 'text/x-juttle', 'text/juttle')),
    'KLexer': ('pip._vendor.pygments.lexers.q', 'K', ('k',), ('*.k',), ()),
    'KalLexer': ('pip._vendor.pygments.lexers.javascript', 'Kal', ('kal',), ('*.kal',), ('text/kal', 'application/kal')),
    'KconfigLexer': ('pip._vendor.pygments.lexers.configs', 'Kconfig', ('kconfig', 'menuconfig', 'linux-config', 'kernel-config'), ('Kconfig*', '*Config.in*', 'external.in*', 'standard-modules.in'), ('text/x-kconfig',)),
    'KernelLogLexer': ('pip._vendor.pygments.lexers.textfmts', 'Kernel log', ('kmsg', 'dmesg'), ('*.kmsg', '*.dmesg'), ()),
    'KokaLexer': ('pip._vendor.pygments.lexers.haskell', 'Koka', ('koka',), ('*.kk', '*.kki'), ('text/x-koka',)),
    'KotlinLexer': ('pip._vendor.pygments.lexers.jvm', 'Kotlin', ('kotlin',), ('*.kt', '*.kts'), ('text/x-kotlin',)),
    'KuinLexer': ('pip._vendor.pygments.lexers.kuin', 'Kuin', ('kuin',), ('*.kn',), ()),
    'LSLLexer': ('pip._vendor.pygments.lexers.scripting', 'LSL', ('lsl',), ('*.lsl',), ('text/x-lsl',)),
    'LassoCssLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Lasso', ('css+lasso',), (), ('text/css+lasso',)),
    'LassoHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Lasso', ('html+lasso',), (), ('text/html+lasso', 'application/x-httpd-lasso', 'application/x-httpd-lasso[89]')),
    'LassoJavascriptLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Lasso', ('javascript+lasso', 'js+lasso'), (), ('application/x-javascript+lasso', 'text/x-javascript+lasso', 'text/javascript+lasso')),
    'LassoLexer': ('pip._vendor.pygments.lexers.javascript', 'Lasso', ('lasso', 'lassoscript'), ('*.lasso', '*.lasso[89]'), ('text/x-lasso',)),
    'LassoXmlLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Lasso', ('xml+lasso',), (), ('application/xml+lasso',)),
    'LeanLexer': ('pip._vendor.pygments.lexers.theorem', 'Lean', ('lean',), ('*.lean',), ('text/x-lean',)),
    'LessCssLexer': ('pip._vendor.pygments.lexers.css', 'LessCss', ('less',), ('*.less',), ('text/x-less-css',)),
    'LighttpdConfLexer': ('pip._vendor.pygments.lexers.configs', 'Lighttpd configuration file', ('lighttpd', 'lighty'), ('lighttpd.conf',), ('text/x-lighttpd-conf',)),
    'LilyPondLexer': ('pip._vendor.pygments.lexers.lilypond', 'LilyPond', ('lilypond',), ('*.ly',), ()),
    'LimboLexer': ('pip._vendor.pygments.lexers.inferno', 'Limbo', ('limbo',), ('*.b',), ('text/limbo',)),
    'LiquidLexer': ('pip._vendor.pygments.lexers.templates', 'liquid', ('liquid',), ('*.liquid',), ()),
    'LiterateAgdaLexer': ('pip._vendor.pygments.lexers.haskell', 'Literate Agda', ('literate-agda', 'lagda'), ('*.lagda',), ('text/x-literate-agda',)),
    'LiterateCryptolLexer': ('pip._vendor.pygments.lexers.haskell', 'Literate Cryptol', ('literate-cryptol', 'lcryptol', 'lcry'), ('*.lcry',), ('text/x-literate-cryptol',)),
    'LiterateHaskellLexer': ('pip._vendor.pygments.lexers.haskell', 'Literate Haskell', ('literate-haskell', 'lhaskell', 'lhs'), ('*.lhs',), ('text/x-literate-haskell',)),
    'LiterateIdrisLexer': ('pip._vendor.pygments.lexers.haskell', 'Literate Idris', ('literate-idris', 'lidris', 'lidr'), ('*.lidr',), ('text/x-literate-idris',)),
    'LiveScriptLexer': ('pip._vendor.pygments.lexers.javascript', 'LiveScript', ('livescript', 'live-script'), ('*.ls',), ('text/livescript',)),
    'LlvmLexer': ('pip._vendor.pygments.lexers.asm', 'LLVM', ('llvm',), ('*.ll',), ('text/x-llvm',)),
    'LlvmMirBodyLexer': ('pip._vendor.pygments.lexers.asm', 'LLVM-MIR Body', ('llvm-mir-body',), (), ()),
    'LlvmMirLexer': ('pip._vendor.pygments.lexers.asm', 'LLVM-MIR', ('llvm-mir',), ('*.mir',), ()),
    'LogosLexer': ('pip._vendor.pygments.lexers.objective', 'Logos', ('logos',), ('*.x', '*.xi', '*.xm', '*.xmi'), ('text/x-logos',)),
    'LogtalkLexer': ('pip._vendor.pygments.lexers.prolog', 'Logtalk', ('logtalk',), ('*.lgt', '*.logtalk'), ('text/x-logtalk',)),
    'LuaLexer': ('pip._vendor.pygments.lexers.scripting', 'Lua', ('lua',), ('*.lua', '*.wlua'), ('text/x-lua', 'application/x-lua')),
    'MCFunctionLexer': ('pip._vendor.pygments.lexers.minecraft', 'MCFunction', ('mcfunction', 'mcf'), ('*.mcfunction',), ('text/mcfunction',)),
    'MCSchemaLexer': ('pip._vendor.pygments.lexers.minecraft', 'MCSchema', ('mcschema',), ('*.mcschema',), ('text/mcschema',)),
    'MIMELexer': ('pip._vendor.pygments.lexers.mime', 'MIME', ('mime',), (), ('multipart/mixed', 'multipart/related', 'multipart/alternative')),
    'MIPSLexer': ('pip._vendor.pygments.lexers.mips', 'MIPS', ('mips',), ('*.mips', '*.MIPS'), ()),
    'MOOCodeLexer': ('pip._vendor.pygments.lexers.scripting', 'MOOCode', ('moocode', 'moo'), ('*.moo',), ('text/x-moocode',)),
    'MSDOSSessionLexer': ('pip._vendor.pygments.lexers.shell', 'MSDOS Session', ('doscon',), (), ()),
    'Macaulay2Lexer': ('pip._vendor.pygments.lexers.macaulay2', 'Macaulay2', ('macaulay2',), ('*.m2',), ()),
    'MakefileLexer': ('pip._vendor.pygments.lexers.make', 'Makefile', ('make', 'makefile', 'mf', 'bsdmake'), ('*.mak', '*.mk', 'Makefile', 'makefile', 'Makefile.*', 'GNUmakefile'), ('text/x-makefile',)),
    'MakoCssLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Mako', ('css+mako',), (), ('text/css+mako',)),
    'MakoHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Mako', ('html+mako',), (), ('text/html+mako',)),
    'MakoJavascriptLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Mako', ('javascript+mako', 'js+mako'), (), ('application/x-javascript+mako', 'text/x-javascript+mako', 'text/javascript+mako')),
    'MakoLexer': ('pip._vendor.pygments.lexers.templates', 'Mako', ('mako',), ('*.mao',), ('application/x-mako',)),
    'MakoXmlLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Mako', ('xml+mako',), (), ('application/xml+mako',)),
    'MaqlLexer': ('pip._vendor.pygments.lexers.business', 'MAQL', ('maql',), ('*.maql',), ('text/x-gooddata-maql', 'application/x-gooddata-maql')),
    'MarkdownLexer': ('pip._vendor.pygments.lexers.markup', 'Markdown', ('markdown', 'md'), ('*.md', '*.markdown'), ('text/x-markdown',)),
    'MaskLexer': ('pip._vendor.pygments.lexers.javascript', 'Mask', ('mask',), ('*.mask',), ('text/x-mask',)),
    'MasonLexer': ('pip._vendor.pygments.lexers.templates', 'Mason', ('mason',), ('*.m', '*.mhtml', '*.mc', '*.mi', 'autohandler', 'dhandler'), ('application/x-mason',)),
    'MathematicaLexer': ('pip._vendor.pygments.lexers.algebra', 'Mathematica', ('mathematica', 'mma', 'nb'), ('*.nb', '*.cdf', '*.nbp', '*.ma'), ('application/mathematica', 'application/vnd.wolfram.mathematica', 'application/vnd.wolfram.mathematica.package', 'application/vnd.wolfram.cdf')),
    'MatlabLexer': ('pip._vendor.pygments.lexers.matlab', 'Matlab', ('matlab',), ('*.m',), ('text/matlab',)),
    'MatlabSessionLexer': ('pip._vendor.pygments.lexers.matlab', 'Matlab session', ('matlabsession',), (), ()),
    'MaximaLexer': ('pip._vendor.pygments.lexers.maxima', 'Maxima', ('maxima', 'macsyma'), ('*.mac', '*.max'), ()),
    'MesonLexer': ('pip._vendor.pygments.lexers.meson', 'Meson', ('meson', 'meson.build'), ('meson.build', 'meson_options.txt'), ('text/x-meson',)),
    'MiniDLexer': ('pip._vendor.pygments.lexers.d', 'MiniD', ('minid',), (), ('text/x-minidsrc',)),
    'MiniScriptLexer': ('pip._vendor.pygments.lexers.scripting', 'MiniScript', ('miniscript', 'ms'), ('*.ms',), ('text/x-minicript', 'application/x-miniscript')),
    'ModelicaLexer': ('pip._vendor.pygments.lexers.modeling', 'Modelica', ('modelica',), ('*.mo',), ('text/x-modelica',)),
    'Modula2Lexer': ('pip._vendor.pygments.lexers.modula2', 'Modula-2', ('modula2', 'm2'), ('*.def', '*.mod'), ('text/x-modula2',)),
    'MoinWikiLexer': ('pip._vendor.pygments.lexers.markup', 'MoinMoin/Trac Wiki markup', ('trac-wiki', 'moin'), (), ('text/x-trac-wiki',)),
    'MonkeyLexer': ('pip._vendor.pygments.lexers.basic', 'Monkey', ('monkey',), ('*.monkey',), ('text/x-monkey',)),
    'MonteLexer': ('pip._vendor.pygments.lexers.monte', 'Monte', ('monte',), ('*.mt',), ()),
    'MoonScriptLexer': ('pip._vendor.pygments.lexers.scripting', 'MoonScript', ('moonscript', 'moon'), ('*.moon',), ('text/x-moonscript', 'application/x-moonscript')),
    'MoselLexer': ('pip._vendor.pygments.lexers.mosel', 'Mosel', ('mosel',), ('*.mos',), ()),
    'MozPreprocCssLexer': ('pip._vendor.pygments.lexers.markup', 'CSS+mozpreproc', ('css+mozpreproc',), ('*.css.in',), ()),
    'MozPreprocHashLexer': ('pip._vendor.pygments.lexers.markup', 'mozhashpreproc', ('mozhashpreproc',), (), ()),
    'MozPreprocJavascriptLexer': ('pip._vendor.pygments.lexers.markup', 'Javascript+mozpreproc', ('javascript+mozpreproc',), ('*.js.in',), ()),
    'MozPreprocPercentLexer': ('pip._vendor.pygments.lexers.markup', 'mozpercentpreproc', ('mozpercentpreproc',), (), ()),
    'MozPreprocXulLexer': ('pip._vendor.pygments.lexers.markup', 'XUL+mozpreproc', ('xul+mozpreproc',), ('*.xul.in',), ()),
    'MqlLexer': ('pip._vendor.pygments.lexers.c_like', 'MQL', ('mql', 'mq4', 'mq5', 'mql4', 'mql5'), ('*.mq4', '*.mq5', '*.mqh'), ('text/x-mql',)),
    'MscgenLexer': ('pip._vendor.pygments.lexers.dsls', 'Mscgen', ('mscgen', 'msc'), ('*.msc',), ()),
    'MuPADLexer': ('pip._vendor.pygments.lexers.algebra', 'MuPAD', ('mupad',), ('*.mu',), ()),
    'MxmlLexer': ('pip._vendor.pygments.lexers.actionscript', 'MXML', ('mxml',), ('*.mxml',), ()),
    'MySqlLexer': ('pip._vendor.pygments.lexers.sql', 'MySQL', ('mysql',), (), ('text/x-mysql',)),
    'MyghtyCssLexer': ('pip._vendor.pygments.lexers.templates', 'CSS+Myghty', ('css+myghty',), (), ('text/css+myghty',)),
    'MyghtyHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Myghty', ('html+myghty',), (), ('text/html+myghty',)),
    'MyghtyJavascriptLexer': ('pip._vendor.pygments.lexers.templates', 'JavaScript+Myghty', ('javascript+myghty', 'js+myghty'), (), ('application/x-javascript+myghty', 'text/x-javascript+myghty', 'text/javascript+mygthy')),
    'MyghtyLexer': ('pip._vendor.pygments.lexers.templates', 'Myghty', ('myghty',), ('*.myt', 'autodelegate'), ('application/x-myghty',)),
    'MyghtyXmlLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Myghty', ('xml+myghty',), (), ('application/xml+myghty',)),
    'NCLLexer': ('pip._vendor.pygments.lexers.ncl', 'NCL', ('ncl',), ('*.ncl',), ('text/ncl',)),
    'NSISLexer': ('pip._vendor.pygments.lexers.installers', 'NSIS', ('nsis', 'nsi', 'nsh'), ('*.nsi', '*.nsh'), ('text/x-nsis',)),
    'NasmLexer': ('pip._vendor.pygments.lexers.asm', 'NASM', ('nasm',), ('*.asm', '*.ASM', '*.nasm'), ('text/x-nasm',)),
    'NasmObjdumpLexer': ('pip._vendor.pygments.lexers.asm', 'objdump-nasm', ('objdump-nasm',), ('*.objdump-intel',), ('text/x-nasm-objdump',)),
    'NemerleLexer': ('pip._vendor.pygments.lexers.dotnet', 'Nemerle', ('nemerle',), ('*.n',), ('text/x-nemerle',)),
    'NesCLexer': ('pip._vendor.pygments.lexers.c_like', 'nesC', ('nesc',), ('*.nc',), ('text/x-nescsrc',)),
    'NestedTextLexer': ('pip._vendor.pygments.lexers.configs', 'NestedText', ('nestedtext', 'nt'), ('*.nt',), ()),
    'NewLispLexer': ('pip._vendor.pygments.lexers.lisp', 'NewLisp', ('newlisp',), ('*.lsp', '*.nl', '*.kif'), ('text/x-newlisp', 'application/x-newlisp')),
    'NewspeakLexer': ('pip._vendor.pygments.lexers.smalltalk', 'Newspeak', ('newspeak',), ('*.ns2',), ('text/x-newspeak',)),
    'NginxConfLexer': ('pip._vendor.pygments.lexers.configs', 'Nginx configuration file', ('nginx',), ('nginx.conf',), ('text/x-nginx-conf',)),
    'NimrodLexer': ('pip._vendor.pygments.lexers.nimrod', 'Nimrod', ('nimrod', 'nim'), ('*.nim', '*.nimrod'), ('text/x-nim',)),
    'NitLexer': ('pip._vendor.pygments.lexers.nit', 'Nit', ('nit',), ('*.nit',), ()),
    'NixLexer': ('pip._vendor.pygments.lexers.nix', 'Nix', ('nixos', 'nix'), ('*.nix',), ('text/x-nix',)),
    'NodeConsoleLexer': ('pip._vendor.pygments.lexers.javascript', 'Node.js REPL console session', ('nodejsrepl',), (), ('text/x-nodejsrepl',)),
    'NotmuchLexer': ('pip._vendor.pygments.lexers.textfmts', 'Notmuch', ('notmuch',), (), ()),
    'NuSMVLexer': ('pip._vendor.pygments.lexers.smv', 'NuSMV', ('nusmv',), ('*.smv',), ()),
    'NumPyLexer': ('pip._vendor.pygments.lexers.python', 'NumPy', ('numpy',), (), ()),
    'ObjdumpLexer': ('pip._vendor.pygments.lexers.asm', 'objdump', ('objdump',), ('*.objdump',), ('text/x-objdump',)),
    'ObjectiveCLexer': ('pip._vendor.pygments.lexers.objective', 'Objective-C', ('objective-c', 'objectivec', 'obj-c', 'objc'), ('*.m', '*.h'), ('text/x-objective-c',)),
    'ObjectiveCppLexer': ('pip._vendor.pygments.lexers.objective', 'Objective-C++', ('objective-c++', 'objectivec++', 'obj-c++', 'objc++'), ('*.mm', '*.hh'), ('text/x-objective-c++',)),
    'ObjectiveJLexer': ('pip._vendor.pygments.lexers.javascript', 'Objective-J', ('objective-j', 'objectivej', 'obj-j', 'objj'), ('*.j',), ('text/x-objective-j',)),
    'OcamlLexer': ('pip._vendor.pygments.lexers.ml', 'OCaml', ('ocaml',), ('*.ml', '*.mli', '*.mll', '*.mly'), ('text/x-ocaml',)),
    'OctaveLexer': ('pip._vendor.pygments.lexers.matlab', 'Octave', ('octave',), ('*.m',), ('text/octave',)),
    'OdinLexer': ('pip._vendor.pygments.lexers.archetype', 'ODIN', ('odin',), ('*.odin',), ('text/odin',)),
    'OmgIdlLexer': ('pip._vendor.pygments.lexers.c_like', 'OMG Interface Definition Language', ('omg-idl',), ('*.idl', '*.pidl'), ()),
    'OocLexer': ('pip._vendor.pygments.lexers.ooc', 'Ooc', ('ooc',), ('*.ooc',), ('text/x-ooc',)),
    'OpaLexer': ('pip._vendor.pygments.lexers.ml', 'Opa', ('opa',), ('*.opa',), ('text/x-opa',)),
    'OpenEdgeLexer': ('pip._vendor.pygments.lexers.business', 'OpenEdge ABL', ('openedge', 'abl', 'progress'), ('*.p', '*.cls'), ('text/x-openedge', 'application/x-openedge')),
    'OutputLexer': ('pip._vendor.pygments.lexers.special', 'Text output', ('output',), (), ()),
    'PacmanConfLexer': ('pip._vendor.pygments.lexers.configs', 'PacmanConf', ('pacmanconf',), ('pacman.conf',), ()),
    'PanLexer': ('pip._vendor.pygments.lexers.dsls', 'Pan', ('pan',), ('*.pan',), ()),
    'ParaSailLexer': ('pip._vendor.pygments.lexers.parasail', 'ParaSail', ('parasail',), ('*.psi', '*.psl'), ('text/x-parasail',)),
    'PawnLexer': ('pip._vendor.pygments.lexers.pawn', 'Pawn', ('pawn',), ('*.p', '*.pwn', '*.inc'), ('text/x-pawn',)),
    'PegLexer': ('pip._vendor.pygments.lexers.grammar_notation', 'PEG', ('peg',), ('*.peg',), ('text/x-peg',)),
    'Perl6Lexer': ('pip._vendor.pygments.lexers.perl', 'Perl6', ('perl6', 'pl6', 'raku'), ('*.pl', '*.pm', '*.nqp', '*.p6', '*.6pl', '*.p6l', '*.pl6', '*.6pm', '*.p6m', '*.pm6', '*.t', '*.raku', '*.rakumod', '*.rakutest', '*.rakudoc'), ('text/x-perl6', 'application/x-perl6')),
    'PerlLexer': ('pip._vendor.pygments.lexers.perl', 'Perl', ('perl', 'pl'), ('*.pl', '*.pm', '*.t', '*.perl'), ('text/x-perl', 'application/x-perl')),
    'PhixLexer': ('pip._vendor.pygments.lexers.phix', 'Phix', ('phix',), ('*.exw',), ('text/x-phix',)),
    'PhpLexer': ('pip._vendor.pygments.lexers.php', 'PHP', ('php', 'php3', 'php4', 'php5'), ('*.php', '*.php[345]', '*.inc'), ('text/x-php',)),
    'PigLexer': ('pip._vendor.pygments.lexers.jvm', 'Pig', ('pig',), ('*.pig',), ('text/x-pig',)),
    'PikeLexer': ('pip._vendor.pygments.lexers.c_like', 'Pike', ('pike',), ('*.pike', '*.pmod'), ('text/x-pike',)),
    'PkgConfigLexer': ('pip._vendor.pygments.lexers.configs', 'PkgConfig', ('pkgconfig',), ('*.pc',), ()),
    'PlPgsqlLexer': ('pip._vendor.pygments.lexers.sql', 'PL/pgSQL', ('plpgsql',), (), ('text/x-plpgsql',)),
    'PointlessLexer': ('pip._vendor.pygments.lexers.pointless', 'Pointless', ('pointless',), ('*.ptls',), ()),
    'PonyLexer': ('pip._vendor.pygments.lexers.pony', 'Pony', ('pony',), ('*.pony',), ()),
    'PortugolLexer': ('pip._vendor.pygments.lexers.pascal', 'Portugol', ('portugol',), ('*.alg', '*.portugol'), ()),
    'PostScriptLexer': ('pip._vendor.pygments.lexers.graphics', 'PostScript', ('postscript', 'postscr'), ('*.ps', '*.eps'), ('application/postscript',)),
    'PostgresConsoleLexer': ('pip._vendor.pygments.lexers.sql', 'PostgreSQL console (psql)', ('psql', 'postgresql-console', 'postgres-console'), (), ('text/x-postgresql-psql',)),
    'PostgresExplainLexer': ('pip._vendor.pygments.lexers.sql', 'PostgreSQL EXPLAIN dialect', ('postgres-explain',), ('*.explain',), ('text/x-postgresql-explain',)),
    'PostgresLexer': ('pip._vendor.pygments.lexers.sql', 'PostgreSQL SQL dialect', ('postgresql', 'postgres'), (), ('text/x-postgresql',)),
    'PovrayLexer': ('pip._vendor.pygments.lexers.graphics', 'POVRay', ('pov',), ('*.pov', '*.inc'), ('text/x-povray',)),
    'PowerShellLexer': ('pip._vendor.pygments.lexers.shell', 'PowerShell', ('powershell', 'pwsh', 'posh', 'ps1', 'psm1'), ('*.ps1', '*.psm1'), ('text/x-powershell',)),
    'PowerShellSessionLexer': ('pip._vendor.pygments.lexers.shell', 'PowerShell Session', ('pwsh-session', 'ps1con'), (), ()),
    'PraatLexer': ('pip._vendor.pygments.lexers.praat', 'Praat', ('praat',), ('*.praat', '*.proc', '*.psc'), ()),
    'ProcfileLexer': ('pip._vendor.pygments.lexers.procfile', 'Procfile', ('procfile',), ('Procfile',), ()),
    'PrologLexer': ('pip._vendor.pygments.lexers.prolog', 'Prolog', ('prolog',), ('*.ecl', '*.prolog', '*.pro', '*.pl'), ('text/x-prolog',)),
    'PromQLLexer': ('pip._vendor.pygments.lexers.promql', 'PromQL', ('promql',), ('*.promql',), ()),
    'PropertiesLexer': ('pip._vendor.pygments.lexers.configs', 'Properties', ('properties', 'jproperties'), ('*.properties',), ('text/x-java-properties',)),
    'ProtoBufLexer': ('pip._vendor.pygments.lexers.dsls', 'Protocol Buffer', ('protobuf', 'proto'), ('*.proto',), ()),
    'PsyshConsoleLexer': ('pip._vendor.pygments.lexers.php', 'PsySH console session for PHP', ('psysh',), (), ()),
    'PugLexer': ('pip._vendor.pygments.lexers.html', 'Pug', ('pug', 'jade'), ('*.pug', '*.jade'), ('text/x-pug', 'text/x-jade')),
    'PuppetLexer': ('pip._vendor.pygments.lexers.dsls', 'Puppet', ('puppet',), ('*.pp',), ()),
    'PyPyLogLexer': ('pip._vendor.pygments.lexers.console', 'PyPy Log', ('pypylog', 'pypy'), ('*.pypylog',), ('application/x-pypylog',)),
    'Python2Lexer': ('pip._vendor.pygments.lexers.python', 'Python 2.x', ('python2', 'py2'), (), ('text/x-python2', 'application/x-python2')),
    'Python2TracebackLexer': ('pip._vendor.pygments.lexers.python', 'Python 2.x Traceback', ('py2tb',), ('*.py2tb',), ('text/x-python2-traceback',)),
    'PythonConsoleLexer': ('pip._vendor.pygments.lexers.python', 'Python console session', ('pycon',), (), ('text/x-python-doctest',)),
    'PythonLexer': ('pip._vendor.pygments.lexers.python', 'Python', ('python', 'py', 'sage', 'python3', 'py3'), ('*.py', '*.pyw', '*.pyi', '*.jy', '*.sage', '*.sc', 'SConstruct', 'SConscript', '*.bzl', 'BUCK', 'BUILD', 'BUILD.bazel', 'WORKSPACE', '*.tac'), ('text/x-python', 'application/x-python', 'text/x-python3', 'application/x-python3')),
    'PythonTracebackLexer': ('pip._vendor.pygments.lexers.python', 'Python Traceback', ('pytb', 'py3tb'), ('*.pytb', '*.py3tb'), ('text/x-python-traceback', 'text/x-python3-traceback')),
    'PythonUL4Lexer': ('pip._vendor.pygments.lexers.ul4', 'Python+UL4', ('py+ul4',), ('*.pyul4',), ()),
    'QBasicLexer': ('pip._vendor.pygments.lexers.basic', 'QBasic', ('qbasic', 'basic'), ('*.BAS', '*.bas'), ('text/basic',)),
    'QLexer': ('pip._vendor.pygments.lexers.q', 'Q', ('q',), ('*.q',), ()),
    'QVToLexer': ('pip._vendor.pygments.lexers.qvt', 'QVTO', ('qvto', 'qvt'), ('*.qvto',), ()),
    'QlikLexer': ('pip._vendor.pygments.lexers.qlik', 'Qlik', ('qlik', 'qlikview', 'qliksense', 'qlikscript'), ('*.qvs', '*.qvw'), ()),
    'QmlLexer': ('pip._vendor.pygments.lexers.webmisc', 'QML', ('qml', 'qbs'), ('*.qml', '*.qbs'), ('application/x-qml', 'application/x-qt.qbs+qml')),
    'RConsoleLexer': ('pip._vendor.pygments.lexers.r', 'RConsole', ('rconsole', 'rout'), ('*.Rout',), ()),
    'RNCCompactLexer': ('pip._vendor.pygments.lexers.rnc', 'Relax-NG Compact', ('rng-compact', 'rnc'), ('*.rnc',), ()),
    'RPMSpecLexer': ('pip._vendor.pygments.lexers.installers', 'RPMSpec', ('spec',), ('*.spec',), ('text/x-rpm-spec',)),
    'RacketLexer': ('pip._vendor.pygments.lexers.lisp', 'Racket', ('racket', 'rkt'), ('*.rkt', '*.rktd', '*.rktl'), ('text/x-racket', 'application/x-racket')),
    'RagelCLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel in C Host', ('ragel-c',), ('*.rl',), ()),
    'RagelCppLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel in CPP Host', ('ragel-cpp',), ('*.rl',), ()),
    'RagelDLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel in D Host', ('ragel-d',), ('*.rl',), ()),
    'RagelEmbeddedLexer': ('pip._vendor.pygments.lexers.parsers', 'Embedded Ragel', ('ragel-em',), ('*.rl',), ()),
    'RagelJavaLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel in Java Host', ('ragel-java',), ('*.rl',), ()),
    'RagelLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel', ('ragel',), (), ()),
    'RagelObjectiveCLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel in Objective C Host', ('ragel-objc',), ('*.rl',), ()),
    'RagelRubyLexer': ('pip._vendor.pygments.lexers.parsers', 'Ragel in Ruby Host', ('ragel-ruby', 'ragel-rb'), ('*.rl',), ()),
    'RawTokenLexer': ('pip._vendor.pygments.lexers.special', 'Raw token data', (), (), ('application/x-pygments-tokens',)),
    'RdLexer': ('pip._vendor.pygments.lexers.r', 'Rd', ('rd',), ('*.Rd',), ('text/x-r-doc',)),
    'ReasonLexer': ('pip._vendor.pygments.lexers.ml', 'ReasonML', ('reasonml', 'reason'), ('*.re', '*.rei'), ('text/x-reasonml',)),
    'RebolLexer': ('pip._vendor.pygments.lexers.rebol', 'REBOL', ('rebol',), ('*.r', '*.r3', '*.reb'), ('text/x-rebol',)),
    'RedLexer': ('pip._vendor.pygments.lexers.rebol', 'Red', ('red', 'red/system'), ('*.red', '*.reds'), ('text/x-red', 'text/x-red-system')),
    'RedcodeLexer': ('pip._vendor.pygments.lexers.esoteric', 'Redcode', ('redcode',), ('*.cw',), ()),
    'RegeditLexer': ('pip._vendor.pygments.lexers.configs', 'reg', ('registry',), ('*.reg',), ('text/x-windows-registry',)),
    'ResourceLexer': ('pip._vendor.pygments.lexers.resource', 'ResourceBundle', ('resourcebundle', 'resource'), (), ()),
    'RexxLexer': ('pip._vendor.pygments.lexers.scripting', 'Rexx', ('rexx', 'arexx'), ('*.rexx', '*.rex', '*.rx', '*.arexx'), ('text/x-rexx',)),
    'RhtmlLexer': ('pip._vendor.pygments.lexers.templates', 'RHTML', ('rhtml', 'html+erb', 'html+ruby'), ('*.rhtml',), ('text/html+ruby',)),
    'RideLexer': ('pip._vendor.pygments.lexers.ride', 'Ride', ('ride',), ('*.ride',), ('text/x-ride',)),
    'RitaLexer': ('pip._vendor.pygments.lexers.rita', 'Rita', ('rita',), ('*.rita',), ('text/rita',)),
    'RoboconfGraphLexer': ('pip._vendor.pygments.lexers.roboconf', 'Roboconf Graph', ('roboconf-graph',), ('*.graph',), ()),
    'RoboconfInstancesLexer': ('pip._vendor.pygments.lexers.roboconf', 'Roboconf Instances', ('roboconf-instances',), ('*.instances',), ()),
    'RobotFrameworkLexer': ('pip._vendor.pygments.lexers.robotframework', 'RobotFramework', ('robotframework',), ('*.robot', '*.resource'), ('text/x-robotframework',)),
    'RqlLexer': ('pip._vendor.pygments.lexers.sql', 'RQL', ('rql',), ('*.rql',), ('text/x-rql',)),
    'RslLexer': ('pip._vendor.pygments.lexers.dsls', 'RSL', ('rsl',), ('*.rsl',), ('text/rsl',)),
    'RstLexer': ('pip._vendor.pygments.lexers.markup', 'reStructuredText', ('restructuredtext', 'rst', 'rest'), ('*.rst', '*.rest'), ('text/x-rst', 'text/prs.fallenstein.rst')),
    'RtsLexer': ('pip._vendor.pygments.lexers.trafficscript', 'TrafficScript', ('trafficscript', 'rts'), ('*.rts',), ()),
    'RubyConsoleLexer': ('pip._vendor.pygments.lexers.ruby', 'Ruby irb session', ('rbcon', 'irb'), (), ('text/x-ruby-shellsession',)),
    'RubyLexer': ('pip._vendor.pygments.lexers.ruby', 'Ruby', ('ruby', 'rb', 'duby'), ('*.rb', '*.rbw', 'Rakefile', '*.rake', '*.gemspec', '*.rbx', '*.duby', 'Gemfile', 'Vagrantfile'), ('text/x-ruby', 'application/x-ruby')),
    'RustLexer': ('pip._vendor.pygments.lexers.rust', 'Rust', ('rust', 'rs'), ('*.rs', '*.rs.in'), ('text/rust', 'text/x-rust')),
    'SASLexer': ('pip._vendor.pygments.lexers.sas', 'SAS', ('sas',), ('*.SAS', '*.sas'), ('text/x-sas', 'text/sas', 'application/x-sas')),
    'SLexer': ('pip._vendor.pygments.lexers.r', 'S', ('splus', 's', 'r'), ('*.S', '*.R', '.Rhistory', '.Rprofile', '.Renviron'), ('text/S-plus', 'text/S', 'text/x-r-source', 'text/x-r', 'text/x-R', 'text/x-r-history', 'text/x-r-profile')),
    'SMLLexer': ('pip._vendor.pygments.lexers.ml', 'Standard ML', ('sml',), ('*.sml', '*.sig', '*.fun'), ('text/x-standardml', 'application/x-standardml')),
    'SNBTLexer': ('pip._vendor.pygments.lexers.minecraft', 'SNBT', ('snbt',), ('*.snbt',), ('text/snbt',)),
    'SarlLexer': ('pip._vendor.pygments.lexers.jvm', 'SARL', ('sarl',), ('*.sarl',), ('text/x-sarl',)),
    'SassLexer': ('pip._vendor.pygments.lexers.css', 'Sass', ('sass',), ('*.sass',), ('text/x-sass',)),
    'SaviLexer': ('pip._vendor.pygments.lexers.savi', 'Savi', ('savi',), ('*.savi',), ()),
    'ScalaLexer': ('pip._vendor.pygments.lexers.jvm', 'Scala', ('scala',), ('*.scala',), ('text/x-scala',)),
    'ScamlLexer': ('pip._vendor.pygments.lexers.html', 'Scaml', ('scaml',), ('*.scaml',), ('text/x-scaml',)),
    'ScdocLexer': ('pip._vendor.pygments.lexers.scdoc', 'scdoc', ('scdoc', 'scd'), ('*.scd', '*.scdoc'), ()),
    'SchemeLexer': ('pip._vendor.pygments.lexers.lisp', 'Scheme', ('scheme', 'scm'), ('*.scm', '*.ss'), ('text/x-scheme', 'application/x-scheme')),
    'ScilabLexer': ('pip._vendor.pygments.lexers.matlab', 'Scilab', ('scilab',), ('*.sci', '*.sce', '*.tst'), ('text/scilab',)),
    'ScssLexer': ('pip._vendor.pygments.lexers.css', 'SCSS', ('scss',), ('*.scss',), ('text/x-scss',)),
    'SedLexer': ('pip._vendor.pygments.lexers.textedit', 'Sed', ('sed', 'gsed', 'ssed'), ('*.sed', '*.[gs]sed'), ('text/x-sed',)),
    'ShExCLexer': ('pip._vendor.pygments.lexers.rdf', 'ShExC', ('shexc', 'shex'), ('*.shex',), ('text/shex',)),
    'ShenLexer': ('pip._vendor.pygments.lexers.lisp', 'Shen', ('shen',), ('*.shen',), ('text/x-shen', 'application/x-shen')),
    'SieveLexer': ('pip._vendor.pygments.lexers.sieve', 'Sieve', ('sieve',), ('*.siv', '*.sieve'), ()),
    'SilverLexer': ('pip._vendor.pygments.lexers.verification', 'Silver', ('silver',), ('*.sil', '*.vpr'), ()),
    'SingularityLexer': ('pip._vendor.pygments.lexers.configs', 'Singularity', ('singularity',), ('*.def', 'Singularity'), ()),
    'SlashLexer': ('pip._vendor.pygments.lexers.slash', 'Slash', ('slash',), ('*.sla',), ()),
    'SlimLexer': ('pip._vendor.pygments.lexers.webmisc', 'Slim', ('slim',), ('*.slim',), ('text/x-slim',)),
    'SlurmBashLexer': ('pip._vendor.pygments.lexers.shell', 'Slurm', ('slurm', 'sbatch'), ('*.sl',), ()),
    'SmaliLexer': ('pip._vendor.pygments.lexers.dalvik', 'Smali', ('smali',), ('*.smali',), ('text/smali',)),
    'SmalltalkLexer': ('pip._vendor.pygments.lexers.smalltalk', 'Smalltalk', ('smalltalk', 'squeak', 'st'), ('*.st',), ('text/x-smalltalk',)),
    'SmartGameFormatLexer': ('pip._vendor.pygments.lexers.sgf', 'SmartGameFormat', ('sgf',), ('*.sgf',), ()),
    'SmartyLexer': ('pip._vendor.pygments.lexers.templates', 'Smarty', ('smarty',), ('*.tpl',), ('application/x-smarty',)),
    'SmithyLexer': ('pip._vendor.pygments.lexers.smithy', 'Smithy', ('smithy',), ('*.smithy',), ()),
    'SnobolLexer': ('pip._vendor.pygments.lexers.snobol', 'Snobol', ('snobol',), ('*.snobol',), ('text/x-snobol',)),
    'SnowballLexer': ('pip._vendor.pygments.lexers.dsls', 'Snowball', ('snowball',), ('*.sbl',), ()),
    'SolidityLexer': ('pip._vendor.pygments.lexers.solidity', 'Solidity', ('solidity',), ('*.sol',), ()),
    'SophiaLexer': ('pip._vendor.pygments.lexers.sophia', 'Sophia', ('sophia',), ('*.aes',), ()),
    'SourcePawnLexer': ('pip._vendor.pygments.lexers.pawn', 'SourcePawn', ('sp',), ('*.sp',), ('text/x-sourcepawn',)),
    'SourcesListLexer': ('pip._vendor.pygments.lexers.installers', 'Debian Sourcelist', ('debsources', 'sourceslist', 'sources.list'), ('sources.list',), ()),
    'SparqlLexer': ('pip._vendor.pygments.lexers.rdf', 'SPARQL', ('sparql',), ('*.rq', '*.sparql'), ('application/sparql-query',)),
    'SpiceLexer': ('pip._vendor.pygments.lexers.spice', 'Spice', ('spice', 'spicelang'), ('*.spice',), ('text/x-spice',)),
    'SqlJinjaLexer': ('pip._vendor.pygments.lexers.templates', 'SQL+Jinja', ('sql+jinja',), ('*.sql', '*.sql.j2', '*.sql.jinja2'), ()),
    'SqlLexer': ('pip._vendor.pygments.lexers.sql', 'SQL', ('sql',), ('*.sql',), ('text/x-sql',)),
    'SqliteConsoleLexer': ('pip._vendor.pygments.lexers.sql', 'sqlite3con', ('sqlite3',), ('*.sqlite3-console',), ('text/x-sqlite3-console',)),
    'SquidConfLexer': ('pip._vendor.pygments.lexers.configs', 'SquidConf', ('squidconf', 'squid.conf', 'squid'), ('squid.conf',), ('text/x-squidconf',)),
    'SrcinfoLexer': ('pip._vendor.pygments.lexers.srcinfo', 'Srcinfo', ('srcinfo',), ('.SRCINFO',), ()),
    'SspLexer': ('pip._vendor.pygments.lexers.templates', 'Scalate Server Page', ('ssp',), ('*.ssp',), ('application/x-ssp',)),
    'StanLexer': ('pip._vendor.pygments.lexers.modeling', 'Stan', ('stan',), ('*.stan',), ()),
    'StataLexer': ('pip._vendor.pygments.lexers.stata', 'Stata', ('stata', 'do'), ('*.do', '*.ado'), ('text/x-stata', 'text/stata', 'application/x-stata')),
    'SuperColliderLexer': ('pip._vendor.pygments.lexers.supercollider', 'SuperCollider', ('supercollider', 'sc'), ('*.sc', '*.scd'), ('application/supercollider', 'text/supercollider')),
    'SwiftLexer': ('pip._vendor.pygments.lexers.objective', 'Swift', ('swift',), ('*.swift',), ('text/x-swift',)),
    'SwigLexer': ('pip._vendor.pygments.lexers.c_like', 'SWIG', ('swig',), ('*.swg', '*.i'), ('text/swig',)),
    'SystemVerilogLexer': ('pip._vendor.pygments.lexers.hdl', 'systemverilog', ('systemverilog', 'sv'), ('*.sv', '*.svh'), ('text/x-systemverilog',)),
    'TAPLexer': ('pip._vendor.pygments.lexers.testing', 'TAP', ('tap',), ('*.tap',), ()),
    'TNTLexer': ('pip._vendor.pygments.lexers.tnt', 'Typographic Number Theory', ('tnt',), ('*.tnt',), ()),
    'TOMLLexer': ('pip._vendor.pygments.lexers.configs', 'TOML', ('toml',), ('*.toml', 'Pipfile', 'poetry.lock'), ()),
    'Tads3Lexer': ('pip._vendor.pygments.lexers.int_fiction', 'TADS 3', ('tads3',), ('*.t',), ()),
    'TalLexer': ('pip._vendor.pygments.lexers.tal', 'Tal', ('tal', 'uxntal'), ('*.tal',), ('text/x-uxntal',)),
    'TasmLexer': ('pip._vendor.pygments.lexers.asm', 'TASM', ('tasm',), ('*.asm', '*.ASM', '*.tasm'), ('text/x-tasm',)),
    'TclLexer': ('pip._vendor.pygments.lexers.tcl', 'Tcl', ('tcl',), ('*.tcl', '*.rvt'), ('text/x-tcl', 'text/x-script.tcl', 'application/x-tcl')),
    'TcshLexer': ('pip._vendor.pygments.lexers.shell', 'Tcsh', ('tcsh', 'csh'), ('*.tcsh', '*.csh'), ('application/x-csh',)),
    'TcshSessionLexer': ('pip._vendor.pygments.lexers.shell', 'Tcsh Session', ('tcshcon',), (), ()),
    'TeaTemplateLexer': ('pip._vendor.pygments.lexers.templates', 'Tea', ('tea',), ('*.tea',), ('text/x-tea',)),
    'TealLexer': ('pip._vendor.pygments.lexers.teal', 'teal', ('teal',), ('*.teal',), ()),
    'TeraTermLexer': ('pip._vendor.pygments.lexers.teraterm', 'Tera Term macro', ('teratermmacro', 'teraterm', 'ttl'), ('*.ttl',), ('text/x-teratermmacro',)),
    'TermcapLexer': ('pip._vendor.pygments.lexers.configs', 'Termcap', ('termcap',), ('termcap', 'termcap.src'), ()),
    'TerminfoLexer': ('pip._vendor.pygments.lexers.configs', 'Terminfo', ('terminfo',), ('terminfo', 'terminfo.src'), ()),
    'TerraformLexer': ('pip._vendor.pygments.lexers.configs', 'Terraform', ('terraform', 'tf', 'hcl'), ('*.tf', '*.hcl'), ('application/x-tf', 'application/x-terraform')),
    'TexLexer': ('pip._vendor.pygments.lexers.markup', 'TeX', ('tex', 'latex'), ('*.tex', '*.aux', '*.toc'), ('text/x-tex', 'text/x-latex')),
    'TextLexer': ('pip._vendor.pygments.lexers.special', 'Text only', ('text',), ('*.txt',), ('text/plain',)),
    'ThingsDBLexer': ('pip._vendor.pygments.lexers.thingsdb', 'ThingsDB', ('ti', 'thingsdb'), ('*.ti',), ()),
    'ThriftLexer': ('pip._vendor.pygments.lexers.dsls', 'Thrift', ('thrift',), ('*.thrift',), ('application/x-thrift',)),
    'TiddlyWiki5Lexer': ('pip._vendor.pygments.lexers.markup', 'tiddler', ('tid',), ('*.tid',), ('text/vnd.tiddlywiki',)),
    'TlbLexer': ('pip._vendor.pygments.lexers.tlb', 'Tl-b', ('tlb',), ('*.tlb',), ()),
    'TodotxtLexer': ('pip._vendor.pygments.lexers.textfmts', 'Todotxt', ('todotxt',), ('todo.txt', '*.todotxt'), ('text/x-todo',)),
    'TransactSqlLexer': ('pip._vendor.pygments.lexers.sql', 'Transact-SQL', ('tsql', 't-sql'), ('*.sql',), ('text/x-tsql',)),
    'TreetopLexer': ('pip._vendor.pygments.lexers.parsers', 'Treetop', ('treetop',), ('*.treetop', '*.tt'), ()),
    'TurtleLexer': ('pip._vendor.pygments.lexers.rdf', 'Turtle', ('turtle',), ('*.ttl',), ('text/turtle', 'application/x-turtle')),
    'TwigHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Twig', ('html+twig',), ('*.twig',), ('text/html+twig',)),
    'TwigLexer': ('pip._vendor.pygments.lexers.templates', 'Twig', ('twig',), (), ('application/x-twig',)),
    'TypeScriptLexer': ('pip._vendor.pygments.lexers.javascript', 'TypeScript', ('typescript', 'ts'), ('*.ts',), ('application/x-typescript', 'text/x-typescript')),
    'TypoScriptCssDataLexer': ('pip._vendor.pygments.lexers.typoscript', 'TypoScriptCssData', ('typoscriptcssdata',), (), ()),
    'TypoScriptHtmlDataLexer': ('pip._vendor.pygments.lexers.typoscript', 'TypoScriptHtmlData', ('typoscripthtmldata',), (), ()),
    'TypoScriptLexer': ('pip._vendor.pygments.lexers.typoscript', 'TypoScript', ('typoscript',), ('*.typoscript',), ('text/x-typoscript',)),
    'UL4Lexer': ('pip._vendor.pygments.lexers.ul4', 'UL4', ('ul4',), ('*.ul4',), ()),
    'UcodeLexer': ('pip._vendor.pygments.lexers.unicon', 'ucode', ('ucode',), ('*.u', '*.u1', '*.u2'), ()),
    'UniconLexer': ('pip._vendor.pygments.lexers.unicon', 'Unicon', ('unicon',), ('*.icn',), ('text/unicon',)),
    'UnixConfigLexer': ('pip._vendor.pygments.lexers.configs', 'Unix/Linux config files', ('unixconfig', 'linuxconfig'), (), ()),
    'UrbiscriptLexer': ('pip._vendor.pygments.lexers.urbi', 'UrbiScript', ('urbiscript',), ('*.u',), ('application/x-urbiscript',)),
    'UsdLexer': ('pip._vendor.pygments.lexers.usd', 'USD', ('usd', 'usda'), ('*.usd', '*.usda'), ()),
    'VBScriptLexer': ('pip._vendor.pygments.lexers.basic', 'VBScript', ('vbscript',), ('*.vbs', '*.VBS'), ()),
    'VCLLexer': ('pip._vendor.pygments.lexers.varnish', 'VCL', ('vcl',), ('*.vcl',), ('text/x-vclsrc',)),
    'VCLSnippetLexer': ('pip._vendor.pygments.lexers.varnish', 'VCLSnippets', ('vclsnippets', 'vclsnippet'), (), ('text/x-vclsnippet',)),
    'VCTreeStatusLexer': ('pip._vendor.pygments.lexers.console', 'VCTreeStatus', ('vctreestatus',), (), ()),
    'VGLLexer': ('pip._vendor.pygments.lexers.dsls', 'VGL', ('vgl',), ('*.rpf',), ()),
    'ValaLexer': ('pip._vendor.pygments.lexers.c_like', 'Vala', ('vala', 'vapi'), ('*.vala', '*.vapi'), ('text/x-vala',)),
    'VbNetAspxLexer': ('pip._vendor.pygments.lexers.dotnet', 'aspx-vb', ('aspx-vb',), ('*.aspx', '*.asax', '*.ascx', '*.ashx', '*.asmx', '*.axd'), ()),
    'VbNetLexer': ('pip._vendor.pygments.lexers.dotnet', 'VB.net', ('vb.net', 'vbnet', 'lobas', 'oobas', 'sobas'), ('*.vb', '*.bas'), ('text/x-vbnet', 'text/x-vba')),
    'VelocityHtmlLexer': ('pip._vendor.pygments.lexers.templates', 'HTML+Velocity', ('html+velocity',), (), ('text/html+velocity',)),
    'VelocityLexer': ('pip._vendor.pygments.lexers.templates', 'Velocity', ('velocity',), ('*.vm', '*.fhtml'), ()),
    'VelocityXmlLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Velocity', ('xml+velocity',), (), ('application/xml+velocity',)),
    'VerilogLexer': ('pip._vendor.pygments.lexers.hdl', 'verilog', ('verilog', 'v'), ('*.v',), ('text/x-verilog',)),
    'VhdlLexer': ('pip._vendor.pygments.lexers.hdl', 'vhdl', ('vhdl',), ('*.vhdl', '*.vhd'), ('text/x-vhdl',)),
    'VimLexer': ('pip._vendor.pygments.lexers.textedit', 'VimL', ('vim',), ('*.vim', '.vimrc', '.exrc', '.gvimrc', '_vimrc', '_exrc', '_gvimrc', 'vimrc', 'gvimrc'), ('text/x-vim',)),
    'WDiffLexer': ('pip._vendor.pygments.lexers.diff', 'WDiff', ('wdiff',), ('*.wdiff',), ()),
    'WatLexer': ('pip._vendor.pygments.lexers.webassembly', 'WebAssembly', ('wast', 'wat'), ('*.wat', '*.wast'), ()),
    'WebIDLLexer': ('pip._vendor.pygments.lexers.webidl', 'Web IDL', ('webidl',), ('*.webidl',), ()),
    'WgslLexer': ('pip._vendor.pygments.lexers.wgsl', 'WebGPU Shading Language', ('wgsl',), ('*.wgsl',), ('text/wgsl',)),
    'WhileyLexer': ('pip._vendor.pygments.lexers.whiley', 'Whiley', ('whiley',), ('*.whiley',), ('text/x-whiley',)),
    'WikitextLexer': ('pip._vendor.pygments.lexers.markup', 'Wikitext', ('wikitext', 'mediawiki'), (), ('text/x-wiki',)),
    'WoWTocLexer': ('pip._vendor.pygments.lexers.wowtoc', 'World of Warcraft TOC', ('wowtoc',), ('*.toc',), ()),
    'WrenLexer': ('pip._vendor.pygments.lexers.wren', 'Wren', ('wren',), ('*.wren',), ()),
    'X10Lexer': ('pip._vendor.pygments.lexers.x10', 'X10', ('x10', 'xten'), ('*.x10',), ('text/x-x10',)),
    'XMLUL4Lexer': ('pip._vendor.pygments.lexers.ul4', 'XML+UL4', ('xml+ul4',), ('*.xmlul4',), ()),
    'XQueryLexer': ('pip._vendor.pygments.lexers.webmisc', 'XQuery', ('xquery', 'xqy', 'xq', 'xql', 'xqm'), ('*.xqy', '*.xquery', '*.xq', '*.xql', '*.xqm'), ('text/xquery', 'application/xquery')),
    'XmlDjangoLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Django/Jinja', ('xml+django', 'xml+jinja'), ('*.xml.j2', '*.xml.jinja2'), ('application/xml+django', 'application/xml+jinja')),
    'XmlErbLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Ruby', ('xml+ruby', 'xml+erb'), (), ('application/xml+ruby',)),
    'XmlLexer': ('pip._vendor.pygments.lexers.html', 'XML', ('xml',), ('*.xml', '*.xsl', '*.rss', '*.xslt', '*.xsd', '*.wsdl', '*.wsf'), ('text/xml', 'application/xml', 'image/svg+xml', 'application/rss+xml', 'application/atom+xml')),
    'XmlPhpLexer': ('pip._vendor.pygments.lexers.templates', 'XML+PHP', ('xml+php',), (), ('application/xml+php',)),
    'XmlSmartyLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Smarty', ('xml+smarty',), (), ('application/xml+smarty',)),
    'XorgLexer': ('pip._vendor.pygments.lexers.xorg', 'Xorg', ('xorg.conf',), ('xorg.conf',), ()),
    'XppLexer': ('pip._vendor.pygments.lexers.dotnet', 'X++', ('xpp', 'x++'), ('*.xpp',), ()),
    'XsltLexer': ('pip._vendor.pygments.lexers.html', 'XSLT', ('xslt',), ('*.xsl', '*.xslt', '*.xpl'), ('application/xsl+xml', 'application/xslt+xml')),
    'XtendLexer': ('pip._vendor.pygments.lexers.jvm', 'Xtend', ('xtend',), ('*.xtend',), ('text/x-xtend',)),
    'XtlangLexer': ('pip._vendor.pygments.lexers.lisp', 'xtlang', ('extempore',), ('*.xtm',), ()),
    'YamlJinjaLexer': ('pip._vendor.pygments.lexers.templates', 'YAML+Jinja', ('yaml+jinja', 'salt', 'sls'), ('*.sls', '*.yaml.j2', '*.yml.j2', '*.yaml.jinja2', '*.yml.jinja2'), ('text/x-yaml+jinja', 'text/x-sls')),
    'YamlLexer': ('pip._vendor.pygments.lexers.data', 'YAML', ('yaml',), ('*.yaml', '*.yml'), ('text/x-yaml',)),
    'YangLexer': ('pip._vendor.pygments.lexers.yang', 'YANG', ('yang',), ('*.yang',), ('application/yang',)),
    'ZeekLexer': ('pip._vendor.pygments.lexers.dsls', 'Zeek', ('zeek', 'bro'), ('*.zeek', '*.bro'), ()),
    'ZephirLexer': ('pip._vendor.pygments.lexers.php', 'Zephir', ('zephir',), ('*.zep',), ()),
    'ZigLexer': ('pip._vendor.pygments.lexers.zig', 'Zig', ('zig',), ('*.zig',), ('text/zig',)),
    'apdlexer': ('pip._vendor.pygments.lexers.apdlexer', 'ANSYS parametric design language', ('ansys', 'apdl'), ('*.ans',), ()),
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/python.py
# ========================================================
"""
    pygments.lexers.python
    ~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Python and related languages.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
import keyword

from pip._vendor.pygments.lexer import DelegatingLexer, Lexer, RegexLexer, include, \
    bygroups, using, default, words, combined, do_insertions, this, line_re
from pip._vendor.pygments.util import get_bool_opt, shebang_matches
from pip._vendor.pygments.token import Text, Comment, Operator, Keyword, Name, String, \
    Number, Punctuation, Generic, Other, Error, Whitespace
from pip._vendor.pygments import unistring as uni

__all__ = ['PythonLexer', 'PythonConsoleLexer', 'PythonTracebackLexer',
           'Python2Lexer', 'Python2TracebackLexer',
           'CythonLexer', 'DgLexer', 'NumPyLexer']


class PythonLexer(RegexLexer):
    """
    For Python source code (version 3.x).

    .. versionadded:: 0.10

    .. versionchanged:: 2.5
       This is now the default ``PythonLexer``.  It is still available as the
       alias ``Python3Lexer``.
    """

    name = 'Python'
    url = 'http://www.python.org'
    aliases = ['python', 'py', 'sage', 'python3', 'py3']
    filenames = [
        '*.py',
        '*.pyw',
        # Type stubs
        '*.pyi',
        # Jython
        '*.jy',
        # Sage
        '*.sage',
        # SCons
        '*.sc',
        'SConstruct',
        'SConscript',
        # Skylark/Starlark (used by Bazel, Buck, and Pants)
        '*.bzl',
        'BUCK',
        'BUILD',
        'BUILD.bazel',
        'WORKSPACE',
        # Twisted Application infrastructure
        '*.tac',
    ]
    mimetypes = ['text/x-python', 'application/x-python',
                 'text/x-python3', 'application/x-python3']

    uni_name = "[%s][%s]*" % (uni.xid_start, uni.xid_continue)

    def innerstring_rules(ttype):
        return [
            # the old style '%s' % (...) string formatting (still valid in Py3)
            (r'%(\(\w+\))?[-#0 +]*([0-9]+|[*])?(\.([0-9]+|[*]))?'
             '[hlL]?[E-GXc-giorsaux%]', String.Interpol),
            # the new style '{}'.format(...) string formatting
            (r'\{'
             r'((\w+)((\.\w+)|(\[[^\]]+\]))*)?'  # field name
             r'(\![sra])?'                       # conversion
             r'(\:(.?[<>=\^])?[-+ ]?#?0?(\d+)?,?(\.\d+)?[E-GXb-gnosx%]?)?'
             r'\}', String.Interpol),

            # backslashes, quotes and formatting signs must be parsed one at a time
            (r'[^\\\'"%{\n]+', ttype),
            (r'[\'"\\]', ttype),
            # unhandled string formatting sign
            (r'%|(\{{1,2})', ttype)
            # newlines are an error (use "nl" state)
        ]

    def fstring_rules(ttype):
        return [
            # Assuming that a '}' is the closing brace after format specifier.
            # Sadly, this means that we won't detect syntax error. But it's
            # more important to parse correct syntax correctly, than to
            # highlight invalid syntax.
            (r'\}', String.Interpol),
            (r'\{', String.Interpol, 'expr-inside-fstring'),
            # backslashes, quotes and formatting signs must be parsed one at a time
            (r'[^\\\'"{}\n]+', ttype),
            (r'[\'"\\]', ttype),
            # newlines are an error (use "nl" state)
        ]

    tokens = {
        'root': [
            (r'\n', Whitespace),
            (r'^(\s*)([rRuUbB]{,2})("""(?:.|\n)*?""")',
             bygroups(Whitespace, String.Affix, String.Doc)),
            (r"^(\s*)([rRuUbB]{,2})('''(?:.|\n)*?''')",
             bygroups(Whitespace, String.Affix, String.Doc)),
            (r'\A#!.+$', Comment.Hashbang),
            (r'#.*$', Comment.Single),
            (r'\\\n', Text),
            (r'\\', Text),
            include('keywords'),
            include('soft-keywords'),
            (r'(def)((?:\s|\\\s)+)', bygroups(Keyword, Text), 'funcname'),
            (r'(class)((?:\s|\\\s)+)', bygroups(Keyword, Text), 'classname'),
            (r'(from)((?:\s|\\\s)+)', bygroups(Keyword.Namespace, Text),
             'fromimport'),
            (r'(import)((?:\s|\\\s)+)', bygroups(Keyword.Namespace, Text),
             'import'),
            include('expr'),
        ],
        'expr': [
            # raw f-strings
            ('(?i)(rf|fr)(""")',
             bygroups(String.Affix, String.Double),
             combined('rfstringescape', 'tdqf')),
            ("(?i)(rf|fr)(''')",
             bygroups(String.Affix, String.Single),
             combined('rfstringescape', 'tsqf')),
            ('(?i)(rf|fr)(")',
             bygroups(String.Affix, String.Double),
             combined('rfstringescape', 'dqf')),
            ("(?i)(rf|fr)(')",
             bygroups(String.Affix, String.Single),
             combined('rfstringescape', 'sqf')),
            # non-raw f-strings
            ('([fF])(""")', bygroups(String.Affix, String.Double),
             combined('fstringescape', 'tdqf')),
            ("([fF])(''')", bygroups(String.Affix, String.Single),
             combined('fstringescape', 'tsqf')),
            ('([fF])(")', bygroups(String.Affix, String.Double),
             combined('fstringescape', 'dqf')),
            ("([fF])(')", bygroups(String.Affix, String.Single),
             combined('fstringescape', 'sqf')),
            # raw bytes and strings
            ('(?i)(rb|br|r)(""")',
             bygroups(String.Affix, String.Double), 'tdqs'),
            ("(?i)(rb|br|r)(''')",
             bygroups(String.Affix, String.Single), 'tsqs'),
            ('(?i)(rb|br|r)(")',
             bygroups(String.Affix, String.Double), 'dqs'),
            ("(?i)(rb|br|r)(')",
             bygroups(String.Affix, String.Single), 'sqs'),
            # non-raw strings
            ('([uU]?)(""")', bygroups(String.Affix, String.Double),
             combined('stringescape', 'tdqs')),
            ("([uU]?)(''')", bygroups(String.Affix, String.Single),
             combined('stringescape', 'tsqs')),
            ('([uU]?)(")', bygroups(String.Affix, String.Double),
             combined('stringescape', 'dqs')),
            ("([uU]?)(')", bygroups(String.Affix, String.Single),
             combined('stringescape', 'sqs')),
            # non-raw bytes
            ('([bB])(""")', bygroups(String.Affix, String.Double),
             combined('bytesescape', 'tdqs')),
            ("([bB])(''')", bygroups(String.Affix, String.Single),
             combined('bytesescape', 'tsqs')),
            ('([bB])(")', bygroups(String.Affix, String.Double),
             combined('bytesescape', 'dqs')),
            ("([bB])(')", bygroups(String.Affix, String.Single),
             combined('bytesescape', 'sqs')),

            (r'[^\S\n]+', Text),
            include('numbers'),
            (r'!=|==|<<|>>|:=|[-~+/*%=<>&^|.]', Operator),
            (r'[]{}:(),;[]', Punctuation),
            (r'(in|is|and|or|not)\b', Operator.Word),
            include('expr-keywords'),
            include('builtins'),
            include('magicfuncs'),
            include('magicvars'),
            include('name'),
        ],
        'expr-inside-fstring': [
            (r'[{([]', Punctuation, 'expr-inside-fstring-inner'),
            # without format specifier
            (r'(=\s*)?'         # debug (https://bugs.python.org/issue36817)
             r'(\![sraf])?'     # conversion
             r'\}', String.Interpol, '#pop'),
            # with format specifier
            # we'll catch the remaining '}' in the outer scope
            (r'(=\s*)?'         # debug (https://bugs.python.org/issue36817)
             r'(\![sraf])?'     # conversion
             r':', String.Interpol, '#pop'),
            (r'\s+', Whitespace),  # allow new lines
            include('expr'),
        ],
        'expr-inside-fstring-inner': [
            (r'[{([]', Punctuation, 'expr-inside-fstring-inner'),
            (r'[])}]', Punctuation, '#pop'),
            (r'\s+', Whitespace),  # allow new lines
            include('expr'),
        ],
        'expr-keywords': [
            # Based on https://docs.python.org/3/reference/expressions.html
            (words((
                'async for', 'await', 'else', 'for', 'if', 'lambda',
                'yield', 'yield from'), suffix=r'\b'),
             Keyword),
            (words(('True', 'False', 'None'), suffix=r'\b'), Keyword.Constant),
        ],
        'keywords': [
            (words((
                'assert', 'async', 'await', 'break', 'continue', 'del', 'elif',
                'else', 'except', 'finally', 'for', 'global', 'if', 'lambda',
                'pass', 'raise', 'nonlocal', 'return', 'try', 'while', 'yield',
                'yield from', 'as', 'with'), suffix=r'\b'),
             Keyword),
            (words(('True', 'False', 'None'), suffix=r'\b'), Keyword.Constant),
        ],
        'soft-keywords': [
            # `match`, `case` and `_` soft keywords
            (r'(^[ \t]*)'              # at beginning of line + possible indentation
             r'(match|case)\b'         # a possible keyword
             r'(?![ \t]*(?:'           # not followed by...
             r'[:,;=^&|@~)\]}]|(?:' +  # characters and keywords that mean this isn't
             r'|'.join(keyword.kwlist) + r')\b))',                 # pattern matching
             bygroups(Text, Keyword), 'soft-keywords-inner'),
        ],
        'soft-keywords-inner': [
            # optional `_` keyword
            (r'(\s+)([^\n_]*)(_\b)', bygroups(Whitespace, using(this), Keyword)),
            default('#pop')
        ],
        'builtins': [
            (words((
                '__import__', 'abs', 'aiter', 'all', 'any', 'bin', 'bool', 'bytearray',
                'breakpoint', 'bytes', 'callable', 'chr', 'classmethod', 'compile',
                'complex', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval',
                'filter', 'float', 'format', 'frozenset', 'getattr', 'globals',
                'hasattr', 'hash', 'hex', 'id', 'input', 'int', 'isinstance',
                'issubclass', 'iter', 'len', 'list', 'locals', 'map', 'max',
                'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow',
                'print', 'property', 'range', 'repr', 'reversed', 'round', 'set',
                'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super',
                'tuple', 'type', 'vars', 'zip'), prefix=r'(?<!\.)', suffix=r'\b'),
             Name.Builtin),
            (r'(?<!\.)(self|Ellipsis|NotImplemented|cls)\b', Name.Builtin.Pseudo),
            (words((
                'ArithmeticError', 'AssertionError', 'AttributeError',
                'BaseException', 'BufferError', 'BytesWarning', 'DeprecationWarning',
                'EOFError', 'EnvironmentError', 'Exception', 'FloatingPointError',
                'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError',
                'ImportWarning', 'IndentationError', 'IndexError', 'KeyError',
                'KeyboardInterrupt', 'LookupError', 'MemoryError', 'NameError',
                'NotImplementedError', 'OSError', 'OverflowError',
                'PendingDeprecationWarning', 'ReferenceError', 'ResourceWarning',
                'RuntimeError', 'RuntimeWarning', 'StopIteration',
                'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit',
                'TabError', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError',
                'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError',
                'UnicodeWarning', 'UserWarning', 'ValueError', 'VMSError',
                'Warning', 'WindowsError', 'ZeroDivisionError',
                # new builtin exceptions from PEP 3151
                'BlockingIOError', 'ChildProcessError', 'ConnectionError',
                'BrokenPipeError', 'ConnectionAbortedError', 'ConnectionRefusedError',
                'ConnectionResetError', 'FileExistsError', 'FileNotFoundError',
                'InterruptedError', 'IsADirectoryError', 'NotADirectoryError',
                'PermissionError', 'ProcessLookupError', 'TimeoutError',
                # others new in Python 3
                'StopAsyncIteration', 'ModuleNotFoundError', 'RecursionError',
                'EncodingWarning'),
                prefix=r'(?<!\.)', suffix=r'\b'),
             Name.Exception),
        ],
        'magicfuncs': [
            (words((
                '__abs__', '__add__', '__aenter__', '__aexit__', '__aiter__',
                '__and__', '__anext__', '__await__', '__bool__', '__bytes__',
                '__call__', '__complex__', '__contains__', '__del__', '__delattr__',
                '__delete__', '__delitem__', '__dir__', '__divmod__', '__enter__',
                '__eq__', '__exit__', '__float__', '__floordiv__', '__format__',
                '__ge__', '__get__', '__getattr__', '__getattribute__',
                '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__',
                '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__',
                '__imul__', '__index__', '__init__', '__instancecheck__',
                '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__',
                '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__',
                '__len__', '__length_hint__', '__lshift__', '__lt__', '__matmul__',
                '__missing__', '__mod__', '__mul__', '__ne__', '__neg__',
                '__new__', '__next__', '__or__', '__pos__', '__pow__',
                '__prepare__', '__radd__', '__rand__', '__rdivmod__', '__repr__',
                '__reversed__', '__rfloordiv__', '__rlshift__', '__rmatmul__',
                '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__',
                '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__',
                '__rxor__', '__set__', '__setattr__', '__setitem__', '__str__',
                '__sub__', '__subclasscheck__', '__truediv__',
                '__xor__'), suffix=r'\b'),
             Name.Function.Magic),
        ],
        'magicvars': [
            (words((
                '__annotations__', '__bases__', '__class__', '__closure__',
                '__code__', '__defaults__', '__dict__', '__doc__', '__file__',
                '__func__', '__globals__', '__kwdefaults__', '__module__',
                '__mro__', '__name__', '__objclass__', '__qualname__',
                '__self__', '__slots__', '__weakref__'), suffix=r'\b'),
             Name.Variable.Magic),
        ],
        'numbers': [
            (r'(\d(?:_?\d)*\.(?:\d(?:_?\d)*)?|(?:\d(?:_?\d)*)?\.\d(?:_?\d)*)'
             r'([eE][+-]?\d(?:_?\d)*)?', Number.Float),
            (r'\d(?:_?\d)*[eE][+-]?\d(?:_?\d)*j?', Number.Float),
            (r'0[oO](?:_?[0-7])+', Number.Oct),
            (r'0[bB](?:_?[01])+', Number.Bin),
            (r'0[xX](?:_?[a-fA-F0-9])+', Number.Hex),
            (r'\d(?:_?\d)*', Number.Integer),
        ],
        'name': [
            (r'@' + uni_name, Name.Decorator),
            (r'@', Operator),  # new matrix multiplication operator
            (uni_name, Name),
        ],
        'funcname': [
            include('magicfuncs'),
            (uni_name, Name.Function, '#pop'),
            default('#pop'),
        ],
        'classname': [
            (uni_name, Name.Class, '#pop'),
        ],
        'import': [
            (r'(\s+)(as)(\s+)', bygroups(Text, Keyword, Text)),
            (r'\.', Name.Namespace),
            (uni_name, Name.Namespace),
            (r'(\s*)(,)(\s*)', bygroups(Text, Operator, Text)),
            default('#pop')  # all else: go back
        ],
        'fromimport': [
            (r'(\s+)(import)\b', bygroups(Text, Keyword.Namespace), '#pop'),
            (r'\.', Name.Namespace),
            # if None occurs here, it's "raise x from None", since None can
            # never be a module name
            (r'None\b', Keyword.Constant, '#pop'),
            (uni_name, Name.Namespace),
            default('#pop'),
        ],
        'rfstringescape': [
            (r'\{\{', String.Escape),
            (r'\}\}', String.Escape),
        ],
        'fstringescape': [
            include('rfstringescape'),
            include('stringescape'),
        ],
        'bytesescape': [
            (r'\\([\\abfnrtv"\']|\n|x[a-fA-F0-9]{2}|[0-7]{1,3})', String.Escape)
        ],
        'stringescape': [
            (r'\\(N\{.*?\}|u[a-fA-F0-9]{4}|U[a-fA-F0-9]{8})', String.Escape),
            include('bytesescape')
        ],
        'fstrings-single': fstring_rules(String.Single),
        'fstrings-double': fstring_rules(String.Double),
        'strings-single': innerstring_rules(String.Single),
        'strings-double': innerstring_rules(String.Double),
        'dqf': [
            (r'"', String.Double, '#pop'),
            (r'\\\\|\\"|\\\n', String.Escape),  # included here for raw strings
            include('fstrings-double')
        ],
        'sqf': [
            (r"'", String.Single, '#pop'),
            (r"\\\\|\\'|\\\n", String.Escape),  # included here for raw strings
            include('fstrings-single')
        ],
        'dqs': [
            (r'"', String.Double, '#pop'),
            (r'\\\\|\\"|\\\n', String.Escape),  # included here for raw strings
            include('strings-double')
        ],
        'sqs': [
            (r"'", String.Single, '#pop'),
            (r"\\\\|\\'|\\\n", String.Escape),  # included here for raw strings
            include('strings-single')
        ],
        'tdqf': [
            (r'"""', String.Double, '#pop'),
            include('fstrings-double'),
            (r'\n', String.Double)
        ],
        'tsqf': [
            (r"'''", String.Single, '#pop'),
            include('fstrings-single'),
            (r'\n', String.Single)
        ],
        'tdqs': [
            (r'"""', String.Double, '#pop'),
            include('strings-double'),
            (r'\n', String.Double)
        ],
        'tsqs': [
            (r"'''", String.Single, '#pop'),
            include('strings-single'),
            (r'\n', String.Single)
        ],
    }

    def analyse_text(text):
        return shebang_matches(text, r'pythonw?(3(\.\d)?)?') or \
            'import ' in text[:1000]


Python3Lexer = PythonLexer


class Python2Lexer(RegexLexer):
    """
    For Python 2.x source code.

    .. versionchanged:: 2.5
       This class has been renamed from ``PythonLexer``.  ``PythonLexer`` now
       refers to the Python 3 variant.  File name patterns like ``*.py`` have
       been moved to Python 3 as well.
    """

    name = 'Python 2.x'
    url = 'http://www.python.org'
    aliases = ['python2', 'py2']
    filenames = []  # now taken over by PythonLexer (3.x)
    mimetypes = ['text/x-python2', 'application/x-python2']

    def innerstring_rules(ttype):
        return [
            # the old style '%s' % (...) string formatting
            (r'%(\(\w+\))?[-#0 +]*([0-9]+|[*])?(\.([0-9]+|[*]))?'
             '[hlL]?[E-GXc-giorsux%]', String.Interpol),
            # backslashes, quotes and formatting signs must be parsed one at a time
            (r'[^\\\'"%\n]+', ttype),
            (r'[\'"\\]', ttype),
            # unhandled string formatting sign
            (r'%', ttype),
            # newlines are an error (use "nl" state)
        ]

    tokens = {
        'root': [
            (r'\n', Whitespace),
            (r'^(\s*)([rRuUbB]{,2})("""(?:.|\n)*?""")',
             bygroups(Whitespace, String.Affix, String.Doc)),
            (r"^(\s*)([rRuUbB]{,2})('''(?:.|\n)*?''')",
             bygroups(Whitespace, String.Affix, String.Doc)),
            (r'[^\S\n]+', Text),
            (r'\A#!.+$', Comment.Hashbang),
            (r'#.*$', Comment.Single),
            (r'[]{}:(),;[]', Punctuation),
            (r'\\\n', Text),
            (r'\\', Text),
            (r'(in|is|and|or|not)\b', Operator.Word),
            (r'!=|==|<<|>>|[-~+/*%=<>&^|.]', Operator),
            include('keywords'),
            (r'(def)((?:\s|\\\s)+)', bygroups(Keyword, Text), 'funcname'),
            (r'(class)((?:\s|\\\s)+)', bygroups(Keyword, Text), 'classname'),
            (r'(from)((?:\s|\\\s)+)', bygroups(Keyword.Namespace, Text),
             'fromimport'),
            (r'(import)((?:\s|\\\s)+)', bygroups(Keyword.Namespace, Text),
             'import'),
            include('builtins'),
            include('magicfuncs'),
            include('magicvars'),
            include('backtick'),
            ('([rR]|[uUbB][rR]|[rR][uUbB])(""")',
             bygroups(String.Affix, String.Double), 'tdqs'),
            ("([rR]|[uUbB][rR]|[rR][uUbB])(''')",
             bygroups(String.Affix, String.Single), 'tsqs'),
            ('([rR]|[uUbB][rR]|[rR][uUbB])(")',
             bygroups(String.Affix, String.Double), 'dqs'),
            ("([rR]|[uUbB][rR]|[rR][uUbB])(')",
             bygroups(String.Affix, String.Single), 'sqs'),
            ('([uUbB]?)(""")', bygroups(String.Affix, String.Double),
             combined('stringescape', 'tdqs')),
            ("([uUbB]?)(''')", bygroups(String.Affix, String.Single),
             combined('stringescape', 'tsqs')),
            ('([uUbB]?)(")', bygroups(String.Affix, String.Double),
             combined('stringescape', 'dqs')),
            ("([uUbB]?)(')", bygroups(String.Affix, String.Single),
             combined('stringescape', 'sqs')),
            include('name'),
            include('numbers'),
        ],
        'keywords': [
            (words((
                'assert', 'break', 'continue', 'del', 'elif', 'else', 'except',
                'exec', 'finally', 'for', 'global', 'if', 'lambda', 'pass',
                'print', 'raise', 'return', 'try', 'while', 'yield',
                'yield from', 'as', 'with'), suffix=r'\b'),
             Keyword),
        ],
        'builtins': [
            (words((
                '__import__', 'abs', 'all', 'any', 'apply', 'basestring', 'bin',
                'bool', 'buffer', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod',
                'cmp', 'coerce', 'compile', 'complex', 'delattr', 'dict', 'dir', 'divmod',
                'enumerate', 'eval', 'execfile', 'exit', 'file', 'filter', 'float',
                'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'hex', 'id',
                'input', 'int', 'intern', 'isinstance', 'issubclass', 'iter', 'len',
                'list', 'locals', 'long', 'map', 'max', 'min', 'next', 'object',
                'oct', 'open', 'ord', 'pow', 'property', 'range', 'raw_input', 'reduce',
                'reload', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice',
                'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type',
                'unichr', 'unicode', 'vars', 'xrange', 'zip'),
                prefix=r'(?<!\.)', suffix=r'\b'),
             Name.Builtin),
            (r'(?<!\.)(self|None|Ellipsis|NotImplemented|False|True|cls'
             r')\b', Name.Builtin.Pseudo),
            (words((
                'ArithmeticError', 'AssertionError', 'AttributeError',
                'BaseException', 'DeprecationWarning', 'EOFError', 'EnvironmentError',
                'Exception', 'FloatingPointError', 'FutureWarning', 'GeneratorExit',
                'IOError', 'ImportError', 'ImportWarning', 'IndentationError',
                'IndexError', 'KeyError', 'KeyboardInterrupt', 'LookupError',
                'MemoryError', 'NameError',
                'NotImplementedError', 'OSError', 'OverflowError', 'OverflowWarning',
                'PendingDeprecationWarning', 'ReferenceError',
                'RuntimeError', 'RuntimeWarning', 'StandardError', 'StopIteration',
                'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit',
                'TabError', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError',
                'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError',
                'UnicodeWarning', 'UserWarning', 'ValueError', 'VMSError', 'Warning',
                'WindowsError', 'ZeroDivisionError'), prefix=r'(?<!\.)', suffix=r'\b'),
             Name.Exception),
        ],
        'magicfuncs': [
            (words((
                '__abs__', '__add__', '__and__', '__call__', '__cmp__', '__coerce__',
                '__complex__', '__contains__', '__del__', '__delattr__', '__delete__',
                '__delitem__', '__delslice__', '__div__', '__divmod__', '__enter__',
                '__eq__', '__exit__', '__float__', '__floordiv__', '__ge__', '__get__',
                '__getattr__', '__getattribute__', '__getitem__', '__getslice__', '__gt__',
                '__hash__', '__hex__', '__iadd__', '__iand__', '__idiv__', '__ifloordiv__',
                '__ilshift__', '__imod__', '__imul__', '__index__', '__init__',
                '__instancecheck__', '__int__', '__invert__', '__iop__', '__ior__',
                '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__',
                '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__',
                '__missing__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__',
                '__nonzero__', '__oct__', '__op__', '__or__', '__pos__', '__pow__',
                '__radd__', '__rand__', '__rcmp__', '__rdiv__', '__rdivmod__', '__repr__',
                '__reversed__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__',
                '__rop__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__',
                '__rtruediv__', '__rxor__', '__set__', '__setattr__', '__setitem__',
                '__setslice__', '__str__', '__sub__', '__subclasscheck__', '__truediv__',
                '__unicode__', '__xor__'), suffix=r'\b'),
             Name.Function.Magic),
        ],
        'magicvars': [
            (words((
                '__bases__', '__class__', '__closure__', '__code__', '__defaults__',
                '__dict__', '__doc__', '__file__', '__func__', '__globals__',
                '__metaclass__', '__module__', '__mro__', '__name__', '__self__',
                '__slots__', '__weakref__'),
                suffix=r'\b'),
             Name.Variable.Magic),
        ],
        'numbers': [
            (r'(\d+\.\d*|\d*\.\d+)([eE][+-]?[0-9]+)?j?', Number.Float),
            (r'\d+[eE][+-]?[0-9]+j?', Number.Float),
            (r'0[0-7]+j?', Number.Oct),
            (r'0[bB][01]+', Number.Bin),
            (r'0[xX][a-fA-F0-9]+', Number.Hex),
            (r'\d+L', Number.Integer.Long),
            (r'\d+j?', Number.Integer)
        ],
        'backtick': [
            ('`.*?`', String.Backtick),
        ],
        'name': [
            (r'@[\w.]+', Name.Decorator),
            (r'[a-zA-Z_]\w*', Name),
        ],
        'funcname': [
            include('magicfuncs'),
            (r'[a-zA-Z_]\w*', Name.Function, '#pop'),
            default('#pop'),
        ],
        'classname': [
            (r'[a-zA-Z_]\w*', Name.Class, '#pop')
        ],
        'import': [
            (r'(?:[ \t]|\\\n)+', Text),
            (r'as\b', Keyword.Namespace),
            (r',', Operator),
            (r'[a-zA-Z_][\w.]*', Name.Namespace),
            default('#pop')  # all else: go back
        ],
        'fromimport': [
            (r'(?:[ \t]|\\\n)+', Text),
            (r'import\b', Keyword.Namespace, '#pop'),
            # if None occurs here, it's "raise x from None", since None can
            # never be a module name
            (r'None\b', Name.Builtin.Pseudo, '#pop'),
            # sadly, in "raise x from y" y will be highlighted as namespace too
            (r'[a-zA-Z_.][\w.]*', Name.Namespace),
            # anything else here also means "raise x from y" and is therefore
            # not an error
            default('#pop'),
        ],
        'stringescape': [
            (r'\\([\\abfnrtv"\']|\n|N\{.*?\}|u[a-fA-F0-9]{4}|'
             r'U[a-fA-F0-9]{8}|x[a-fA-F0-9]{2}|[0-7]{1,3})', String.Escape)
        ],
        'strings-single': innerstring_rules(String.Single),
        'strings-double': innerstring_rules(String.Double),
        'dqs': [
            (r'"', String.Double, '#pop'),
            (r'\\\\|\\"|\\\n', String.Escape),  # included here for raw strings
            include('strings-double')
        ],
        'sqs': [
            (r"'", String.Single, '#pop'),
            (r"\\\\|\\'|\\\n", String.Escape),  # included here for raw strings
            include('strings-single')
        ],
        'tdqs': [
            (r'"""', String.Double, '#pop'),
            include('strings-double'),
            (r'\n', String.Double)
        ],
        'tsqs': [
            (r"'''", String.Single, '#pop'),
            include('strings-single'),
            (r'\n', String.Single)
        ],
    }

    def analyse_text(text):
        return shebang_matches(text, r'pythonw?2(\.\d)?')

class _PythonConsoleLexerBase(RegexLexer):
    name = 'Python console session'
    aliases = ['pycon']
    mimetypes = ['text/x-python-doctest']

    """Auxiliary lexer for `PythonConsoleLexer`.

    Code tokens are output as ``Token.Other.Code``, traceback tokens as
    ``Token.Other.Traceback``.
    """
    tokens = {
        'root': [
            (r'(>>> )(.*\n)', bygroups(Generic.Prompt, Other.Code), 'continuations'),
            # This happens, e.g., when tracebacks are embedded in documentation;
            # trailing whitespaces are often stripped in such contexts.
            (r'(>>>)(\n)', bygroups(Generic.Prompt, Whitespace)),
            (r'(\^C)?Traceback \(most recent call last\):\n', Other.Traceback, 'traceback'),
            # SyntaxError starts with this
            (r'  File "[^"]+", line \d+', Other.Traceback, 'traceback'),
            (r'.*\n', Generic.Output),
        ],
        'continuations': [
            (r'(\.\.\. )(.*\n)', bygroups(Generic.Prompt, Other.Code)),
            # See above.
            (r'(\.\.\.)(\n)', bygroups(Generic.Prompt, Whitespace)),
            default('#pop'),
        ],
        'traceback': [
            # As soon as we see a traceback, consume everything until the next
            # >>> prompt.
            (r'(?=>>>( |$))', Text, '#pop'),
            (r'(KeyboardInterrupt)(\n)', bygroups(Name.Class, Whitespace)),
            (r'.*\n', Other.Traceback),
        ],
    }

class PythonConsoleLexer(DelegatingLexer):
    """
    For Python console output or doctests, such as:

    .. sourcecode:: pycon

        >>> a = 'foo'
        >>> print(a)
        foo
        >>> 1 / 0
        Traceback (most recent call last):
          File "<stdin>", line 1, in <module>
        ZeroDivisionError: integer division or modulo by zero

    Additional options:

    `python3`
        Use Python 3 lexer for code.  Default is ``True``.

        .. versionadded:: 1.0
        .. versionchanged:: 2.5
           Now defaults to ``True``.
    """

    name = 'Python console session'
    aliases = ['pycon']
    mimetypes = ['text/x-python-doctest']

    def __init__(self, **options):
        python3 = get_bool_opt(options, 'python3', True)
        if python3:
            pylexer = PythonLexer
            tblexer = PythonTracebackLexer
        else:
            pylexer = Python2Lexer
            tblexer = Python2TracebackLexer
        # We have two auxiliary lexers. Use DelegatingLexer twice with
        # different tokens.  TODO: DelegatingLexer should support this
        # directly, by accepting a tuplet of auxiliary lexers and a tuple of
        # distinguishing tokens. Then we wouldn't need this intermediary
        # class.
        class _ReplaceInnerCode(DelegatingLexer):
            def __init__(self, **options):
                super().__init__(pylexer, _PythonConsoleLexerBase, Other.Code, **options)
        super().__init__(tblexer, _ReplaceInnerCode, Other.Traceback, **options)

class PythonTracebackLexer(RegexLexer):
    """
    For Python 3.x tracebacks, with support for chained exceptions.

    .. versionadded:: 1.0

    .. versionchanged:: 2.5
       This is now the default ``PythonTracebackLexer``.  It is still available
       as the alias ``Python3TracebackLexer``.
    """

    name = 'Python Traceback'
    aliases = ['pytb', 'py3tb']
    filenames = ['*.pytb', '*.py3tb']
    mimetypes = ['text/x-python-traceback', 'text/x-python3-traceback']

    tokens = {
        'root': [
            (r'\n', Whitespace),
            (r'^(\^C)?Traceback \(most recent call last\):\n', Generic.Traceback, 'intb'),
            (r'^During handling of the above exception, another '
             r'exception occurred:\n\n', Generic.Traceback),
            (r'^The above exception was the direct cause of the '
             r'following exception:\n\n', Generic.Traceback),
            (r'^(?=  File "[^"]+", line \d+)', Generic.Traceback, 'intb'),
            (r'^.*\n', Other),
        ],
        'intb': [
            (r'^(  File )("[^"]+")(, line )(\d+)(, in )(.+)(\n)',
             bygroups(Text, Name.Builtin, Text, Number, Text, Name, Whitespace)),
            (r'^(  File )("[^"]+")(, line )(\d+)(\n)',
             bygroups(Text, Name.Builtin, Text, Number, Whitespace)),
            (r'^(    )(.+)(\n)',
             bygroups(Whitespace, using(PythonLexer), Whitespace), 'markers'),
            (r'^([ \t]*)(\.\.\.)(\n)',
             bygroups(Whitespace, Comment, Whitespace)),  # for doctests...
            (r'^([^:]+)(: )(.+)(\n)',
             bygroups(Generic.Error, Text, Name, Whitespace), '#pop'),
            (r'^([a-zA-Z_][\w.]*)(:?\n)',
             bygroups(Generic.Error, Whitespace), '#pop'),
            default('#pop'),
        ],
        'markers': [
            # Either `PEP 657 <https://www.python.org/dev/peps/pep-0657/>`
            # error locations in Python 3.11+, or single-caret markers
            # for syntax errors before that.
            (r'^( {4,})([~^]+)(\n)',
             bygroups(Whitespace, Punctuation.Marker, Whitespace),
             '#pop'),
            default('#pop'),
        ],
    }


Python3TracebackLexer = PythonTracebackLexer


class Python2TracebackLexer(RegexLexer):
    """
    For Python tracebacks.

    .. versionadded:: 0.7

    .. versionchanged:: 2.5
       This class has been renamed from ``PythonTracebackLexer``.
       ``PythonTracebackLexer`` now refers to the Python 3 variant.
    """

    name = 'Python 2.x Traceback'
    aliases = ['py2tb']
    filenames = ['*.py2tb']
    mimetypes = ['text/x-python2-traceback']

    tokens = {
        'root': [
            # Cover both (most recent call last) and (innermost last)
            # The optional ^C allows us to catch keyboard interrupt signals.
            (r'^(\^C)?(Traceback.*\n)',
             bygroups(Text, Generic.Traceback), 'intb'),
            # SyntaxError starts with this.
            (r'^(?=  File "[^"]+", line \d+)', Generic.Traceback, 'intb'),
            (r'^.*\n', Other),
        ],
        'intb': [
            (r'^(  File )("[^"]+")(, line )(\d+)(, in )(.+)(\n)',
             bygroups(Text, Name.Builtin, Text, Number, Text, Name, Whitespace)),
            (r'^(  File )("[^"]+")(, line )(\d+)(\n)',
             bygroups(Text, Name.Builtin, Text, Number, Whitespace)),
            (r'^(    )(.+)(\n)',
             bygroups(Text, using(Python2Lexer), Whitespace), 'marker'),
            (r'^([ \t]*)(\.\.\.)(\n)',
             bygroups(Text, Comment, Whitespace)),  # for doctests...
            (r'^([^:]+)(: )(.+)(\n)',
             bygroups(Generic.Error, Text, Name, Whitespace), '#pop'),
            (r'^([a-zA-Z_]\w*)(:?\n)',
             bygroups(Generic.Error, Whitespace), '#pop')
        ],
        'marker': [
            # For syntax errors.
            (r'( {4,})(\^)', bygroups(Text, Punctuation.Marker), '#pop'),
            default('#pop'),
        ],
    }


class CythonLexer(RegexLexer):
    """
    For Pyrex and Cython source code.

    .. versionadded:: 1.1
    """

    name = 'Cython'
    url = 'http://cython.org'
    aliases = ['cython', 'pyx', 'pyrex']
    filenames = ['*.pyx', '*.pxd', '*.pxi']
    mimetypes = ['text/x-cython', 'application/x-cython']

    tokens = {
        'root': [
            (r'\n', Whitespace),
            (r'^(\s*)("""(?:.|\n)*?""")', bygroups(Whitespace, String.Doc)),
            (r"^(\s*)('''(?:.|\n)*?''')", bygroups(Whitespace, String.Doc)),
            (r'[^\S\n]+', Text),
            (r'#.*$', Comment),
            (r'[]{}:(),;[]', Punctuation),
            (r'\\\n', Whitespace),
            (r'\\', Text),
            (r'(in|is|and|or|not)\b', Operator.Word),
            (r'(<)([a-zA-Z0-9.?]+)(>)',
             bygroups(Punctuation, Keyword.Type, Punctuation)),
            (r'!=|==|<<|>>|[-~+/*%=<>&^|.?]', Operator),
            (r'(from)(\d+)(<=)(\s+)(<)(\d+)(:)',
             bygroups(Keyword, Number.Integer, Operator, Name, Operator,
                      Name, Punctuation)),
            include('keywords'),
            (r'(def|property)(\s+)', bygroups(Keyword, Text), 'funcname'),
            (r'(cp?def)(\s+)', bygroups(Keyword, Text), 'cdef'),
            # (should actually start a block with only cdefs)
            (r'(cdef)(:)', bygroups(Keyword, Punctuation)),
            (r'(class|struct)(\s+)', bygroups(Keyword, Text), 'classname'),
            (r'(from)(\s+)', bygroups(Keyword, Text), 'fromimport'),
            (r'(c?import)(\s+)', bygroups(Keyword, Text), 'import'),
            include('builtins'),
            include('backtick'),
            ('(?:[rR]|[uU][rR]|[rR][uU])"""', String, 'tdqs'),
            ("(?:[rR]|[uU][rR]|[rR][uU])'''", String, 'tsqs'),
            ('(?:[rR]|[uU][rR]|[rR][uU])"', String, 'dqs'),
            ("(?:[rR]|[uU][rR]|[rR][uU])'", String, 'sqs'),
            ('[uU]?"""', String, combined('stringescape', 'tdqs')),
            ("[uU]?'''", String, combined('stringescape', 'tsqs')),
            ('[uU]?"', String, combined('stringescape', 'dqs')),
            ("[uU]?'", String, combined('stringescape', 'sqs')),
            include('name'),
            include('numbers'),
        ],
        'keywords': [
            (words((
                'assert', 'async', 'await', 'break', 'by', 'continue', 'ctypedef', 'del', 'elif',
                'else', 'except', 'except?', 'exec', 'finally', 'for', 'fused', 'gil',
                'global', 'if', 'include', 'lambda', 'nogil', 'pass', 'print',
                'raise', 'return', 'try', 'while', 'yield', 'as', 'with'), suffix=r'\b'),
             Keyword),
            (r'(DEF|IF|ELIF|ELSE)\b', Comment.Preproc),
        ],
        'builtins': [
            (words((
                '__import__', 'abs', 'all', 'any', 'apply', 'basestring', 'bin', 'bint',
                'bool', 'buffer', 'bytearray', 'bytes', 'callable', 'chr',
                'classmethod', 'cmp', 'coerce', 'compile', 'complex', 'delattr',
                'dict', 'dir', 'divmod', 'enumerate', 'eval', 'execfile', 'exit',
                'file', 'filter', 'float', 'frozenset', 'getattr', 'globals',
                'hasattr', 'hash', 'hex', 'id', 'input', 'int', 'intern', 'isinstance',
                'issubclass', 'iter', 'len', 'list', 'locals', 'long', 'map', 'max',
                'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'property', 'Py_ssize_t',
                'range', 'raw_input', 'reduce', 'reload', 'repr', 'reversed',
                'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod',
                'str', 'sum', 'super', 'tuple', 'type', 'unichr', 'unicode', 'unsigned',
                'vars', 'xrange', 'zip'), prefix=r'(?<!\.)', suffix=r'\b'),
             Name.Builtin),
            (r'(?<!\.)(self|None|Ellipsis|NotImplemented|False|True|NULL'
             r')\b', Name.Builtin.Pseudo),
            (words((
                'ArithmeticError', 'AssertionError', 'AttributeError',
                'BaseException', 'DeprecationWarning', 'EOFError', 'EnvironmentError',
                'Exception', 'FloatingPointError', 'FutureWarning', 'GeneratorExit',
                'IOError', 'ImportError', 'ImportWarning', 'IndentationError',
                'IndexError', 'KeyError', 'KeyboardInterrupt', 'LookupError',
                'MemoryError', 'NameError', 'NotImplemented', 'NotImplementedError',
                'OSError', 'OverflowError', 'OverflowWarning',
                'PendingDeprecationWarning', 'ReferenceError', 'RuntimeError',
                'RuntimeWarning', 'StandardError', 'StopIteration', 'SyntaxError',
                'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError',
                'TypeError', 'UnboundLocalError', 'UnicodeDecodeError',
                'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError',
                'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning',
                'ZeroDivisionError'), prefix=r'(?<!\.)', suffix=r'\b'),
             Name.Exception),
        ],
        'numbers': [
            (r'(\d+\.?\d*|\d*\.\d+)([eE][+-]?[0-9]+)?', Number.Float),
            (r'0\d+', Number.Oct),
            (r'0[xX][a-fA-F0-9]+', Number.Hex),
            (r'\d+L', Number.Integer.Long),
            (r'\d+', Number.Integer)
        ],
        'backtick': [
            ('`.*?`', String.Backtick),
        ],
        'name': [
            (r'@\w+', Name.Decorator),
            (r'[a-zA-Z_]\w*', Name),
        ],
        'funcname': [
            (r'[a-zA-Z_]\w*', Name.Function, '#pop')
        ],
        'cdef': [
            (r'(public|readonly|extern|api|inline)\b', Keyword.Reserved),
            (r'(struct|enum|union|class)\b', Keyword),
            (r'([a-zA-Z_]\w*)(\s*)(?=[(:#=]|$)',
             bygroups(Name.Function, Text), '#pop'),
            (r'([a-zA-Z_]\w*)(\s*)(,)',
             bygroups(Name.Function, Text, Punctuation)),
            (r'from\b', Keyword, '#pop'),
            (r'as\b', Keyword),
            (r':', Punctuation, '#pop'),
            (r'(?=["\'])', Text, '#pop'),
            (r'[a-zA-Z_]\w*', Keyword.Type),
            (r'.', Text),
        ],
        'classname': [
            (r'[a-zA-Z_]\w*', Name.Class, '#pop')
        ],
        'import': [
            (r'(\s+)(as)(\s+)', bygroups(Text, Keyword, Text)),
            (r'[a-zA-Z_][\w.]*', Name.Namespace),
            (r'(\s*)(,)(\s*)', bygroups(Text, Operator, Text)),
            default('#pop')  # all else: go back
        ],
        'fromimport': [
            (r'(\s+)(c?import)\b', bygroups(Text, Keyword), '#pop'),
            (r'[a-zA-Z_.][\w.]*', Name.Namespace),
            # ``cdef foo from "header"``, or ``for foo from 0 < i < 10``
            default('#pop'),
        ],
        'stringescape': [
            (r'\\([\\abfnrtv"\']|\n|N\{.*?\}|u[a-fA-F0-9]{4}|'
             r'U[a-fA-F0-9]{8}|x[a-fA-F0-9]{2}|[0-7]{1,3})', String.Escape)
        ],
        'strings': [
            (r'%(\([a-zA-Z0-9]+\))?[-#0 +]*([0-9]+|[*])?(\.([0-9]+|[*]))?'
             '[hlL]?[E-GXc-giorsux%]', String.Interpol),
            (r'[^\\\'"%\n]+', String),
            # quotes, percents and backslashes must be parsed one at a time
            (r'[\'"\\]', String),
            # unhandled string formatting sign
            (r'%', String)
            # newlines are an error (use "nl" state)
        ],
        'nl': [
            (r'\n', String)
        ],
        'dqs': [
            (r'"', String, '#pop'),
            (r'\\\\|\\"|\\\n', String.Escape),  # included here again for raw strings
            include('strings')
        ],
        'sqs': [
            (r"'", String, '#pop'),
            (r"\\\\|\\'|\\\n", String.Escape),  # included here again for raw strings
            include('strings')
        ],
        'tdqs': [
            (r'"""', String, '#pop'),
            include('strings'),
            include('nl')
        ],
        'tsqs': [
            (r"'''", String, '#pop'),
            include('strings'),
            include('nl')
        ],
    }


class DgLexer(RegexLexer):
    """
    Lexer for dg,
    a functional and object-oriented programming language
    running on the CPython 3 VM.

    .. versionadded:: 1.6
    """
    name = 'dg'
    aliases = ['dg']
    filenames = ['*.dg']
    mimetypes = ['text/x-dg']

    tokens = {
        'root': [
            (r'\s+', Text),
            (r'#.*?$', Comment.Single),

            (r'(?i)0b[01]+', Number.Bin),
            (r'(?i)0o[0-7]+', Number.Oct),
            (r'(?i)0x[0-9a-f]+', Number.Hex),
            (r'(?i)[+-]?[0-9]+\.[0-9]+(e[+-]?[0-9]+)?j?', Number.Float),
            (r'(?i)[+-]?[0-9]+e[+-]?\d+j?', Number.Float),
            (r'(?i)[+-]?[0-9]+j?', Number.Integer),

            (r"(?i)(br|r?b?)'''", String, combined('stringescape', 'tsqs', 'string')),
            (r'(?i)(br|r?b?)"""', String, combined('stringescape', 'tdqs', 'string')),
            (r"(?i)(br|r?b?)'", String, combined('stringescape', 'sqs', 'string')),
            (r'(?i)(br|r?b?)"', String, combined('stringescape', 'dqs', 'string')),

            (r"`\w+'*`", Operator),
            (r'\b(and|in|is|or|where)\b', Operator.Word),
            (r'[!$%&*+\-./:<-@\\^|~;,]+', Operator),

            (words((
                'bool', 'bytearray', 'bytes', 'classmethod', 'complex', 'dict', 'dict\'',
                'float', 'frozenset', 'int', 'list', 'list\'', 'memoryview', 'object',
                'property', 'range', 'set', 'set\'', 'slice', 'staticmethod', 'str',
                'super', 'tuple', 'tuple\'', 'type'),
                   prefix=r'(?<!\.)', suffix=r'(?![\'\w])'),
             Name.Builtin),
            (words((
                '__import__', 'abs', 'all', 'any', 'bin', 'bind', 'chr', 'cmp', 'compile',
                'complex', 'delattr', 'dir', 'divmod', 'drop', 'dropwhile', 'enumerate',
                'eval', 'exhaust', 'filter', 'flip', 'foldl1?', 'format', 'fst',
                'getattr', 'globals', 'hasattr', 'hash', 'head', 'hex', 'id', 'init',
                'input', 'isinstance', 'issubclass', 'iter', 'iterate', 'last', 'len',
                'locals', 'map', 'max', 'min', 'next', 'oct', 'open', 'ord', 'pow',
                'print', 'repr', 'reversed', 'round', 'setattr', 'scanl1?', 'snd',
                'sorted', 'sum', 'tail', 'take', 'takewhile', 'vars', 'zip'),
                   prefix=r'(?<!\.)', suffix=r'(?![\'\w])'),
             Name.Builtin),
            (r"(?<!\.)(self|Ellipsis|NotImplemented|None|True|False)(?!['\w])",
             Name.Builtin.Pseudo),

            (r"(?<!\.)[A-Z]\w*(Error|Exception|Warning)'*(?!['\w])",
             Name.Exception),
            (r"(?<!\.)(Exception|GeneratorExit|KeyboardInterrupt|StopIteration|"
             r"SystemExit)(?!['\w])", Name.Exception),

            (r"(?<![\w.])(except|finally|for|if|import|not|otherwise|raise|"
             r"subclass|while|with|yield)(?!['\w])", Keyword.Reserved),

            (r"[A-Z_]+'*(?!['\w])", Name),
            (r"[A-Z]\w+'*(?!['\w])", Keyword.Type),
            (r"\w+'*", Name),

            (r'[()]', Punctuation),
            (r'.', Error),
        ],
        'stringescape': [
            (r'\\([\\abfnrtv"\']|\n|N\{.*?\}|u[a-fA-F0-9]{4}|'
             r'U[a-fA-F0-9]{8}|x[a-fA-F0-9]{2}|[0-7]{1,3})', String.Escape)
        ],
        'string': [
            (r'%(\(\w+\))?[-#0 +]*([0-9]+|[*])?(\.([0-9]+|[*]))?'
             '[hlL]?[E-GXc-giorsux%]', String.Interpol),
            (r'[^\\\'"%\n]+', String),
            # quotes, percents and backslashes must be parsed one at a time
            (r'[\'"\\]', String),
            # unhandled string formatting sign
            (r'%', String),
            (r'\n', String)
        ],
        'dqs': [
            (r'"', String, '#pop')
        ],
        'sqs': [
            (r"'", String, '#pop')
        ],
        'tdqs': [
            (r'"""', String, '#pop')
        ],
        'tsqs': [
            (r"'''", String, '#pop')
        ],
    }


class NumPyLexer(PythonLexer):
    """
    A Python lexer recognizing Numerical Python builtins.

    .. versionadded:: 0.10
    """

    name = 'NumPy'
    url = 'https://numpy.org/'
    aliases = ['numpy']

    # override the mimetypes to not inherit them from python
    mimetypes = []
    filenames = []

    EXTRA_KEYWORDS = {
        'abs', 'absolute', 'accumulate', 'add', 'alen', 'all', 'allclose',
        'alltrue', 'alterdot', 'amax', 'amin', 'angle', 'any', 'append',
        'apply_along_axis', 'apply_over_axes', 'arange', 'arccos', 'arccosh',
        'arcsin', 'arcsinh', 'arctan', 'arctan2', 'arctanh', 'argmax', 'argmin',
        'argsort', 'argwhere', 'around', 'array', 'array2string', 'array_equal',
        'array_equiv', 'array_repr', 'array_split', 'array_str', 'arrayrange',
        'asanyarray', 'asarray', 'asarray_chkfinite', 'ascontiguousarray',
        'asfarray', 'asfortranarray', 'asmatrix', 'asscalar', 'astype',
        'atleast_1d', 'atleast_2d', 'atleast_3d', 'average', 'bartlett',
        'base_repr', 'beta', 'binary_repr', 'bincount', 'binomial',
        'bitwise_and', 'bitwise_not', 'bitwise_or', 'bitwise_xor', 'blackman',
        'bmat', 'broadcast', 'byte_bounds', 'bytes', 'byteswap', 'c_',
        'can_cast', 'ceil', 'choose', 'clip', 'column_stack', 'common_type',
        'compare_chararrays', 'compress', 'concatenate', 'conj', 'conjugate',
        'convolve', 'copy', 'corrcoef', 'correlate', 'cos', 'cosh', 'cov',
        'cross', 'cumprod', 'cumproduct', 'cumsum', 'delete', 'deprecate',
        'diag', 'diagflat', 'diagonal', 'diff', 'digitize', 'disp', 'divide',
        'dot', 'dsplit', 'dstack', 'dtype', 'dump', 'dumps', 'ediff1d', 'empty',
        'empty_like', 'equal', 'exp', 'expand_dims', 'expm1', 'extract', 'eye',
        'fabs', 'fastCopyAndTranspose', 'fft', 'fftfreq', 'fftshift', 'fill',
        'finfo', 'fix', 'flat', 'flatnonzero', 'flatten', 'fliplr', 'flipud',
        'floor', 'floor_divide', 'fmod', 'frexp', 'fromarrays', 'frombuffer',
        'fromfile', 'fromfunction', 'fromiter', 'frompyfunc', 'fromstring',
        'generic', 'get_array_wrap', 'get_include', 'get_numarray_include',
        'get_numpy_include', 'get_printoptions', 'getbuffer', 'getbufsize',
        'geterr', 'geterrcall', 'geterrobj', 'getfield', 'gradient', 'greater',
        'greater_equal', 'gumbel', 'hamming', 'hanning', 'histogram',
        'histogram2d', 'histogramdd', 'hsplit', 'hstack', 'hypot', 'i0',
        'identity', 'ifft', 'imag', 'index_exp', 'indices', 'inf', 'info',
        'inner', 'insert', 'int_asbuffer', 'interp', 'intersect1d',
        'intersect1d_nu', 'inv', 'invert', 'iscomplex', 'iscomplexobj',
        'isfinite', 'isfortran', 'isinf', 'isnan', 'isneginf', 'isposinf',
        'isreal', 'isrealobj', 'isscalar', 'issctype', 'issubclass_',
        'issubdtype', 'issubsctype', 'item', 'itemset', 'iterable', 'ix_',
        'kaiser', 'kron', 'ldexp', 'left_shift', 'less', 'less_equal', 'lexsort',
        'linspace', 'load', 'loads', 'loadtxt', 'log', 'log10', 'log1p', 'log2',
        'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'logspace',
        'lstsq', 'mat', 'matrix', 'max', 'maximum', 'maximum_sctype',
        'may_share_memory', 'mean', 'median', 'meshgrid', 'mgrid', 'min',
        'minimum', 'mintypecode', 'mod', 'modf', 'msort', 'multiply', 'nan',
        'nan_to_num', 'nanargmax', 'nanargmin', 'nanmax', 'nanmin', 'nansum',
        'ndenumerate', 'ndim', 'ndindex', 'negative', 'newaxis', 'newbuffer',
        'newbyteorder', 'nonzero', 'not_equal', 'obj2sctype', 'ogrid', 'ones',
        'ones_like', 'outer', 'permutation', 'piecewise', 'pinv', 'pkgload',
        'place', 'poisson', 'poly', 'poly1d', 'polyadd', 'polyder', 'polydiv',
        'polyfit', 'polyint', 'polymul', 'polysub', 'polyval', 'power', 'prod',
        'product', 'ptp', 'put', 'putmask', 'r_', 'randint', 'random_integers',
        'random_sample', 'ranf', 'rank', 'ravel', 'real', 'real_if_close',
        'recarray', 'reciprocal', 'reduce', 'remainder', 'repeat', 'require',
        'reshape', 'resize', 'restoredot', 'right_shift', 'rint', 'roll',
        'rollaxis', 'roots', 'rot90', 'round', 'round_', 'row_stack', 's_',
        'sample', 'savetxt', 'sctype2char', 'searchsorted', 'seed', 'select',
        'set_numeric_ops', 'set_printoptions', 'set_string_function',
        'setbufsize', 'setdiff1d', 'seterr', 'seterrcall', 'seterrobj',
        'setfield', 'setflags', 'setmember1d', 'setxor1d', 'shape',
        'show_config', 'shuffle', 'sign', 'signbit', 'sin', 'sinc', 'sinh',
        'size', 'slice', 'solve', 'sometrue', 'sort', 'sort_complex', 'source',
        'split', 'sqrt', 'square', 'squeeze', 'standard_normal', 'std',
        'subtract', 'sum', 'svd', 'swapaxes', 'take', 'tan', 'tanh', 'tensordot',
        'test', 'tile', 'tofile', 'tolist', 'tostring', 'trace', 'transpose',
        'trapz', 'tri', 'tril', 'trim_zeros', 'triu', 'true_divide', 'typeDict',
        'typename', 'uniform', 'union1d', 'unique', 'unique1d', 'unravel_index',
        'unwrap', 'vander', 'var', 'vdot', 'vectorize', 'view', 'vonmises',
        'vsplit', 'vstack', 'weibull', 'where', 'who', 'zeros', 'zeros_like'
    }

    def get_tokens_unprocessed(self, text):
        for index, token, value in \
                PythonLexer.get_tokens_unprocessed(self, text):
            if token is Name and value in self.EXTRA_KEYWORDS:
                yield index, Keyword.Pseudo, value
            else:
                yield index, token, value

    def analyse_text(text):
        ltext = text[:1000]
        return (shebang_matches(text, r'pythonw?(3(\.\d)?)?') or
                'import ' in ltext) \
            and ('import numpy' in ltext or 'from numpy import' in ltext)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/__init__.py
# ========================================================
"""
    pygments.lexers
    ~~~~~~~~~~~~~~~

    Pygments lexers.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
import sys
import types
import fnmatch
from os.path import basename

from pip._vendor.pygments.lexers._mapping import LEXERS
from pip._vendor.pygments.modeline import get_filetype_from_buffer
from pip._vendor.pygments.plugin import find_plugin_lexers
from pip._vendor.pygments.util import ClassNotFound, guess_decode

COMPAT = {
    'Python3Lexer': 'PythonLexer',
    'Python3TracebackLexer': 'PythonTracebackLexer',
}

__all__ = ['get_lexer_by_name', 'get_lexer_for_filename', 'find_lexer_class',
           'guess_lexer', 'load_lexer_from_file'] + list(LEXERS) + list(COMPAT)

_lexer_cache = {}
_pattern_cache = {}


def _fn_matches(fn, glob):
    """Return whether the supplied file name fn matches pattern filename."""
    if glob not in _pattern_cache:
        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))
        return pattern.match(fn)
    return _pattern_cache[glob].match(fn)


def _load_lexers(module_name):
    """Load a lexer (and all others in the module too)."""
    mod = __import__(module_name, None, None, ['__all__'])
    for lexer_name in mod.__all__:
        cls = getattr(mod, lexer_name)
        _lexer_cache[cls.name] = cls


def get_all_lexers(plugins=True):
    """Return a generator of tuples in the form ``(name, aliases,
    filenames, mimetypes)`` of all know lexers.

    If *plugins* is true (the default), plugin lexers supplied by entrypoints
    are also returned.  Otherwise, only builtin ones are considered.
    """
    for item in LEXERS.values():
        yield item[1:]
    if plugins:
        for lexer in find_plugin_lexers():
            yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes


def find_lexer_class(name):
    """
    Return the `Lexer` subclass that with the *name* attribute as given by
    the *name* argument.
    """
    if name in _lexer_cache:
        return _lexer_cache[name]
    # lookup builtin lexers
    for module_name, lname, aliases, _, _ in LEXERS.values():
        if name == lname:
            _load_lexers(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if cls.name == name:
            return cls


def find_lexer_class_by_name(_alias):
    """
    Return the `Lexer` subclass that has `alias` in its aliases list, without
    instantiating it.

    Like `get_lexer_by_name`, but does not instantiate the class.

    Will raise :exc:`pygments.util.ClassNotFound` if no lexer with that alias is
    found.

    .. versionadded:: 2.2
    """
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)
    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name]
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls
    raise ClassNotFound('no lexer for alias %r found' % _alias)


def get_lexer_by_name(_alias, **options):
    """
    Return an instance of a `Lexer` subclass that has `alias` in its
    aliases list. The lexer is given the `options` at its
    instantiation.

    Will raise :exc:`pygments.util.ClassNotFound` if no lexer with that alias is
    found.
    """
    if not _alias:
        raise ClassNotFound('no lexer for alias %r found' % _alias)

    # lookup builtin lexers
    for module_name, name, aliases, _, _ in LEXERS.values():
        if _alias.lower() in aliases:
            if name not in _lexer_cache:
                _load_lexers(module_name)
            return _lexer_cache[name](**options)
    # continue with lexers from setuptools entrypoints
    for cls in find_plugin_lexers():
        if _alias.lower() in cls.aliases:
            return cls(**options)
    raise ClassNotFound('no lexer for alias %r found' % _alias)


def load_lexer_from_file(filename, lexername="CustomLexer", **options):
    """Load a lexer from a file.

    This method expects a file located relative to the current working
    directory, which contains a Lexer class. By default, it expects the
    Lexer to be name CustomLexer; you can specify your own class name
    as the second argument to this function.

    Users should be very careful with the input, because this method
    is equivalent to running eval on the input file.

    Raises ClassNotFound if there are any problems importing the Lexer.

    .. versionadded:: 2.2
    """
    try:
        # This empty dict will contain the namespace for the exec'd file
        custom_namespace = {}
        with open(filename, 'rb') as f:
            exec(f.read(), custom_namespace)
        # Retrieve the class `lexername` from that namespace
        if lexername not in custom_namespace:
            raise ClassNotFound('no valid %s class found in %s' %
                                (lexername, filename))
        lexer_class = custom_namespace[lexername]
        # And finally instantiate it with the options
        return lexer_class(**options)
    except OSError as err:
        raise ClassNotFound('cannot read %s: %s' % (filename, err))
    except ClassNotFound:
        raise
    except Exception as err:
        raise ClassNotFound('error when loading custom lexer: %s' % err)


def find_lexer_class_for_filename(_fn, code=None):
    """Get a lexer for a filename.

    If multiple lexers match the filename pattern, use ``analyse_text()`` to
    figure out which one is more appropriate.

    Returns None if not found.
    """
    matches = []
    fn = basename(_fn)
    for modname, name, _, filenames, _ in LEXERS.values():
        for filename in filenames:
            if _fn_matches(fn, filename):
                if name not in _lexer_cache:
                    _load_lexers(modname)
                matches.append((_lexer_cache[name], filename))
    for cls in find_plugin_lexers():
        for filename in cls.filenames:
            if _fn_matches(fn, filename):
                matches.append((cls, filename))

    if isinstance(code, bytes):
        # decode it, since all analyse_text functions expect unicode
        code = guess_decode(code)

    def get_rating(info):
        cls, filename = info
        # explicit patterns get a bonus
        bonus = '*' not in filename and 0.5 or 0
        # The class _always_ defines analyse_text because it's included in
        # the Lexer class.  The default implementation returns None which
        # gets turned into 0.0.  Run scripts/detect_missing_analyse_text.py
        # to find lexers which need it overridden.
        if code:
            return cls.analyse_text(code) + bonus, cls.__name__
        return cls.priority + bonus, cls.__name__

    if matches:
        matches.sort(key=get_rating)
        # print "Possible lexers, after sort:", matches
        return matches[-1][0]


def get_lexer_for_filename(_fn, code=None, **options):
    """Get a lexer for a filename.

    Return a `Lexer` subclass instance that has a filename pattern
    matching `fn`. The lexer is given the `options` at its
    instantiation.

    Raise :exc:`pygments.util.ClassNotFound` if no lexer for that filename
    is found.

    If multiple lexers match the filename pattern, use their ``analyse_text()``
    methods to figure out which one is more appropriate.
    """
    res = find_lexer_class_for_filename(_fn, code)
    if not res:
        raise ClassNotFound('no lexer for filename %r found' % _fn)
    return res(**options)


def get_lexer_for_mimetype(_mime, **options):
    """
    Return a `Lexer` subclass instance that has `mime` in its mimetype
    list. The lexer is given the `options` at its instantiation.

    Will raise :exc:`pygments.util.ClassNotFound` if not lexer for that mimetype
    is found.
    """
    for modname, name, _, _, mimetypes in LEXERS.values():
        if _mime in mimetypes:
            if name not in _lexer_cache:
                _load_lexers(modname)
            return _lexer_cache[name](**options)
    for cls in find_plugin_lexers():
        if _mime in cls.mimetypes:
            return cls(**options)
    raise ClassNotFound('no lexer for mimetype %r found' % _mime)


def _iter_lexerclasses(plugins=True):
    """Return an iterator over all lexer classes."""
    for key in sorted(LEXERS):
        module_name, name = LEXERS[key][:2]
        if name not in _lexer_cache:
            _load_lexers(module_name)
        yield _lexer_cache[name]
    if plugins:
        yield from find_plugin_lexers()


def guess_lexer_for_filename(_fn, _text, **options):
    """
    As :func:`guess_lexer()`, but only lexers which have a pattern in `filenames`
    or `alias_filenames` that matches `filename` are taken into consideration.

    :exc:`pygments.util.ClassNotFound` is raised if no lexer thinks it can
    handle the content.
    """
    fn = basename(_fn)
    primary = {}
    matching_lexers = set()
    for lexer in _iter_lexerclasses():
        for filename in lexer.filenames:
            if _fn_matches(fn, filename):
                matching_lexers.add(lexer)
                primary[lexer] = True
        for filename in lexer.alias_filenames:
            if _fn_matches(fn, filename):
                matching_lexers.add(lexer)
                primary[lexer] = False
    if not matching_lexers:
        raise ClassNotFound('no lexer for filename %r found' % fn)
    if len(matching_lexers) == 1:
        return matching_lexers.pop()(**options)
    result = []
    for lexer in matching_lexers:
        rv = lexer.analyse_text(_text)
        if rv == 1.0:
            return lexer(**options)
        result.append((rv, lexer))

    def type_sort(t):
        # sort by:
        # - analyse score
        # - is primary filename pattern?
        # - priority
        # - last resort: class name
        return (t[0], primary[t[1]], t[1].priority, t[1].__name__)
    result.sort(key=type_sort)

    return result[-1][1](**options)


def guess_lexer(_text, **options):
    """
    Return a `Lexer` subclass instance that's guessed from the text in
    `text`. For that, the :meth:`.analyse_text()` method of every known lexer
    class is called with the text as argument, and the lexer which returned the
    highest value will be instantiated and returned.

    :exc:`pygments.util.ClassNotFound` is raised if no lexer thinks it can
    handle the content.
    """

    if not isinstance(_text, str):
        inencoding = options.get('inencoding', options.get('encoding'))
        if inencoding:
            _text = _text.decode(inencoding or 'utf8')
        else:
            _text, _ = guess_decode(_text)

    # try to get a vim modeline first
    ft = get_filetype_from_buffer(_text)

    if ft is not None:
        try:
            return get_lexer_by_name(ft, **options)
        except ClassNotFound:
            pass

    best_lexer = [0.0, None]
    for lexer in _iter_lexerclasses():
        rv = lexer.analyse_text(_text)
        if rv == 1.0:
            return lexer(**options)
        if rv > best_lexer[0]:
            best_lexer[:] = (rv, lexer)
    if not best_lexer[0] or best_lexer[1] is None:
        raise ClassNotFound('no lexer matching the text found')
    return best_lexer[1](**options)


class _automodule(types.ModuleType):
    """Automatically import lexers."""

    def __getattr__(self, name):
        info = LEXERS.get(name)
        if info:
            _load_lexers(info[0])
            cls = _lexer_cache[info[1]]
            setattr(self, name, cls)
            return cls
        if name in COMPAT:
            return getattr(self, COMPAT[name])
        raise AttributeError(name)


oldmod = sys.modules[__name__]
newmod = _automodule(__name__)
newmod.__dict__.update(oldmod.__dict__)
sys.modules[__name__] = newmod
del newmod.newmod, newmod.oldmod, newmod.sys, newmod.types


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatter.py
# ========================================================
"""
    pygments.formatter
    ~~~~~~~~~~~~~~~~~~

    Base formatter class.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import codecs

from pip._vendor.pygments.util import get_bool_opt
from pip._vendor.pygments.styles import get_style_by_name

__all__ = ['Formatter']


def _lookup_style(style):
    if isinstance(style, str):
        return get_style_by_name(style)
    return style


class Formatter:
    """
    Converts a token stream to text.

    Formatters should have attributes to help selecting them. These
    are similar to the corresponding :class:`~pygments.lexer.Lexer`
    attributes.

    .. autoattribute:: name
       :no-value:

    .. autoattribute:: aliases
       :no-value:

    .. autoattribute:: filenames
       :no-value:

    You can pass options as keyword arguments to the constructor.
    All formatters accept these basic options:

    ``style``
        The style to use, can be a string or a Style subclass
        (default: "default"). Not used by e.g. the
        TerminalFormatter.
    ``full``
        Tells the formatter to output a "full" document, i.e.
        a complete self-contained document. This doesn't have
        any effect for some formatters (default: false).
    ``title``
        If ``full`` is true, the title that should be used to
        caption the document (default: '').
    ``encoding``
        If given, must be an encoding name. This will be used to
        convert the Unicode token strings to byte strings in the
        output. If it is "" or None, Unicode strings will be written
        to the output file, which most file-like objects do not
        support (default: None).
    ``outencoding``
        Overrides ``encoding`` if given.

    """

    #: Full name for the formatter, in human-readable form.
    name = None

    #: A list of short, unique identifiers that can be used to lookup
    #: the formatter from a list, e.g. using :func:`.get_formatter_by_name()`.
    aliases = []

    #: A list of fnmatch patterns that match filenames for which this
    #: formatter can produce output. The patterns in this list should be unique
    #: among all formatters.
    filenames = []

    #: If True, this formatter outputs Unicode strings when no encoding
    #: option is given.
    unicodeoutput = True

    def __init__(self, **options):
        """
        As with lexers, this constructor takes arbitrary optional arguments,
        and if you override it, you should first process your own options, then
        call the base class implementation.
        """
        self.style = _lookup_style(options.get('style', 'default'))
        self.full = get_bool_opt(options, 'full', False)
        self.title = options.get('title', '')
        self.encoding = options.get('encoding', None) or None
        if self.encoding in ('guess', 'chardet'):
            # can happen for e.g. pygmentize -O encoding=guess
            self.encoding = 'utf-8'
        self.encoding = options.get('outencoding') or self.encoding
        self.options = options

    def get_style_defs(self, arg=''):
        """
        This method must return statements or declarations suitable to define
        the current style for subsequent highlighted text (e.g. CSS classes
        in the `HTMLFormatter`).

        The optional argument `arg` can be used to modify the generation and
        is formatter dependent (it is standardized because it can be given on
        the command line).

        This method is called by the ``-S`` :doc:`command-line option <cmdline>`,
        the `arg` is then given by the ``-a`` option.
        """
        return ''

    def format(self, tokensource, outfile):
        """
        This method must format the tokens from the `tokensource` iterable and
        write the formatted version to the file object `outfile`.

        Formatter options can control how exactly the tokens are converted.
        """
        if self.encoding:
            # wrap the outfile in a StreamWriter
            outfile = codecs.lookup(self.encoding)[3](outfile)
        return self.format_unencoded(tokensource, outfile)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/irc.py
# ========================================================
"""
    pygments.formatters.irc
    ~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for IRC output

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.token import Keyword, Name, Comment, String, Error, \
    Number, Operator, Generic, Token, Whitespace
from pip._vendor.pygments.util import get_choice_opt


__all__ = ['IRCFormatter']


#: Map token types to a tuple of color values for light and dark
#: backgrounds.
IRC_COLORS = {
    Token:              ('',            ''),

    Whitespace:         ('gray',   'brightblack'),
    Comment:            ('gray',   'brightblack'),
    Comment.Preproc:    ('cyan',        'brightcyan'),
    Keyword:            ('blue',    'brightblue'),
    Keyword.Type:       ('cyan',        'brightcyan'),
    Operator.Word:      ('magenta',      'brightcyan'),
    Name.Builtin:       ('cyan',        'brightcyan'),
    Name.Function:      ('green',   'brightgreen'),
    Name.Namespace:     ('_cyan_',      '_brightcyan_'),
    Name.Class:         ('_green_', '_brightgreen_'),
    Name.Exception:     ('cyan',        'brightcyan'),
    Name.Decorator:     ('brightblack',    'gray'),
    Name.Variable:      ('red',     'brightred'),
    Name.Constant:      ('red',     'brightred'),
    Name.Attribute:     ('cyan',        'brightcyan'),
    Name.Tag:           ('brightblue',        'brightblue'),
    String:             ('yellow',       'yellow'),
    Number:             ('blue',    'brightblue'),

    Generic.Deleted:    ('brightred',        'brightred'),
    Generic.Inserted:   ('green',  'brightgreen'),
    Generic.Heading:    ('**',         '**'),
    Generic.Subheading: ('*magenta*',   '*brightmagenta*'),
    Generic.Error:      ('brightred',        'brightred'),

    Error:              ('_brightred_',      '_brightred_'),
}


IRC_COLOR_MAP = {
    'white': 0,
    'black': 1,
    'blue': 2,
    'brightgreen': 3,
    'brightred': 4,
    'yellow': 5,
    'magenta': 6,
    'orange': 7,
    'green': 7, #compat w/ ansi
    'brightyellow': 8,
    'lightgreen': 9,
    'brightcyan': 9, # compat w/ ansi
    'cyan': 10,
    'lightblue': 11,
    'red': 11, # compat w/ ansi
    'brightblue': 12,
    'brightmagenta': 13,
    'brightblack': 14,
    'gray': 15,
}

def ircformat(color, text):
    if len(color) < 1:
        return text
    add = sub = ''
    if '_' in color: # italic
        add += '\x1D'
        sub = '\x1D' + sub
        color = color.strip('_')
    if '*' in color: # bold
        add += '\x02'
        sub = '\x02' + sub
        color = color.strip('*')
    # underline (\x1F) not supported
    # backgrounds (\x03FF,BB) not supported
    if len(color) > 0: # actual color - may have issues with ircformat("red", "blah")+"10" type stuff
        add += '\x03' + str(IRC_COLOR_MAP[color]).zfill(2)
        sub = '\x03' + sub
    return add + text + sub
    return '<'+add+'>'+text+'</'+sub+'>'


class IRCFormatter(Formatter):
    r"""
    Format tokens with IRC color sequences

    The `get_style_defs()` method doesn't do anything special since there is
    no support for common styles.

    Options accepted:

    `bg`
        Set to ``"light"`` or ``"dark"`` depending on the terminal's background
        (default: ``"light"``).

    `colorscheme`
        A dictionary mapping token types to (lightbg, darkbg) color names or
        ``None`` (default: ``None`` = use builtin colorscheme).

    `linenos`
        Set to ``True`` to have line numbers in the output as well
        (default: ``False`` = no line numbers).
    """
    name = 'IRC'
    aliases = ['irc', 'IRC']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        self.darkbg = get_choice_opt(options, 'bg',
                                     ['light', 'dark'], 'light') == 'dark'
        self.colorscheme = options.get('colorscheme', None) or IRC_COLORS
        self.linenos = options.get('linenos', False)
        self._lineno = 0

    def _write_lineno(self, outfile):
        if self.linenos:
            self._lineno += 1
            outfile.write("%04d: " % self._lineno)

    def format_unencoded(self, tokensource, outfile):
        self._write_lineno(outfile)

        for ttype, value in tokensource:
            color = self.colorscheme.get(ttype)
            while color is None:
                ttype = ttype[:-1]
                color = self.colorscheme.get(ttype)
            if color:
                color = color[self.darkbg]
                spl = value.split('\n')
                for line in spl[:-1]:
                    if line:
                        outfile.write(ircformat(color, line))
                    outfile.write('\n')
                    self._write_lineno(outfile)
                if spl[-1]:
                    outfile.write(ircformat(color, spl[-1]))
            else:
                outfile.write(value)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/latex.py
# ========================================================
"""
    pygments.formatters.latex
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for LaTeX fancyvrb output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from io import StringIO

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.lexer import Lexer, do_insertions
from pip._vendor.pygments.token import Token, STANDARD_TYPES
from pip._vendor.pygments.util import get_bool_opt, get_int_opt


__all__ = ['LatexFormatter']


def escape_tex(text, commandprefix):
    return text.replace('\\', '\x00'). \
                replace('{', '\x01'). \
                replace('}', '\x02'). \
                replace('\x00', r'\%sZbs{}' % commandprefix). \
                replace('\x01', r'\%sZob{}' % commandprefix). \
                replace('\x02', r'\%sZcb{}' % commandprefix). \
                replace('^', r'\%sZca{}' % commandprefix). \
                replace('_', r'\%sZus{}' % commandprefix). \
                replace('&', r'\%sZam{}' % commandprefix). \
                replace('<', r'\%sZlt{}' % commandprefix). \
                replace('>', r'\%sZgt{}' % commandprefix). \
                replace('#', r'\%sZsh{}' % commandprefix). \
                replace('%', r'\%sZpc{}' % commandprefix). \
                replace('$', r'\%sZdl{}' % commandprefix). \
                replace('-', r'\%sZhy{}' % commandprefix). \
                replace("'", r'\%sZsq{}' % commandprefix). \
                replace('"', r'\%sZdq{}' % commandprefix). \
                replace('~', r'\%sZti{}' % commandprefix)


DOC_TEMPLATE = r'''
\documentclass{%(docclass)s}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage[%(encoding)s]{inputenc}
%(preamble)s

%(styledefs)s

\begin{document}

\section*{%(title)s}

%(code)s
\end{document}
'''

## Small explanation of the mess below :)
#
# The previous version of the LaTeX formatter just assigned a command to
# each token type defined in the current style.  That obviously is
# problematic if the highlighted code is produced for a different style
# than the style commands themselves.
#
# This version works much like the HTML formatter which assigns multiple
# CSS classes to each <span> tag, from the most specific to the least
# specific token type, thus falling back to the parent token type if one
# is not defined.  Here, the classes are there too and use the same short
# forms given in token.STANDARD_TYPES.
#
# Highlighted code now only uses one custom command, which by default is
# \PY and selectable by the commandprefix option (and in addition the
# escapes \PYZat, \PYZlb and \PYZrb which haven't been renamed for
# backwards compatibility purposes).
#
# \PY has two arguments: the classes, separated by +, and the text to
# render in that style.  The classes are resolved into the respective
# style commands by magic, which serves to ignore unknown classes.
#
# The magic macros are:
# * \PY@it, \PY@bf, etc. are unconditionally wrapped around the text
#   to render in \PY@do.  Their definition determines the style.
# * \PY@reset resets \PY@it etc. to do nothing.
# * \PY@toks parses the list of classes, using magic inspired by the
#   keyval package (but modified to use plusses instead of commas
#   because fancyvrb redefines commas inside its environments).
# * \PY@tok processes one class, calling the \PY@tok@classname command
#   if it exists.
# * \PY@tok@classname sets the \PY@it etc. to reflect the chosen style
#   for its class.
# * \PY resets the style, parses the classnames and then calls \PY@do.
#
# Tip: to read this code, print it out in substituted form using e.g.
# >>> print STYLE_TEMPLATE % {'cp': 'PY'}

STYLE_TEMPLATE = r'''
\makeatletter
\def\%(cp)s@reset{\let\%(cp)s@it=\relax \let\%(cp)s@bf=\relax%%
    \let\%(cp)s@ul=\relax \let\%(cp)s@tc=\relax%%
    \let\%(cp)s@bc=\relax \let\%(cp)s@ff=\relax}
\def\%(cp)s@tok#1{\csname %(cp)s@tok@#1\endcsname}
\def\%(cp)s@toks#1+{\ifx\relax#1\empty\else%%
    \%(cp)s@tok{#1}\expandafter\%(cp)s@toks\fi}
\def\%(cp)s@do#1{\%(cp)s@bc{\%(cp)s@tc{\%(cp)s@ul{%%
    \%(cp)s@it{\%(cp)s@bf{\%(cp)s@ff{#1}}}}}}}
\def\%(cp)s#1#2{\%(cp)s@reset\%(cp)s@toks#1+\relax+\%(cp)s@do{#2}}

%(styles)s

\def\%(cp)sZbs{\char`\\}
\def\%(cp)sZus{\char`\_}
\def\%(cp)sZob{\char`\{}
\def\%(cp)sZcb{\char`\}}
\def\%(cp)sZca{\char`\^}
\def\%(cp)sZam{\char`\&}
\def\%(cp)sZlt{\char`\<}
\def\%(cp)sZgt{\char`\>}
\def\%(cp)sZsh{\char`\#}
\def\%(cp)sZpc{\char`\%%}
\def\%(cp)sZdl{\char`\$}
\def\%(cp)sZhy{\char`\-}
\def\%(cp)sZsq{\char`\'}
\def\%(cp)sZdq{\char`\"}
\def\%(cp)sZti{\char`\~}
%% for compatibility with earlier versions
\def\%(cp)sZat{@}
\def\%(cp)sZlb{[}
\def\%(cp)sZrb{]}
\makeatother
'''


def _get_ttype_name(ttype):
    fname = STANDARD_TYPES.get(ttype)
    if fname:
        return fname
    aname = ''
    while fname is None:
        aname = ttype[-1] + aname
        ttype = ttype.parent
        fname = STANDARD_TYPES.get(ttype)
    return fname + aname


class LatexFormatter(Formatter):
    r"""
    Format tokens as LaTeX code. This needs the `fancyvrb` and `color`
    standard packages.

    Without the `full` option, code is formatted as one ``Verbatim``
    environment, like this:

    .. sourcecode:: latex

        \begin{Verbatim}[commandchars=\\\{\}]
        \PY{k}{def }\PY{n+nf}{foo}(\PY{n}{bar}):
            \PY{k}{pass}
        \end{Verbatim}

    Wrapping can be disabled using the `nowrap` option.

    The special command used here (``\PY``) and all the other macros it needs
    are output by the `get_style_defs` method.

    With the `full` option, a complete LaTeX document is output, including
    the command definitions in the preamble.

    The `get_style_defs()` method of a `LatexFormatter` returns a string
    containing ``\def`` commands defining the macros needed inside the
    ``Verbatim`` environments.

    Additional options accepted:

    `nowrap`
        If set to ``True``, don't wrap the tokens at all, not even inside a
        ``\begin{Verbatim}`` environment. This disables most other options
        (default: ``False``).

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``).

    `full`
        Tells the formatter to output a "full" document, i.e. a complete
        self-contained document (default: ``False``).

    `title`
        If `full` is true, the title that should be used to caption the
        document (default: ``''``).

    `docclass`
        If the `full` option is enabled, this is the document class to use
        (default: ``'article'``).

    `preamble`
        If the `full` option is enabled, this can be further preamble commands,
        e.g. ``\usepackage`` (default: ``''``).

    `linenos`
        If set to ``True``, output line numbers (default: ``False``).

    `linenostart`
        The line number for the first line (default: ``1``).

    `linenostep`
        If set to a number n > 1, only every nth line number is printed.

    `verboptions`
        Additional options given to the Verbatim environment (see the *fancyvrb*
        docs for possible values) (default: ``''``).

    `commandprefix`
        The LaTeX commands used to produce colored output are constructed
        using this prefix and some letters (default: ``'PY'``).

        .. versionadded:: 0.7
        .. versionchanged:: 0.10
           The default is now ``'PY'`` instead of ``'C'``.

    `texcomments`
        If set to ``True``, enables LaTeX comment lines.  That is, LaTex markup
        in comment tokens is not escaped so that LaTeX can render it (default:
        ``False``).

        .. versionadded:: 1.2

    `mathescape`
        If set to ``True``, enables LaTeX math mode escape in comments. That
        is, ``'$...$'`` inside a comment will trigger math mode (default:
        ``False``).

        .. versionadded:: 1.2

    `escapeinside`
        If set to a string of length 2, enables escaping to LaTeX. Text
        delimited by these 2 characters is read as LaTeX code and
        typeset accordingly. It has no effect in string literals. It has
        no effect in comments if `texcomments` or `mathescape` is
        set. (default: ``''``).

        .. versionadded:: 2.0

    `envname`
        Allows you to pick an alternative environment name replacing Verbatim.
        The alternate environment still has to support Verbatim's option syntax.
        (default: ``'Verbatim'``).

        .. versionadded:: 2.0
    """
    name = 'LaTeX'
    aliases = ['latex', 'tex']
    filenames = ['*.tex']

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        self.nowrap = get_bool_opt(options, 'nowrap', False)
        self.docclass = options.get('docclass', 'article')
        self.preamble = options.get('preamble', '')
        self.linenos = get_bool_opt(options, 'linenos', False)
        self.linenostart = abs(get_int_opt(options, 'linenostart', 1))
        self.linenostep = abs(get_int_opt(options, 'linenostep', 1))
        self.verboptions = options.get('verboptions', '')
        self.nobackground = get_bool_opt(options, 'nobackground', False)
        self.commandprefix = options.get('commandprefix', 'PY')
        self.texcomments = get_bool_opt(options, 'texcomments', False)
        self.mathescape = get_bool_opt(options, 'mathescape', False)
        self.escapeinside = options.get('escapeinside', '')
        if len(self.escapeinside) == 2:
            self.left = self.escapeinside[0]
            self.right = self.escapeinside[1]
        else:
            self.escapeinside = ''
        self.envname = options.get('envname', 'Verbatim')

        self._create_stylesheet()

    def _create_stylesheet(self):
        t2n = self.ttype2name = {Token: ''}
        c2d = self.cmd2def = {}
        cp = self.commandprefix

        def rgbcolor(col):
            if col:
                return ','.join(['%.2f' % (int(col[i] + col[i + 1], 16) / 255.0)
                                 for i in (0, 2, 4)])
            else:
                return '1,1,1'

        for ttype, ndef in self.style:
            name = _get_ttype_name(ttype)
            cmndef = ''
            if ndef['bold']:
                cmndef += r'\let\$$@bf=\textbf'
            if ndef['italic']:
                cmndef += r'\let\$$@it=\textit'
            if ndef['underline']:
                cmndef += r'\let\$$@ul=\underline'
            if ndef['roman']:
                cmndef += r'\let\$$@ff=\textrm'
            if ndef['sans']:
                cmndef += r'\let\$$@ff=\textsf'
            if ndef['mono']:
                cmndef += r'\let\$$@ff=\textsf'
            if ndef['color']:
                cmndef += (r'\def\$$@tc##1{\textcolor[rgb]{%s}{##1}}' %
                           rgbcolor(ndef['color']))
            if ndef['border']:
                cmndef += (r'\def\$$@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}'
                           r'\fcolorbox[rgb]{%s}{%s}{\strut ##1}}}' %
                           (rgbcolor(ndef['border']),
                            rgbcolor(ndef['bgcolor'])))
            elif ndef['bgcolor']:
                cmndef += (r'\def\$$@bc##1{{\setlength{\fboxsep}{0pt}'
                           r'\colorbox[rgb]{%s}{\strut ##1}}}' %
                           rgbcolor(ndef['bgcolor']))
            if cmndef == '':
                continue
            cmndef = cmndef.replace('$$', cp)
            t2n[ttype] = name
            c2d[name] = cmndef

    def get_style_defs(self, arg=''):
        """
        Return the command sequences needed to define the commands
        used to format text in the verbatim environment. ``arg`` is ignored.
        """
        cp = self.commandprefix
        styles = []
        for name, definition in self.cmd2def.items():
            styles.append(r'\@namedef{%s@tok@%s}{%s}' % (cp, name, definition))
        return STYLE_TEMPLATE % {'cp': self.commandprefix,
                                 'styles': '\n'.join(styles)}

    def format_unencoded(self, tokensource, outfile):
        # TODO: add support for background colors
        t2n = self.ttype2name
        cp = self.commandprefix

        if self.full:
            realoutfile = outfile
            outfile = StringIO()

        if not self.nowrap:
            outfile.write('\\begin{' + self.envname + '}[commandchars=\\\\\\{\\}')
            if self.linenos:
                start, step = self.linenostart, self.linenostep
                outfile.write(',numbers=left' +
                              (start and ',firstnumber=%d' % start or '') +
                              (step and ',stepnumber=%d' % step or ''))
            if self.mathescape or self.texcomments or self.escapeinside:
                outfile.write(',codes={\\catcode`\\$=3\\catcode`\\^=7'
                              '\\catcode`\\_=8\\relax}')
            if self.verboptions:
                outfile.write(',' + self.verboptions)
            outfile.write(']\n')

        for ttype, value in tokensource:
            if ttype in Token.Comment:
                if self.texcomments:
                    # Try to guess comment starting lexeme and escape it ...
                    start = value[0:1]
                    for i in range(1, len(value)):
                        if start[0] != value[i]:
                            break
                        start += value[i]

                    value = value[len(start):]
                    start = escape_tex(start, cp)

                    # ... but do not escape inside comment.
                    value = start + value
                elif self.mathescape:
                    # Only escape parts not inside a math environment.
                    parts = value.split('$')
                    in_math = False
                    for i, part in enumerate(parts):
                        if not in_math:
                            parts[i] = escape_tex(part, cp)
                        in_math = not in_math
                    value = '$'.join(parts)
                elif self.escapeinside:
                    text = value
                    value = ''
                    while text:
                        a, sep1, text = text.partition(self.left)
                        if sep1:
                            b, sep2, text = text.partition(self.right)
                            if sep2:
                                value += escape_tex(a, cp) + b
                            else:
                                value += escape_tex(a + sep1 + b, cp)
                        else:
                            value += escape_tex(a, cp)
                else:
                    value = escape_tex(value, cp)
            elif ttype not in Token.Escape:
                value = escape_tex(value, cp)
            styles = []
            while ttype is not Token:
                try:
                    styles.append(t2n[ttype])
                except KeyError:
                    # not in current style
                    styles.append(_get_ttype_name(ttype))
                ttype = ttype.parent
            styleval = '+'.join(reversed(styles))
            if styleval:
                spl = value.split('\n')
                for line in spl[:-1]:
                    if line:
                        outfile.write("\\%s{%s}{%s}" % (cp, styleval, line))
                    outfile.write('\n')
                if spl[-1]:
                    outfile.write("\\%s{%s}{%s}" % (cp, styleval, spl[-1]))
            else:
                outfile.write(value)

        if not self.nowrap:
            outfile.write('\\end{' + self.envname + '}\n')

        if self.full:
            encoding = self.encoding or 'utf8'
            # map known existings encodings from LaTeX distribution
            encoding = {
                'utf_8': 'utf8',
                'latin_1': 'latin1',
                'iso_8859_1': 'latin1',
            }.get(encoding.replace('-', '_'), encoding)
            realoutfile.write(DOC_TEMPLATE %
                dict(docclass  = self.docclass,
                     preamble  = self.preamble,
                     title     = self.title,
                     encoding  = encoding,
                     styledefs = self.get_style_defs(),
                     code      = outfile.getvalue()))


class LatexEmbeddedLexer(Lexer):
    """
    This lexer takes one lexer as argument, the lexer for the language
    being formatted, and the left and right delimiters for escaped text.

    First everything is scanned using the language lexer to obtain
    strings and comments. All other consecutive tokens are merged and
    the resulting text is scanned for escaped segments, which are given
    the Token.Escape type. Finally text that is not escaped is scanned
    again with the language lexer.
    """
    def __init__(self, left, right, lang, **options):
        self.left = left
        self.right = right
        self.lang = lang
        Lexer.__init__(self, **options)

    def get_tokens_unprocessed(self, text):
        # find and remove all the escape tokens (replace with an empty string)
        # this is very similar to DelegatingLexer.get_tokens_unprocessed.
        buffered = ''
        insertions = []
        insertion_buf = []
        for i, t, v in self._find_safe_escape_tokens(text):
            if t is None:
                if insertion_buf:
                    insertions.append((len(buffered), insertion_buf))
                    insertion_buf = []
                buffered += v
            else:
                insertion_buf.append((i, t, v))
        if insertion_buf:
            insertions.append((len(buffered), insertion_buf))
        return do_insertions(insertions,
                             self.lang.get_tokens_unprocessed(buffered))

    def _find_safe_escape_tokens(self, text):
        """ find escape tokens that are not in strings or comments """
        for i, t, v in self._filter_to(
            self.lang.get_tokens_unprocessed(text),
            lambda t: t in Token.Comment or t in Token.String
        ):
            if t is None:
                for i2, t2, v2 in self._find_escape_tokens(v):
                    yield i + i2, t2, v2
            else:
                yield i, None, v

    def _filter_to(self, it, pred):
        """ Keep only the tokens that match `pred`, merge the others together """
        buf = ''
        idx = 0
        for i, t, v in it:
            if pred(t):
                if buf:
                    yield idx, None, buf
                    buf = ''
                yield i, t, v
            else:
                if not buf:
                    idx = i
                buf += v
        if buf:
            yield idx, None, buf

    def _find_escape_tokens(self, text):
        """ Find escape tokens within text, give token=None otherwise """
        index = 0
        while text:
            a, sep1, text = text.partition(self.left)
            if a:
                yield index, None, a
                index += len(a)
            if sep1:
                b, sep2, text = text.partition(self.right)
                if sep2:
                    yield index + len(sep1), Token.Escape, b
                    index += len(sep1) + len(b) + len(sep2)
                else:
                    yield index, Token.Error, sep1
                    index += len(sep1)
                    text = b


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/html.py
# ========================================================
"""
    pygments.formatters.html
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for HTML output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import functools
import os
import sys
import os.path
from io import StringIO

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.token import Token, Text, STANDARD_TYPES
from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt

try:
    import ctags
except ImportError:
    ctags = None

__all__ = ['HtmlFormatter']


_escape_html_table = {
    ord('&'): '&amp;',
    ord('<'): '&lt;',
    ord('>'): '&gt;',
    ord('"'): '&quot;',
    ord("'"): '&#39;',
}


def escape_html(text, table=_escape_html_table):
    """Escape &, <, > as well as single and double quotes for HTML."""
    return text.translate(table)


def webify(color):
    if color.startswith('calc') or color.startswith('var'):
        return color
    else:
        return '#' + color


def _get_ttype_class(ttype):
    fname = STANDARD_TYPES.get(ttype)
    if fname:
        return fname
    aname = ''
    while fname is None:
        aname = '-' + ttype[-1] + aname
        ttype = ttype.parent
        fname = STANDARD_TYPES.get(ttype)
    return fname + aname


CSSFILE_TEMPLATE = '''\
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
%(styledefs)s
'''

DOC_HEADER = '''\
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title>%(title)s</title>
  <meta http-equiv="content-type" content="text/html; charset=%(encoding)s">
  <style type="text/css">
''' + CSSFILE_TEMPLATE + '''
  </style>
</head>
<body>
<h2>%(title)s</h2>

'''

DOC_HEADER_EXTERNALCSS = '''\
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">

<html>
<head>
  <title>%(title)s</title>
  <meta http-equiv="content-type" content="text/html; charset=%(encoding)s">
  <link rel="stylesheet" href="%(cssfile)s" type="text/css">
</head>
<body>
<h2>%(title)s</h2>

'''

DOC_FOOTER = '''\
</body>
</html>
'''


class HtmlFormatter(Formatter):
    r"""
    Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed
    in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option).
    The ``<div>``'s CSS class can be set by the `cssclass` option.

    If the `linenos` option is set to ``"table"``, the ``<pre>`` is
    additionally wrapped inside a ``<table>`` which has one row and two
    cells: one containing the line numbers and one containing the code.
    Example:

    .. sourcecode:: html

        <div class="highlight" >
        <table><tr>
          <td class="linenos" title="click to toggle"
            onclick="with (this.firstChild.style)
                     { display = (display == '') ? 'none' : '' }">
            <pre>1
            2</pre>
          </td>
          <td class="code">
            <pre><span class="Ke">def </span><span class="NaFu">foo</span>(bar):
              <span class="Ke">pass</span>
            </pre>
          </td>
        </tr></table></div>

    (whitespace added to improve clarity).

    A list of lines can be specified using the `hl_lines` option to make these
    lines highlighted (as of Pygments 0.11).

    With the `full` option, a complete HTML 4 document is output, including
    the style definitions inside a ``<style>`` tag, or in a separate file if
    the `cssfile` option is given.

    When `tagsfile` is set to the path of a ctags index file, it is used to
    generate hyperlinks from names to their definition.  You must enable
    `lineanchors` and run ctags with the `-n` option for this to work.  The
    `python-ctags` module from PyPI must be installed to use this feature;
    otherwise a `RuntimeError` will be raised.

    The `get_style_defs(arg='')` method of a `HtmlFormatter` returns a string
    containing CSS rules for the CSS classes used by the formatter. The
    argument `arg` can be used to specify additional CSS selectors that
    are prepended to the classes. A call `fmter.get_style_defs('td .code')`
    would result in the following CSS classes:

    .. sourcecode:: css

        td .code .kw { font-weight: bold; color: #00FF00 }
        td .code .cm { color: #999999 }
        ...

    If you have Pygments 0.6 or higher, you can also pass a list or tuple to the
    `get_style_defs()` method to request multiple prefixes for the tokens:

    .. sourcecode:: python

        formatter.get_style_defs(['div.syntax pre', 'pre.syntax'])

    The output would then look like this:

    .. sourcecode:: css

        div.syntax pre .kw,
        pre.syntax .kw { font-weight: bold; color: #00FF00 }
        div.syntax pre .cm,
        pre.syntax .cm { color: #999999 }
        ...

    Additional options accepted:

    `nowrap`
        If set to ``True``, don't add a ``<pre>`` and a ``<div>`` tag
        around the tokens. This disables most other options (default: ``False``).

    `full`
        Tells the formatter to output a "full" document, i.e. a complete
        self-contained document (default: ``False``).

    `title`
        If `full` is true, the title that should be used to caption the
        document (default: ``''``).

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``). This option has no effect if the `cssfile`
        and `noclobber_cssfile` option are given and the file specified in
        `cssfile` exists.

    `noclasses`
        If set to true, token ``<span>`` tags (as well as line number elements)
        will not use CSS classes, but inline styles. This is not recommended
        for larger pieces of code since it increases output size by quite a bit
        (default: ``False``).

    `classprefix`
        Since the token types use relatively short class names, they may clash
        with some of your own class names. In this case you can use the
        `classprefix` option to give a string to prepend to all Pygments-generated
        CSS class names for token types.
        Note that this option also affects the output of `get_style_defs()`.

    `cssclass`
        CSS class for the wrapping ``<div>`` tag (default: ``'highlight'``).
        If you set this option, the default selector for `get_style_defs()`
        will be this class.

        .. versionadded:: 0.9
           If you select the ``'table'`` line numbers, the wrapping table will
           have a CSS class of this string plus ``'table'``, the default is
           accordingly ``'highlighttable'``.

    `cssstyles`
        Inline CSS styles for the wrapping ``<div>`` tag (default: ``''``).

    `prestyles`
        Inline CSS styles for the ``<pre>`` tag (default: ``''``).

        .. versionadded:: 0.11

    `cssfile`
        If the `full` option is true and this option is given, it must be the
        name of an external file. If the filename does not include an absolute
        path, the file's path will be assumed to be relative to the main output
        file's path, if the latter can be found. The stylesheet is then written
        to this file instead of the HTML file.

        .. versionadded:: 0.6

    `noclobber_cssfile`
        If `cssfile` is given and the specified file exists, the css file will
        not be overwritten. This allows the use of the `full` option in
        combination with a user specified css file. Default is ``False``.

        .. versionadded:: 1.1

    `linenos`
        If set to ``'table'``, output line numbers as a table with two cells,
        one containing the line numbers, the other the whole code.  This is
        copy-and-paste-friendly, but may cause alignment problems with some
        browsers or fonts.  If set to ``'inline'``, the line numbers will be
        integrated in the ``<pre>`` tag that contains the code (that setting
        is *new in Pygments 0.8*).

        For compatibility with Pygments 0.7 and earlier, every true value
        except ``'inline'`` means the same as ``'table'`` (in particular, that
        means also ``True``).

        The default value is ``False``, which means no line numbers at all.

        **Note:** with the default ("table") line number mechanism, the line
        numbers and code can have different line heights in Internet Explorer
        unless you give the enclosing ``<pre>`` tags an explicit ``line-height``
        CSS property (you get the default line spacing with ``line-height:
        125%``).

    `hl_lines`
        Specify a list of lines to be highlighted. The line numbers are always
        relative to the input (i.e. the first line is line 1) and are
        independent of `linenostart`.

        .. versionadded:: 0.11

    `linenostart`
        The line number for the first line (default: ``1``).

    `linenostep`
        If set to a number n > 1, only every nth line number is printed.

    `linenospecial`
        If set to a number n > 0, every nth line number is given the CSS
        class ``"special"`` (default: ``0``).

    `nobackground`
        If set to ``True``, the formatter won't output the background color
        for the wrapping element (this automatically defaults to ``False``
        when there is no wrapping element [eg: no argument for the
        `get_syntax_defs` method given]) (default: ``False``).

        .. versionadded:: 0.6

    `lineseparator`
        This string is output between lines of code. It defaults to ``"\n"``,
        which is enough to break a line inside ``<pre>`` tags, but you can
        e.g. set it to ``"<br>"`` to get HTML line breaks.

        .. versionadded:: 0.7

    `lineanchors`
        If set to a nonempty string, e.g. ``foo``, the formatter will wrap each
        output line in an anchor tag with an ``id`` (and `name`) of ``foo-linenumber``.
        This allows easy linking to certain lines.

        .. versionadded:: 0.9

    `linespans`
        If set to a nonempty string, e.g. ``foo``, the formatter will wrap each
        output line in a span tag with an ``id`` of ``foo-linenumber``.
        This allows easy access to lines via javascript.

        .. versionadded:: 1.6

    `anchorlinenos`
        If set to `True`, will wrap line numbers in <a> tags. Used in
        combination with `linenos` and `lineanchors`.

    `tagsfile`
        If set to the path of a ctags file, wrap names in anchor tags that
        link to their definitions. `lineanchors` should be used, and the
        tags file should specify line numbers (see the `-n` option to ctags).

        .. versionadded:: 1.6

    `tagurlformat`
        A string formatting pattern used to generate links to ctags definitions.
        Available variables are `%(path)s`, `%(fname)s` and `%(fext)s`.
        Defaults to an empty string, resulting in just `#prefix-number` links.

        .. versionadded:: 1.6

    `filename`
        A string used to generate a filename when rendering ``<pre>`` blocks,
        for example if displaying source code. If `linenos` is set to
        ``'table'`` then the filename will be rendered in an initial row
        containing a single `<th>` which spans both columns.

        .. versionadded:: 2.1

    `wrapcode`
        Wrap the code inside ``<pre>`` blocks using ``<code>``, as recommended
        by the HTML5 specification.

        .. versionadded:: 2.4

    `debug_token_types`
        Add ``title`` attributes to all token ``<span>`` tags that show the
        name of the token.

        .. versionadded:: 2.10


    **Subclassing the HTML formatter**

    .. versionadded:: 0.7

    The HTML formatter is now built in a way that allows easy subclassing, thus
    customizing the output HTML code. The `format()` method calls
    `self._format_lines()` which returns a generator that yields tuples of ``(1,
    line)``, where the ``1`` indicates that the ``line`` is a line of the
    formatted source code.

    If the `nowrap` option is set, the generator is the iterated over and the
    resulting HTML is output.

    Otherwise, `format()` calls `self.wrap()`, which wraps the generator with
    other generators. These may add some HTML code to the one generated by
    `_format_lines()`, either by modifying the lines generated by the latter,
    then yielding them again with ``(1, line)``, and/or by yielding other HTML
    code before or after the lines, with ``(0, html)``. The distinction between
    source lines and other code makes it possible to wrap the generator multiple
    times.

    The default `wrap()` implementation adds a ``<div>`` and a ``<pre>`` tag.

    A custom `HtmlFormatter` subclass could look like this:

    .. sourcecode:: python

        class CodeHtmlFormatter(HtmlFormatter):

            def wrap(self, source, *, include_div):
                return self._wrap_code(source)

            def _wrap_code(self, source):
                yield 0, '<code>'
                for i, t in source:
                    if i == 1:
                        # it's a line of formatted code
                        t += '<br>'
                    yield i, t
                yield 0, '</code>'

    This results in wrapping the formatted lines with a ``<code>`` tag, where the
    source lines are broken using ``<br>`` tags.

    After calling `wrap()`, the `format()` method also adds the "line numbers"
    and/or "full document" wrappers if the respective options are set. Then, all
    HTML yielded by the wrapped generator is output.
    """

    name = 'HTML'
    aliases = ['html']
    filenames = ['*.html', '*.htm']

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        self.title = self._decodeifneeded(self.title)
        self.nowrap = get_bool_opt(options, 'nowrap', False)
        self.noclasses = get_bool_opt(options, 'noclasses', False)
        self.classprefix = options.get('classprefix', '')
        self.cssclass = self._decodeifneeded(options.get('cssclass', 'highlight'))
        self.cssstyles = self._decodeifneeded(options.get('cssstyles', ''))
        self.prestyles = self._decodeifneeded(options.get('prestyles', ''))
        self.cssfile = self._decodeifneeded(options.get('cssfile', ''))
        self.noclobber_cssfile = get_bool_opt(options, 'noclobber_cssfile', False)
        self.tagsfile = self._decodeifneeded(options.get('tagsfile', ''))
        self.tagurlformat = self._decodeifneeded(options.get('tagurlformat', ''))
        self.filename = self._decodeifneeded(options.get('filename', ''))
        self.wrapcode = get_bool_opt(options, 'wrapcode', False)
        self.span_element_openers = {}
        self.debug_token_types = get_bool_opt(options, 'debug_token_types', False)

        if self.tagsfile:
            if not ctags:
                raise RuntimeError('The "ctags" package must to be installed '
                                   'to be able to use the "tagsfile" feature.')
            self._ctags = ctags.CTags(self.tagsfile)

        linenos = options.get('linenos', False)
        if linenos == 'inline':
            self.linenos = 2
        elif linenos:
            # compatibility with <= 0.7
            self.linenos = 1
        else:
            self.linenos = 0
        self.linenostart = abs(get_int_opt(options, 'linenostart', 1))
        self.linenostep = abs(get_int_opt(options, 'linenostep', 1))
        self.linenospecial = abs(get_int_opt(options, 'linenospecial', 0))
        self.nobackground = get_bool_opt(options, 'nobackground', False)
        self.lineseparator = options.get('lineseparator', '\n')
        self.lineanchors = options.get('lineanchors', '')
        self.linespans = options.get('linespans', '')
        self.anchorlinenos = get_bool_opt(options, 'anchorlinenos', False)
        self.hl_lines = set()
        for lineno in get_list_opt(options, 'hl_lines', []):
            try:
                self.hl_lines.add(int(lineno))
            except ValueError:
                pass

        self._create_stylesheet()

    def _get_css_class(self, ttype):
        """Return the css class of this token type prefixed with
        the classprefix option."""
        ttypeclass = _get_ttype_class(ttype)
        if ttypeclass:
            return self.classprefix + ttypeclass
        return ''

    def _get_css_classes(self, ttype):
        """Return the CSS classes of this token type prefixed with the classprefix option."""
        cls = self._get_css_class(ttype)
        while ttype not in STANDARD_TYPES:
            ttype = ttype.parent
            cls = self._get_css_class(ttype) + ' ' + cls
        return cls or ''

    def _get_css_inline_styles(self, ttype):
        """Return the inline CSS styles for this token type."""
        cclass = self.ttype2class.get(ttype)
        while cclass is None:
            ttype = ttype.parent
            cclass = self.ttype2class.get(ttype)
        return cclass or ''

    def _create_stylesheet(self):
        t2c = self.ttype2class = {Token: ''}
        c2s = self.class2style = {}
        for ttype, ndef in self.style:
            name = self._get_css_class(ttype)
            style = ''
            if ndef['color']:
                style += 'color: %s; ' % webify(ndef['color'])
            if ndef['bold']:
                style += 'font-weight: bold; '
            if ndef['italic']:
                style += 'font-style: italic; '
            if ndef['underline']:
                style += 'text-decoration: underline; '
            if ndef['bgcolor']:
                style += 'background-color: %s; ' % webify(ndef['bgcolor'])
            if ndef['border']:
                style += 'border: 1px solid %s; ' % webify(ndef['border'])
            if style:
                t2c[ttype] = name
                # save len(ttype) to enable ordering the styles by
                # hierarchy (necessary for CSS cascading rules!)
                c2s[name] = (style[:-2], ttype, len(ttype))

    def get_style_defs(self, arg=None):
        """
        Return CSS style definitions for the classes produced by the current
        highlighting style. ``arg`` can be a string or list of selectors to
        insert before the token type classes.
        """
        style_lines = []

        style_lines.extend(self.get_linenos_style_defs())
        style_lines.extend(self.get_background_style_defs(arg))
        style_lines.extend(self.get_token_style_defs(arg))

        return '\n'.join(style_lines)

    def get_token_style_defs(self, arg=None):
        prefix = self.get_css_prefix(arg)

        styles = [
            (level, ttype, cls, style)
            for cls, (style, ttype, level) in self.class2style.items()
            if cls and style
        ]
        styles.sort()

        lines = [
            '%s { %s } /* %s */' % (prefix(cls), style, repr(ttype)[6:])
            for (level, ttype, cls, style) in styles
        ]

        return lines

    def get_background_style_defs(self, arg=None):
        prefix = self.get_css_prefix(arg)
        bg_color = self.style.background_color
        hl_color = self.style.highlight_color

        lines = []

        if arg and not self.nobackground and bg_color is not None:
            text_style = ''
            if Text in self.ttype2class:
                text_style = ' ' + self.class2style[self.ttype2class[Text]][0]
            lines.insert(
                0, '%s{ background: %s;%s }' % (
                    prefix(''), bg_color, text_style
                )
            )
        if hl_color is not None:
            lines.insert(
                0, '%s { background-color: %s }' % (prefix('hll'), hl_color)
            )

        return lines

    def get_linenos_style_defs(self):
        lines = [
            'pre { %s }' % self._pre_style,
            'td.linenos .normal { %s }' % self._linenos_style,
            'span.linenos { %s }' % self._linenos_style,
            'td.linenos .special { %s }' % self._linenos_special_style,
            'span.linenos.special { %s }' % self._linenos_special_style,
        ]

        return lines

    def get_css_prefix(self, arg):
        if arg is None:
            arg = ('cssclass' in self.options and '.'+self.cssclass or '')
        if isinstance(arg, str):
            args = [arg]
        else:
            args = list(arg)

        def prefix(cls):
            if cls:
                cls = '.' + cls
            tmp = []
            for arg in args:
                tmp.append((arg and arg + ' ' or '') + cls)
            return ', '.join(tmp)

        return prefix

    @property
    def _pre_style(self):
        return 'line-height: 125%;'

    @property
    def _linenos_style(self):
        return 'color: %s; background-color: %s; padding-left: 5px; padding-right: 5px;' % (
            self.style.line_number_color,
            self.style.line_number_background_color
        )

    @property
    def _linenos_special_style(self):
        return 'color: %s; background-color: %s; padding-left: 5px; padding-right: 5px;' % (
            self.style.line_number_special_color,
            self.style.line_number_special_background_color
        )

    def _decodeifneeded(self, value):
        if isinstance(value, bytes):
            if self.encoding:
                return value.decode(self.encoding)
            return value.decode()
        return value

    def _wrap_full(self, inner, outfile):
        if self.cssfile:
            if os.path.isabs(self.cssfile):
                # it's an absolute filename
                cssfilename = self.cssfile
            else:
                try:
                    filename = outfile.name
                    if not filename or filename[0] == '<':
                        # pseudo files, e.g. name == '<fdopen>'
                        raise AttributeError
                    cssfilename = os.path.join(os.path.dirname(filename),
                                               self.cssfile)
                except AttributeError:
                    print('Note: Cannot determine output file name, '
                          'using current directory as base for the CSS file name',
                          file=sys.stderr)
                    cssfilename = self.cssfile
            # write CSS file only if noclobber_cssfile isn't given as an option.
            try:
                if not os.path.exists(cssfilename) or not self.noclobber_cssfile:
                    with open(cssfilename, "w", encoding="utf-8") as cf:
                        cf.write(CSSFILE_TEMPLATE %
                                 {'styledefs': self.get_style_defs('body')})
            except OSError as err:
                err.strerror = 'Error writing CSS file: ' + err.strerror
                raise

            yield 0, (DOC_HEADER_EXTERNALCSS %
                      dict(title=self.title,
                           cssfile=self.cssfile,
                           encoding=self.encoding))
        else:
            yield 0, (DOC_HEADER %
                      dict(title=self.title,
                           styledefs=self.get_style_defs('body'),
                           encoding=self.encoding))

        yield from inner
        yield 0, DOC_FOOTER

    def _wrap_tablelinenos(self, inner):
        dummyoutfile = StringIO()
        lncount = 0
        for t, line in inner:
            if t:
                lncount += 1
            dummyoutfile.write(line)

        fl = self.linenostart
        mw = len(str(lncount + fl - 1))
        sp = self.linenospecial
        st = self.linenostep
        anchor_name = self.lineanchors or self.linespans
        aln = self.anchorlinenos
        nocls = self.noclasses

        lines = []

        for i in range(fl, fl+lncount):
            print_line = i % st == 0
            special_line = sp and i % sp == 0

            if print_line:
                line = '%*d' % (mw, i)
                if aln:
                    line = '<a href="#%s-%d">%s</a>' % (anchor_name, i, line)
            else:
                line = ' ' * mw

            if nocls:
                if special_line:
                    style = ' style="%s"' % self._linenos_special_style
                else:
                    style = ' style="%s"' % self._linenos_style
            else:
                if special_line:
                    style = ' class="special"'
                else:
                    style = ' class="normal"'

            if style:
                line = '<span%s>%s</span>' % (style, line)

            lines.append(line)

        ls = '\n'.join(lines)

        # If a filename was specified, we can't put it into the code table as it
        # would misalign the line numbers. Hence we emit a separate row for it.
        filename_tr = ""
        if self.filename:
            filename_tr = (
                '<tr><th colspan="2" class="filename">'
                '<span class="filename">' + self.filename + '</span>'
                '</th></tr>')

        # in case you wonder about the seemingly redundant <div> here: since the
        # content in the other cell also is wrapped in a div, some browsers in
        # some configurations seem to mess up the formatting...
        yield 0, (f'<table class="{self.cssclass}table">' + filename_tr +
            '<tr><td class="linenos"><div class="linenodiv"><pre>' +
            ls + '</pre></div></td><td class="code">')
        yield 0, '<div>'
        yield 0, dummyoutfile.getvalue()
        yield 0, '</div>'
        yield 0, '</td></tr></table>'


    def _wrap_inlinelinenos(self, inner):
        # need a list of lines since we need the width of a single number :(
        inner_lines = list(inner)
        sp = self.linenospecial
        st = self.linenostep
        num = self.linenostart
        mw = len(str(len(inner_lines) + num - 1))
        anchor_name = self.lineanchors or self.linespans
        aln = self.anchorlinenos
        nocls = self.noclasses

        for _, inner_line in inner_lines:
            print_line = num % st == 0
            special_line = sp and num % sp == 0

            if print_line:
                line = '%*d' % (mw, num)
            else:
                line = ' ' * mw

            if nocls:
                if special_line:
                    style = ' style="%s"' % self._linenos_special_style
                else:
                    style = ' style="%s"' % self._linenos_style
            else:
                if special_line:
                    style = ' class="linenos special"'
                else:
                    style = ' class="linenos"'

            if style:
                linenos = '<span%s>%s</span>' % (style, line)
            else:
                linenos = line

            if aln:
                yield 1, ('<a href="#%s-%d">%s</a>' % (anchor_name, num, linenos) +
                          inner_line)
            else:
                yield 1, linenos + inner_line
            num += 1

    def _wrap_lineanchors(self, inner):
        s = self.lineanchors
        # subtract 1 since we have to increment i *before* yielding
        i = self.linenostart - 1
        for t, line in inner:
            if t:
                i += 1
                href = "" if self.linenos else ' href="#%s-%d"' % (s, i)
                yield 1, '<a id="%s-%d" name="%s-%d"%s></a>' % (s, i, s, i, href) + line
            else:
                yield 0, line

    def _wrap_linespans(self, inner):
        s = self.linespans
        i = self.linenostart - 1
        for t, line in inner:
            if t:
                i += 1
                yield 1, '<span id="%s-%d">%s</span>' % (s, i, line)
            else:
                yield 0, line

    def _wrap_div(self, inner):
        style = []
        if (self.noclasses and not self.nobackground and
                self.style.background_color is not None):
            style.append('background: %s' % (self.style.background_color,))
        if self.cssstyles:
            style.append(self.cssstyles)
        style = '; '.join(style)

        yield 0, ('<div' + (self.cssclass and ' class="%s"' % self.cssclass) +
                  (style and (' style="%s"' % style)) + '>')
        yield from inner
        yield 0, '</div>\n'

    def _wrap_pre(self, inner):
        style = []
        if self.prestyles:
            style.append(self.prestyles)
        if self.noclasses:
            style.append(self._pre_style)
        style = '; '.join(style)

        if self.filename and self.linenos != 1:
            yield 0, ('<span class="filename">' + self.filename + '</span>')

        # the empty span here is to keep leading empty lines from being
        # ignored by HTML parsers
        yield 0, ('<pre' + (style and ' style="%s"' % style) + '><span></span>')
        yield from inner
        yield 0, '</pre>'

    def _wrap_code(self, inner):
        yield 0, '<code>'
        yield from inner
        yield 0, '</code>'

    @functools.lru_cache(maxsize=100)
    def _translate_parts(self, value):
        """HTML-escape a value and split it by newlines."""
        return value.translate(_escape_html_table).split('\n')

    def _format_lines(self, tokensource):
        """
        Just format the tokens, without any wrapping tags.
        Yield individual lines.
        """
        nocls = self.noclasses
        lsep = self.lineseparator
        tagsfile = self.tagsfile

        lspan = ''
        line = []
        for ttype, value in tokensource:
            try:
                cspan = self.span_element_openers[ttype]
            except KeyError:
                title = ' title="%s"' % '.'.join(ttype) if self.debug_token_types else ''
                if nocls:
                    css_style = self._get_css_inline_styles(ttype)
                    if css_style:
                        css_style = self.class2style[css_style][0]
                        cspan = '<span style="%s"%s>' % (css_style, title)
                    else:
                        cspan = ''
                else:
                    css_class = self._get_css_classes(ttype)
                    if css_class:
                        cspan = '<span class="%s"%s>' % (css_class, title)
                    else:
                        cspan = ''
                self.span_element_openers[ttype] = cspan

            parts = self._translate_parts(value)

            if tagsfile and ttype in Token.Name:
                filename, linenumber = self._lookup_ctag(value)
                if linenumber:
                    base, filename = os.path.split(filename)
                    if base:
                        base += '/'
                    filename, extension = os.path.splitext(filename)
                    url = self.tagurlformat % {'path': base, 'fname': filename,
                                               'fext': extension}
                    parts[0] = "<a href=\"%s#%s-%d\">%s" % \
                        (url, self.lineanchors, linenumber, parts[0])
                    parts[-1] = parts[-1] + "</a>"

            # for all but the last line
            for part in parts[:-1]:
                if line:
                    # Also check for part being non-empty, so we avoid creating
                    # empty <span> tags
                    if lspan != cspan and part:
                        line.extend(((lspan and '</span>'), cspan, part,
                                     (cspan and '</span>'), lsep))
                    else:  # both are the same, or the current part was empty
                        line.extend((part, (lspan and '</span>'), lsep))
                    yield 1, ''.join(line)
                    line = []
                elif part:
                    yield 1, ''.join((cspan, part, (cspan and '</span>'), lsep))
                else:
                    yield 1, lsep
            # for the last line
            if line and parts[-1]:
                if lspan != cspan:
                    line.extend(((lspan and '</span>'), cspan, parts[-1]))
                    lspan = cspan
                else:
                    line.append(parts[-1])
            elif parts[-1]:
                line = [cspan, parts[-1]]
                lspan = cspan
            # else we neither have to open a new span nor set lspan

        if line:
            line.extend(((lspan and '</span>'), lsep))
            yield 1, ''.join(line)

    def _lookup_ctag(self, token):
        entry = ctags.TagEntry()
        if self._ctags.find(entry, token.encode(), 0):
            return entry['file'], entry['lineNumber']
        else:
            return None, None

    def _highlight_lines(self, tokensource):
        """
        Highlighted the lines specified in the `hl_lines` option by
        post-processing the token stream coming from `_format_lines`.
        """
        hls = self.hl_lines

        for i, (t, value) in enumerate(tokensource):
            if t != 1:
                yield t, value
            if i + 1 in hls:  # i + 1 because Python indexes start at 0
                if self.noclasses:
                    style = ''
                    if self.style.highlight_color is not None:
                        style = (' style="background-color: %s"' %
                                 (self.style.highlight_color,))
                    yield 1, '<span%s>%s</span>' % (style, value)
                else:
                    yield 1, '<span class="hll">%s</span>' % value
            else:
                yield 1, value

    def wrap(self, source):
        """
        Wrap the ``source``, which is a generator yielding
        individual lines, in custom generators. See docstring
        for `format`. Can be overridden.
        """

        output = source
        if self.wrapcode:
            output = self._wrap_code(output)

        output = self._wrap_pre(output)

        return output

    def format_unencoded(self, tokensource, outfile):
        """
        The formatting process uses several nested generators; which of
        them are used is determined by the user's options.

        Each generator should take at least one argument, ``inner``,
        and wrap the pieces of text generated by this.

        Always yield 2-tuples: (code, text). If "code" is 1, the text
        is part of the original tokensource being highlighted, if it's
        0, the text is some piece of wrapping. This makes it possible to
        use several different wrappers that process the original source
        linewise, e.g. line number generators.
        """
        source = self._format_lines(tokensource)

        # As a special case, we wrap line numbers before line highlighting
        # so the line numbers get wrapped in the highlighting tag.
        if not self.nowrap and self.linenos == 2:
            source = self._wrap_inlinelinenos(source)

        if self.hl_lines:
            source = self._highlight_lines(source)

        if not self.nowrap:
            if self.lineanchors:
                source = self._wrap_lineanchors(source)
            if self.linespans:
                source = self._wrap_linespans(source)
            source = self.wrap(source)
            if self.linenos == 1:
                source = self._wrap_tablelinenos(source)
            source = self._wrap_div(source)
            if self.full:
                source = self._wrap_full(source, outfile)

        for t, piece in source:
            outfile.write(piece)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/img.py
# ========================================================
"""
    pygments.formatters.img
    ~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for Pixmap output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import os
import sys

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \
    get_choice_opt

import subprocess

# Import this carefully
try:
    from PIL import Image, ImageDraw, ImageFont
    pil_available = True
except ImportError:
    pil_available = False

try:
    import _winreg
except ImportError:
    try:
        import winreg as _winreg
    except ImportError:
        _winreg = None

__all__ = ['ImageFormatter', 'GifImageFormatter', 'JpgImageFormatter',
           'BmpImageFormatter']


# For some unknown reason every font calls it something different
STYLES = {
    'NORMAL':     ['', 'Roman', 'Book', 'Normal', 'Regular', 'Medium'],
    'ITALIC':     ['Oblique', 'Italic'],
    'BOLD':       ['Bold'],
    'BOLDITALIC': ['Bold Oblique', 'Bold Italic'],
}

# A sane default for modern systems
DEFAULT_FONT_NAME_NIX = 'DejaVu Sans Mono'
DEFAULT_FONT_NAME_WIN = 'Courier New'
DEFAULT_FONT_NAME_MAC = 'Menlo'


class PilNotAvailable(ImportError):
    """When Python imaging library is not available"""


class FontNotFound(Exception):
    """When there are no usable fonts specified"""


class FontManager:
    """
    Manages a set of fonts: normal, italic, bold, etc...
    """

    def __init__(self, font_name, font_size=14):
        self.font_name = font_name
        self.font_size = font_size
        self.fonts = {}
        self.encoding = None
        if sys.platform.startswith('win'):
            if not font_name:
                self.font_name = DEFAULT_FONT_NAME_WIN
            self._create_win()
        elif sys.platform.startswith('darwin'):
            if not font_name:
                self.font_name = DEFAULT_FONT_NAME_MAC
            self._create_mac()
        else:
            if not font_name:
                self.font_name = DEFAULT_FONT_NAME_NIX
            self._create_nix()

    def _get_nix_font_path(self, name, style):
        proc = subprocess.Popen(['fc-list', "%s:style=%s" % (name, style), 'file'],
                                stdout=subprocess.PIPE, stderr=None)
        stdout, _ = proc.communicate()
        if proc.returncode == 0:
            lines = stdout.splitlines()
            for line in lines:
                if line.startswith(b'Fontconfig warning:'):
                    continue
                path = line.decode().strip().strip(':')
                if path:
                    return path
            return None

    def _create_nix(self):
        for name in STYLES['NORMAL']:
            path = self._get_nix_font_path(self.font_name, name)
            if path is not None:
                self.fonts['NORMAL'] = ImageFont.truetype(path, self.font_size)
                break
        else:
            raise FontNotFound('No usable fonts named: "%s"' %
                               self.font_name)
        for style in ('ITALIC', 'BOLD', 'BOLDITALIC'):
            for stylename in STYLES[style]:
                path = self._get_nix_font_path(self.font_name, stylename)
                if path is not None:
                    self.fonts[style] = ImageFont.truetype(path, self.font_size)
                    break
            else:
                if style == 'BOLDITALIC':
                    self.fonts[style] = self.fonts['BOLD']
                else:
                    self.fonts[style] = self.fonts['NORMAL']

    def _get_mac_font_path(self, font_map, name, style):
        return font_map.get((name + ' ' + style).strip().lower())

    def _create_mac(self):
        font_map = {}
        for font_dir in (os.path.join(os.getenv("HOME"), 'Library/Fonts/'),
                         '/Library/Fonts/', '/System/Library/Fonts/'):
            font_map.update(
                (os.path.splitext(f)[0].lower(), os.path.join(font_dir, f))
                for f in os.listdir(font_dir)
                if f.lower().endswith(('ttf', 'ttc')))

        for name in STYLES['NORMAL']:
            path = self._get_mac_font_path(font_map, self.font_name, name)
            if path is not None:
                self.fonts['NORMAL'] = ImageFont.truetype(path, self.font_size)
                break
        else:
            raise FontNotFound('No usable fonts named: "%s"' %
                               self.font_name)
        for style in ('ITALIC', 'BOLD', 'BOLDITALIC'):
            for stylename in STYLES[style]:
                path = self._get_mac_font_path(font_map, self.font_name, stylename)
                if path is not None:
                    self.fonts[style] = ImageFont.truetype(path, self.font_size)
                    break
            else:
                if style == 'BOLDITALIC':
                    self.fonts[style] = self.fonts['BOLD']
                else:
                    self.fonts[style] = self.fonts['NORMAL']

    def _lookup_win(self, key, basename, styles, fail=False):
        for suffix in ('', ' (TrueType)'):
            for style in styles:
                try:
                    valname = '%s%s%s' % (basename, style and ' '+style, suffix)
                    val, _ = _winreg.QueryValueEx(key, valname)
                    return val
                except OSError:
                    continue
        else:
            if fail:
                raise FontNotFound('Font %s (%s) not found in registry' %
                                   (basename, styles[0]))
            return None

    def _create_win(self):
        lookuperror = None
        keynames = [ (_winreg.HKEY_CURRENT_USER, r'Software\Microsoft\Windows NT\CurrentVersion\Fonts'),
                     (_winreg.HKEY_CURRENT_USER, r'Software\Microsoft\Windows\CurrentVersion\Fonts'),
                     (_winreg.HKEY_LOCAL_MACHINE, r'Software\Microsoft\Windows NT\CurrentVersion\Fonts'),
                     (_winreg.HKEY_LOCAL_MACHINE, r'Software\Microsoft\Windows\CurrentVersion\Fonts') ]
        for keyname in keynames:
            try:
                key = _winreg.OpenKey(*keyname)
                try:
                    path = self._lookup_win(key, self.font_name, STYLES['NORMAL'], True)
                    self.fonts['NORMAL'] = ImageFont.truetype(path, self.font_size)
                    for style in ('ITALIC', 'BOLD', 'BOLDITALIC'):
                        path = self._lookup_win(key, self.font_name, STYLES[style])
                        if path:
                            self.fonts[style] = ImageFont.truetype(path, self.font_size)
                        else:
                            if style == 'BOLDITALIC':
                                self.fonts[style] = self.fonts['BOLD']
                            else:
                                self.fonts[style] = self.fonts['NORMAL']
                    return
                except FontNotFound as err:
                    lookuperror = err
                finally:
                    _winreg.CloseKey(key)
            except OSError:
                pass
        else:
            # If we get here, we checked all registry keys and had no luck
            # We can be in one of two situations now:
            # * All key lookups failed. In this case lookuperror is None and we
            #   will raise a generic error
            # * At least one lookup failed with a FontNotFound error. In this
            #   case, we will raise that as a more specific error
            if lookuperror:
                raise lookuperror
            raise FontNotFound('Can\'t open Windows font registry key')

    def get_char_size(self):
        """
        Get the character size.
        """
        return self.get_text_size('M')

    def get_text_size(self, text):
        """
        Get the text size (width, height).
        """
        font = self.fonts['NORMAL']
        if hasattr(font, 'getbbox'):  # Pillow >= 9.2.0
            return font.getbbox(text)[2:4]
        else:
            return font.getsize(text)

    def get_font(self, bold, oblique):
        """
        Get the font based on bold and italic flags.
        """
        if bold and oblique:
            return self.fonts['BOLDITALIC']
        elif bold:
            return self.fonts['BOLD']
        elif oblique:
            return self.fonts['ITALIC']
        else:
            return self.fonts['NORMAL']


class ImageFormatter(Formatter):
    """
    Create a PNG image from source code. This uses the Python Imaging Library to
    generate a pixmap from the source code.

    .. versionadded:: 0.10

    Additional options accepted:

    `image_format`
        An image format to output to that is recognised by PIL, these include:

        * "PNG" (default)
        * "JPEG"
        * "BMP"
        * "GIF"

    `line_pad`
        The extra spacing (in pixels) between each line of text.

        Default: 2

    `font_name`
        The font name to be used as the base font from which others, such as
        bold and italic fonts will be generated.  This really should be a
        monospace font to look sane.

        Default: "Courier New" on Windows, "Menlo" on Mac OS, and
                 "DejaVu Sans Mono" on \\*nix

    `font_size`
        The font size in points to be used.

        Default: 14

    `image_pad`
        The padding, in pixels to be used at each edge of the resulting image.

        Default: 10

    `line_numbers`
        Whether line numbers should be shown: True/False

        Default: True

    `line_number_start`
        The line number of the first line.

        Default: 1

    `line_number_step`
        The step used when printing line numbers.

        Default: 1

    `line_number_bg`
        The background colour (in "#123456" format) of the line number bar, or
        None to use the style background color.

        Default: "#eed"

    `line_number_fg`
        The text color of the line numbers (in "#123456"-like format).

        Default: "#886"

    `line_number_chars`
        The number of columns of line numbers allowable in the line number
        margin.

        Default: 2

    `line_number_bold`
        Whether line numbers will be bold: True/False

        Default: False

    `line_number_italic`
        Whether line numbers will be italicized: True/False

        Default: False

    `line_number_separator`
        Whether a line will be drawn between the line number area and the
        source code area: True/False

        Default: True

    `line_number_pad`
        The horizontal padding (in pixels) between the line number margin, and
        the source code area.

        Default: 6

    `hl_lines`
        Specify a list of lines to be highlighted.

        .. versionadded:: 1.2

        Default: empty list

    `hl_color`
        Specify the color for highlighting lines.

        .. versionadded:: 1.2

        Default: highlight color of the selected style
    """

    # Required by the pygments mapper
    name = 'img'
    aliases = ['img', 'IMG', 'png']
    filenames = ['*.png']

    unicodeoutput = False

    default_image_format = 'png'

    def __init__(self, **options):
        """
        See the class docstring for explanation of options.
        """
        if not pil_available:
            raise PilNotAvailable(
                'Python Imaging Library is required for this formatter')
        Formatter.__init__(self, **options)
        self.encoding = 'latin1'  # let pygments.format() do the right thing
        # Read the style
        self.styles = dict(self.style)
        if self.style.background_color is None:
            self.background_color = '#fff'
        else:
            self.background_color = self.style.background_color
        # Image options
        self.image_format = get_choice_opt(
            options, 'image_format', ['png', 'jpeg', 'gif', 'bmp'],
            self.default_image_format, normcase=True)
        self.image_pad = get_int_opt(options, 'image_pad', 10)
        self.line_pad = get_int_opt(options, 'line_pad', 2)
        # The fonts
        fontsize = get_int_opt(options, 'font_size', 14)
        self.fonts = FontManager(options.get('font_name', ''), fontsize)
        self.fontw, self.fonth = self.fonts.get_char_size()
        # Line number options
        self.line_number_fg = options.get('line_number_fg', '#886')
        self.line_number_bg = options.get('line_number_bg', '#eed')
        self.line_number_chars = get_int_opt(options,
                                             'line_number_chars', 2)
        self.line_number_bold = get_bool_opt(options,
                                             'line_number_bold', False)
        self.line_number_italic = get_bool_opt(options,
                                               'line_number_italic', False)
        self.line_number_pad = get_int_opt(options, 'line_number_pad', 6)
        self.line_numbers = get_bool_opt(options, 'line_numbers', True)
        self.line_number_separator = get_bool_opt(options,
                                                  'line_number_separator', True)
        self.line_number_step = get_int_opt(options, 'line_number_step', 1)
        self.line_number_start = get_int_opt(options, 'line_number_start', 1)
        if self.line_numbers:
            self.line_number_width = (self.fontw * self.line_number_chars +
                                      self.line_number_pad * 2)
        else:
            self.line_number_width = 0
        self.hl_lines = []
        hl_lines_str = get_list_opt(options, 'hl_lines', [])
        for line in hl_lines_str:
            try:
                self.hl_lines.append(int(line))
            except ValueError:
                pass
        self.hl_color = options.get('hl_color',
                                    self.style.highlight_color) or '#f90'
        self.drawables = []

    def get_style_defs(self, arg=''):
        raise NotImplementedError('The -S option is meaningless for the image '
                                  'formatter. Use -O style=<stylename> instead.')

    def _get_line_height(self):
        """
        Get the height of a line.
        """
        return self.fonth + self.line_pad

    def _get_line_y(self, lineno):
        """
        Get the Y coordinate of a line number.
        """
        return lineno * self._get_line_height() + self.image_pad

    def _get_char_width(self):
        """
        Get the width of a character.
        """
        return self.fontw

    def _get_char_x(self, linelength):
        """
        Get the X coordinate of a character position.
        """
        return linelength + self.image_pad + self.line_number_width

    def _get_text_pos(self, linelength, lineno):
        """
        Get the actual position for a character and line position.
        """
        return self._get_char_x(linelength), self._get_line_y(lineno)

    def _get_linenumber_pos(self, lineno):
        """
        Get the actual position for the start of a line number.
        """
        return (self.image_pad, self._get_line_y(lineno))

    def _get_text_color(self, style):
        """
        Get the correct color for the token from the style.
        """
        if style['color'] is not None:
            fill = '#' + style['color']
        else:
            fill = '#000'
        return fill

    def _get_text_bg_color(self, style):
        """
        Get the correct background color for the token from the style.
        """
        if style['bgcolor'] is not None:
            bg_color = '#' + style['bgcolor']
        else:
            bg_color = None
        return bg_color

    def _get_style_font(self, style):
        """
        Get the correct font for the style.
        """
        return self.fonts.get_font(style['bold'], style['italic'])

    def _get_image_size(self, maxlinelength, maxlineno):
        """
        Get the required image size.
        """
        return (self._get_char_x(maxlinelength) + self.image_pad,
                self._get_line_y(maxlineno + 0) + self.image_pad)

    def _draw_linenumber(self, posno, lineno):
        """
        Remember a line number drawable to paint later.
        """
        self._draw_text(
            self._get_linenumber_pos(posno),
            str(lineno).rjust(self.line_number_chars),
            font=self.fonts.get_font(self.line_number_bold,
                                     self.line_number_italic),
            text_fg=self.line_number_fg,
            text_bg=None,
        )

    def _draw_text(self, pos, text, font, text_fg, text_bg):
        """
        Remember a single drawable tuple to paint later.
        """
        self.drawables.append((pos, text, font, text_fg, text_bg))

    def _create_drawables(self, tokensource):
        """
        Create drawables for the token content.
        """
        lineno = charno = maxcharno = 0
        maxlinelength = linelength = 0
        for ttype, value in tokensource:
            while ttype not in self.styles:
                ttype = ttype.parent
            style = self.styles[ttype]
            # TODO: make sure tab expansion happens earlier in the chain.  It
            # really ought to be done on the input, as to do it right here is
            # quite complex.
            value = value.expandtabs(4)
            lines = value.splitlines(True)
            # print lines
            for i, line in enumerate(lines):
                temp = line.rstrip('\n')
                if temp:
                    self._draw_text(
                        self._get_text_pos(linelength, lineno),
                        temp,
                        font = self._get_style_font(style),
                        text_fg = self._get_text_color(style),
                        text_bg = self._get_text_bg_color(style),
                    )
                    temp_width, _ = self.fonts.get_text_size(temp)
                    linelength += temp_width
                    maxlinelength = max(maxlinelength, linelength)
                    charno += len(temp)
                    maxcharno = max(maxcharno, charno)
                if line.endswith('\n'):
                    # add a line for each extra line in the value
                    linelength = 0
                    charno = 0
                    lineno += 1
        self.maxlinelength = maxlinelength
        self.maxcharno = maxcharno
        self.maxlineno = lineno

    def _draw_line_numbers(self):
        """
        Create drawables for the line numbers.
        """
        if not self.line_numbers:
            return
        for p in range(self.maxlineno):
            n = p + self.line_number_start
            if (n % self.line_number_step) == 0:
                self._draw_linenumber(p, n)

    def _paint_line_number_bg(self, im):
        """
        Paint the line number background on the image.
        """
        if not self.line_numbers:
            return
        if self.line_number_fg is None:
            return
        draw = ImageDraw.Draw(im)
        recth = im.size[-1]
        rectw = self.image_pad + self.line_number_width - self.line_number_pad
        draw.rectangle([(0, 0), (rectw, recth)],
                       fill=self.line_number_bg)
        if self.line_number_separator:
            draw.line([(rectw, 0), (rectw, recth)], fill=self.line_number_fg)
        del draw

    def format(self, tokensource, outfile):
        """
        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
        tuples and write it into ``outfile``.

        This implementation calculates where it should draw each token on the
        pixmap, then calculates the required pixmap size and draws the items.
        """
        self._create_drawables(tokensource)
        self._draw_line_numbers()
        im = Image.new(
            'RGB',
            self._get_image_size(self.maxlinelength, self.maxlineno),
            self.background_color
        )
        self._paint_line_number_bg(im)
        draw = ImageDraw.Draw(im)
        # Highlight
        if self.hl_lines:
            x = self.image_pad + self.line_number_width - self.line_number_pad + 1
            recth = self._get_line_height()
            rectw = im.size[0] - x
            for linenumber in self.hl_lines:
                y = self._get_line_y(linenumber - 1)
                draw.rectangle([(x, y), (x + rectw, y + recth)],
                               fill=self.hl_color)
        for pos, value, font, text_fg, text_bg in self.drawables:
            if text_bg:
                text_size = draw.textsize(text=value, font=font)
                draw.rectangle([pos[0], pos[1], pos[0] + text_size[0], pos[1] + text_size[1]], fill=text_bg)
            draw.text(pos, value, font=font, fill=text_fg)
        im.save(outfile, self.image_format.upper())


# Add one formatter per format, so that the "-f gif" option gives the correct result
# when used in pygmentize.

class GifImageFormatter(ImageFormatter):
    """
    Create a GIF image from source code. This uses the Python Imaging Library to
    generate a pixmap from the source code.

    .. versionadded:: 1.0
    """

    name = 'img_gif'
    aliases = ['gif']
    filenames = ['*.gif']
    default_image_format = 'gif'


class JpgImageFormatter(ImageFormatter):
    """
    Create a JPEG image from source code. This uses the Python Imaging Library to
    generate a pixmap from the source code.

    .. versionadded:: 1.0
    """

    name = 'img_jpg'
    aliases = ['jpg', 'jpeg']
    filenames = ['*.jpg']
    default_image_format = 'jpeg'


class BmpImageFormatter(ImageFormatter):
    """
    Create a bitmap image from source code. This uses the Python Imaging Library to
    generate a pixmap from the source code.

    .. versionadded:: 1.0
    """

    name = 'img_bmp'
    aliases = ['bmp', 'bitmap']
    filenames = ['*.bmp']
    default_image_format = 'bmp'


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/_mapping.py
# ========================================================
# Automatically generated by scripts/gen_mapfiles.py.
# DO NOT EDIT BY HAND; run `tox -e mapfiles` instead.

FORMATTERS = {
    'BBCodeFormatter': ('pygments.formatters.bbcode', 'BBCode', ('bbcode', 'bb'), (), 'Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.'),
    'BmpImageFormatter': ('pygments.formatters.img', 'img_bmp', ('bmp', 'bitmap'), ('*.bmp',), 'Create a bitmap image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
    'GifImageFormatter': ('pygments.formatters.img', 'img_gif', ('gif',), ('*.gif',), 'Create a GIF image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
    'GroffFormatter': ('pygments.formatters.groff', 'groff', ('groff', 'troff', 'roff'), (), 'Format tokens with groff escapes to change their color and font style.'),
    'HtmlFormatter': ('pygments.formatters.html', 'HTML', ('html',), ('*.html', '*.htm'), "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option."),
    'IRCFormatter': ('pygments.formatters.irc', 'IRC', ('irc', 'IRC'), (), 'Format tokens with IRC color sequences'),
    'ImageFormatter': ('pygments.formatters.img', 'img', ('img', 'IMG', 'png'), ('*.png',), 'Create a PNG image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
    'JpgImageFormatter': ('pygments.formatters.img', 'img_jpg', ('jpg', 'jpeg'), ('*.jpg',), 'Create a JPEG image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
    'LatexFormatter': ('pygments.formatters.latex', 'LaTeX', ('latex', 'tex'), ('*.tex',), 'Format tokens as LaTeX code. This needs the `fancyvrb` and `color` standard packages.'),
    'NullFormatter': ('pygments.formatters.other', 'Text only', ('text', 'null'), ('*.txt',), 'Output the text unchanged without any formatting.'),
    'PangoMarkupFormatter': ('pygments.formatters.pangomarkup', 'Pango Markup', ('pango', 'pangomarkup'), (), 'Format tokens as Pango Markup code. It can then be rendered to an SVG.'),
    'RawTokenFormatter': ('pygments.formatters.other', 'Raw tokens', ('raw', 'tokens'), ('*.raw',), 'Format tokens as a raw representation for storing token streams.'),
    'RtfFormatter': ('pygments.formatters.rtf', 'RTF', ('rtf',), ('*.rtf',), 'Format tokens as RTF markup. This formatter automatically outputs full RTF documents with color information and other useful stuff. Perfect for Copy and Paste into Microsoft(R) Word(R) documents.'),
    'SvgFormatter': ('pygments.formatters.svg', 'SVG', ('svg',), ('*.svg',), 'Format tokens as an SVG graphics file.  This formatter is still experimental. Each line of code is a ``<text>`` element with explicit ``x`` and ``y`` coordinates containing ``<tspan>`` elements with the individual token styles.'),
    'Terminal256Formatter': ('pygments.formatters.terminal256', 'Terminal256', ('terminal256', 'console256', '256'), (), 'Format tokens with ANSI color sequences, for output in a 256-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.'),
    'TerminalFormatter': ('pygments.formatters.terminal', 'Terminal', ('terminal', 'console'), (), 'Format tokens with ANSI color sequences, for output in a text console. Color sequences are terminated at newlines, so that paging the output works correctly.'),
    'TerminalTrueColorFormatter': ('pygments.formatters.terminal256', 'TerminalTrueColor', ('terminal16m', 'console16m', '16m'), (), 'Format tokens with ANSI color sequences, for output in a true-color terminal or console.  Like in `TerminalFormatter` color sequences are terminated at newlines, so that paging the output works correctly.'),
    'TestcaseFormatter': ('pygments.formatters.other', 'Testcase', ('testcase',), (), 'Format tokens as appropriate for a new testcase.'),
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/__init__.py
# ========================================================
"""
    pygments.formatters
    ~~~~~~~~~~~~~~~~~~~

    Pygments formatters.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
import sys
import types
import fnmatch
from os.path import basename

from pip._vendor.pygments.formatters._mapping import FORMATTERS
from pip._vendor.pygments.plugin import find_plugin_formatters
from pip._vendor.pygments.util import ClassNotFound

__all__ = ['get_formatter_by_name', 'get_formatter_for_filename',
           'get_all_formatters', 'load_formatter_from_file'] + list(FORMATTERS)

_formatter_cache = {}  # classes by name
_pattern_cache = {}


def _fn_matches(fn, glob):
    """Return whether the supplied file name fn matches pattern filename."""
    if glob not in _pattern_cache:
        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))
        return pattern.match(fn)
    return _pattern_cache[glob].match(fn)


def _load_formatters(module_name):
    """Load a formatter (and all others in the module too)."""
    mod = __import__(module_name, None, None, ['__all__'])
    for formatter_name in mod.__all__:
        cls = getattr(mod, formatter_name)
        _formatter_cache[cls.name] = cls


def get_all_formatters():
    """Return a generator for all formatter classes."""
    # NB: this returns formatter classes, not info like get_all_lexers().
    for info in FORMATTERS.values():
        if info[1] not in _formatter_cache:
            _load_formatters(info[0])
        yield _formatter_cache[info[1]]
    for _, formatter in find_plugin_formatters():
        yield formatter


def find_formatter_class(alias):
    """Lookup a formatter by alias.

    Returns None if not found.
    """
    for module_name, name, aliases, _, _ in FORMATTERS.values():
        if alias in aliases:
            if name not in _formatter_cache:
                _load_formatters(module_name)
            return _formatter_cache[name]
    for _, cls in find_plugin_formatters():
        if alias in cls.aliases:
            return cls


def get_formatter_by_name(_alias, **options):
    """
    Return an instance of a :class:`.Formatter` subclass that has `alias` in its
    aliases list. The formatter is given the `options` at its instantiation.

    Will raise :exc:`pygments.util.ClassNotFound` if no formatter with that
    alias is found.
    """
    cls = find_formatter_class(_alias)
    if cls is None:
        raise ClassNotFound("no formatter found for name %r" % _alias)
    return cls(**options)


def load_formatter_from_file(filename, formattername="CustomFormatter", **options):
    """
    Return a `Formatter` subclass instance loaded from the provided file, relative
    to the current directory.

    The file is expected to contain a Formatter class named ``formattername``
    (by default, CustomFormatter). Users should be very careful with the input, because
    this method is equivalent to running ``eval()`` on the input file. The formatter is
    given the `options` at its instantiation.

    :exc:`pygments.util.ClassNotFound` is raised if there are any errors loading
    the formatter.

    .. versionadded:: 2.2
    """
    try:
        # This empty dict will contain the namespace for the exec'd file
        custom_namespace = {}
        with open(filename, 'rb') as f:
            exec(f.read(), custom_namespace)
        # Retrieve the class `formattername` from that namespace
        if formattername not in custom_namespace:
            raise ClassNotFound('no valid %s class found in %s' %
                                (formattername, filename))
        formatter_class = custom_namespace[formattername]
        # And finally instantiate it with the options
        return formatter_class(**options)
    except OSError as err:
        raise ClassNotFound('cannot read %s: %s' % (filename, err))
    except ClassNotFound:
        raise
    except Exception as err:
        raise ClassNotFound('error when loading custom formatter: %s' % err)


def get_formatter_for_filename(fn, **options):
    """
    Return a :class:`.Formatter` subclass instance that has a filename pattern
    matching `fn`. The formatter is given the `options` at its instantiation.

    Will raise :exc:`pygments.util.ClassNotFound` if no formatter for that filename
    is found.
    """
    fn = basename(fn)
    for modname, name, _, filenames, _ in FORMATTERS.values():
        for filename in filenames:
            if _fn_matches(fn, filename):
                if name not in _formatter_cache:
                    _load_formatters(modname)
                return _formatter_cache[name](**options)
    for cls in find_plugin_formatters():
        for filename in cls.filenames:
            if _fn_matches(fn, filename):
                return cls(**options)
    raise ClassNotFound("no formatter found for file name %r" % fn)


class _automodule(types.ModuleType):
    """Automatically import formatters."""

    def __getattr__(self, name):
        info = FORMATTERS.get(name)
        if info:
            _load_formatters(info[0])
            cls = _formatter_cache[info[1]]
            setattr(self, name, cls)
            return cls
        raise AttributeError(name)


oldmod = sys.modules[__name__]
newmod = _automodule(__name__)
newmod.__dict__.update(oldmod.__dict__)
sys.modules[__name__] = newmod
del newmod.newmod, newmod.oldmod, newmod.sys, newmod.types


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/rtf.py
# ========================================================
"""
    pygments.formatters.rtf
    ~~~~~~~~~~~~~~~~~~~~~~~

    A formatter that generates RTF files.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.util import get_int_opt, surrogatepair


__all__ = ['RtfFormatter']


class RtfFormatter(Formatter):
    """
    Format tokens as RTF markup. This formatter automatically outputs full RTF
    documents with color information and other useful stuff. Perfect for Copy and
    Paste into Microsoft(R) Word(R) documents.

    Please note that ``encoding`` and ``outencoding`` options are ignored.
    The RTF format is ASCII natively, but handles unicode characters correctly
    thanks to escape sequences.

    .. versionadded:: 0.6

    Additional options accepted:

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``).

    `fontface`
        The used font family, for example ``Bitstream Vera Sans``. Defaults to
        some generic font which is supposed to have fixed width.

    `fontsize`
        Size of the font used. Size is specified in half points. The
        default is 24 half-points, giving a size 12 font.

        .. versionadded:: 2.0
    """
    name = 'RTF'
    aliases = ['rtf']
    filenames = ['*.rtf']

    def __init__(self, **options):
        r"""
        Additional options accepted:

        ``fontface``
            Name of the font used. Could for example be ``'Courier New'``
            to further specify the default which is ``'\fmodern'``. The RTF
            specification claims that ``\fmodern`` are "Fixed-pitch serif
            and sans serif fonts". Hope every RTF implementation thinks
            the same about modern...

        """
        Formatter.__init__(self, **options)
        self.fontface = options.get('fontface') or ''
        self.fontsize = get_int_opt(options, 'fontsize', 0)

    def _escape(self, text):
        return text.replace('\\', '\\\\') \
                   .replace('{', '\\{') \
                   .replace('}', '\\}')

    def _escape_text(self, text):
        # empty strings, should give a small performance improvement
        if not text:
            return ''

        # escape text
        text = self._escape(text)

        buf = []
        for c in text:
            cn = ord(c)
            if cn < (2**7):
                # ASCII character
                buf.append(str(c))
            elif (2**7) <= cn < (2**16):
                # single unicode escape sequence
                buf.append('{\\u%d}' % cn)
            elif (2**16) <= cn:
                # RTF limits unicode to 16 bits.
                # Force surrogate pairs
                buf.append('{\\u%d}{\\u%d}' % surrogatepair(cn))

        return ''.join(buf).replace('\n', '\\par\n')

    def format_unencoded(self, tokensource, outfile):
        # rtf 1.8 header
        outfile.write('{\\rtf1\\ansi\\uc0\\deff0'
                      '{\\fonttbl{\\f0\\fmodern\\fprq1\\fcharset0%s;}}'
                      '{\\colortbl;' % (self.fontface and
                                        ' ' + self._escape(self.fontface) or
                                        ''))

        # convert colors and save them in a mapping to access them later.
        color_mapping = {}
        offset = 1
        for _, style in self.style:
            for color in style['color'], style['bgcolor'], style['border']:
                if color and color not in color_mapping:
                    color_mapping[color] = offset
                    outfile.write('\\red%d\\green%d\\blue%d;' % (
                        int(color[0:2], 16),
                        int(color[2:4], 16),
                        int(color[4:6], 16)
                    ))
                    offset += 1
        outfile.write('}\\f0 ')
        if self.fontsize:
            outfile.write('\\fs%d' % self.fontsize)

        # highlight stream
        for ttype, value in tokensource:
            while not self.style.styles_token(ttype) and ttype.parent:
                ttype = ttype.parent
            style = self.style.style_for_token(ttype)
            buf = []
            if style['bgcolor']:
                buf.append('\\cb%d' % color_mapping[style['bgcolor']])
            if style['color']:
                buf.append('\\cf%d' % color_mapping[style['color']])
            if style['bold']:
                buf.append('\\b')
            if style['italic']:
                buf.append('\\i')
            if style['underline']:
                buf.append('\\ul')
            if style['border']:
                buf.append('\\chbrdr\\chcfpat%d' %
                           color_mapping[style['border']])
            start = ''.join(buf)
            if start:
                outfile.write('{%s ' % start)
            outfile.write(self._escape_text(value))
            if start:
                outfile.write('}')

        outfile.write('}')


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/bbcode.py
# ========================================================
"""
    pygments.formatters.bbcode
    ~~~~~~~~~~~~~~~~~~~~~~~~~~

    BBcode formatter.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""


from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.util import get_bool_opt

__all__ = ['BBCodeFormatter']


class BBCodeFormatter(Formatter):
    """
    Format tokens with BBcodes. These formatting codes are used by many
    bulletin boards, so you can highlight your sourcecode with pygments before
    posting it there.

    This formatter has no support for background colors and borders, as there
    are no common BBcode tags for that.

    Some board systems (e.g. phpBB) don't support colors in their [code] tag,
    so you can't use the highlighting together with that tag.
    Text in a [code] tag usually is shown with a monospace font (which this
    formatter can do with the ``monofont`` option) and no spaces (which you
    need for indentation) are removed.

    Additional options accepted:

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``).

    `codetag`
        If set to true, put the output into ``[code]`` tags (default:
        ``false``)

    `monofont`
        If set to true, add a tag to show the code with a monospace font
        (default: ``false``).
    """
    name = 'BBCode'
    aliases = ['bbcode', 'bb']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        self._code = get_bool_opt(options, 'codetag', False)
        self._mono = get_bool_opt(options, 'monofont', False)

        self.styles = {}
        self._make_styles()

    def _make_styles(self):
        for ttype, ndef in self.style:
            start = end = ''
            if ndef['color']:
                start += '[color=#%s]' % ndef['color']
                end = '[/color]' + end
            if ndef['bold']:
                start += '[b]'
                end = '[/b]' + end
            if ndef['italic']:
                start += '[i]'
                end = '[/i]' + end
            if ndef['underline']:
                start += '[u]'
                end = '[/u]' + end
            # there are no common BBcodes for background-color and border

            self.styles[ttype] = start, end

    def format_unencoded(self, tokensource, outfile):
        if self._code:
            outfile.write('[code]')
        if self._mono:
            outfile.write('[font=monospace]')

        lastval = ''
        lasttype = None

        for ttype, value in tokensource:
            while ttype not in self.styles:
                ttype = ttype.parent
            if ttype == lasttype:
                lastval += value
            else:
                if lastval:
                    start, end = self.styles[lasttype]
                    outfile.write(''.join((start, lastval, end)))
                lastval = value
                lasttype = ttype

        if lastval:
            start, end = self.styles[lasttype]
            outfile.write(''.join((start, lastval, end)))

        if self._mono:
            outfile.write('[/font]')
        if self._code:
            outfile.write('[/code]')
        if self._code or self._mono:
            outfile.write('\n')


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/pangomarkup.py
# ========================================================
"""
    pygments.formatters.pangomarkup
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for Pango markup output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.formatter import Formatter


__all__ = ['PangoMarkupFormatter']


_escape_table = {
    ord('&'): '&amp;',
    ord('<'): '&lt;',
}


def escape_special_chars(text, table=_escape_table):
    """Escape & and < for Pango Markup."""
    return text.translate(table)


class PangoMarkupFormatter(Formatter):
    """
    Format tokens as Pango Markup code. It can then be rendered to an SVG.

    .. versionadded:: 2.9
    """

    name = 'Pango Markup'
    aliases = ['pango', 'pangomarkup']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)

        self.styles = {}

        for token, style in self.style:
            start = ''
            end = ''
            if style['color']:
                start += '<span fgcolor="#%s">' % style['color']
                end = '</span>' + end
            if style['bold']:
                start += '<b>'
                end = '</b>' + end
            if style['italic']:
                start += '<i>'
                end = '</i>' + end
            if style['underline']:
                start += '<u>'
                end = '</u>' + end
            self.styles[token] = (start, end)

    def format_unencoded(self, tokensource, outfile):
        lastval = ''
        lasttype = None

        outfile.write('<tt>')

        for ttype, value in tokensource:
            while ttype not in self.styles:
                ttype = ttype.parent
            if ttype == lasttype:
                lastval += escape_special_chars(value)
            else:
                if lastval:
                    stylebegin, styleend = self.styles[lasttype]
                    outfile.write(stylebegin + lastval + styleend)
                lastval = escape_special_chars(value)
                lasttype = ttype

        if lastval:
            stylebegin, styleend = self.styles[lasttype]
            outfile.write(stylebegin + lastval + styleend)

        outfile.write('</tt>')


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/groff.py
# ========================================================
"""
    pygments.formatters.groff
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for groff output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import math
from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.util import get_bool_opt, get_int_opt

__all__ = ['GroffFormatter']


class GroffFormatter(Formatter):
    """
    Format tokens with groff escapes to change their color and font style.

    .. versionadded:: 2.11

    Additional options accepted:

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``).

    `monospaced`
        If set to true, monospace font will be used (default: ``true``).

    `linenos`
        If set to true, print the line numbers (default: ``false``).

    `wrap`
        Wrap lines to the specified number of characters. Disabled if set to 0
        (default: ``0``).
    """

    name = 'groff'
    aliases = ['groff','troff','roff']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)

        self.monospaced = get_bool_opt(options, 'monospaced', True)
        self.linenos = get_bool_opt(options, 'linenos', False)
        self._lineno = 0
        self.wrap = get_int_opt(options, 'wrap', 0)
        self._linelen = 0

        self.styles = {}
        self._make_styles()


    def _make_styles(self):
        regular = '\\f[CR]' if self.monospaced else '\\f[R]'
        bold = '\\f[CB]' if self.monospaced else '\\f[B]'
        italic = '\\f[CI]' if self.monospaced else '\\f[I]'

        for ttype, ndef in self.style:
            start = end = ''
            if ndef['color']:
                start += '\\m[%s]' % ndef['color']
                end = '\\m[]' + end
            if ndef['bold']:
                start += bold
                end = regular + end
            if ndef['italic']:
                start += italic
                end = regular + end
            if ndef['bgcolor']:
                start += '\\M[%s]' % ndef['bgcolor']
                end = '\\M[]' + end

            self.styles[ttype] = start, end


    def _define_colors(self, outfile):
        colors = set()
        for _, ndef in self.style:
            if ndef['color'] is not None:
                colors.add(ndef['color'])

        for color in sorted(colors):
            outfile.write('.defcolor ' + color + ' rgb #' + color + '\n')


    def _write_lineno(self, outfile):
        self._lineno += 1
        outfile.write("%s% 4d " % (self._lineno != 1 and '\n' or '', self._lineno))


    def _wrap_line(self, line):
        length = len(line.rstrip('\n'))
        space = '     ' if self.linenos else ''
        newline = ''

        if length > self.wrap:
            for i in range(0, math.floor(length / self.wrap)):
                chunk = line[i*self.wrap:i*self.wrap+self.wrap]
                newline += (chunk + '\n' + space)
            remainder = length % self.wrap
            if remainder > 0:
                newline += line[-remainder-1:]
                self._linelen = remainder
        elif self._linelen + length > self.wrap:
            newline = ('\n' + space) + line
            self._linelen = length
        else:
            newline = line
            self._linelen += length

        return newline


    def _escape_chars(self, text):
        text = text.replace('\\', '\\[u005C]'). \
                    replace('.', '\\[char46]'). \
                    replace('\'', '\\[u0027]'). \
                    replace('`', '\\[u0060]'). \
                    replace('~', '\\[u007E]')
        copy = text

        for char in copy:
            if len(char) != len(char.encode()):
                uni = char.encode('unicode_escape') \
                    .decode()[1:] \
                    .replace('x', 'u00') \
                    .upper()
                text = text.replace(char, '\\[u' + uni[1:] + ']')

        return text


    def format_unencoded(self, tokensource, outfile):
        self._define_colors(outfile)

        outfile.write('.nf\n\\f[CR]\n')

        if self.linenos:
            self._write_lineno(outfile)

        for ttype, value in tokensource:
            while ttype not in self.styles:
                ttype = ttype.parent
            start, end = self.styles[ttype]

            for line in value.splitlines(True):
                if self.wrap > 0:
                    line = self._wrap_line(line)

                if start and end:
                    text = self._escape_chars(line.rstrip('\n'))
                    if text != '':
                        outfile.write(''.join((start, text, end)))
                else:
                    outfile.write(self._escape_chars(line.rstrip('\n')))

                if line.endswith('\n'):
                    if self.linenos:
                        self._write_lineno(outfile)
                        self._linelen = 0
                    else:
                        outfile.write('\n')
                        self._linelen = 0

        outfile.write('\n.fi')


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/other.py
# ========================================================
"""
    pygments.formatters.other
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Other formatters: NullFormatter, RawTokenFormatter.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.util import get_choice_opt
from pip._vendor.pygments.token import Token
from pip._vendor.pygments.console import colorize

__all__ = ['NullFormatter', 'RawTokenFormatter', 'TestcaseFormatter']


class NullFormatter(Formatter):
    """
    Output the text unchanged without any formatting.
    """
    name = 'Text only'
    aliases = ['text', 'null']
    filenames = ['*.txt']

    def format(self, tokensource, outfile):
        enc = self.encoding
        for ttype, value in tokensource:
            if enc:
                outfile.write(value.encode(enc))
            else:
                outfile.write(value)


class RawTokenFormatter(Formatter):
    r"""
    Format tokens as a raw representation for storing token streams.

    The format is ``tokentype<TAB>repr(tokenstring)\n``. The output can later
    be converted to a token stream with the `RawTokenLexer`, described in the
    :doc:`lexer list <lexers>`.

    Only two options are accepted:

    `compress`
        If set to ``'gz'`` or ``'bz2'``, compress the output with the given
        compression algorithm after encoding (default: ``''``).
    `error_color`
        If set to a color name, highlight error tokens using that color.  If
        set but with no value, defaults to ``'red'``.

        .. versionadded:: 0.11

    """
    name = 'Raw tokens'
    aliases = ['raw', 'tokens']
    filenames = ['*.raw']

    unicodeoutput = False

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        # We ignore self.encoding if it is set, since it gets set for lexer
        # and formatter if given with -Oencoding on the command line.
        # The RawTokenFormatter outputs only ASCII. Override here.
        self.encoding = 'ascii'  # let pygments.format() do the right thing
        self.compress = get_choice_opt(options, 'compress',
                                       ['', 'none', 'gz', 'bz2'], '')
        self.error_color = options.get('error_color', None)
        if self.error_color is True:
            self.error_color = 'red'
        if self.error_color is not None:
            try:
                colorize(self.error_color, '')
            except KeyError:
                raise ValueError("Invalid color %r specified" %
                                 self.error_color)

    def format(self, tokensource, outfile):
        try:
            outfile.write(b'')
        except TypeError:
            raise TypeError('The raw tokens formatter needs a binary '
                            'output file')
        if self.compress == 'gz':
            import gzip
            outfile = gzip.GzipFile('', 'wb', 9, outfile)

            write = outfile.write
            flush = outfile.close
        elif self.compress == 'bz2':
            import bz2
            compressor = bz2.BZ2Compressor(9)

            def write(text):
                outfile.write(compressor.compress(text))

            def flush():
                outfile.write(compressor.flush())
                outfile.flush()
        else:
            write = outfile.write
            flush = outfile.flush

        if self.error_color:
            for ttype, value in tokensource:
                line = b"%r\t%r\n" % (ttype, value)
                if ttype is Token.Error:
                    write(colorize(self.error_color, line))
                else:
                    write(line)
        else:
            for ttype, value in tokensource:
                write(b"%r\t%r\n" % (ttype, value))
        flush()


TESTCASE_BEFORE = '''\
    def testNeedsName(lexer):
        fragment = %r
        tokens = [
'''
TESTCASE_AFTER = '''\
        ]
        assert list(lexer.get_tokens(fragment)) == tokens
'''


class TestcaseFormatter(Formatter):
    """
    Format tokens as appropriate for a new testcase.

    .. versionadded:: 2.0
    """
    name = 'Testcase'
    aliases = ['testcase']

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        if self.encoding is not None and self.encoding != 'utf-8':
            raise ValueError("Only None and utf-8 are allowed encodings.")

    def format(self, tokensource, outfile):
        indentation = ' ' * 12
        rawbuf = []
        outbuf = []
        for ttype, value in tokensource:
            rawbuf.append(value)
            outbuf.append('%s(%s, %r),\n' % (indentation, ttype, value))

        before = TESTCASE_BEFORE % (''.join(rawbuf),)
        during = ''.join(outbuf)
        after = TESTCASE_AFTER
        if self.encoding is None:
            outfile.write(before + during + after)
        else:
            outfile.write(before.encode('utf-8'))
            outfile.write(during.encode('utf-8'))
            outfile.write(after.encode('utf-8'))
        outfile.flush()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/terminal.py
# ========================================================
"""
    pygments.formatters.terminal
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for terminal output with ANSI sequences.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.token import Keyword, Name, Comment, String, Error, \
    Number, Operator, Generic, Token, Whitespace
from pip._vendor.pygments.console import ansiformat
from pip._vendor.pygments.util import get_choice_opt


__all__ = ['TerminalFormatter']


#: Map token types to a tuple of color values for light and dark
#: backgrounds.
TERMINAL_COLORS = {
    Token:              ('',            ''),

    Whitespace:         ('gray',   'brightblack'),
    Comment:            ('gray',   'brightblack'),
    Comment.Preproc:    ('cyan',        'brightcyan'),
    Keyword:            ('blue',    'brightblue'),
    Keyword.Type:       ('cyan',        'brightcyan'),
    Operator.Word:      ('magenta',      'brightmagenta'),
    Name.Builtin:       ('cyan',        'brightcyan'),
    Name.Function:      ('green',   'brightgreen'),
    Name.Namespace:     ('_cyan_',      '_brightcyan_'),
    Name.Class:         ('_green_', '_brightgreen_'),
    Name.Exception:     ('cyan',        'brightcyan'),
    Name.Decorator:     ('brightblack',    'gray'),
    Name.Variable:      ('red',     'brightred'),
    Name.Constant:      ('red',     'brightred'),
    Name.Attribute:     ('cyan',        'brightcyan'),
    Name.Tag:           ('brightblue',        'brightblue'),
    String:             ('yellow',       'yellow'),
    Number:             ('blue',    'brightblue'),

    Generic.Deleted:    ('brightred',        'brightred'),
    Generic.Inserted:   ('green',  'brightgreen'),
    Generic.Heading:    ('**',         '**'),
    Generic.Subheading: ('*magenta*',   '*brightmagenta*'),
    Generic.Prompt:     ('**',         '**'),
    Generic.Error:      ('brightred',        'brightred'),

    Error:              ('_brightred_',      '_brightred_'),
}


class TerminalFormatter(Formatter):
    r"""
    Format tokens with ANSI color sequences, for output in a text console.
    Color sequences are terminated at newlines, so that paging the output
    works correctly.

    The `get_style_defs()` method doesn't do anything special since there is
    no support for common styles.

    Options accepted:

    `bg`
        Set to ``"light"`` or ``"dark"`` depending on the terminal's background
        (default: ``"light"``).

    `colorscheme`
        A dictionary mapping token types to (lightbg, darkbg) color names or
        ``None`` (default: ``None`` = use builtin colorscheme).

    `linenos`
        Set to ``True`` to have line numbers on the terminal output as well
        (default: ``False`` = no line numbers).
    """
    name = 'Terminal'
    aliases = ['terminal', 'console']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        self.darkbg = get_choice_opt(options, 'bg',
                                     ['light', 'dark'], 'light') == 'dark'
        self.colorscheme = options.get('colorscheme', None) or TERMINAL_COLORS
        self.linenos = options.get('linenos', False)
        self._lineno = 0

    def format(self, tokensource, outfile):
        return Formatter.format(self, tokensource, outfile)

    def _write_lineno(self, outfile):
        self._lineno += 1
        outfile.write("%s%04d: " % (self._lineno != 1 and '\n' or '', self._lineno))

    def _get_color(self, ttype):
        # self.colorscheme is a dict containing usually generic types, so we
        # have to walk the tree of dots.  The base Token type must be a key,
        # even if it's empty string, as in the default above.
        colors = self.colorscheme.get(ttype)
        while colors is None:
            ttype = ttype.parent
            colors = self.colorscheme.get(ttype)
        return colors[self.darkbg]

    def format_unencoded(self, tokensource, outfile):
        if self.linenos:
            self._write_lineno(outfile)

        for ttype, value in tokensource:
            color = self._get_color(ttype)

            for line in value.splitlines(True):
                if color:
                    outfile.write(ansiformat(color, line.rstrip('\n')))
                else:
                    outfile.write(line.rstrip('\n'))
                if line.endswith('\n'):
                    if self.linenos:
                        self._write_lineno(outfile)
                    else:
                        outfile.write('\n')

        if self.linenos:
            outfile.write("\n")


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/terminal256.py
# ========================================================
"""
    pygments.formatters.terminal256
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for 256-color terminal output with ANSI sequences.

    RGB-to-XTERM color conversion routines adapted from xterm256-conv
    tool (http://frexx.de/xterm-256-notes/data/xterm256-conv2.tar.bz2)
    by Wolfgang Frisch.

    Formatter version 1.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

# TODO:
#  - Options to map style's bold/underline/italic/border attributes
#    to some ANSI attrbutes (something like 'italic=underline')
#  - An option to output "style RGB to xterm RGB/index" conversion table
#  - An option to indicate that we are running in "reverse background"
#    xterm. This means that default colors are white-on-black, not
#    black-on-while, so colors like "white background" need to be converted
#    to "white background, black foreground", etc...

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.console import codes
from pip._vendor.pygments.style import ansicolors


__all__ = ['Terminal256Formatter', 'TerminalTrueColorFormatter']


class EscapeSequence:
    def __init__(self, fg=None, bg=None, bold=False, underline=False, italic=False):
        self.fg = fg
        self.bg = bg
        self.bold = bold
        self.underline = underline
        self.italic = italic

    def escape(self, attrs):
        if len(attrs):
            return "\x1b[" + ";".join(attrs) + "m"
        return ""

    def color_string(self):
        attrs = []
        if self.fg is not None:
            if self.fg in ansicolors:
                esc = codes[self.fg.replace('ansi','')]
                if ';01m' in esc:
                    self.bold = True
                # extract fg color code.
                attrs.append(esc[2:4])
            else:
                attrs.extend(("38", "5", "%i" % self.fg))
        if self.bg is not None:
            if self.bg in ansicolors:
                esc = codes[self.bg.replace('ansi','')]
                # extract fg color code, add 10 for bg.
                attrs.append(str(int(esc[2:4])+10))
            else:
                attrs.extend(("48", "5", "%i" % self.bg))
        if self.bold:
            attrs.append("01")
        if self.underline:
            attrs.append("04")
        if self.italic:
            attrs.append("03")
        return self.escape(attrs)

    def true_color_string(self):
        attrs = []
        if self.fg:
            attrs.extend(("38", "2", str(self.fg[0]), str(self.fg[1]), str(self.fg[2])))
        if self.bg:
            attrs.extend(("48", "2", str(self.bg[0]), str(self.bg[1]), str(self.bg[2])))
        if self.bold:
            attrs.append("01")
        if self.underline:
            attrs.append("04")
        if self.italic:
            attrs.append("03")
        return self.escape(attrs)

    def reset_string(self):
        attrs = []
        if self.fg is not None:
            attrs.append("39")
        if self.bg is not None:
            attrs.append("49")
        if self.bold or self.underline or self.italic:
            attrs.append("00")
        return self.escape(attrs)


class Terminal256Formatter(Formatter):
    """
    Format tokens with ANSI color sequences, for output in a 256-color
    terminal or console.  Like in `TerminalFormatter` color sequences
    are terminated at newlines, so that paging the output works correctly.

    The formatter takes colors from a style defined by the `style` option
    and converts them to nearest ANSI 256-color escape sequences. Bold and
    underline attributes from the style are preserved (and displayed).

    .. versionadded:: 0.9

    .. versionchanged:: 2.2
       If the used style defines foreground colors in the form ``#ansi*``, then
       `Terminal256Formatter` will map these to non extended foreground color.
       See :ref:`AnsiTerminalStyle` for more information.

    .. versionchanged:: 2.4
       The ANSI color names have been updated with names that are easier to
       understand and align with colornames of other projects and terminals.
       See :ref:`this table <new-ansi-color-names>` for more information.


    Options accepted:

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``).

    `linenos`
        Set to ``True`` to have line numbers on the terminal output as well
        (default: ``False`` = no line numbers).
    """
    name = 'Terminal256'
    aliases = ['terminal256', 'console256', '256']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)

        self.xterm_colors = []
        self.best_match = {}
        self.style_string = {}

        self.usebold = 'nobold' not in options
        self.useunderline = 'nounderline' not in options
        self.useitalic = 'noitalic' not in options

        self._build_color_table()  # build an RGB-to-256 color conversion table
        self._setup_styles()  # convert selected style's colors to term. colors

        self.linenos = options.get('linenos', False)
        self._lineno = 0

    def _build_color_table(self):
        # colors 0..15: 16 basic colors

        self.xterm_colors.append((0x00, 0x00, 0x00))  # 0
        self.xterm_colors.append((0xcd, 0x00, 0x00))  # 1
        self.xterm_colors.append((0x00, 0xcd, 0x00))  # 2
        self.xterm_colors.append((0xcd, 0xcd, 0x00))  # 3
        self.xterm_colors.append((0x00, 0x00, 0xee))  # 4
        self.xterm_colors.append((0xcd, 0x00, 0xcd))  # 5
        self.xterm_colors.append((0x00, 0xcd, 0xcd))  # 6
        self.xterm_colors.append((0xe5, 0xe5, 0xe5))  # 7
        self.xterm_colors.append((0x7f, 0x7f, 0x7f))  # 8
        self.xterm_colors.append((0xff, 0x00, 0x00))  # 9
        self.xterm_colors.append((0x00, 0xff, 0x00))  # 10
        self.xterm_colors.append((0xff, 0xff, 0x00))  # 11
        self.xterm_colors.append((0x5c, 0x5c, 0xff))  # 12
        self.xterm_colors.append((0xff, 0x00, 0xff))  # 13
        self.xterm_colors.append((0x00, 0xff, 0xff))  # 14
        self.xterm_colors.append((0xff, 0xff, 0xff))  # 15

        # colors 16..232: the 6x6x6 color cube

        valuerange = (0x00, 0x5f, 0x87, 0xaf, 0xd7, 0xff)

        for i in range(217):
            r = valuerange[(i // 36) % 6]
            g = valuerange[(i // 6) % 6]
            b = valuerange[i % 6]
            self.xterm_colors.append((r, g, b))

        # colors 233..253: grayscale

        for i in range(1, 22):
            v = 8 + i * 10
            self.xterm_colors.append((v, v, v))

    def _closest_color(self, r, g, b):
        distance = 257*257*3  # "infinity" (>distance from #000000 to #ffffff)
        match = 0

        for i in range(0, 254):
            values = self.xterm_colors[i]

            rd = r - values[0]
            gd = g - values[1]
            bd = b - values[2]
            d = rd*rd + gd*gd + bd*bd

            if d < distance:
                match = i
                distance = d
        return match

    def _color_index(self, color):
        index = self.best_match.get(color, None)
        if color in ansicolors:
            # strip the `ansi/#ansi` part and look up code
            index = color
            self.best_match[color] = index
        if index is None:
            try:
                rgb = int(str(color), 16)
            except ValueError:
                rgb = 0

            r = (rgb >> 16) & 0xff
            g = (rgb >> 8) & 0xff
            b = rgb & 0xff
            index = self._closest_color(r, g, b)
            self.best_match[color] = index
        return index

    def _setup_styles(self):
        for ttype, ndef in self.style:
            escape = EscapeSequence()
            # get foreground from ansicolor if set
            if ndef['ansicolor']:
                escape.fg = self._color_index(ndef['ansicolor'])
            elif ndef['color']:
                escape.fg = self._color_index(ndef['color'])
            if ndef['bgansicolor']:
                escape.bg = self._color_index(ndef['bgansicolor'])
            elif ndef['bgcolor']:
                escape.bg = self._color_index(ndef['bgcolor'])
            if self.usebold and ndef['bold']:
                escape.bold = True
            if self.useunderline and ndef['underline']:
                escape.underline = True
            if self.useitalic and ndef['italic']:
                escape.italic = True
            self.style_string[str(ttype)] = (escape.color_string(),
                                             escape.reset_string())

    def _write_lineno(self, outfile):
        self._lineno += 1
        outfile.write("%s%04d: " % (self._lineno != 1 and '\n' or '', self._lineno))

    def format(self, tokensource, outfile):
        return Formatter.format(self, tokensource, outfile)

    def format_unencoded(self, tokensource, outfile):
        if self.linenos:
            self._write_lineno(outfile)

        for ttype, value in tokensource:
            not_found = True
            while ttype and not_found:
                try:
                    # outfile.write( "<" + str(ttype) + ">" )
                    on, off = self.style_string[str(ttype)]

                    # Like TerminalFormatter, add "reset colors" escape sequence
                    # on newline.
                    spl = value.split('\n')
                    for line in spl[:-1]:
                        if line:
                            outfile.write(on + line + off)
                        if self.linenos:
                            self._write_lineno(outfile)
                        else:
                            outfile.write('\n')

                    if spl[-1]:
                        outfile.write(on + spl[-1] + off)

                    not_found = False
                    # outfile.write( '#' + str(ttype) + '#' )

                except KeyError:
                    # ottype = ttype
                    ttype = ttype.parent
                    # outfile.write( '!' + str(ottype) + '->' + str(ttype) + '!' )

            if not_found:
                outfile.write(value)

        if self.linenos:
            outfile.write("\n")



class TerminalTrueColorFormatter(Terminal256Formatter):
    r"""
    Format tokens with ANSI color sequences, for output in a true-color
    terminal or console.  Like in `TerminalFormatter` color sequences
    are terminated at newlines, so that paging the output works correctly.

    .. versionadded:: 2.1

    Options accepted:

    `style`
        The style to use, can be a string or a Style subclass (default:
        ``'default'``).
    """
    name = 'TerminalTrueColor'
    aliases = ['terminal16m', 'console16m', '16m']
    filenames = []

    def _build_color_table(self):
        pass

    def _color_tuple(self, color):
        try:
            rgb = int(str(color), 16)
        except ValueError:
            return None
        r = (rgb >> 16) & 0xff
        g = (rgb >> 8) & 0xff
        b = rgb & 0xff
        return (r, g, b)

    def _setup_styles(self):
        for ttype, ndef in self.style:
            escape = EscapeSequence()
            if ndef['color']:
                escape.fg = self._color_tuple(ndef['color'])
            if ndef['bgcolor']:
                escape.bg = self._color_tuple(ndef['bgcolor'])
            if self.usebold and ndef['bold']:
                escape.bold = True
            if self.useunderline and ndef['underline']:
                escape.underline = True
            if self.useitalic and ndef['italic']:
                escape.italic = True
            self.style_string[str(ttype)] = (escape.true_color_string(),
                                             escape.reset_string())


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/svg.py
# ========================================================
"""
    pygments.formatters.svg
    ~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for SVG output.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.token import Comment
from pip._vendor.pygments.util import get_bool_opt, get_int_opt

__all__ = ['SvgFormatter']


def escape_html(text):
    """Escape &, <, > as well as single and double quotes for HTML."""
    return text.replace('&', '&amp;').  \
                replace('<', '&lt;').   \
                replace('>', '&gt;').   \
                replace('"', '&quot;'). \
                replace("'", '&#39;')


class2style = {}

class SvgFormatter(Formatter):
    """
    Format tokens as an SVG graphics file.  This formatter is still experimental.
    Each line of code is a ``<text>`` element with explicit ``x`` and ``y``
    coordinates containing ``<tspan>`` elements with the individual token styles.

    By default, this formatter outputs a full SVG document including doctype
    declaration and the ``<svg>`` root element.

    .. versionadded:: 0.9

    Additional options accepted:

    `nowrap`
        Don't wrap the SVG ``<text>`` elements in ``<svg><g>`` elements and
        don't add a XML declaration and a doctype.  If true, the `fontfamily`
        and `fontsize` options are ignored.  Defaults to ``False``.

    `fontfamily`
        The value to give the wrapping ``<g>`` element's ``font-family``
        attribute, defaults to ``"monospace"``.

    `fontsize`
        The value to give the wrapping ``<g>`` element's ``font-size``
        attribute, defaults to ``"14px"``.

    `linenos`
        If ``True``, add line numbers (default: ``False``).

    `linenostart`
        The line number for the first line (default: ``1``).

    `linenostep`
        If set to a number n > 1, only every nth line number is printed.
        
    `linenowidth`
        Maximum width devoted to line numbers (default: ``3*ystep``, sufficient
        for up to 4-digit line numbers. Increase width for longer code blocks).  
        
    `xoffset`
        Starting offset in X direction, defaults to ``0``.

    `yoffset`
        Starting offset in Y direction, defaults to the font size if it is given
        in pixels, or ``20`` else.  (This is necessary since text coordinates
        refer to the text baseline, not the top edge.)

    `ystep`
        Offset to add to the Y coordinate for each subsequent line.  This should
        roughly be the text size plus 5.  It defaults to that value if the text
        size is given in pixels, or ``25`` else.

    `spacehack`
        Convert spaces in the source to ``&#160;``, which are non-breaking
        spaces.  SVG provides the ``xml:space`` attribute to control how
        whitespace inside tags is handled, in theory, the ``preserve`` value
        could be used to keep all whitespace as-is.  However, many current SVG
        viewers don't obey that rule, so this option is provided as a workaround
        and defaults to ``True``.
    """
    name = 'SVG'
    aliases = ['svg']
    filenames = ['*.svg']

    def __init__(self, **options):
        Formatter.__init__(self, **options)
        self.nowrap = get_bool_opt(options, 'nowrap', False)
        self.fontfamily = options.get('fontfamily', 'monospace')
        self.fontsize = options.get('fontsize', '14px')
        self.xoffset = get_int_opt(options, 'xoffset', 0)
        fs = self.fontsize.strip()
        if fs.endswith('px'): fs = fs[:-2].strip()
        try:
            int_fs = int(fs)
        except:
            int_fs = 20
        self.yoffset = get_int_opt(options, 'yoffset', int_fs)
        self.ystep = get_int_opt(options, 'ystep', int_fs + 5)
        self.spacehack = get_bool_opt(options, 'spacehack', True)
        self.linenos = get_bool_opt(options,'linenos',False)
        self.linenostart = get_int_opt(options,'linenostart',1)
        self.linenostep = get_int_opt(options,'linenostep',1)
        self.linenowidth = get_int_opt(options,'linenowidth', 3*self.ystep)
        self._stylecache = {}

    def format_unencoded(self, tokensource, outfile):
        """
        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
        tuples and write it into ``outfile``.

        For our implementation we put all lines in their own 'line group'.
        """
        x = self.xoffset
        y = self.yoffset
        if not self.nowrap:
            if self.encoding:
                outfile.write('<?xml version="1.0" encoding="%s"?>\n' %
                              self.encoding)
            else:
                outfile.write('<?xml version="1.0"?>\n')
            outfile.write('<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.0//EN" '
                          '"http://www.w3.org/TR/2001/REC-SVG-20010904/DTD/'
                          'svg10.dtd">\n')
            outfile.write('<svg xmlns="http://www.w3.org/2000/svg">\n')
            outfile.write('<g font-family="%s" font-size="%s">\n' %
                          (self.fontfamily, self.fontsize))
        
        counter = self.linenostart 
        counter_step = self.linenostep
        counter_style = self._get_style(Comment)
        line_x = x
        
        if self.linenos:
            if counter % counter_step == 0:
                outfile.write('<text x="%s" y="%s" %s text-anchor="end">%s</text>' %
                    (x+self.linenowidth,y,counter_style,counter))
            line_x += self.linenowidth + self.ystep
            counter += 1

        outfile.write('<text x="%s" y="%s" xml:space="preserve">' % (line_x, y))
        for ttype, value in tokensource:
            style = self._get_style(ttype)
            tspan = style and '<tspan' + style + '>' or ''
            tspanend = tspan and '</tspan>' or ''
            value = escape_html(value)
            if self.spacehack:
                value = value.expandtabs().replace(' ', '&#160;')
            parts = value.split('\n')
            for part in parts[:-1]:
                outfile.write(tspan + part + tspanend)
                y += self.ystep
                outfile.write('</text>\n')
                if self.linenos and counter % counter_step == 0:
                    outfile.write('<text x="%s" y="%s" text-anchor="end" %s>%s</text>' %
                        (x+self.linenowidth,y,counter_style,counter))
                
                counter += 1
                outfile.write('<text x="%s" y="%s" ' 'xml:space="preserve">' % (line_x,y))
            outfile.write(tspan + parts[-1] + tspanend)
        outfile.write('</text>')

        if not self.nowrap:
            outfile.write('</g></svg>\n')

    def _get_style(self, tokentype):
        if tokentype in self._stylecache:
            return self._stylecache[tokentype]
        otokentype = tokentype
        while not self.style.styles_token(tokentype):
            tokentype = tokentype.parent
        value = self.style.style_for_token(tokentype)
        result = ''
        if value['color']:
            result = ' fill="#' + value['color'] + '"'
        if value['bold']:
            result += ' font-weight="bold"'
        if value['italic']:
            result += ' font-style="italic"'
        self._stylecache[otokentype] = result
        return result


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/filters/__init__.py
# ========================================================
"""
    pygments.filters
    ~~~~~~~~~~~~~~~~

    Module containing filter lookup functions and default
    filters.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re

from pip._vendor.pygments.token import String, Comment, Keyword, Name, Error, Whitespace, \
    string_to_tokentype
from pip._vendor.pygments.filter import Filter
from pip._vendor.pygments.util import get_list_opt, get_int_opt, get_bool_opt, \
    get_choice_opt, ClassNotFound, OptionError
from pip._vendor.pygments.plugin import find_plugin_filters


def find_filter_class(filtername):
    """Lookup a filter by name. Return None if not found."""
    if filtername in FILTERS:
        return FILTERS[filtername]
    for name, cls in find_plugin_filters():
        if name == filtername:
            return cls
    return None


def get_filter_by_name(filtername, **options):
    """Return an instantiated filter.

    Options are passed to the filter initializer if wanted.
    Raise a ClassNotFound if not found.
    """
    cls = find_filter_class(filtername)
    if cls:
        return cls(**options)
    else:
        raise ClassNotFound('filter %r not found' % filtername)


def get_all_filters():
    """Return a generator of all filter names."""
    yield from FILTERS
    for name, _ in find_plugin_filters():
        yield name


def _replace_special(ttype, value, regex, specialttype,
                     replacefunc=lambda x: x):
    last = 0
    for match in regex.finditer(value):
        start, end = match.start(), match.end()
        if start != last:
            yield ttype, value[last:start]
        yield specialttype, replacefunc(value[start:end])
        last = end
    if last != len(value):
        yield ttype, value[last:]


class CodeTagFilter(Filter):
    """Highlight special code tags in comments and docstrings.

    Options accepted:

    `codetags` : list of strings
       A list of strings that are flagged as code tags.  The default is to
       highlight ``XXX``, ``TODO``, ``FIXME``, ``BUG`` and ``NOTE``.

    .. versionchanged:: 2.13
       Now recognizes ``FIXME`` by default.
    """

    def __init__(self, **options):
        Filter.__init__(self, **options)
        tags = get_list_opt(options, 'codetags',
                            ['XXX', 'TODO', 'FIXME', 'BUG', 'NOTE'])
        self.tag_re = re.compile(r'\b(%s)\b' % '|'.join([
            re.escape(tag) for tag in tags if tag
        ]))

    def filter(self, lexer, stream):
        regex = self.tag_re
        for ttype, value in stream:
            if ttype in String.Doc or \
               ttype in Comment and \
               ttype not in Comment.Preproc:
                yield from _replace_special(ttype, value, regex, Comment.Special)
            else:
                yield ttype, value


class SymbolFilter(Filter):
    """Convert mathematical symbols such as \\<longrightarrow> in Isabelle
    or \\longrightarrow in LaTeX into Unicode characters.

    This is mostly useful for HTML or console output when you want to
    approximate the source rendering you'd see in an IDE.

    Options accepted:

    `lang` : string
       The symbol language. Must be one of ``'isabelle'`` or
       ``'latex'``.  The default is ``'isabelle'``.
    """

    latex_symbols = {
        '\\alpha'                : '\U000003b1',
        '\\beta'                 : '\U000003b2',
        '\\gamma'                : '\U000003b3',
        '\\delta'                : '\U000003b4',
        '\\varepsilon'           : '\U000003b5',
        '\\zeta'                 : '\U000003b6',
        '\\eta'                  : '\U000003b7',
        '\\vartheta'             : '\U000003b8',
        '\\iota'                 : '\U000003b9',
        '\\kappa'                : '\U000003ba',
        '\\lambda'               : '\U000003bb',
        '\\mu'                   : '\U000003bc',
        '\\nu'                   : '\U000003bd',
        '\\xi'                   : '\U000003be',
        '\\pi'                   : '\U000003c0',
        '\\varrho'               : '\U000003c1',
        '\\sigma'                : '\U000003c3',
        '\\tau'                  : '\U000003c4',
        '\\upsilon'              : '\U000003c5',
        '\\varphi'               : '\U000003c6',
        '\\chi'                  : '\U000003c7',
        '\\psi'                  : '\U000003c8',
        '\\omega'                : '\U000003c9',
        '\\Gamma'                : '\U00000393',
        '\\Delta'                : '\U00000394',
        '\\Theta'                : '\U00000398',
        '\\Lambda'               : '\U0000039b',
        '\\Xi'                   : '\U0000039e',
        '\\Pi'                   : '\U000003a0',
        '\\Sigma'                : '\U000003a3',
        '\\Upsilon'              : '\U000003a5',
        '\\Phi'                  : '\U000003a6',
        '\\Psi'                  : '\U000003a8',
        '\\Omega'                : '\U000003a9',
        '\\leftarrow'            : '\U00002190',
        '\\longleftarrow'        : '\U000027f5',
        '\\rightarrow'           : '\U00002192',
        '\\longrightarrow'       : '\U000027f6',
        '\\Leftarrow'            : '\U000021d0',
        '\\Longleftarrow'        : '\U000027f8',
        '\\Rightarrow'           : '\U000021d2',
        '\\Longrightarrow'       : '\U000027f9',
        '\\leftrightarrow'       : '\U00002194',
        '\\longleftrightarrow'   : '\U000027f7',
        '\\Leftrightarrow'       : '\U000021d4',
        '\\Longleftrightarrow'   : '\U000027fa',
        '\\mapsto'               : '\U000021a6',
        '\\longmapsto'           : '\U000027fc',
        '\\relbar'               : '\U00002500',
        '\\Relbar'               : '\U00002550',
        '\\hookleftarrow'        : '\U000021a9',
        '\\hookrightarrow'       : '\U000021aa',
        '\\leftharpoondown'      : '\U000021bd',
        '\\rightharpoondown'     : '\U000021c1',
        '\\leftharpoonup'        : '\U000021bc',
        '\\rightharpoonup'       : '\U000021c0',
        '\\rightleftharpoons'    : '\U000021cc',
        '\\leadsto'              : '\U0000219d',
        '\\downharpoonleft'      : '\U000021c3',
        '\\downharpoonright'     : '\U000021c2',
        '\\upharpoonleft'        : '\U000021bf',
        '\\upharpoonright'       : '\U000021be',
        '\\restriction'          : '\U000021be',
        '\\uparrow'              : '\U00002191',
        '\\Uparrow'              : '\U000021d1',
        '\\downarrow'            : '\U00002193',
        '\\Downarrow'            : '\U000021d3',
        '\\updownarrow'          : '\U00002195',
        '\\Updownarrow'          : '\U000021d5',
        '\\langle'               : '\U000027e8',
        '\\rangle'               : '\U000027e9',
        '\\lceil'                : '\U00002308',
        '\\rceil'                : '\U00002309',
        '\\lfloor'               : '\U0000230a',
        '\\rfloor'               : '\U0000230b',
        '\\flqq'                 : '\U000000ab',
        '\\frqq'                 : '\U000000bb',
        '\\bot'                  : '\U000022a5',
        '\\top'                  : '\U000022a4',
        '\\wedge'                : '\U00002227',
        '\\bigwedge'             : '\U000022c0',
        '\\vee'                  : '\U00002228',
        '\\bigvee'               : '\U000022c1',
        '\\forall'               : '\U00002200',
        '\\exists'               : '\U00002203',
        '\\nexists'              : '\U00002204',
        '\\neg'                  : '\U000000ac',
        '\\Box'                  : '\U000025a1',
        '\\Diamond'              : '\U000025c7',
        '\\vdash'                : '\U000022a2',
        '\\models'               : '\U000022a8',
        '\\dashv'                : '\U000022a3',
        '\\surd'                 : '\U0000221a',
        '\\le'                   : '\U00002264',
        '\\ge'                   : '\U00002265',
        '\\ll'                   : '\U0000226a',
        '\\gg'                   : '\U0000226b',
        '\\lesssim'              : '\U00002272',
        '\\gtrsim'               : '\U00002273',
        '\\lessapprox'           : '\U00002a85',
        '\\gtrapprox'            : '\U00002a86',
        '\\in'                   : '\U00002208',
        '\\notin'                : '\U00002209',
        '\\subset'               : '\U00002282',
        '\\supset'               : '\U00002283',
        '\\subseteq'             : '\U00002286',
        '\\supseteq'             : '\U00002287',
        '\\sqsubset'             : '\U0000228f',
        '\\sqsupset'             : '\U00002290',
        '\\sqsubseteq'           : '\U00002291',
        '\\sqsupseteq'           : '\U00002292',
        '\\cap'                  : '\U00002229',
        '\\bigcap'               : '\U000022c2',
        '\\cup'                  : '\U0000222a',
        '\\bigcup'               : '\U000022c3',
        '\\sqcup'                : '\U00002294',
        '\\bigsqcup'             : '\U00002a06',
        '\\sqcap'                : '\U00002293',
        '\\Bigsqcap'             : '\U00002a05',
        '\\setminus'             : '\U00002216',
        '\\propto'               : '\U0000221d',
        '\\uplus'                : '\U0000228e',
        '\\bigplus'              : '\U00002a04',
        '\\sim'                  : '\U0000223c',
        '\\doteq'                : '\U00002250',
        '\\simeq'                : '\U00002243',
        '\\approx'               : '\U00002248',
        '\\asymp'                : '\U0000224d',
        '\\cong'                 : '\U00002245',
        '\\equiv'                : '\U00002261',
        '\\Join'                 : '\U000022c8',
        '\\bowtie'               : '\U00002a1d',
        '\\prec'                 : '\U0000227a',
        '\\succ'                 : '\U0000227b',
        '\\preceq'               : '\U0000227c',
        '\\succeq'               : '\U0000227d',
        '\\parallel'             : '\U00002225',
        '\\mid'                  : '\U000000a6',
        '\\pm'                   : '\U000000b1',
        '\\mp'                   : '\U00002213',
        '\\times'                : '\U000000d7',
        '\\div'                  : '\U000000f7',
        '\\cdot'                 : '\U000022c5',
        '\\star'                 : '\U000022c6',
        '\\circ'                 : '\U00002218',
        '\\dagger'               : '\U00002020',
        '\\ddagger'              : '\U00002021',
        '\\lhd'                  : '\U000022b2',
        '\\rhd'                  : '\U000022b3',
        '\\unlhd'                : '\U000022b4',
        '\\unrhd'                : '\U000022b5',
        '\\triangleleft'         : '\U000025c3',
        '\\triangleright'        : '\U000025b9',
        '\\triangle'             : '\U000025b3',
        '\\triangleq'            : '\U0000225c',
        '\\oplus'                : '\U00002295',
        '\\bigoplus'             : '\U00002a01',
        '\\otimes'               : '\U00002297',
        '\\bigotimes'            : '\U00002a02',
        '\\odot'                 : '\U00002299',
        '\\bigodot'              : '\U00002a00',
        '\\ominus'               : '\U00002296',
        '\\oslash'               : '\U00002298',
        '\\dots'                 : '\U00002026',
        '\\cdots'                : '\U000022ef',
        '\\sum'                  : '\U00002211',
        '\\prod'                 : '\U0000220f',
        '\\coprod'               : '\U00002210',
        '\\infty'                : '\U0000221e',
        '\\int'                  : '\U0000222b',
        '\\oint'                 : '\U0000222e',
        '\\clubsuit'             : '\U00002663',
        '\\diamondsuit'          : '\U00002662',
        '\\heartsuit'            : '\U00002661',
        '\\spadesuit'            : '\U00002660',
        '\\aleph'                : '\U00002135',
        '\\emptyset'             : '\U00002205',
        '\\nabla'                : '\U00002207',
        '\\partial'              : '\U00002202',
        '\\flat'                 : '\U0000266d',
        '\\natural'              : '\U0000266e',
        '\\sharp'                : '\U0000266f',
        '\\angle'                : '\U00002220',
        '\\copyright'            : '\U000000a9',
        '\\textregistered'       : '\U000000ae',
        '\\textonequarter'       : '\U000000bc',
        '\\textonehalf'          : '\U000000bd',
        '\\textthreequarters'    : '\U000000be',
        '\\textordfeminine'      : '\U000000aa',
        '\\textordmasculine'     : '\U000000ba',
        '\\euro'                 : '\U000020ac',
        '\\pounds'               : '\U000000a3',
        '\\yen'                  : '\U000000a5',
        '\\textcent'             : '\U000000a2',
        '\\textcurrency'         : '\U000000a4',
        '\\textdegree'           : '\U000000b0',
    }

    isabelle_symbols = {
        '\\<zero>'                 : '\U0001d7ec',
        '\\<one>'                  : '\U0001d7ed',
        '\\<two>'                  : '\U0001d7ee',
        '\\<three>'                : '\U0001d7ef',
        '\\<four>'                 : '\U0001d7f0',
        '\\<five>'                 : '\U0001d7f1',
        '\\<six>'                  : '\U0001d7f2',
        '\\<seven>'                : '\U0001d7f3',
        '\\<eight>'                : '\U0001d7f4',
        '\\<nine>'                 : '\U0001d7f5',
        '\\<A>'                    : '\U0001d49c',
        '\\<B>'                    : '\U0000212c',
        '\\<C>'                    : '\U0001d49e',
        '\\<D>'                    : '\U0001d49f',
        '\\<E>'                    : '\U00002130',
        '\\<F>'                    : '\U00002131',
        '\\<G>'                    : '\U0001d4a2',
        '\\<H>'                    : '\U0000210b',
        '\\<I>'                    : '\U00002110',
        '\\<J>'                    : '\U0001d4a5',
        '\\<K>'                    : '\U0001d4a6',
        '\\<L>'                    : '\U00002112',
        '\\<M>'                    : '\U00002133',
        '\\<N>'                    : '\U0001d4a9',
        '\\<O>'                    : '\U0001d4aa',
        '\\<P>'                    : '\U0001d4ab',
        '\\<Q>'                    : '\U0001d4ac',
        '\\<R>'                    : '\U0000211b',
        '\\<S>'                    : '\U0001d4ae',
        '\\<T>'                    : '\U0001d4af',
        '\\<U>'                    : '\U0001d4b0',
        '\\<V>'                    : '\U0001d4b1',
        '\\<W>'                    : '\U0001d4b2',
        '\\<X>'                    : '\U0001d4b3',
        '\\<Y>'                    : '\U0001d4b4',
        '\\<Z>'                    : '\U0001d4b5',
        '\\<a>'                    : '\U0001d5ba',
        '\\<b>'                    : '\U0001d5bb',
        '\\<c>'                    : '\U0001d5bc',
        '\\<d>'                    : '\U0001d5bd',
        '\\<e>'                    : '\U0001d5be',
        '\\<f>'                    : '\U0001d5bf',
        '\\<g>'                    : '\U0001d5c0',
        '\\<h>'                    : '\U0001d5c1',
        '\\<i>'                    : '\U0001d5c2',
        '\\<j>'                    : '\U0001d5c3',
        '\\<k>'                    : '\U0001d5c4',
        '\\<l>'                    : '\U0001d5c5',
        '\\<m>'                    : '\U0001d5c6',
        '\\<n>'                    : '\U0001d5c7',
        '\\<o>'                    : '\U0001d5c8',
        '\\<p>'                    : '\U0001d5c9',
        '\\<q>'                    : '\U0001d5ca',
        '\\<r>'                    : '\U0001d5cb',
        '\\<s>'                    : '\U0001d5cc',
        '\\<t>'                    : '\U0001d5cd',
        '\\<u>'                    : '\U0001d5ce',
        '\\<v>'                    : '\U0001d5cf',
        '\\<w>'                    : '\U0001d5d0',
        '\\<x>'                    : '\U0001d5d1',
        '\\<y>'                    : '\U0001d5d2',
        '\\<z>'                    : '\U0001d5d3',
        '\\<AA>'                   : '\U0001d504',
        '\\<BB>'                   : '\U0001d505',
        '\\<CC>'                   : '\U0000212d',
        '\\<DD>'                   : '\U0001d507',
        '\\<EE>'                   : '\U0001d508',
        '\\<FF>'                   : '\U0001d509',
        '\\<GG>'                   : '\U0001d50a',
        '\\<HH>'                   : '\U0000210c',
        '\\<II>'                   : '\U00002111',
        '\\<JJ>'                   : '\U0001d50d',
        '\\<KK>'                   : '\U0001d50e',
        '\\<LL>'                   : '\U0001d50f',
        '\\<MM>'                   : '\U0001d510',
        '\\<NN>'                   : '\U0001d511',
        '\\<OO>'                   : '\U0001d512',
        '\\<PP>'                   : '\U0001d513',
        '\\<QQ>'                   : '\U0001d514',
        '\\<RR>'                   : '\U0000211c',
        '\\<SS>'                   : '\U0001d516',
        '\\<TT>'                   : '\U0001d517',
        '\\<UU>'                   : '\U0001d518',
        '\\<VV>'                   : '\U0001d519',
        '\\<WW>'                   : '\U0001d51a',
        '\\<XX>'                   : '\U0001d51b',
        '\\<YY>'                   : '\U0001d51c',
        '\\<ZZ>'                   : '\U00002128',
        '\\<aa>'                   : '\U0001d51e',
        '\\<bb>'                   : '\U0001d51f',
        '\\<cc>'                   : '\U0001d520',
        '\\<dd>'                   : '\U0001d521',
        '\\<ee>'                   : '\U0001d522',
        '\\<ff>'                   : '\U0001d523',
        '\\<gg>'                   : '\U0001d524',
        '\\<hh>'                   : '\U0001d525',
        '\\<ii>'                   : '\U0001d526',
        '\\<jj>'                   : '\U0001d527',
        '\\<kk>'                   : '\U0001d528',
        '\\<ll>'                   : '\U0001d529',
        '\\<mm>'                   : '\U0001d52a',
        '\\<nn>'                   : '\U0001d52b',
        '\\<oo>'                   : '\U0001d52c',
        '\\<pp>'                   : '\U0001d52d',
        '\\<qq>'                   : '\U0001d52e',
        '\\<rr>'                   : '\U0001d52f',
        '\\<ss>'                   : '\U0001d530',
        '\\<tt>'                   : '\U0001d531',
        '\\<uu>'                   : '\U0001d532',
        '\\<vv>'                   : '\U0001d533',
        '\\<ww>'                   : '\U0001d534',
        '\\<xx>'                   : '\U0001d535',
        '\\<yy>'                   : '\U0001d536',
        '\\<zz>'                   : '\U0001d537',
        '\\<alpha>'                : '\U000003b1',
        '\\<beta>'                 : '\U000003b2',
        '\\<gamma>'                : '\U000003b3',
        '\\<delta>'                : '\U000003b4',
        '\\<epsilon>'              : '\U000003b5',
        '\\<zeta>'                 : '\U000003b6',
        '\\<eta>'                  : '\U000003b7',
        '\\<theta>'                : '\U000003b8',
        '\\<iota>'                 : '\U000003b9',
        '\\<kappa>'                : '\U000003ba',
        '\\<lambda>'               : '\U000003bb',
        '\\<mu>'                   : '\U000003bc',
        '\\<nu>'                   : '\U000003bd',
        '\\<xi>'                   : '\U000003be',
        '\\<pi>'                   : '\U000003c0',
        '\\<rho>'                  : '\U000003c1',
        '\\<sigma>'                : '\U000003c3',
        '\\<tau>'                  : '\U000003c4',
        '\\<upsilon>'              : '\U000003c5',
        '\\<phi>'                  : '\U000003c6',
        '\\<chi>'                  : '\U000003c7',
        '\\<psi>'                  : '\U000003c8',
        '\\<omega>'                : '\U000003c9',
        '\\<Gamma>'                : '\U00000393',
        '\\<Delta>'                : '\U00000394',
        '\\<Theta>'                : '\U00000398',
        '\\<Lambda>'               : '\U0000039b',
        '\\<Xi>'                   : '\U0000039e',
        '\\<Pi>'                   : '\U000003a0',
        '\\<Sigma>'                : '\U000003a3',
        '\\<Upsilon>'              : '\U000003a5',
        '\\<Phi>'                  : '\U000003a6',
        '\\<Psi>'                  : '\U000003a8',
        '\\<Omega>'                : '\U000003a9',
        '\\<bool>'                 : '\U0001d539',
        '\\<complex>'              : '\U00002102',
        '\\<nat>'                  : '\U00002115',
        '\\<rat>'                  : '\U0000211a',
        '\\<real>'                 : '\U0000211d',
        '\\<int>'                  : '\U00002124',
        '\\<leftarrow>'            : '\U00002190',
        '\\<longleftarrow>'        : '\U000027f5',
        '\\<rightarrow>'           : '\U00002192',
        '\\<longrightarrow>'       : '\U000027f6',
        '\\<Leftarrow>'            : '\U000021d0',
        '\\<Longleftarrow>'        : '\U000027f8',
        '\\<Rightarrow>'           : '\U000021d2',
        '\\<Longrightarrow>'       : '\U000027f9',
        '\\<leftrightarrow>'       : '\U00002194',
        '\\<longleftrightarrow>'   : '\U000027f7',
        '\\<Leftrightarrow>'       : '\U000021d4',
        '\\<Longleftrightarrow>'   : '\U000027fa',
        '\\<mapsto>'               : '\U000021a6',
        '\\<longmapsto>'           : '\U000027fc',
        '\\<midarrow>'             : '\U00002500',
        '\\<Midarrow>'             : '\U00002550',
        '\\<hookleftarrow>'        : '\U000021a9',
        '\\<hookrightarrow>'       : '\U000021aa',
        '\\<leftharpoondown>'      : '\U000021bd',
        '\\<rightharpoondown>'     : '\U000021c1',
        '\\<leftharpoonup>'        : '\U000021bc',
        '\\<rightharpoonup>'       : '\U000021c0',
        '\\<rightleftharpoons>'    : '\U000021cc',
        '\\<leadsto>'              : '\U0000219d',
        '\\<downharpoonleft>'      : '\U000021c3',
        '\\<downharpoonright>'     : '\U000021c2',
        '\\<upharpoonleft>'        : '\U000021bf',
        '\\<upharpoonright>'       : '\U000021be',
        '\\<restriction>'          : '\U000021be',
        '\\<Colon>'                : '\U00002237',
        '\\<up>'                   : '\U00002191',
        '\\<Up>'                   : '\U000021d1',
        '\\<down>'                 : '\U00002193',
        '\\<Down>'                 : '\U000021d3',
        '\\<updown>'               : '\U00002195',
        '\\<Updown>'               : '\U000021d5',
        '\\<langle>'               : '\U000027e8',
        '\\<rangle>'               : '\U000027e9',
        '\\<lceil>'                : '\U00002308',
        '\\<rceil>'                : '\U00002309',
        '\\<lfloor>'               : '\U0000230a',
        '\\<rfloor>'               : '\U0000230b',
        '\\<lparr>'                : '\U00002987',
        '\\<rparr>'                : '\U00002988',
        '\\<lbrakk>'               : '\U000027e6',
        '\\<rbrakk>'               : '\U000027e7',
        '\\<lbrace>'               : '\U00002983',
        '\\<rbrace>'               : '\U00002984',
        '\\<guillemotleft>'        : '\U000000ab',
        '\\<guillemotright>'       : '\U000000bb',
        '\\<bottom>'               : '\U000022a5',
        '\\<top>'                  : '\U000022a4',
        '\\<and>'                  : '\U00002227',
        '\\<And>'                  : '\U000022c0',
        '\\<or>'                   : '\U00002228',
        '\\<Or>'                   : '\U000022c1',
        '\\<forall>'               : '\U00002200',
        '\\<exists>'               : '\U00002203',
        '\\<nexists>'              : '\U00002204',
        '\\<not>'                  : '\U000000ac',
        '\\<box>'                  : '\U000025a1',
        '\\<diamond>'              : '\U000025c7',
        '\\<turnstile>'            : '\U000022a2',
        '\\<Turnstile>'            : '\U000022a8',
        '\\<tturnstile>'           : '\U000022a9',
        '\\<TTurnstile>'           : '\U000022ab',
        '\\<stileturn>'            : '\U000022a3',
        '\\<surd>'                 : '\U0000221a',
        '\\<le>'                   : '\U00002264',
        '\\<ge>'                   : '\U00002265',
        '\\<lless>'                : '\U0000226a',
        '\\<ggreater>'             : '\U0000226b',
        '\\<lesssim>'              : '\U00002272',
        '\\<greatersim>'           : '\U00002273',
        '\\<lessapprox>'           : '\U00002a85',
        '\\<greaterapprox>'        : '\U00002a86',
        '\\<in>'                   : '\U00002208',
        '\\<notin>'                : '\U00002209',
        '\\<subset>'               : '\U00002282',
        '\\<supset>'               : '\U00002283',
        '\\<subseteq>'             : '\U00002286',
        '\\<supseteq>'             : '\U00002287',
        '\\<sqsubset>'             : '\U0000228f',
        '\\<sqsupset>'             : '\U00002290',
        '\\<sqsubseteq>'           : '\U00002291',
        '\\<sqsupseteq>'           : '\U00002292',
        '\\<inter>'                : '\U00002229',
        '\\<Inter>'                : '\U000022c2',
        '\\<union>'                : '\U0000222a',
        '\\<Union>'                : '\U000022c3',
        '\\<squnion>'              : '\U00002294',
        '\\<Squnion>'              : '\U00002a06',
        '\\<sqinter>'              : '\U00002293',
        '\\<Sqinter>'              : '\U00002a05',
        '\\<setminus>'             : '\U00002216',
        '\\<propto>'               : '\U0000221d',
        '\\<uplus>'                : '\U0000228e',
        '\\<Uplus>'                : '\U00002a04',
        '\\<noteq>'                : '\U00002260',
        '\\<sim>'                  : '\U0000223c',
        '\\<doteq>'                : '\U00002250',
        '\\<simeq>'                : '\U00002243',
        '\\<approx>'               : '\U00002248',
        '\\<asymp>'                : '\U0000224d',
        '\\<cong>'                 : '\U00002245',
        '\\<smile>'                : '\U00002323',
        '\\<equiv>'                : '\U00002261',
        '\\<frown>'                : '\U00002322',
        '\\<Join>'                 : '\U000022c8',
        '\\<bowtie>'               : '\U00002a1d',
        '\\<prec>'                 : '\U0000227a',
        '\\<succ>'                 : '\U0000227b',
        '\\<preceq>'               : '\U0000227c',
        '\\<succeq>'               : '\U0000227d',
        '\\<parallel>'             : '\U00002225',
        '\\<bar>'                  : '\U000000a6',
        '\\<plusminus>'            : '\U000000b1',
        '\\<minusplus>'            : '\U00002213',
        '\\<times>'                : '\U000000d7',
        '\\<div>'                  : '\U000000f7',
        '\\<cdot>'                 : '\U000022c5',
        '\\<star>'                 : '\U000022c6',
        '\\<bullet>'               : '\U00002219',
        '\\<circ>'                 : '\U00002218',
        '\\<dagger>'               : '\U00002020',
        '\\<ddagger>'              : '\U00002021',
        '\\<lhd>'                  : '\U000022b2',
        '\\<rhd>'                  : '\U000022b3',
        '\\<unlhd>'                : '\U000022b4',
        '\\<unrhd>'                : '\U000022b5',
        '\\<triangleleft>'         : '\U000025c3',
        '\\<triangleright>'        : '\U000025b9',
        '\\<triangle>'             : '\U000025b3',
        '\\<triangleq>'            : '\U0000225c',
        '\\<oplus>'                : '\U00002295',
        '\\<Oplus>'                : '\U00002a01',
        '\\<otimes>'               : '\U00002297',
        '\\<Otimes>'               : '\U00002a02',
        '\\<odot>'                 : '\U00002299',
        '\\<Odot>'                 : '\U00002a00',
        '\\<ominus>'               : '\U00002296',
        '\\<oslash>'               : '\U00002298',
        '\\<dots>'                 : '\U00002026',
        '\\<cdots>'                : '\U000022ef',
        '\\<Sum>'                  : '\U00002211',
        '\\<Prod>'                 : '\U0000220f',
        '\\<Coprod>'               : '\U00002210',
        '\\<infinity>'             : '\U0000221e',
        '\\<integral>'             : '\U0000222b',
        '\\<ointegral>'            : '\U0000222e',
        '\\<clubsuit>'             : '\U00002663',
        '\\<diamondsuit>'          : '\U00002662',
        '\\<heartsuit>'            : '\U00002661',
        '\\<spadesuit>'            : '\U00002660',
        '\\<aleph>'                : '\U00002135',
        '\\<emptyset>'             : '\U00002205',
        '\\<nabla>'                : '\U00002207',
        '\\<partial>'              : '\U00002202',
        '\\<flat>'                 : '\U0000266d',
        '\\<natural>'              : '\U0000266e',
        '\\<sharp>'                : '\U0000266f',
        '\\<angle>'                : '\U00002220',
        '\\<copyright>'            : '\U000000a9',
        '\\<registered>'           : '\U000000ae',
        '\\<hyphen>'               : '\U000000ad',
        '\\<inverse>'              : '\U000000af',
        '\\<onequarter>'           : '\U000000bc',
        '\\<onehalf>'              : '\U000000bd',
        '\\<threequarters>'        : '\U000000be',
        '\\<ordfeminine>'          : '\U000000aa',
        '\\<ordmasculine>'         : '\U000000ba',
        '\\<section>'              : '\U000000a7',
        '\\<paragraph>'            : '\U000000b6',
        '\\<exclamdown>'           : '\U000000a1',
        '\\<questiondown>'         : '\U000000bf',
        '\\<euro>'                 : '\U000020ac',
        '\\<pounds>'               : '\U000000a3',
        '\\<yen>'                  : '\U000000a5',
        '\\<cent>'                 : '\U000000a2',
        '\\<currency>'             : '\U000000a4',
        '\\<degree>'               : '\U000000b0',
        '\\<amalg>'                : '\U00002a3f',
        '\\<mho>'                  : '\U00002127',
        '\\<lozenge>'              : '\U000025ca',
        '\\<wp>'                   : '\U00002118',
        '\\<wrong>'                : '\U00002240',
        '\\<struct>'               : '\U000022c4',
        '\\<acute>'                : '\U000000b4',
        '\\<index>'                : '\U00000131',
        '\\<dieresis>'             : '\U000000a8',
        '\\<cedilla>'              : '\U000000b8',
        '\\<hungarumlaut>'         : '\U000002dd',
        '\\<some>'                 : '\U000003f5',
        '\\<newline>'              : '\U000023ce',
        '\\<open>'                 : '\U00002039',
        '\\<close>'                : '\U0000203a',
        '\\<here>'                 : '\U00002302',
        '\\<^sub>'                 : '\U000021e9',
        '\\<^sup>'                 : '\U000021e7',
        '\\<^bold>'                : '\U00002759',
        '\\<^bsub>'                : '\U000021d8',
        '\\<^esub>'                : '\U000021d9',
        '\\<^bsup>'                : '\U000021d7',
        '\\<^esup>'                : '\U000021d6',
    }

    lang_map = {'isabelle' : isabelle_symbols, 'latex' : latex_symbols}

    def __init__(self, **options):
        Filter.__init__(self, **options)
        lang = get_choice_opt(options, 'lang',
                              ['isabelle', 'latex'], 'isabelle')
        self.symbols = self.lang_map[lang]

    def filter(self, lexer, stream):
        for ttype, value in stream:
            if value in self.symbols:
                yield ttype, self.symbols[value]
            else:
                yield ttype, value


class KeywordCaseFilter(Filter):
    """Convert keywords to lowercase or uppercase or capitalize them, which
    means first letter uppercase, rest lowercase.

    This can be useful e.g. if you highlight Pascal code and want to adapt the
    code to your styleguide.

    Options accepted:

    `case` : string
       The casing to convert keywords to. Must be one of ``'lower'``,
       ``'upper'`` or ``'capitalize'``.  The default is ``'lower'``.
    """

    def __init__(self, **options):
        Filter.__init__(self, **options)
        case = get_choice_opt(options, 'case',
                              ['lower', 'upper', 'capitalize'], 'lower')
        self.convert = getattr(str, case)

    def filter(self, lexer, stream):
        for ttype, value in stream:
            if ttype in Keyword:
                yield ttype, self.convert(value)
            else:
                yield ttype, value


class NameHighlightFilter(Filter):
    """Highlight a normal Name (and Name.*) token with a different token type.

    Example::

        filter = NameHighlightFilter(
            names=['foo', 'bar', 'baz'],
            tokentype=Name.Function,
        )

    This would highlight the names "foo", "bar" and "baz"
    as functions. `Name.Function` is the default token type.

    Options accepted:

    `names` : list of strings
      A list of names that should be given the different token type.
      There is no default.
    `tokentype` : TokenType or string
      A token type or a string containing a token type name that is
      used for highlighting the strings in `names`.  The default is
      `Name.Function`.
    """

    def __init__(self, **options):
        Filter.__init__(self, **options)
        self.names = set(get_list_opt(options, 'names', []))
        tokentype = options.get('tokentype')
        if tokentype:
            self.tokentype = string_to_tokentype(tokentype)
        else:
            self.tokentype = Name.Function

    def filter(self, lexer, stream):
        for ttype, value in stream:
            if ttype in Name and value in self.names:
                yield self.tokentype, value
            else:
                yield ttype, value


class ErrorToken(Exception):
    pass


class RaiseOnErrorTokenFilter(Filter):
    """Raise an exception when the lexer generates an error token.

    Options accepted:

    `excclass` : Exception class
      The exception class to raise.
      The default is `pygments.filters.ErrorToken`.

    .. versionadded:: 0.8
    """

    def __init__(self, **options):
        Filter.__init__(self, **options)
        self.exception = options.get('excclass', ErrorToken)
        try:
            # issubclass() will raise TypeError if first argument is not a class
            if not issubclass(self.exception, Exception):
                raise TypeError
        except TypeError:
            raise OptionError('excclass option is not an exception class')

    def filter(self, lexer, stream):
        for ttype, value in stream:
            if ttype is Error:
                raise self.exception(value)
            yield ttype, value


class VisibleWhitespaceFilter(Filter):
    """Convert tabs, newlines and/or spaces to visible characters.

    Options accepted:

    `spaces` : string or bool
      If this is a one-character string, spaces will be replaces by this string.
      If it is another true value, spaces will be replaced by ```` (unicode
      MIDDLE DOT).  If it is a false value, spaces will not be replaced.  The
      default is ``False``.
    `tabs` : string or bool
      The same as for `spaces`, but the default replacement character is ````
      (unicode RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK).  The default value
      is ``False``.  Note: this will not work if the `tabsize` option for the
      lexer is nonzero, as tabs will already have been expanded then.
    `tabsize` : int
      If tabs are to be replaced by this filter (see the `tabs` option), this
      is the total number of characters that a tab should be expanded to.
      The default is ``8``.
    `newlines` : string or bool
      The same as for `spaces`, but the default replacement character is ````
      (unicode PILCROW SIGN).  The default value is ``False``.
    `wstokentype` : bool
      If true, give whitespace the special `Whitespace` token type.  This allows
      styling the visible whitespace differently (e.g. greyed out), but it can
      disrupt background colors.  The default is ``True``.

    .. versionadded:: 0.8
    """

    def __init__(self, **options):
        Filter.__init__(self, **options)
        for name, default in [('spaces',   ''),
                              ('tabs',     ''),
                              ('newlines', '')]:
            opt = options.get(name, False)
            if isinstance(opt, str) and len(opt) == 1:
                setattr(self, name, opt)
            else:
                setattr(self, name, (opt and default or ''))
        tabsize = get_int_opt(options, 'tabsize', 8)
        if self.tabs:
            self.tabs += ' ' * (tabsize - 1)
        if self.newlines:
            self.newlines += '\n'
        self.wstt = get_bool_opt(options, 'wstokentype', True)

    def filter(self, lexer, stream):
        if self.wstt:
            spaces = self.spaces or ' '
            tabs = self.tabs or '\t'
            newlines = self.newlines or '\n'
            regex = re.compile(r'\s')

            def replacefunc(wschar):
                if wschar == ' ':
                    return spaces
                elif wschar == '\t':
                    return tabs
                elif wschar == '\n':
                    return newlines
                return wschar

            for ttype, value in stream:
                yield from _replace_special(ttype, value, regex, Whitespace,
                                            replacefunc)
        else:
            spaces, tabs, newlines = self.spaces, self.tabs, self.newlines
            # simpler processing
            for ttype, value in stream:
                if spaces:
                    value = value.replace(' ', spaces)
                if tabs:
                    value = value.replace('\t', tabs)
                if newlines:
                    value = value.replace('\n', newlines)
                yield ttype, value


class GobbleFilter(Filter):
    """Gobbles source code lines (eats initial characters).

    This filter drops the first ``n`` characters off every line of code.  This
    may be useful when the source code fed to the lexer is indented by a fixed
    amount of space that isn't desired in the output.

    Options accepted:

    `n` : int
       The number of characters to gobble.

    .. versionadded:: 1.2
    """
    def __init__(self, **options):
        Filter.__init__(self, **options)
        self.n = get_int_opt(options, 'n', 0)

    def gobble(self, value, left):
        if left < len(value):
            return value[left:], 0
        else:
            return '', left - len(value)

    def filter(self, lexer, stream):
        n = self.n
        left = n  # How many characters left to gobble.
        for ttype, value in stream:
            # Remove ``left`` tokens from first line, ``n`` from all others.
            parts = value.split('\n')
            (parts[0], left) = self.gobble(parts[0], left)
            for i in range(1, len(parts)):
                (parts[i], left) = self.gobble(parts[i], n)
            value = '\n'.join(parts)

            if value != '':
                yield ttype, value


class TokenMergeFilter(Filter):
    """Merges consecutive tokens with the same token type in the output
    stream of a lexer.

    .. versionadded:: 1.2
    """
    def __init__(self, **options):
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        current_type = None
        current_value = None
        for ttype, value in stream:
            if ttype is current_type:
                current_value += value
            else:
                if current_type is not None:
                    yield current_type, current_value
                current_type = ttype
                current_value = value
        if current_type is not None:
            yield current_type, current_value


FILTERS = {
    'codetagify':     CodeTagFilter,
    'keywordcase':    KeywordCaseFilter,
    'highlight':      NameHighlightFilter,
    'raiseonerror':   RaiseOnErrorTokenFilter,
    'whitespace':     VisibleWhitespaceFilter,
    'gobble':         GobbleFilter,
    'tokenmerge':     TokenMergeFilter,
    'symbols':        SymbolFilter,
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/util.py
# ========================================================
"""
    pygments.util
    ~~~~~~~~~~~~~

    Utility functions.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
from io import TextIOWrapper


split_path_re = re.compile(r'[/\\ ]')
doctype_lookup_re = re.compile(r'''
    <!DOCTYPE\s+(
     [a-zA-Z_][a-zA-Z0-9]*
     (?: \s+      # optional in HTML5
     [a-zA-Z_][a-zA-Z0-9]*\s+
     "[^"]*")?
     )
     [^>]*>
''', re.DOTALL | re.MULTILINE | re.VERBOSE)
tag_re = re.compile(r'<(.+?)(\s.*?)?>.*?</.+?>',
                    re.IGNORECASE | re.DOTALL | re.MULTILINE)
xml_decl_re = re.compile(r'\s*<\?xml[^>]*\?>', re.I)


class ClassNotFound(ValueError):
    """Raised if one of the lookup functions didn't find a matching class."""


class OptionError(Exception):
    """
    This exception will be raised by all option processing functions if
    the type or value of the argument is not correct.
    """

def get_choice_opt(options, optname, allowed, default=None, normcase=False):
    """
    If the key `optname` from the dictionary is not in the sequence
    `allowed`, raise an error, otherwise return it.
    """
    string = options.get(optname, default)
    if normcase:
        string = string.lower()
    if string not in allowed:
        raise OptionError('Value for option %s must be one of %s' %
                          (optname, ', '.join(map(str, allowed))))
    return string


def get_bool_opt(options, optname, default=None):
    """
    Intuitively, this is `options.get(optname, default)`, but restricted to
    Boolean value. The Booleans can be represented as string, in order to accept
    Boolean value from the command line arguments. If the key `optname` is
    present in the dictionary `options` and is not associated with a Boolean,
    raise an `OptionError`. If it is absent, `default` is returned instead.

    The valid string values for ``True`` are ``1``, ``yes``, ``true`` and
    ``on``, the ones for ``False`` are ``0``, ``no``, ``false`` and ``off``
    (matched case-insensitively).
    """
    string = options.get(optname, default)
    if isinstance(string, bool):
        return string
    elif isinstance(string, int):
        return bool(string)
    elif not isinstance(string, str):
        raise OptionError('Invalid type %r for option %s; use '
                          '1/0, yes/no, true/false, on/off' % (
                              string, optname))
    elif string.lower() in ('1', 'yes', 'true', 'on'):
        return True
    elif string.lower() in ('0', 'no', 'false', 'off'):
        return False
    else:
        raise OptionError('Invalid value %r for option %s; use '
                          '1/0, yes/no, true/false, on/off' % (
                              string, optname))


def get_int_opt(options, optname, default=None):
    """As :func:`get_bool_opt`, but interpret the value as an integer."""
    string = options.get(optname, default)
    try:
        return int(string)
    except TypeError:
        raise OptionError('Invalid type %r for option %s; you '
                          'must give an integer value' % (
                              string, optname))
    except ValueError:
        raise OptionError('Invalid value %r for option %s; you '
                          'must give an integer value' % (
                              string, optname))

def get_list_opt(options, optname, default=None):
    """
    If the key `optname` from the dictionary `options` is a string,
    split it at whitespace and return it. If it is already a list
    or a tuple, it is returned as a list.
    """
    val = options.get(optname, default)
    if isinstance(val, str):
        return val.split()
    elif isinstance(val, (list, tuple)):
        return list(val)
    else:
        raise OptionError('Invalid type %r for option %s; you '
                          'must give a list value' % (
                              val, optname))


def docstring_headline(obj):
    if not obj.__doc__:
        return ''
    res = []
    for line in obj.__doc__.strip().splitlines():
        if line.strip():
            res.append(" " + line.strip())
        else:
            break
    return ''.join(res).lstrip()


def make_analysator(f):
    """Return a static text analyser function that returns float values."""
    def text_analyse(text):
        try:
            rv = f(text)
        except Exception:
            return 0.0
        if not rv:
            return 0.0
        try:
            return min(1.0, max(0.0, float(rv)))
        except (ValueError, TypeError):
            return 0.0
    text_analyse.__doc__ = f.__doc__
    return staticmethod(text_analyse)


def shebang_matches(text, regex):
    r"""Check if the given regular expression matches the last part of the
    shebang if one exists.

        >>> from pygments.util import shebang_matches
        >>> shebang_matches('#!/usr/bin/env python', r'python(2\.\d)?')
        True
        >>> shebang_matches('#!/usr/bin/python2.4', r'python(2\.\d)?')
        True
        >>> shebang_matches('#!/usr/bin/python-ruby', r'python(2\.\d)?')
        False
        >>> shebang_matches('#!/usr/bin/python/ruby', r'python(2\.\d)?')
        False
        >>> shebang_matches('#!/usr/bin/startsomethingwith python',
        ...                 r'python(2\.\d)?')
        True

    It also checks for common windows executable file extensions::

        >>> shebang_matches('#!C:\\Python2.4\\Python.exe', r'python(2\.\d)?')
        True

    Parameters (``'-f'`` or ``'--foo'`` are ignored so ``'perl'`` does
    the same as ``'perl -e'``)

    Note that this method automatically searches the whole string (eg:
    the regular expression is wrapped in ``'^$'``)
    """
    index = text.find('\n')
    if index >= 0:
        first_line = text[:index].lower()
    else:
        first_line = text.lower()
    if first_line.startswith('#!'):
        try:
            found = [x for x in split_path_re.split(first_line[2:].strip())
                     if x and not x.startswith('-')][-1]
        except IndexError:
            return False
        regex = re.compile(r'^%s(\.(exe|cmd|bat|bin))?$' % regex, re.IGNORECASE)
        if regex.search(found) is not None:
            return True
    return False


def doctype_matches(text, regex):
    """Check if the doctype matches a regular expression (if present).

    Note that this method only checks the first part of a DOCTYPE.
    eg: 'html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"'
    """
    m = doctype_lookup_re.search(text)
    if m is None:
        return False
    doctype = m.group(1)
    return re.compile(regex, re.I).match(doctype.strip()) is not None


def html_doctype_matches(text):
    """Check if the file looks like it has a html doctype."""
    return doctype_matches(text, r'html')


_looks_like_xml_cache = {}


def looks_like_xml(text):
    """Check if a doctype exists or if we have some tags."""
    if xml_decl_re.match(text):
        return True
    key = hash(text)
    try:
        return _looks_like_xml_cache[key]
    except KeyError:
        m = doctype_lookup_re.search(text)
        if m is not None:
            return True
        rv = tag_re.search(text[:1000]) is not None
        _looks_like_xml_cache[key] = rv
        return rv


def surrogatepair(c):
    """Given a unicode character code with length greater than 16 bits,
    return the two 16 bit surrogate pair.
    """
    # From example D28 of:
    # http://www.unicode.org/book/ch03.pdf
    return (0xd7c0 + (c >> 10), (0xdc00 + (c & 0x3ff)))


def format_lines(var_name, seq, raw=False, indent_level=0):
    """Formats a sequence of strings for output."""
    lines = []
    base_indent = ' ' * indent_level * 4
    inner_indent = ' ' * (indent_level + 1) * 4
    lines.append(base_indent + var_name + ' = (')
    if raw:
        # These should be preformatted reprs of, say, tuples.
        for i in seq:
            lines.append(inner_indent + i + ',')
    else:
        for i in seq:
            # Force use of single quotes
            r = repr(i + '"')
            lines.append(inner_indent + r[:-2] + r[-1] + ',')
    lines.append(base_indent + ')')
    return '\n'.join(lines)


def duplicates_removed(it, already_seen=()):
    """
    Returns a list with duplicates removed from the iterable `it`.

    Order is preserved.
    """
    lst = []
    seen = set()
    for i in it:
        if i in seen or i in already_seen:
            continue
        lst.append(i)
        seen.add(i)
    return lst


class Future:
    """Generic class to defer some work.

    Handled specially in RegexLexerMeta, to support regex string construction at
    first use.
    """
    def get(self):
        raise NotImplementedError


def guess_decode(text):
    """Decode *text* with guessed encoding.

    First try UTF-8; this should fail for non-UTF-8 encodings.
    Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """
    try:
        text = text.decode('utf-8')
        return text, 'utf-8'
    except UnicodeDecodeError:
        try:
            import locale
            prefencoding = locale.getpreferredencoding()
            text = text.decode()
            return text, prefencoding
        except (UnicodeDecodeError, LookupError):
            text = text.decode('latin1')
            return text, 'latin1'


def guess_decode_from_terminal(text, term):
    """Decode *text* coming from terminal *term*.

    First try the terminal encoding, if given.
    Then try UTF-8.  Then try the preferred locale encoding.
    Fall back to latin-1, which always works.
    """
    if getattr(term, 'encoding', None):
        try:
            text = text.decode(term.encoding)
        except UnicodeDecodeError:
            pass
        else:
            return text, term.encoding
    return guess_decode(text)


def terminal_encoding(term):
    """Return our best guess of encoding for the given *term*."""
    if getattr(term, 'encoding', None):
        return term.encoding
    import locale
    return locale.getpreferredencoding()


class UnclosingTextIOWrapper(TextIOWrapper):
    # Don't close underlying buffer on destruction.
    def close(self):
        self.flush()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pygments/regexopt.py
# ========================================================
"""
    pygments.regexopt
    ~~~~~~~~~~~~~~~~~

    An algorithm that generates optimized regexes for matching long lists of
    literal strings.

    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""

import re
from re import escape
from os.path import commonprefix
from itertools import groupby
from operator import itemgetter

CS_ESCAPE = re.compile(r'[\[\^\\\-\]]')
FIRST_ELEMENT = itemgetter(0)


def make_charset(letters):
    return '[' + CS_ESCAPE.sub(lambda m: '\\' + m.group(), ''.join(letters)) + ']'


def regex_opt_inner(strings, open_paren):
    """Return a regex that matches any string in the sorted list of strings."""
    close_paren = open_paren and ')' or ''
    # print strings, repr(open_paren)
    if not strings:
        # print '-> nothing left'
        return ''
    first = strings[0]
    if len(strings) == 1:
        # print '-> only 1 string'
        return open_paren + escape(first) + close_paren
    if not first:
        # print '-> first string empty'
        return open_paren + regex_opt_inner(strings[1:], '(?:') \
            + '?' + close_paren
    if len(first) == 1:
        # multiple one-char strings? make a charset
        oneletter = []
        rest = []
        for s in strings:
            if len(s) == 1:
                oneletter.append(s)
            else:
                rest.append(s)
        if len(oneletter) > 1:  # do we have more than one oneletter string?
            if rest:
                # print '-> 1-character + rest'
                return open_paren + regex_opt_inner(rest, '') + '|' \
                    + make_charset(oneletter) + close_paren
            # print '-> only 1-character'
            return open_paren + make_charset(oneletter) + close_paren
    prefix = commonprefix(strings)
    if prefix:
        plen = len(prefix)
        # we have a prefix for all strings
        # print '-> prefix:', prefix
        return open_paren + escape(prefix) \
            + regex_opt_inner([s[plen:] for s in strings], '(?:') \
            + close_paren
    # is there a suffix?
    strings_rev = [s[::-1] for s in strings]
    suffix = commonprefix(strings_rev)
    if suffix:
        slen = len(suffix)
        # print '-> suffix:', suffix[::-1]
        return open_paren \
            + regex_opt_inner(sorted(s[:-slen] for s in strings), '(?:') \
            + escape(suffix[::-1]) + close_paren
    # recurse on common 1-string prefixes
    # print '-> last resort'
    return open_paren + \
        '|'.join(regex_opt_inner(list(group[1]), '')
                 for group in groupby(strings, lambda s: s[0] == first[0])) \
        + close_paren


def regex_opt(strings, prefix='', suffix=''):
    """Return a compiled regex that matches any string in the given list.

    The strings to match must be literal strings, not regexes.  They will be
    regex-escaped.

    *prefix* and *suffix* are pre- and appended to the final regex.
    """
    strings = sorted(strings)
    return prefix + regex_opt_inner(strings, '(') + suffix


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/diagram/__init__.py
# ========================================================
# mypy: ignore-errors
import railroad
from pip._vendor import pyparsing
import typing
from typing import (
    List,
    NamedTuple,
    Generic,
    TypeVar,
    Dict,
    Callable,
    Set,
    Iterable,
)
from jinja2 import Template
from io import StringIO
import inspect


jinja2_template_source = """\
{% if not embed %}
<!DOCTYPE html>
<html>
<head>
{% endif %}
    {% if not head %}
        <style>
            .railroad-heading {
                font-family: monospace;
            }
        </style>
    {% else %}
        {{ head | safe }}
    {% endif %}
{% if not embed %}
</head>
<body>
{% endif %}
{{ body | safe }}
{% for diagram in diagrams %}
    <div class="railroad-group">
        <h1 class="railroad-heading">{{ diagram.title }}</h1>
        <div class="railroad-description">{{ diagram.text }}</div>
        <div class="railroad-svg">
            {{ diagram.svg }}
        </div>
    </div>
{% endfor %}
{% if not embed %}
</body>
</html>
{% endif %}
"""

template = Template(jinja2_template_source)

# Note: ideally this would be a dataclass, but we're supporting Python 3.5+ so we can't do this yet
NamedDiagram = NamedTuple(
    "NamedDiagram",
    [("name", str), ("diagram", typing.Optional[railroad.DiagramItem]), ("index", int)],
)
"""
A simple structure for associating a name with a railroad diagram
"""

T = TypeVar("T")


class EachItem(railroad.Group):
    """
    Custom railroad item to compose a:
    - Group containing a
      - OneOrMore containing a
        - Choice of the elements in the Each
    with the group label indicating that all must be matched
    """

    all_label = "[ALL]"

    def __init__(self, *items):
        choice_item = railroad.Choice(len(items) - 1, *items)
        one_or_more_item = railroad.OneOrMore(item=choice_item)
        super().__init__(one_or_more_item, label=self.all_label)


class AnnotatedItem(railroad.Group):
    """
    Simple subclass of Group that creates an annotation label
    """

    def __init__(self, label: str, item):
        super().__init__(item=item, label="[{}]".format(label) if label else label)


class EditablePartial(Generic[T]):
    """
    Acts like a functools.partial, but can be edited. In other words, it represents a type that hasn't yet been
    constructed.
    """

    # We need this here because the railroad constructors actually transform the data, so can't be called until the
    # entire tree is assembled

    def __init__(self, func: Callable[..., T], args: list, kwargs: dict):
        self.func = func
        self.args = args
        self.kwargs = kwargs

    @classmethod
    def from_call(cls, func: Callable[..., T], *args, **kwargs) -> "EditablePartial[T]":
        """
        If you call this function in the same way that you would call the constructor, it will store the arguments
        as you expect. For example EditablePartial.from_call(Fraction, 1, 3)() == Fraction(1, 3)
        """
        return EditablePartial(func=func, args=list(args), kwargs=kwargs)

    @property
    def name(self):
        return self.kwargs["name"]

    def __call__(self) -> T:
        """
        Evaluate the partial and return the result
        """
        args = self.args.copy()
        kwargs = self.kwargs.copy()

        # This is a helpful hack to allow you to specify varargs parameters (e.g. *args) as keyword args (e.g.
        # args=['list', 'of', 'things'])
        arg_spec = inspect.getfullargspec(self.func)
        if arg_spec.varargs in self.kwargs:
            args += kwargs.pop(arg_spec.varargs)

        return self.func(*args, **kwargs)


def railroad_to_html(diagrams: List[NamedDiagram], embed=False, **kwargs) -> str:
    """
    Given a list of NamedDiagram, produce a single HTML string that visualises those diagrams
    :params kwargs: kwargs to be passed in to the template
    """
    data = []
    for diagram in diagrams:
        if diagram.diagram is None:
            continue
        io = StringIO()
        try:
            css = kwargs.get('css')
            diagram.diagram.writeStandalone(io.write, css=css)
        except AttributeError:
            diagram.diagram.writeSvg(io.write)
        title = diagram.name
        if diagram.index == 0:
            title += " (root)"
        data.append({"title": title, "text": "", "svg": io.getvalue()})

    return template.render(diagrams=data, embed=embed, **kwargs)


def resolve_partial(partial: "EditablePartial[T]") -> T:
    """
    Recursively resolves a collection of Partials into whatever type they are
    """
    if isinstance(partial, EditablePartial):
        partial.args = resolve_partial(partial.args)
        partial.kwargs = resolve_partial(partial.kwargs)
        return partial()
    elif isinstance(partial, list):
        return [resolve_partial(x) for x in partial]
    elif isinstance(partial, dict):
        return {key: resolve_partial(x) for key, x in partial.items()}
    else:
        return partial


def to_railroad(
    element: pyparsing.ParserElement,
    diagram_kwargs: typing.Optional[dict] = None,
    vertical: int = 3,
    show_results_names: bool = False,
    show_groups: bool = False,
) -> List[NamedDiagram]:
    """
    Convert a pyparsing element tree into a list of diagrams. This is the recommended entrypoint to diagram
    creation if you want to access the Railroad tree before it is converted to HTML
    :param element: base element of the parser being diagrammed
    :param diagram_kwargs: kwargs to pass to the Diagram() constructor
    :param vertical: (optional) - int - limit at which number of alternatives should be
       shown vertically instead of horizontally
    :param show_results_names - bool to indicate whether results name annotations should be
       included in the diagram
    :param show_groups - bool to indicate whether groups should be highlighted with an unlabeled
       surrounding box
    """
    # Convert the whole tree underneath the root
    lookup = ConverterState(diagram_kwargs=diagram_kwargs or {})
    _to_diagram_element(
        element,
        lookup=lookup,
        parent=None,
        vertical=vertical,
        show_results_names=show_results_names,
        show_groups=show_groups,
    )

    root_id = id(element)
    # Convert the root if it hasn't been already
    if root_id in lookup:
        if not element.customName:
            lookup[root_id].name = ""
        lookup[root_id].mark_for_extraction(root_id, lookup, force=True)

    # Now that we're finished, we can convert from intermediate structures into Railroad elements
    diags = list(lookup.diagrams.values())
    if len(diags) > 1:
        # collapse out duplicate diags with the same name
        seen = set()
        deduped_diags = []
        for d in diags:
            # don't extract SkipTo elements, they are uninformative as subdiagrams
            if d.name == "...":
                continue
            if d.name is not None and d.name not in seen:
                seen.add(d.name)
                deduped_diags.append(d)
        resolved = [resolve_partial(partial) for partial in deduped_diags]
    else:
        # special case - if just one diagram, always display it, even if
        # it has no name
        resolved = [resolve_partial(partial) for partial in diags]
    return sorted(resolved, key=lambda diag: diag.index)


def _should_vertical(
    specification: int, exprs: Iterable[pyparsing.ParserElement]
) -> bool:
    """
    Returns true if we should return a vertical list of elements
    """
    if specification is None:
        return False
    else:
        return len(_visible_exprs(exprs)) >= specification


class ElementState:
    """
    State recorded for an individual pyparsing Element
    """

    # Note: this should be a dataclass, but we have to support Python 3.5
    def __init__(
        self,
        element: pyparsing.ParserElement,
        converted: EditablePartial,
        parent: EditablePartial,
        number: int,
        name: str = None,
        parent_index: typing.Optional[int] = None,
    ):
        #: The pyparsing element that this represents
        self.element: pyparsing.ParserElement = element
        #: The name of the element
        self.name: typing.Optional[str] = name
        #: The output Railroad element in an unconverted state
        self.converted: EditablePartial = converted
        #: The parent Railroad element, which we store so that we can extract this if it's duplicated
        self.parent: EditablePartial = parent
        #: The order in which we found this element, used for sorting diagrams if this is extracted into a diagram
        self.number: int = number
        #: The index of this inside its parent
        self.parent_index: typing.Optional[int] = parent_index
        #: If true, we should extract this out into a subdiagram
        self.extract: bool = False
        #: If true, all of this element's children have been filled out
        self.complete: bool = False

    def mark_for_extraction(
        self, el_id: int, state: "ConverterState", name: str = None, force: bool = False
    ):
        """
        Called when this instance has been seen twice, and thus should eventually be extracted into a sub-diagram
        :param el_id: id of the element
        :param state: element/diagram state tracker
        :param name: name to use for this element's text
        :param force: If true, force extraction now, regardless of the state of this. Only useful for extracting the
        root element when we know we're finished
        """
        self.extract = True

        # Set the name
        if not self.name:
            if name:
                # Allow forcing a custom name
                self.name = name
            elif self.element.customName:
                self.name = self.element.customName
            else:
                self.name = ""

        # Just because this is marked for extraction doesn't mean we can do it yet. We may have to wait for children
        # to be added
        # Also, if this is just a string literal etc, don't bother extracting it
        if force or (self.complete and _worth_extracting(self.element)):
            state.extract_into_diagram(el_id)


class ConverterState:
    """
    Stores some state that persists between recursions into the element tree
    """

    def __init__(self, diagram_kwargs: typing.Optional[dict] = None):
        #: A dictionary mapping ParserElements to state relating to them
        self._element_diagram_states: Dict[int, ElementState] = {}
        #: A dictionary mapping ParserElement IDs to subdiagrams generated from them
        self.diagrams: Dict[int, EditablePartial[NamedDiagram]] = {}
        #: The index of the next unnamed element
        self.unnamed_index: int = 1
        #: The index of the next element. This is used for sorting
        self.index: int = 0
        #: Shared kwargs that are used to customize the construction of diagrams
        self.diagram_kwargs: dict = diagram_kwargs or {}
        self.extracted_diagram_names: Set[str] = set()

    def __setitem__(self, key: int, value: ElementState):
        self._element_diagram_states[key] = value

    def __getitem__(self, key: int) -> ElementState:
        return self._element_diagram_states[key]

    def __delitem__(self, key: int):
        del self._element_diagram_states[key]

    def __contains__(self, key: int):
        return key in self._element_diagram_states

    def generate_unnamed(self) -> int:
        """
        Generate a number used in the name of an otherwise unnamed diagram
        """
        self.unnamed_index += 1
        return self.unnamed_index

    def generate_index(self) -> int:
        """
        Generate a number used to index a diagram
        """
        self.index += 1
        return self.index

    def extract_into_diagram(self, el_id: int):
        """
        Used when we encounter the same token twice in the same tree. When this
        happens, we replace all instances of that token with a terminal, and
        create a new subdiagram for the token
        """
        position = self[el_id]

        # Replace the original definition of this element with a regular block
        if position.parent:
            ret = EditablePartial.from_call(railroad.NonTerminal, text=position.name)
            if "item" in position.parent.kwargs:
                position.parent.kwargs["item"] = ret
            elif "items" in position.parent.kwargs:
                position.parent.kwargs["items"][position.parent_index] = ret

        # If the element we're extracting is a group, skip to its content but keep the title
        if position.converted.func == railroad.Group:
            content = position.converted.kwargs["item"]
        else:
            content = position.converted

        self.diagrams[el_id] = EditablePartial.from_call(
            NamedDiagram,
            name=position.name,
            diagram=EditablePartial.from_call(
                railroad.Diagram, content, **self.diagram_kwargs
            ),
            index=position.number,
        )

        del self[el_id]


def _worth_extracting(element: pyparsing.ParserElement) -> bool:
    """
    Returns true if this element is worth having its own sub-diagram. Simply, if any of its children
    themselves have children, then its complex enough to extract
    """
    children = element.recurse()
    return any(child.recurse() for child in children)


def _apply_diagram_item_enhancements(fn):
    """
    decorator to ensure enhancements to a diagram item (such as results name annotations)
    get applied on return from _to_diagram_element (we do this since there are several
    returns in _to_diagram_element)
    """

    def _inner(
        element: pyparsing.ParserElement,
        parent: typing.Optional[EditablePartial],
        lookup: ConverterState = None,
        vertical: int = None,
        index: int = 0,
        name_hint: str = None,
        show_results_names: bool = False,
        show_groups: bool = False,
    ) -> typing.Optional[EditablePartial]:
        ret = fn(
            element,
            parent,
            lookup,
            vertical,
            index,
            name_hint,
            show_results_names,
            show_groups,
        )

        # apply annotation for results name, if present
        if show_results_names and ret is not None:
            element_results_name = element.resultsName
            if element_results_name:
                # add "*" to indicate if this is a "list all results" name
                element_results_name += "" if element.modalResults else "*"
                ret = EditablePartial.from_call(
                    railroad.Group, item=ret, label=element_results_name
                )

        return ret

    return _inner


def _visible_exprs(exprs: Iterable[pyparsing.ParserElement]):
    non_diagramming_exprs = (
        pyparsing.ParseElementEnhance,
        pyparsing.PositionToken,
        pyparsing.And._ErrorStop,
    )
    return [
        e
        for e in exprs
        if not (e.customName or e.resultsName or isinstance(e, non_diagramming_exprs))
    ]


@_apply_diagram_item_enhancements
def _to_diagram_element(
    element: pyparsing.ParserElement,
    parent: typing.Optional[EditablePartial],
    lookup: ConverterState = None,
    vertical: int = None,
    index: int = 0,
    name_hint: str = None,
    show_results_names: bool = False,
    show_groups: bool = False,
) -> typing.Optional[EditablePartial]:
    """
    Recursively converts a PyParsing Element to a railroad Element
    :param lookup: The shared converter state that keeps track of useful things
    :param index: The index of this element within the parent
    :param parent: The parent of this element in the output tree
    :param vertical: Controls at what point we make a list of elements vertical. If this is an integer (the default),
    it sets the threshold of the number of items before we go vertical. If True, always go vertical, if False, never
    do so
    :param name_hint: If provided, this will override the generated name
    :param show_results_names: bool flag indicating whether to add annotations for results names
    :returns: The converted version of the input element, but as a Partial that hasn't yet been constructed
    :param show_groups: bool flag indicating whether to show groups using bounding box
    """
    exprs = element.recurse()
    name = name_hint or element.customName or element.__class__.__name__

    # Python's id() is used to provide a unique identifier for elements
    el_id = id(element)

    element_results_name = element.resultsName

    # Here we basically bypass processing certain wrapper elements if they contribute nothing to the diagram
    if not element.customName:
        if isinstance(
            element,
            (
                # pyparsing.TokenConverter,
                # pyparsing.Forward,
                pyparsing.Located,
            ),
        ):
            # However, if this element has a useful custom name, and its child does not, we can pass it on to the child
            if exprs:
                if not exprs[0].customName:
                    propagated_name = name
                else:
                    propagated_name = None

                return _to_diagram_element(
                    element.expr,
                    parent=parent,
                    lookup=lookup,
                    vertical=vertical,
                    index=index,
                    name_hint=propagated_name,
                    show_results_names=show_results_names,
                    show_groups=show_groups,
                )

    # If the element isn't worth extracting, we always treat it as the first time we say it
    if _worth_extracting(element):
        if el_id in lookup:
            # If we've seen this element exactly once before, we are only just now finding out that it's a duplicate,
            # so we have to extract it into a new diagram.
            looked_up = lookup[el_id]
            looked_up.mark_for_extraction(el_id, lookup, name=name_hint)
            ret = EditablePartial.from_call(railroad.NonTerminal, text=looked_up.name)
            return ret

        elif el_id in lookup.diagrams:
            # If we have seen the element at least twice before, and have already extracted it into a subdiagram, we
            # just put in a marker element that refers to the sub-diagram
            ret = EditablePartial.from_call(
                railroad.NonTerminal, text=lookup.diagrams[el_id].kwargs["name"]
            )
            return ret

    # Recursively convert child elements
    # Here we find the most relevant Railroad element for matching pyparsing Element
    # We use ``items=[]`` here to hold the place for where the child elements will go once created
    if isinstance(element, pyparsing.And):
        # detect And's created with ``expr*N`` notation - for these use a OneOrMore with a repeat
        # (all will have the same name, and resultsName)
        if not exprs:
            return None
        if len(set((e.name, e.resultsName) for e in exprs)) == 1:
            ret = EditablePartial.from_call(
                railroad.OneOrMore, item="", repeat=str(len(exprs))
            )
        elif _should_vertical(vertical, exprs):
            ret = EditablePartial.from_call(railroad.Stack, items=[])
        else:
            ret = EditablePartial.from_call(railroad.Sequence, items=[])
    elif isinstance(element, (pyparsing.Or, pyparsing.MatchFirst)):
        if not exprs:
            return None
        if _should_vertical(vertical, exprs):
            ret = EditablePartial.from_call(railroad.Choice, 0, items=[])
        else:
            ret = EditablePartial.from_call(railroad.HorizontalChoice, items=[])
    elif isinstance(element, pyparsing.Each):
        if not exprs:
            return None
        ret = EditablePartial.from_call(EachItem, items=[])
    elif isinstance(element, pyparsing.NotAny):
        ret = EditablePartial.from_call(AnnotatedItem, label="NOT", item="")
    elif isinstance(element, pyparsing.FollowedBy):
        ret = EditablePartial.from_call(AnnotatedItem, label="LOOKAHEAD", item="")
    elif isinstance(element, pyparsing.PrecededBy):
        ret = EditablePartial.from_call(AnnotatedItem, label="LOOKBEHIND", item="")
    elif isinstance(element, pyparsing.Group):
        if show_groups:
            ret = EditablePartial.from_call(AnnotatedItem, label="", item="")
        else:
            ret = EditablePartial.from_call(railroad.Group, label="", item="")
    elif isinstance(element, pyparsing.TokenConverter):
        label = type(element).__name__.lower()
        if label == "tokenconverter":
            ret = EditablePartial.from_call(railroad.Sequence, items=[])
        else:
            ret = EditablePartial.from_call(AnnotatedItem, label=label, item="")
    elif isinstance(element, pyparsing.Opt):
        ret = EditablePartial.from_call(railroad.Optional, item="")
    elif isinstance(element, pyparsing.OneOrMore):
        ret = EditablePartial.from_call(railroad.OneOrMore, item="")
    elif isinstance(element, pyparsing.ZeroOrMore):
        ret = EditablePartial.from_call(railroad.ZeroOrMore, item="")
    elif isinstance(element, pyparsing.Group):
        ret = EditablePartial.from_call(
            railroad.Group, item=None, label=element_results_name
        )
    elif isinstance(element, pyparsing.Empty) and not element.customName:
        # Skip unnamed "Empty" elements
        ret = None
    elif isinstance(element, pyparsing.ParseElementEnhance):
        ret = EditablePartial.from_call(railroad.Sequence, items=[])
    elif len(exprs) > 0 and not element_results_name:
        ret = EditablePartial.from_call(railroad.Group, item="", label=name)
    elif len(exprs) > 0:
        ret = EditablePartial.from_call(railroad.Sequence, items=[])
    else:
        terminal = EditablePartial.from_call(railroad.Terminal, element.defaultName)
        ret = terminal

    if ret is None:
        return

    # Indicate this element's position in the tree so we can extract it if necessary
    lookup[el_id] = ElementState(
        element=element,
        converted=ret,
        parent=parent,
        parent_index=index,
        number=lookup.generate_index(),
    )
    if element.customName:
        lookup[el_id].mark_for_extraction(el_id, lookup, element.customName)

    i = 0
    for expr in exprs:
        # Add a placeholder index in case we have to extract the child before we even add it to the parent
        if "items" in ret.kwargs:
            ret.kwargs["items"].insert(i, None)

        item = _to_diagram_element(
            expr,
            parent=ret,
            lookup=lookup,
            vertical=vertical,
            index=i,
            show_results_names=show_results_names,
            show_groups=show_groups,
        )

        # Some elements don't need to be shown in the diagram
        if item is not None:
            if "item" in ret.kwargs:
                ret.kwargs["item"] = item
            elif "items" in ret.kwargs:
                # If we've already extracted the child, don't touch this index, since it's occupied by a nonterminal
                ret.kwargs["items"][i] = item
                i += 1
        elif "items" in ret.kwargs:
            # If we're supposed to skip this element, remove it from the parent
            del ret.kwargs["items"][i]

    # If all this items children are none, skip this item
    if ret and (
        ("items" in ret.kwargs and len(ret.kwargs["items"]) == 0)
        or ("item" in ret.kwargs and ret.kwargs["item"] is None)
    ):
        ret = EditablePartial.from_call(railroad.Terminal, name)

    # Mark this element as "complete", ie it has all of its children
    if el_id in lookup:
        lookup[el_id].complete = True

    if el_id in lookup and lookup[el_id].extract and lookup[el_id].complete:
        lookup.extract_into_diagram(el_id)
        if ret is not None:
            ret = EditablePartial.from_call(
                railroad.NonTerminal, text=lookup.diagrams[el_id].kwargs["name"]
            )

    return ret


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/results.py
# ========================================================
# results.py
from collections.abc import (
    MutableMapping,
    Mapping,
    MutableSequence,
    Iterator,
    Sequence,
    Container,
)
import pprint
from typing import Tuple, Any, Dict, Set, List

str_type: Tuple[type, ...] = (str, bytes)
_generator_type = type((_ for _ in ()))


class _ParseResultsWithOffset:
    tup: Tuple["ParseResults", int]
    __slots__ = ["tup"]

    def __init__(self, p1: "ParseResults", p2: int):
        self.tup: Tuple[ParseResults, int] = (p1, p2)

    def __getitem__(self, i):
        return self.tup[i]

    def __getstate__(self):
        return self.tup

    def __setstate__(self, *args):
        self.tup = args[0]


class ParseResults:
    """Structured parse results, to provide multiple means of access to
    the parsed data:

    - as a list (``len(results)``)
    - by list index (``results[0], results[1]``, etc.)
    - by attribute (``results.<results_name>`` - see :class:`ParserElement.set_results_name`)

    Example::

        integer = Word(nums)
        date_str = (integer.set_results_name("year") + '/'
                    + integer.set_results_name("month") + '/'
                    + integer.set_results_name("day"))
        # equivalent form:
        # date_str = (integer("year") + '/'
        #             + integer("month") + '/'
        #             + integer("day"))

        # parse_string returns a ParseResults object
        result = date_str.parse_string("1999/12/31")

        def test(s, fn=repr):
            print(f"{s} -> {fn(eval(s))}")
        test("list(result)")
        test("result[0]")
        test("result['month']")
        test("result.day")
        test("'month' in result")
        test("'minutes' in result")
        test("result.dump()", str)

    prints::

        list(result) -> ['1999', '/', '12', '/', '31']
        result[0] -> '1999'
        result['month'] -> '12'
        result.day -> '31'
        'month' in result -> True
        'minutes' in result -> False
        result.dump() -> ['1999', '/', '12', '/', '31']
        - day: '31'
        - month: '12'
        - year: '1999'
    """

    _null_values: Tuple[Any, ...] = (None, [], ())

    _name: str
    _parent: "ParseResults"
    _all_names: Set[str]
    _modal: bool
    _toklist: List[Any]
    _tokdict: Dict[str, Any]

    __slots__ = (
        "_name",
        "_parent",
        "_all_names",
        "_modal",
        "_toklist",
        "_tokdict",
    )

    class List(list):
        """
        Simple wrapper class to distinguish parsed list results that should be preserved
        as actual Python lists, instead of being converted to :class:`ParseResults`::

            LBRACK, RBRACK = map(pp.Suppress, "[]")
            element = pp.Forward()
            item = ppc.integer
            element_list = LBRACK + pp.DelimitedList(element) + RBRACK

            # add parse actions to convert from ParseResults to actual Python collection types
            def as_python_list(t):
                return pp.ParseResults.List(t.as_list())
            element_list.add_parse_action(as_python_list)

            element <<= item | element_list

            element.run_tests('''
                100
                [2,3,4]
                [[2, 1],3,4]
                [(2, 1),3,4]
                (2,3,4)
                ''', post_parse=lambda s, r: (r[0], type(r[0])))

        prints::

            100
            (100, <class 'int'>)

            [2,3,4]
            ([2, 3, 4], <class 'list'>)

            [[2, 1],3,4]
            ([[2, 1], 3, 4], <class 'list'>)

        (Used internally by :class:`Group` when `aslist=True`.)
        """

        def __new__(cls, contained=None):
            if contained is None:
                contained = []

            if not isinstance(contained, list):
                raise TypeError(
                    f"{cls.__name__} may only be constructed with a list, not {type(contained).__name__}"
                )

            return list.__new__(cls)

    def __new__(cls, toklist=None, name=None, **kwargs):
        if isinstance(toklist, ParseResults):
            return toklist
        self = object.__new__(cls)
        self._name = None
        self._parent = None
        self._all_names = set()

        if toklist is None:
            self._toklist = []
        elif isinstance(toklist, (list, _generator_type)):
            self._toklist = (
                [toklist[:]]
                if isinstance(toklist, ParseResults.List)
                else list(toklist)
            )
        else:
            self._toklist = [toklist]
        self._tokdict = dict()
        return self

    # Performance tuning: we construct a *lot* of these, so keep this
    # constructor as small and fast as possible
    def __init__(
        self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance
    ):
        self._tokdict: Dict[str, _ParseResultsWithOffset]
        self._modal = modal
        if name is not None and name != "":
            if isinstance(name, int):
                name = str(name)
            if not modal:
                self._all_names = {name}
            self._name = name
            if toklist not in self._null_values:
                if isinstance(toklist, (str_type, type)):
                    toklist = [toklist]
                if asList:
                    if isinstance(toklist, ParseResults):
                        self[name] = _ParseResultsWithOffset(
                            ParseResults(toklist._toklist), 0
                        )
                    else:
                        self[name] = _ParseResultsWithOffset(
                            ParseResults(toklist[0]), 0
                        )
                    self[name]._name = name
                else:
                    try:
                        self[name] = toklist[0]
                    except (KeyError, TypeError, IndexError):
                        if toklist is not self:
                            self[name] = toklist
                        else:
                            self._name = name

    def __getitem__(self, i):
        if isinstance(i, (int, slice)):
            return self._toklist[i]
        else:
            if i not in self._all_names:
                return self._tokdict[i][-1][0]
            else:
                return ParseResults([v[0] for v in self._tokdict[i]])

    def __setitem__(self, k, v, isinstance=isinstance):
        if isinstance(v, _ParseResultsWithOffset):
            self._tokdict[k] = self._tokdict.get(k, list()) + [v]
            sub = v[0]
        elif isinstance(k, (int, slice)):
            self._toklist[k] = v
            sub = v
        else:
            self._tokdict[k] = self._tokdict.get(k, list()) + [
                _ParseResultsWithOffset(v, 0)
            ]
            sub = v
        if isinstance(sub, ParseResults):
            sub._parent = self

    def __delitem__(self, i):
        if isinstance(i, (int, slice)):
            mylen = len(self._toklist)
            del self._toklist[i]

            # convert int to slice
            if isinstance(i, int):
                if i < 0:
                    i += mylen
                i = slice(i, i + 1)
            # get removed indices
            removed = list(range(*i.indices(mylen)))
            removed.reverse()
            # fixup indices in token dictionary
            for name, occurrences in self._tokdict.items():
                for j in removed:
                    for k, (value, position) in enumerate(occurrences):
                        occurrences[k] = _ParseResultsWithOffset(
                            value, position - (position > j)
                        )
        else:
            del self._tokdict[i]

    def __contains__(self, k) -> bool:
        return k in self._tokdict

    def __len__(self) -> int:
        return len(self._toklist)

    def __bool__(self) -> bool:
        return not not (self._toklist or self._tokdict)

    def __iter__(self) -> Iterator:
        return iter(self._toklist)

    def __reversed__(self) -> Iterator:
        return iter(self._toklist[::-1])

    def keys(self):
        return iter(self._tokdict)

    def values(self):
        return (self[k] for k in self.keys())

    def items(self):
        return ((k, self[k]) for k in self.keys())

    def haskeys(self) -> bool:
        """
        Since ``keys()`` returns an iterator, this method is helpful in bypassing
        code that looks for the existence of any defined results names."""
        return not not self._tokdict

    def pop(self, *args, **kwargs):
        """
        Removes and returns item at specified index (default= ``last``).
        Supports both ``list`` and ``dict`` semantics for ``pop()``. If
        passed no argument or an integer argument, it will use ``list``
        semantics and pop tokens from the list of parsed tokens. If passed
        a non-integer argument (most likely a string), it will use ``dict``
        semantics and pop the corresponding value from any defined results
        names. A second default return value argument is supported, just as in
        ``dict.pop()``.

        Example::

            numlist = Word(nums)[...]
            print(numlist.parse_string("0 123 321")) # -> ['0', '123', '321']

            def remove_first(tokens):
                tokens.pop(0)
            numlist.add_parse_action(remove_first)
            print(numlist.parse_string("0 123 321")) # -> ['123', '321']

            label = Word(alphas)
            patt = label("LABEL") + Word(nums)[1, ...]
            print(patt.parse_string("AAB 123 321").dump())

            # Use pop() in a parse action to remove named result (note that corresponding value is not
            # removed from list form of results)
            def remove_LABEL(tokens):
                tokens.pop("LABEL")
                return tokens
            patt.add_parse_action(remove_LABEL)
            print(patt.parse_string("AAB 123 321").dump())

        prints::

            ['AAB', '123', '321']
            - LABEL: 'AAB'

            ['AAB', '123', '321']
        """
        if not args:
            args = [-1]
        for k, v in kwargs.items():
            if k == "default":
                args = (args[0], v)
            else:
                raise TypeError(f"pop() got an unexpected keyword argument {k!r}")
        if isinstance(args[0], int) or len(args) == 1 or args[0] in self:
            index = args[0]
            ret = self[index]
            del self[index]
            return ret
        else:
            defaultvalue = args[1]
            return defaultvalue

    def get(self, key, default_value=None):
        """
        Returns named result matching the given key, or if there is no
        such name, then returns the given ``default_value`` or ``None`` if no
        ``default_value`` is specified.

        Similar to ``dict.get()``.

        Example::

            integer = Word(nums)
            date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

            result = date_str.parse_string("1999/12/31")
            print(result.get("year")) # -> '1999'
            print(result.get("hour", "not specified")) # -> 'not specified'
            print(result.get("hour")) # -> None
        """
        if key in self:
            return self[key]
        else:
            return default_value

    def insert(self, index, ins_string):
        """
        Inserts new element at location index in the list of parsed tokens.

        Similar to ``list.insert()``.

        Example::

            numlist = Word(nums)[...]
            print(numlist.parse_string("0 123 321")) # -> ['0', '123', '321']

            # use a parse action to insert the parse location in the front of the parsed results
            def insert_locn(locn, tokens):
                tokens.insert(0, locn)
            numlist.add_parse_action(insert_locn)
            print(numlist.parse_string("0 123 321")) # -> [0, '0', '123', '321']
        """
        self._toklist.insert(index, ins_string)
        # fixup indices in token dictionary
        for name, occurrences in self._tokdict.items():
            for k, (value, position) in enumerate(occurrences):
                occurrences[k] = _ParseResultsWithOffset(
                    value, position + (position > index)
                )

    def append(self, item):
        """
        Add single element to end of ``ParseResults`` list of elements.

        Example::

            numlist = Word(nums)[...]
            print(numlist.parse_string("0 123 321")) # -> ['0', '123', '321']

            # use a parse action to compute the sum of the parsed integers, and add it to the end
            def append_sum(tokens):
                tokens.append(sum(map(int, tokens)))
            numlist.add_parse_action(append_sum)
            print(numlist.parse_string("0 123 321")) # -> ['0', '123', '321', 444]
        """
        self._toklist.append(item)

    def extend(self, itemseq):
        """
        Add sequence of elements to end of ``ParseResults`` list of elements.

        Example::

            patt = Word(alphas)[1, ...]

            # use a parse action to append the reverse of the matched strings, to make a palindrome
            def make_palindrome(tokens):
                tokens.extend(reversed([t[::-1] for t in tokens]))
                return ''.join(tokens)
            patt.add_parse_action(make_palindrome)
            print(patt.parse_string("lskdj sdlkjf lksd")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'
        """
        if isinstance(itemseq, ParseResults):
            self.__iadd__(itemseq)
        else:
            self._toklist.extend(itemseq)

    def clear(self):
        """
        Clear all elements and results names.
        """
        del self._toklist[:]
        self._tokdict.clear()

    def __getattr__(self, name):
        try:
            return self[name]
        except KeyError:
            if name.startswith("__"):
                raise AttributeError(name)
            return ""

    def __add__(self, other: "ParseResults") -> "ParseResults":
        ret = self.copy()
        ret += other
        return ret

    def __iadd__(self, other: "ParseResults") -> "ParseResults":
        if not other:
            return self

        if other._tokdict:
            offset = len(self._toklist)
            addoffset = lambda a: offset if a < 0 else a + offset
            otheritems = other._tokdict.items()
            otherdictitems = [
                (k, _ParseResultsWithOffset(v[0], addoffset(v[1])))
                for k, vlist in otheritems
                for v in vlist
            ]
            for k, v in otherdictitems:
                self[k] = v
                if isinstance(v[0], ParseResults):
                    v[0]._parent = self

        self._toklist += other._toklist
        self._all_names |= other._all_names
        return self

    def __radd__(self, other) -> "ParseResults":
        if isinstance(other, int) and other == 0:
            # useful for merging many ParseResults using sum() builtin
            return self.copy()
        else:
            # this may raise a TypeError - so be it
            return other + self

    def __repr__(self) -> str:
        return f"{type(self).__name__}({self._toklist!r}, {self.as_dict()})"

    def __str__(self) -> str:
        return (
            "["
            + ", ".join(
                [
                    str(i) if isinstance(i, ParseResults) else repr(i)
                    for i in self._toklist
                ]
            )
            + "]"
        )

    def _asStringList(self, sep=""):
        out = []
        for item in self._toklist:
            if out and sep:
                out.append(sep)
            if isinstance(item, ParseResults):
                out += item._asStringList()
            else:
                out.append(str(item))
        return out

    def as_list(self) -> list:
        """
        Returns the parse results as a nested list of matching tokens, all converted to strings.

        Example::

            patt = Word(alphas)[1, ...]
            result = patt.parse_string("sldkj lsdkj sldkj")
            # even though the result prints in string-like form, it is actually a pyparsing ParseResults
            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']

            # Use as_list() to create an actual list
            result_list = result.as_list()
            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']
        """
        return [
            res.as_list() if isinstance(res, ParseResults) else res
            for res in self._toklist
        ]

    def as_dict(self) -> dict:
        """
        Returns the named parse results as a nested dictionary.

        Example::

            integer = Word(nums)
            date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

            result = date_str.parse_string('12/31/1999')
            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})

            result_dict = result.as_dict()
            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}

            # even though a ParseResults supports dict-like access, sometime you just need to have a dict
            import json
            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable
            print(json.dumps(result.as_dict())) # -> {"month": "31", "day": "1999", "year": "12"}
        """

        def to_item(obj):
            if isinstance(obj, ParseResults):
                return obj.as_dict() if obj.haskeys() else [to_item(v) for v in obj]
            else:
                return obj

        return dict((k, to_item(v)) for k, v in self.items())

    def copy(self) -> "ParseResults":
        """
        Returns a new shallow copy of a :class:`ParseResults` object. `ParseResults`
        items contained within the source are shared with the copy. Use
        :class:`ParseResults.deepcopy()` to create a copy with its own separate
        content values.
        """
        ret = ParseResults(self._toklist)
        ret._tokdict = self._tokdict.copy()
        ret._parent = self._parent
        ret._all_names |= self._all_names
        ret._name = self._name
        return ret

    def deepcopy(self) -> "ParseResults":
        """
        Returns a new deep copy of a :class:`ParseResults` object.
        """
        ret = self.copy()
        # replace values with copies if they are of known mutable types
        for i, obj in enumerate(self._toklist):
            if isinstance(obj, ParseResults):
                self._toklist[i] = obj.deepcopy()
            elif isinstance(obj, (str, bytes)):
                pass
            elif isinstance(obj, MutableMapping):
                self._toklist[i] = dest = type(obj)()
                for k, v in obj.items():
                    dest[k] = v.deepcopy() if isinstance(v, ParseResults) else v
            elif isinstance(obj, Container):
                self._toklist[i] = type(obj)(
                    v.deepcopy() if isinstance(v, ParseResults) else v for v in obj
                )
        return ret

    def get_name(self):
        r"""
        Returns the results name for this token expression. Useful when several
        different expressions might match at a particular location.

        Example::

            integer = Word(nums)
            ssn_expr = Regex(r"\d\d\d-\d\d-\d\d\d\d")
            house_number_expr = Suppress('#') + Word(nums, alphanums)
            user_data = (Group(house_number_expr)("house_number")
                        | Group(ssn_expr)("ssn")
                        | Group(integer)("age"))
            user_info = user_data[1, ...]

            result = user_info.parse_string("22 111-22-3333 #221B")
            for item in result:
                print(item.get_name(), ':', item[0])

        prints::

            age : 22
            ssn : 111-22-3333
            house_number : 221B
        """
        if self._name:
            return self._name
        elif self._parent:
            par: "ParseResults" = self._parent
            parent_tokdict_items = par._tokdict.items()
            return next(
                (
                    k
                    for k, vlist in parent_tokdict_items
                    for v, loc in vlist
                    if v is self
                ),
                None,
            )
        elif (
            len(self) == 1
            and len(self._tokdict) == 1
            and next(iter(self._tokdict.values()))[0][1] in (0, -1)
        ):
            return next(iter(self._tokdict.keys()))
        else:
            return None

    def dump(self, indent="", full=True, include_list=True, _depth=0) -> str:
        """
        Diagnostic method for listing out the contents of
        a :class:`ParseResults`. Accepts an optional ``indent`` argument so
        that this string can be embedded in a nested display of other data.

        Example::

            integer = Word(nums)
            date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

            result = date_str.parse_string('1999/12/31')
            print(result.dump())

        prints::

            ['1999', '/', '12', '/', '31']
            - day: '31'
            - month: '12'
            - year: '1999'
        """
        out = []
        NL = "\n"
        out.append(indent + str(self.as_list()) if include_list else "")

        if full:
            if self.haskeys():
                items = sorted((str(k), v) for k, v in self.items())
                for k, v in items:
                    if out:
                        out.append(NL)
                    out.append(f"{indent}{('  ' * _depth)}- {k}: ")
                    if isinstance(v, ParseResults):
                        if v:
                            out.append(
                                v.dump(
                                    indent=indent,
                                    full=full,
                                    include_list=include_list,
                                    _depth=_depth + 1,
                                )
                            )
                        else:
                            out.append(str(v))
                    else:
                        out.append(repr(v))
            if any(isinstance(vv, ParseResults) for vv in self):
                v = self
                for i, vv in enumerate(v):
                    if isinstance(vv, ParseResults):
                        out.append(
                            "\n{}{}[{}]:\n{}{}{}".format(
                                indent,
                                ("  " * (_depth)),
                                i,
                                indent,
                                ("  " * (_depth + 1)),
                                vv.dump(
                                    indent=indent,
                                    full=full,
                                    include_list=include_list,
                                    _depth=_depth + 1,
                                ),
                            )
                        )
                    else:
                        out.append(
                            "\n%s%s[%d]:\n%s%s%s"
                            % (
                                indent,
                                ("  " * (_depth)),
                                i,
                                indent,
                                ("  " * (_depth + 1)),
                                str(vv),
                            )
                        )

        return "".join(out)

    def pprint(self, *args, **kwargs):
        """
        Pretty-printer for parsed results as a list, using the
        `pprint <https://docs.python.org/3/library/pprint.html>`_ module.
        Accepts additional positional or keyword args as defined for
        `pprint.pprint <https://docs.python.org/3/library/pprint.html#pprint.pprint>`_ .

        Example::

            ident = Word(alphas, alphanums)
            num = Word(nums)
            func = Forward()
            term = ident | num | Group('(' + func + ')')
            func <<= ident + Group(Optional(DelimitedList(term)))
            result = func.parse_string("fna a,b,(fnb c,d,200),100")
            result.pprint(width=40)

        prints::

            ['fna',
             ['a',
              'b',
              ['(', 'fnb', ['c', 'd', '200'], ')'],
              '100']]
        """
        pprint.pprint(self.as_list(), *args, **kwargs)

    # add support for pickle protocol
    def __getstate__(self):
        return (
            self._toklist,
            (
                self._tokdict.copy(),
                None,
                self._all_names,
                self._name,
            ),
        )

    def __setstate__(self, state):
        self._toklist, (self._tokdict, par, inAccumNames, self._name) = state
        self._all_names = set(inAccumNames)
        self._parent = None

    def __getnewargs__(self):
        return self._toklist, self._name

    def __dir__(self):
        return dir(type(self)) + list(self.keys())

    @classmethod
    def from_dict(cls, other, name=None) -> "ParseResults":
        """
        Helper classmethod to construct a ``ParseResults`` from a ``dict``, preserving the
        name-value relations as results names. If an optional ``name`` argument is
        given, a nested ``ParseResults`` will be returned.
        """

        def is_iterable(obj):
            try:
                iter(obj)
            except Exception:
                return False
            # str's are iterable, but in pyparsing, we don't want to iterate over them
            else:
                return not isinstance(obj, str_type)

        ret = cls([])
        for k, v in other.items():
            if isinstance(v, Mapping):
                ret += cls.from_dict(v, name=k)
            else:
                ret += cls([v], name=k, asList=is_iterable(v))
        if name is not None:
            ret = cls([ret], name=name)
        return ret

    asList = as_list
    """Deprecated - use :class:`as_list`"""
    asDict = as_dict
    """Deprecated - use :class:`as_dict`"""
    getName = get_name
    """Deprecated - use :class:`get_name`"""


MutableMapping.register(ParseResults)
MutableSequence.register(ParseResults)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/testing.py
# ========================================================
# testing.py

from contextlib import contextmanager
import typing

from .core import (
    ParserElement,
    ParseException,
    Keyword,
    __diag__,
    __compat__,
)


class pyparsing_test:
    """
    namespace class for classes useful in writing unit tests
    """

    class reset_pyparsing_context:
        """
        Context manager to be used when writing unit tests that modify pyparsing config values:
        - packrat parsing
        - bounded recursion parsing
        - default whitespace characters.
        - default keyword characters
        - literal string auto-conversion class
        - __diag__ settings

        Example::

            with reset_pyparsing_context():
                # test that literals used to construct a grammar are automatically suppressed
                ParserElement.inlineLiteralsUsing(Suppress)

                term = Word(alphas) | Word(nums)
                group = Group('(' + term[...] + ')')

                # assert that the '()' characters are not included in the parsed tokens
                self.assertParseAndCheckList(group, "(abc 123 def)", ['abc', '123', 'def'])

            # after exiting context manager, literals are converted to Literal expressions again
        """

        def __init__(self):
            self._save_context = {}

        def save(self):
            self._save_context["default_whitespace"] = ParserElement.DEFAULT_WHITE_CHARS
            self._save_context["default_keyword_chars"] = Keyword.DEFAULT_KEYWORD_CHARS

            self._save_context[
                "literal_string_class"
            ] = ParserElement._literalStringClass

            self._save_context["verbose_stacktrace"] = ParserElement.verbose_stacktrace

            self._save_context["packrat_enabled"] = ParserElement._packratEnabled
            if ParserElement._packratEnabled:
                self._save_context[
                    "packrat_cache_size"
                ] = ParserElement.packrat_cache.size
            else:
                self._save_context["packrat_cache_size"] = None
            self._save_context["packrat_parse"] = ParserElement._parse
            self._save_context[
                "recursion_enabled"
            ] = ParserElement._left_recursion_enabled

            self._save_context["__diag__"] = {
                name: getattr(__diag__, name) for name in __diag__._all_names
            }

            self._save_context["__compat__"] = {
                "collect_all_And_tokens": __compat__.collect_all_And_tokens
            }

            return self

        def restore(self):
            # reset pyparsing global state
            if (
                ParserElement.DEFAULT_WHITE_CHARS
                != self._save_context["default_whitespace"]
            ):
                ParserElement.set_default_whitespace_chars(
                    self._save_context["default_whitespace"]
                )

            ParserElement.verbose_stacktrace = self._save_context["verbose_stacktrace"]

            Keyword.DEFAULT_KEYWORD_CHARS = self._save_context["default_keyword_chars"]
            ParserElement.inlineLiteralsUsing(
                self._save_context["literal_string_class"]
            )

            for name, value in self._save_context["__diag__"].items():
                (__diag__.enable if value else __diag__.disable)(name)

            ParserElement._packratEnabled = False
            if self._save_context["packrat_enabled"]:
                ParserElement.enable_packrat(self._save_context["packrat_cache_size"])
            else:
                ParserElement._parse = self._save_context["packrat_parse"]
            ParserElement._left_recursion_enabled = self._save_context[
                "recursion_enabled"
            ]

            __compat__.collect_all_And_tokens = self._save_context["__compat__"]

            return self

        def copy(self):
            ret = type(self)()
            ret._save_context.update(self._save_context)
            return ret

        def __enter__(self):
            return self.save()

        def __exit__(self, *args):
            self.restore()

    class TestParseResultsAsserts:
        """
        A mixin class to add parse results assertion methods to normal unittest.TestCase classes.
        """

        def assertParseResultsEquals(
            self, result, expected_list=None, expected_dict=None, msg=None
        ):
            """
            Unit test assertion to compare a :class:`ParseResults` object with an optional ``expected_list``,
            and compare any defined results names with an optional ``expected_dict``.
            """
            if expected_list is not None:
                self.assertEqual(expected_list, result.as_list(), msg=msg)
            if expected_dict is not None:
                self.assertEqual(expected_dict, result.as_dict(), msg=msg)

        def assertParseAndCheckList(
            self, expr, test_string, expected_list, msg=None, verbose=True
        ):
            """
            Convenience wrapper assert to test a parser element and input string, and assert that
            the resulting ``ParseResults.asList()`` is equal to the ``expected_list``.
            """
            result = expr.parse_string(test_string, parse_all=True)
            if verbose:
                print(result.dump())
            else:
                print(result.as_list())
            self.assertParseResultsEquals(result, expected_list=expected_list, msg=msg)

        def assertParseAndCheckDict(
            self, expr, test_string, expected_dict, msg=None, verbose=True
        ):
            """
            Convenience wrapper assert to test a parser element and input string, and assert that
            the resulting ``ParseResults.asDict()`` is equal to the ``expected_dict``.
            """
            result = expr.parse_string(test_string, parseAll=True)
            if verbose:
                print(result.dump())
            else:
                print(result.as_list())
            self.assertParseResultsEquals(result, expected_dict=expected_dict, msg=msg)

        def assertRunTestResults(
            self, run_tests_report, expected_parse_results=None, msg=None
        ):
            """
            Unit test assertion to evaluate output of ``ParserElement.runTests()``. If a list of
            list-dict tuples is given as the ``expected_parse_results`` argument, then these are zipped
            with the report tuples returned by ``runTests`` and evaluated using ``assertParseResultsEquals``.
            Finally, asserts that the overall ``runTests()`` success value is ``True``.

            :param run_tests_report: tuple(bool, [tuple(str, ParseResults or Exception)]) returned from runTests
            :param expected_parse_results (optional): [tuple(str, list, dict, Exception)]
            """
            run_test_success, run_test_results = run_tests_report

            if expected_parse_results is not None:
                merged = [
                    (*rpt, expected)
                    for rpt, expected in zip(run_test_results, expected_parse_results)
                ]
                for test_string, result, expected in merged:
                    # expected should be a tuple containing a list and/or a dict or an exception,
                    # and optional failure message string
                    # an empty tuple will skip any result validation
                    fail_msg = next(
                        (exp for exp in expected if isinstance(exp, str)), None
                    )
                    expected_exception = next(
                        (
                            exp
                            for exp in expected
                            if isinstance(exp, type) and issubclass(exp, Exception)
                        ),
                        None,
                    )
                    if expected_exception is not None:
                        with self.assertRaises(
                            expected_exception=expected_exception, msg=fail_msg or msg
                        ):
                            if isinstance(result, Exception):
                                raise result
                    else:
                        expected_list = next(
                            (exp for exp in expected if isinstance(exp, list)), None
                        )
                        expected_dict = next(
                            (exp for exp in expected if isinstance(exp, dict)), None
                        )
                        if (expected_list, expected_dict) != (None, None):
                            self.assertParseResultsEquals(
                                result,
                                expected_list=expected_list,
                                expected_dict=expected_dict,
                                msg=fail_msg or msg,
                            )
                        else:
                            # warning here maybe?
                            print(f"no validation for {test_string!r}")

            # do this last, in case some specific test results can be reported instead
            self.assertTrue(
                run_test_success, msg=msg if msg is not None else "failed runTests"
            )

        @contextmanager
        def assertRaisesParseException(self, exc_type=ParseException, msg=None):
            with self.assertRaises(exc_type, msg=msg):
                yield

    @staticmethod
    def with_line_numbers(
        s: str,
        start_line: typing.Optional[int] = None,
        end_line: typing.Optional[int] = None,
        expand_tabs: bool = True,
        eol_mark: str = "|",
        mark_spaces: typing.Optional[str] = None,
        mark_control: typing.Optional[str] = None,
    ) -> str:
        """
        Helpful method for debugging a parser - prints a string with line and column numbers.
        (Line and column numbers are 1-based.)

        :param s: tuple(bool, str - string to be printed with line and column numbers
        :param start_line: int - (optional) starting line number in s to print (default=1)
        :param end_line: int - (optional) ending line number in s to print (default=len(s))
        :param expand_tabs: bool - (optional) expand tabs to spaces, to match the pyparsing default
        :param eol_mark: str - (optional) string to mark the end of lines, helps visualize trailing spaces (default="|")
        :param mark_spaces: str - (optional) special character to display in place of spaces
        :param mark_control: str - (optional) convert non-printing control characters to a placeholding
                                 character; valid values:
                                 - "unicode" - replaces control chars with Unicode symbols, such as "" and ""
                                 - any single character string - replace control characters with given string
                                 - None (default) - string is displayed as-is

        :return: str - input string with leading line numbers and column number headers
        """
        if expand_tabs:
            s = s.expandtabs()
        if mark_control is not None:
            mark_control = typing.cast(str, mark_control)
            if mark_control == "unicode":
                transtable_map = {
                    c: u for c, u in zip(range(0, 33), range(0x2400, 0x2433))
                }
                transtable_map[127] = 0x2421
                tbl = str.maketrans(transtable_map)
                eol_mark = ""
            else:
                ord_mark_control = ord(mark_control)
                tbl = str.maketrans(
                    {c: ord_mark_control for c in list(range(0, 32)) + [127]}
                )
            s = s.translate(tbl)
        if mark_spaces is not None and mark_spaces != " ":
            if mark_spaces == "unicode":
                tbl = str.maketrans({9: 0x2409, 32: 0x2423})
                s = s.translate(tbl)
            else:
                s = s.replace(" ", mark_spaces)
        if start_line is None:
            start_line = 1
        if end_line is None:
            end_line = len(s)
        end_line = min(end_line, len(s))
        start_line = min(max(1, start_line), end_line)

        if mark_control != "unicode":
            s_lines = s.splitlines()[start_line - 1 : end_line]
        else:
            s_lines = [line + "" for line in s.split("")[start_line - 1 : end_line]]
        if not s_lines:
            return ""

        lineno_width = len(str(end_line))
        max_line_len = max(len(line) for line in s_lines)
        lead = " " * (lineno_width + 1)
        if max_line_len >= 99:
            header0 = (
                lead
                + "".join(
                    f"{' ' * 99}{(i + 1) % 100}"
                    for i in range(max(max_line_len // 100, 1))
                )
                + "\n"
            )
        else:
            header0 = ""
        header1 = (
            header0
            + lead
            + "".join(f"         {(i + 1) % 10}" for i in range(-(-max_line_len // 10)))
            + "\n"
        )
        header2 = lead + "1234567890" * (-(-max_line_len // 10)) + "\n"
        return (
            header1
            + header2
            + "\n".join(
                f"{i:{lineno_width}d}:{line}{eol_mark}"
                for i, line in enumerate(s_lines, start=start_line)
            )
            + "\n"
        )


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/exceptions.py
# ========================================================
# exceptions.py

import re
import sys
import typing

from .util import (
    col,
    line,
    lineno,
    _collapse_string_to_ranges,
    replaced_by_pep8,
)
from .unicode import pyparsing_unicode as ppu


class ExceptionWordUnicode(ppu.Latin1, ppu.LatinA, ppu.LatinB, ppu.Greek, ppu.Cyrillic):
    pass


_extract_alphanums = _collapse_string_to_ranges(ExceptionWordUnicode.alphanums)
_exception_word_extractor = re.compile("([" + _extract_alphanums + "]{1,16})|.")


class ParseBaseException(Exception):
    """base exception class for all parsing runtime exceptions"""

    loc: int
    msg: str
    pstr: str
    parser_element: typing.Any  # "ParserElement"
    args: typing.Tuple[str, int, typing.Optional[str]]

    __slots__ = (
        "loc",
        "msg",
        "pstr",
        "parser_element",
        "args",
    )

    # Performance tuning: we construct a *lot* of these, so keep this
    # constructor as small and fast as possible
    def __init__(
        self,
        pstr: str,
        loc: int = 0,
        msg: typing.Optional[str] = None,
        elem=None,
    ):
        self.loc = loc
        if msg is None:
            self.msg = pstr
            self.pstr = ""
        else:
            self.msg = msg
            self.pstr = pstr
        self.parser_element = elem
        self.args = (pstr, loc, msg)

    @staticmethod
    def explain_exception(exc, depth=16):
        """
        Method to take an exception and translate the Python internal traceback into a list
        of the pyparsing expressions that caused the exception to be raised.

        Parameters:

        - exc - exception raised during parsing (need not be a ParseException, in support
          of Python exceptions that might be raised in a parse action)
        - depth (default=16) - number of levels back in the stack trace to list expression
          and function names; if None, the full stack trace names will be listed; if 0, only
          the failing input line, marker, and exception string will be shown

        Returns a multi-line string listing the ParserElements and/or function names in the
        exception's stack trace.
        """
        import inspect
        from .core import ParserElement

        if depth is None:
            depth = sys.getrecursionlimit()
        ret = []
        if isinstance(exc, ParseBaseException):
            ret.append(exc.line)
            ret.append(" " * (exc.column - 1) + "^")
        ret.append(f"{type(exc).__name__}: {exc}")

        if depth > 0:
            callers = inspect.getinnerframes(exc.__traceback__, context=depth)
            seen = set()
            for i, ff in enumerate(callers[-depth:]):
                frm = ff[0]

                f_self = frm.f_locals.get("self", None)
                if isinstance(f_self, ParserElement):
                    if not frm.f_code.co_name.startswith(
                        ("parseImpl", "_parseNoCache")
                    ):
                        continue
                    if id(f_self) in seen:
                        continue
                    seen.add(id(f_self))

                    self_type = type(f_self)
                    ret.append(
                        f"{self_type.__module__}.{self_type.__name__} - {f_self}"
                    )

                elif f_self is not None:
                    self_type = type(f_self)
                    ret.append(f"{self_type.__module__}.{self_type.__name__}")

                else:
                    code = frm.f_code
                    if code.co_name in ("wrapper", "<module>"):
                        continue

                    ret.append(code.co_name)

                depth -= 1
                if not depth:
                    break

        return "\n".join(ret)

    @classmethod
    def _from_exception(cls, pe):
        """
        internal factory method to simplify creating one type of ParseException
        from another - avoids having __init__ signature conflicts among subclasses
        """
        return cls(pe.pstr, pe.loc, pe.msg, pe.parser_element)

    @property
    def line(self) -> str:
        """
        Return the line of text where the exception occurred.
        """
        return line(self.loc, self.pstr)

    @property
    def lineno(self) -> int:
        """
        Return the 1-based line number of text where the exception occurred.
        """
        return lineno(self.loc, self.pstr)

    @property
    def col(self) -> int:
        """
        Return the 1-based column on the line of text where the exception occurred.
        """
        return col(self.loc, self.pstr)

    @property
    def column(self) -> int:
        """
        Return the 1-based column on the line of text where the exception occurred.
        """
        return col(self.loc, self.pstr)

    # pre-PEP8 compatibility
    @property
    def parserElement(self):
        return self.parser_element

    @parserElement.setter
    def parserElement(self, elem):
        self.parser_element = elem

    def __str__(self) -> str:
        if self.pstr:
            if self.loc >= len(self.pstr):
                foundstr = ", found end of text"
            else:
                # pull out next word at error location
                found_match = _exception_word_extractor.match(self.pstr, self.loc)
                if found_match is not None:
                    found = found_match.group(0)
                else:
                    found = self.pstr[self.loc : self.loc + 1]
                foundstr = (", found %r" % found).replace(r"\\", "\\")
        else:
            foundstr = ""
        return f"{self.msg}{foundstr}  (at char {self.loc}), (line:{self.lineno}, col:{self.column})"

    def __repr__(self):
        return str(self)

    def mark_input_line(
        self, marker_string: typing.Optional[str] = None, *, markerString: str = ">!<"
    ) -> str:
        """
        Extracts the exception line from the input string, and marks
        the location of the exception with a special symbol.
        """
        markerString = marker_string if marker_string is not None else markerString
        line_str = self.line
        line_column = self.column - 1
        if markerString:
            line_str = "".join(
                (line_str[:line_column], markerString, line_str[line_column:])
            )
        return line_str.strip()

    def explain(self, depth=16) -> str:
        """
        Method to translate the Python internal traceback into a list
        of the pyparsing expressions that caused the exception to be raised.

        Parameters:

        - depth (default=16) - number of levels back in the stack trace to list expression
          and function names; if None, the full stack trace names will be listed; if 0, only
          the failing input line, marker, and exception string will be shown

        Returns a multi-line string listing the ParserElements and/or function names in the
        exception's stack trace.

        Example::

            expr = pp.Word(pp.nums) * 3
            try:
                expr.parse_string("123 456 A789")
            except pp.ParseException as pe:
                print(pe.explain(depth=0))

        prints::

            123 456 A789
                    ^
            ParseException: Expected W:(0-9), found 'A'  (at char 8), (line:1, col:9)

        Note: the diagnostic output will include string representations of the expressions
        that failed to parse. These representations will be more helpful if you use `set_name` to
        give identifiable names to your expressions. Otherwise they will use the default string
        forms, which may be cryptic to read.

        Note: pyparsing's default truncation of exception tracebacks may also truncate the
        stack of expressions that are displayed in the ``explain`` output. To get the full listing
        of parser expressions, you may have to set ``ParserElement.verbose_stacktrace = True``
        """
        return self.explain_exception(self, depth)

    # fmt: off
    @replaced_by_pep8(mark_input_line)
    def markInputline(self): ...
    # fmt: on


class ParseException(ParseBaseException):
    """
    Exception thrown when a parse expression doesn't match the input string

    Example::

        try:
            Word(nums).set_name("integer").parse_string("ABC")
        except ParseException as pe:
            print(pe)
            print("column: {}".format(pe.column))

    prints::

       Expected integer (at char 0), (line:1, col:1)
        column: 1

    """


class ParseFatalException(ParseBaseException):
    """
    User-throwable exception thrown when inconsistent parse content
    is found; stops all parsing immediately
    """


class ParseSyntaxException(ParseFatalException):
    """
    Just like :class:`ParseFatalException`, but thrown internally
    when an :class:`ErrorStop<And._ErrorStop>` ('-' operator) indicates
    that parsing is to stop immediately because an unbacktrackable
    syntax error has been found.
    """


class RecursiveGrammarException(Exception):
    """
    Exception thrown by :class:`ParserElement.validate` if the
    grammar could be left-recursive; parser may need to enable
    left recursion using :class:`ParserElement.enable_left_recursion<ParserElement.enable_left_recursion>`
    """

    def __init__(self, parseElementList):
        self.parseElementTrace = parseElementList

    def __str__(self) -> str:
        return f"RecursiveGrammarException: {self.parseElementTrace}"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/__init__.py
# ========================================================
# module pyparsing.py
#
# Copyright (c) 2003-2022  Paul T. McGuire
#
# Permission is hereby granted, free of charge, to any person obtaining
# a copy of this software and associated documentation files (the
# "Software"), to deal in the Software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish,
# distribute, sublicense, and/or sell copies of the Software, and to
# permit persons to whom the Software is furnished to do so, subject to
# the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#

__doc__ = """
pyparsing module - Classes and methods to define and execute parsing grammars
=============================================================================

The pyparsing module is an alternative approach to creating and
executing simple grammars, vs. the traditional lex/yacc approach, or the
use of regular expressions.  With pyparsing, you don't need to learn
a new syntax for defining grammars or matching expressions - the parsing
module provides a library of classes that you use to construct the
grammar directly in Python.

Here is a program to parse "Hello, World!" (or any greeting of the form
``"<salutation>, <addressee>!"``), built up using :class:`Word`,
:class:`Literal`, and :class:`And` elements
(the :meth:`'+'<ParserElement.__add__>` operators create :class:`And` expressions,
and the strings are auto-converted to :class:`Literal` expressions)::

    from pip._vendor.pyparsing import Word, alphas

    # define grammar of a greeting
    greet = Word(alphas) + "," + Word(alphas) + "!"

    hello = "Hello, World!"
    print(hello, "->", greet.parse_string(hello))

The program outputs the following::

    Hello, World! -> ['Hello', ',', 'World', '!']

The Python representation of the grammar is quite readable, owing to the
self-explanatory class names, and the use of :class:`'+'<And>`,
:class:`'|'<MatchFirst>`, :class:`'^'<Or>` and :class:`'&'<Each>` operators.

The :class:`ParseResults` object returned from
:class:`ParserElement.parse_string` can be
accessed as a nested list, a dictionary, or an object with named
attributes.

The pyparsing module handles some of the problems that are typically
vexing when writing text parsers:

  - extra or missing whitespace (the above program will also handle
    "Hello,World!", "Hello  ,  World  !", etc.)
  - quoted strings
  - embedded comments


Getting Started -
-----------------
Visit the classes :class:`ParserElement` and :class:`ParseResults` to
see the base classes that most other pyparsing
classes inherit from. Use the docstrings for examples of how to:

 - construct literal match expressions from :class:`Literal` and
   :class:`CaselessLiteral` classes
 - construct character word-group expressions using the :class:`Word`
   class
 - see how to create repetitive expressions using :class:`ZeroOrMore`
   and :class:`OneOrMore` classes
 - use :class:`'+'<And>`, :class:`'|'<MatchFirst>`, :class:`'^'<Or>`,
   and :class:`'&'<Each>` operators to combine simple expressions into
   more complex ones
 - associate names with your parsed results using
   :class:`ParserElement.set_results_name`
 - access the parsed data, which is returned as a :class:`ParseResults`
   object
 - find some helpful expression short-cuts like :class:`DelimitedList`
   and :class:`one_of`
 - find more useful common expressions in the :class:`pyparsing_common`
   namespace class
"""
from typing import NamedTuple


class version_info(NamedTuple):
    major: int
    minor: int
    micro: int
    releaselevel: str
    serial: int

    @property
    def __version__(self):
        return (
            f"{self.major}.{self.minor}.{self.micro}"
            + (
                f"{'r' if self.releaselevel[0] == 'c' else ''}{self.releaselevel[0]}{self.serial}",
                "",
            )[self.releaselevel == "final"]
        )

    def __str__(self):
        return f"{__name__} {self.__version__} / {__version_time__}"

    def __repr__(self):
        return f"{__name__}.{type(self).__name__}({', '.join('{}={!r}'.format(*nv) for nv in zip(self._fields, self))})"


__version_info__ = version_info(3, 1, 0, "final", 1)
__version_time__ = "18 Jun 2023 14:05 UTC"
__version__ = __version_info__.__version__
__versionTime__ = __version_time__
__author__ = "Paul McGuire <ptmcg.gm+pyparsing@gmail.com>"

from .util import *
from .exceptions import *
from .actions import *
from .core import __diag__, __compat__
from .results import *
from .core import *  # type: ignore[misc, assignment]
from .core import _builtin_exprs as core_builtin_exprs
from .helpers import *  # type: ignore[misc, assignment]
from .helpers import _builtin_exprs as helper_builtin_exprs

from .unicode import unicode_set, UnicodeRangeList, pyparsing_unicode as unicode
from .testing import pyparsing_test as testing
from .common import (
    pyparsing_common as common,
    _builtin_exprs as common_builtin_exprs,
)

# define backward compat synonyms
if "pyparsing_unicode" not in globals():
    pyparsing_unicode = unicode  # type: ignore[misc]
if "pyparsing_common" not in globals():
    pyparsing_common = common  # type: ignore[misc]
if "pyparsing_test" not in globals():
    pyparsing_test = testing  # type: ignore[misc]

core_builtin_exprs += common_builtin_exprs + helper_builtin_exprs


__all__ = [
    "__version__",
    "__version_time__",
    "__author__",
    "__compat__",
    "__diag__",
    "And",
    "AtLineStart",
    "AtStringStart",
    "CaselessKeyword",
    "CaselessLiteral",
    "CharsNotIn",
    "CloseMatch",
    "Combine",
    "DelimitedList",
    "Dict",
    "Each",
    "Empty",
    "FollowedBy",
    "Forward",
    "GoToColumn",
    "Group",
    "IndentedBlock",
    "Keyword",
    "LineEnd",
    "LineStart",
    "Literal",
    "Located",
    "PrecededBy",
    "MatchFirst",
    "NoMatch",
    "NotAny",
    "OneOrMore",
    "OnlyOnce",
    "OpAssoc",
    "Opt",
    "Optional",
    "Or",
    "ParseBaseException",
    "ParseElementEnhance",
    "ParseException",
    "ParseExpression",
    "ParseFatalException",
    "ParseResults",
    "ParseSyntaxException",
    "ParserElement",
    "PositionToken",
    "QuotedString",
    "RecursiveGrammarException",
    "Regex",
    "SkipTo",
    "StringEnd",
    "StringStart",
    "Suppress",
    "Token",
    "TokenConverter",
    "White",
    "Word",
    "WordEnd",
    "WordStart",
    "ZeroOrMore",
    "Char",
    "alphanums",
    "alphas",
    "alphas8bit",
    "any_close_tag",
    "any_open_tag",
    "autoname_elements",
    "c_style_comment",
    "col",
    "common_html_entity",
    "condition_as_parse_action",
    "counted_array",
    "cpp_style_comment",
    "dbl_quoted_string",
    "dbl_slash_comment",
    "delimited_list",
    "dict_of",
    "empty",
    "hexnums",
    "html_comment",
    "identchars",
    "identbodychars",
    "infix_notation",
    "java_style_comment",
    "line",
    "line_end",
    "line_start",
    "lineno",
    "make_html_tags",
    "make_xml_tags",
    "match_only_at_col",
    "match_previous_expr",
    "match_previous_literal",
    "nested_expr",
    "null_debug_action",
    "nums",
    "one_of",
    "original_text_for",
    "printables",
    "punc8bit",
    "pyparsing_common",
    "pyparsing_test",
    "pyparsing_unicode",
    "python_style_comment",
    "quoted_string",
    "remove_quotes",
    "replace_with",
    "replace_html_entity",
    "rest_of_line",
    "sgl_quoted_string",
    "srange",
    "string_end",
    "string_start",
    "token_map",
    "trace_parse_action",
    "ungroup",
    "unicode_set",
    "unicode_string",
    "with_attribute",
    "with_class",
    # pre-PEP8 compatibility names
    "__versionTime__",
    "anyCloseTag",
    "anyOpenTag",
    "cStyleComment",
    "commonHTMLEntity",
    "conditionAsParseAction",
    "countedArray",
    "cppStyleComment",
    "dblQuotedString",
    "dblSlashComment",
    "delimitedList",
    "dictOf",
    "htmlComment",
    "indentedBlock",
    "infixNotation",
    "javaStyleComment",
    "lineEnd",
    "lineStart",
    "locatedExpr",
    "makeHTMLTags",
    "makeXMLTags",
    "matchOnlyAtCol",
    "matchPreviousExpr",
    "matchPreviousLiteral",
    "nestedExpr",
    "nullDebugAction",
    "oneOf",
    "opAssoc",
    "originalTextFor",
    "pythonStyleComment",
    "quotedString",
    "removeQuotes",
    "replaceHTMLEntity",
    "replaceWith",
    "restOfLine",
    "sglQuotedString",
    "stringEnd",
    "stringStart",
    "tokenMap",
    "traceParseAction",
    "unicodeString",
    "withAttribute",
    "withClass",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/helpers.py
# ========================================================
# helpers.py
import html.entities
import re
import sys
import typing

from . import __diag__
from .core import *
from .util import (
    _bslash,
    _flatten,
    _escape_regex_range_chars,
    replaced_by_pep8,
)


#
# global helpers
#
def counted_array(
    expr: ParserElement,
    int_expr: typing.Optional[ParserElement] = None,
    *,
    intExpr: typing.Optional[ParserElement] = None,
) -> ParserElement:
    """Helper to define a counted list of expressions.

    This helper defines a pattern of the form::

        integer expr expr expr...

    where the leading integer tells how many expr expressions follow.
    The matched tokens returns the array of expr tokens as a list - the
    leading count token is suppressed.

    If ``int_expr`` is specified, it should be a pyparsing expression
    that produces an integer value.

    Example::

        counted_array(Word(alphas)).parse_string('2 ab cd ef')  # -> ['ab', 'cd']

        # in this parser, the leading integer value is given in binary,
        # '10' indicating that 2 values are in the array
        binary_constant = Word('01').set_parse_action(lambda t: int(t[0], 2))
        counted_array(Word(alphas), int_expr=binary_constant).parse_string('10 ab cd ef')  # -> ['ab', 'cd']

        # if other fields must be parsed after the count but before the
        # list items, give the fields results names and they will
        # be preserved in the returned ParseResults:
        count_with_metadata = integer + Word(alphas)("type")
        typed_array = counted_array(Word(alphanums), int_expr=count_with_metadata)("items")
        result = typed_array.parse_string("3 bool True True False")
        print(result.dump())

        # prints
        # ['True', 'True', 'False']
        # - items: ['True', 'True', 'False']
        # - type: 'bool'
    """
    intExpr = intExpr or int_expr
    array_expr = Forward()

    def count_field_parse_action(s, l, t):
        nonlocal array_expr
        n = t[0]
        array_expr <<= (expr * n) if n else Empty()
        # clear list contents, but keep any named results
        del t[:]

    if intExpr is None:
        intExpr = Word(nums).set_parse_action(lambda t: int(t[0]))
    else:
        intExpr = intExpr.copy()
    intExpr.set_name("arrayLen")
    intExpr.add_parse_action(count_field_parse_action, call_during_try=True)
    return (intExpr + array_expr).set_name("(len) " + str(expr) + "...")


def match_previous_literal(expr: ParserElement) -> ParserElement:
    """Helper to define an expression that is indirectly defined from
    the tokens matched in a previous expression, that is, it looks for
    a 'repeat' of a previous expression.  For example::

        first = Word(nums)
        second = match_previous_literal(first)
        match_expr = first + ":" + second

    will match ``"1:1"``, but not ``"1:2"``.  Because this
    matches a previous literal, will also match the leading
    ``"1:1"`` in ``"1:10"``. If this is not desired, use
    :class:`match_previous_expr`. Do *not* use with packrat parsing
    enabled.
    """
    rep = Forward()

    def copy_token_to_repeater(s, l, t):
        if t:
            if len(t) == 1:
                rep << t[0]
            else:
                # flatten t tokens
                tflat = _flatten(t.as_list())
                rep << And(Literal(tt) for tt in tflat)
        else:
            rep << Empty()

    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)
    rep.set_name("(prev) " + str(expr))
    return rep


def match_previous_expr(expr: ParserElement) -> ParserElement:
    """Helper to define an expression that is indirectly defined from
    the tokens matched in a previous expression, that is, it looks for
    a 'repeat' of a previous expression.  For example::

        first = Word(nums)
        second = match_previous_expr(first)
        match_expr = first + ":" + second

    will match ``"1:1"``, but not ``"1:2"``.  Because this
    matches by expressions, will *not* match the leading ``"1:1"``
    in ``"1:10"``; the expressions are evaluated first, and then
    compared, so ``"1"`` is compared with ``"10"``. Do *not* use
    with packrat parsing enabled.
    """
    rep = Forward()
    e2 = expr.copy()
    rep <<= e2

    def copy_token_to_repeater(s, l, t):
        matchTokens = _flatten(t.as_list())

        def must_match_these_tokens(s, l, t):
            theseTokens = _flatten(t.as_list())
            if theseTokens != matchTokens:
                raise ParseException(
                    s, l, f"Expected {matchTokens}, found{theseTokens}"
                )

        rep.set_parse_action(must_match_these_tokens, callDuringTry=True)

    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)
    rep.set_name("(prev) " + str(expr))
    return rep


def one_of(
    strs: Union[typing.Iterable[str], str],
    caseless: bool = False,
    use_regex: bool = True,
    as_keyword: bool = False,
    *,
    useRegex: bool = True,
    asKeyword: bool = False,
) -> ParserElement:
    """Helper to quickly define a set of alternative :class:`Literal` s,
    and makes sure to do longest-first testing when there is a conflict,
    regardless of the input order, but returns
    a :class:`MatchFirst` for best performance.

    Parameters:

    - ``strs`` - a string of space-delimited literals, or a collection of
      string literals
    - ``caseless`` - treat all literals as caseless - (default= ``False``)
    - ``use_regex`` - as an optimization, will
      generate a :class:`Regex` object; otherwise, will generate
      a :class:`MatchFirst` object (if ``caseless=True`` or ``as_keyword=True``, or if
      creating a :class:`Regex` raises an exception) - (default= ``True``)
    - ``as_keyword`` - enforce :class:`Keyword`-style matching on the
      generated expressions - (default= ``False``)
    - ``asKeyword`` and ``useRegex`` are retained for pre-PEP8 compatibility,
      but will be removed in a future release

    Example::

        comp_oper = one_of("< = > <= >= !=")
        var = Word(alphas)
        number = Word(nums)
        term = var | number
        comparison_expr = term + comp_oper + term
        print(comparison_expr.search_string("B = 12  AA=23 B<=AA AA>12"))

    prints::

        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]
    """
    asKeyword = asKeyword or as_keyword
    useRegex = useRegex and use_regex

    if (
        isinstance(caseless, str_type)
        and __diag__.warn_on_multiple_string_args_to_oneof
    ):
        warnings.warn(
            "More than one string argument passed to one_of, pass"
            " choices as a list or space-delimited string",
            stacklevel=2,
        )

    if caseless:
        isequal = lambda a, b: a.upper() == b.upper()
        masks = lambda a, b: b.upper().startswith(a.upper())
        parseElementClass = CaselessKeyword if asKeyword else CaselessLiteral
    else:
        isequal = lambda a, b: a == b
        masks = lambda a, b: b.startswith(a)
        parseElementClass = Keyword if asKeyword else Literal

    symbols: List[str] = []
    if isinstance(strs, str_type):
        strs = typing.cast(str, strs)
        symbols = strs.split()
    elif isinstance(strs, Iterable):
        symbols = list(strs)
    else:
        raise TypeError("Invalid argument to one_of, expected string or iterable")
    if not symbols:
        return NoMatch()

    # reorder given symbols to take care to avoid masking longer choices with shorter ones
    # (but only if the given symbols are not just single characters)
    if any(len(sym) > 1 for sym in symbols):
        i = 0
        while i < len(symbols) - 1:
            cur = symbols[i]
            for j, other in enumerate(symbols[i + 1 :]):
                if isequal(other, cur):
                    del symbols[i + j + 1]
                    break
                elif masks(cur, other):
                    del symbols[i + j + 1]
                    symbols.insert(i, other)
                    break
            else:
                i += 1

    if useRegex:
        re_flags: int = re.IGNORECASE if caseless else 0

        try:
            if all(len(sym) == 1 for sym in symbols):
                # symbols are just single characters, create range regex pattern
                patt = f"[{''.join(_escape_regex_range_chars(sym) for sym in symbols)}]"
            else:
                patt = "|".join(re.escape(sym) for sym in symbols)

            # wrap with \b word break markers if defining as keywords
            if asKeyword:
                patt = rf"\b(?:{patt})\b"

            ret = Regex(patt, flags=re_flags).set_name(" | ".join(symbols))

            if caseless:
                # add parse action to return symbols as specified, not in random
                # casing as found in input string
                symbol_map = {sym.lower(): sym for sym in symbols}
                ret.add_parse_action(lambda s, l, t: symbol_map[t[0].lower()])

            return ret

        except re.error:
            warnings.warn(
                "Exception creating Regex for one_of, building MatchFirst", stacklevel=2
            )

    # last resort, just use MatchFirst
    return MatchFirst(parseElementClass(sym) for sym in symbols).set_name(
        " | ".join(symbols)
    )


def dict_of(key: ParserElement, value: ParserElement) -> ParserElement:
    """Helper to easily and clearly define a dictionary by specifying
    the respective patterns for the key and value.  Takes care of
    defining the :class:`Dict`, :class:`ZeroOrMore`, and
    :class:`Group` tokens in the proper order.  The key pattern
    can include delimiting markers or punctuation, as long as they are
    suppressed, thereby leaving the significant key text.  The value
    pattern can include named results, so that the :class:`Dict` results
    can include named token fields.

    Example::

        text = "shape: SQUARE posn: upper left color: light blue texture: burlap"
        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join))
        print(attr_expr[1, ...].parse_string(text).dump())

        attr_label = label
        attr_value = Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join)

        # similar to Dict, but simpler call format
        result = dict_of(attr_label, attr_value).parse_string(text)
        print(result.dump())
        print(result['shape'])
        print(result.shape)  # object attribute access works too
        print(result.as_dict())

    prints::

        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]
        - color: 'light blue'
        - posn: 'upper left'
        - shape: 'SQUARE'
        - texture: 'burlap'
        SQUARE
        SQUARE
        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}
    """
    return Dict(OneOrMore(Group(key + value)))


def original_text_for(
    expr: ParserElement, as_string: bool = True, *, asString: bool = True
) -> ParserElement:
    """Helper to return the original, untokenized text for a given
    expression.  Useful to restore the parsed fields of an HTML start
    tag into the raw tag text itself, or to revert separate tokens with
    intervening whitespace back to the original matching input text. By
    default, returns a string containing the original parsed text.

    If the optional ``as_string`` argument is passed as
    ``False``, then the return value is
    a :class:`ParseResults` containing any results names that
    were originally matched, and a single token containing the original
    matched text from the input string.  So if the expression passed to
    :class:`original_text_for` contains expressions with defined
    results names, you must set ``as_string`` to ``False`` if you
    want to preserve those results name values.

    The ``asString`` pre-PEP8 argument is retained for compatibility,
    but will be removed in a future release.

    Example::

        src = "this is test <b> bold <i>text</i> </b> normal text "
        for tag in ("b", "i"):
            opener, closer = make_html_tags(tag)
            patt = original_text_for(opener + ... + closer)
            print(patt.search_string(src)[0])

    prints::

        ['<b> bold <i>text</i> </b>']
        ['<i>text</i>']
    """
    asString = asString and as_string

    locMarker = Empty().set_parse_action(lambda s, loc, t: loc)
    endlocMarker = locMarker.copy()
    endlocMarker.callPreparse = False
    matchExpr = locMarker("_original_start") + expr + endlocMarker("_original_end")
    if asString:
        extractText = lambda s, l, t: s[t._original_start : t._original_end]
    else:

        def extractText(s, l, t):
            t[:] = [s[t.pop("_original_start") : t.pop("_original_end")]]

    matchExpr.set_parse_action(extractText)
    matchExpr.ignoreExprs = expr.ignoreExprs
    matchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)
    return matchExpr


def ungroup(expr: ParserElement) -> ParserElement:
    """Helper to undo pyparsing's default grouping of And expressions,
    even if all but one are non-empty.
    """
    return TokenConverter(expr).add_parse_action(lambda t: t[0])


def locatedExpr(expr: ParserElement) -> ParserElement:
    """
    (DEPRECATED - future code should use the :class:`Located` class)
    Helper to decorate a returned token with its starting and ending
    locations in the input string.

    This helper adds the following results names:

    - ``locn_start`` - location where matched expression begins
    - ``locn_end`` - location where matched expression ends
    - ``value`` - the actual parsed results

    Be careful if the input text contains ``<TAB>`` characters, you
    may want to call :class:`ParserElement.parse_with_tabs`

    Example::

        wd = Word(alphas)
        for match in locatedExpr(wd).search_string("ljsdf123lksdjjf123lkkjj1222"):
            print(match)

    prints::

        [[0, 'ljsdf', 5]]
        [[8, 'lksdjjf', 15]]
        [[18, 'lkkjj', 23]]
    """
    locator = Empty().set_parse_action(lambda ss, ll, tt: ll)
    return Group(
        locator("locn_start")
        + expr("value")
        + locator.copy().leaveWhitespace()("locn_end")
    )


def nested_expr(
    opener: Union[str, ParserElement] = "(",
    closer: Union[str, ParserElement] = ")",
    content: typing.Optional[ParserElement] = None,
    ignore_expr: ParserElement = quoted_string(),
    *,
    ignoreExpr: ParserElement = quoted_string(),
) -> ParserElement:
    """Helper method for defining nested lists enclosed in opening and
    closing delimiters (``"("`` and ``")"`` are the default).

    Parameters:

    - ``opener`` - opening character for a nested list
      (default= ``"("``); can also be a pyparsing expression
    - ``closer`` - closing character for a nested list
      (default= ``")"``); can also be a pyparsing expression
    - ``content`` - expression for items within the nested lists
      (default= ``None``)
    - ``ignore_expr`` - expression for ignoring opening and closing delimiters
      (default= :class:`quoted_string`)
    - ``ignoreExpr`` - this pre-PEP8 argument is retained for compatibility
      but will be removed in a future release

    If an expression is not provided for the content argument, the
    nested expression will capture all whitespace-delimited content
    between delimiters as a list of separate values.

    Use the ``ignore_expr`` argument to define expressions that may
    contain opening or closing characters that should not be treated as
    opening or closing characters for nesting, such as quoted_string or
    a comment expression.  Specify multiple expressions using an
    :class:`Or` or :class:`MatchFirst`. The default is
    :class:`quoted_string`, but if no expressions are to be ignored, then
    pass ``None`` for this argument.

    Example::

        data_type = one_of("void int short long char float double")
        decl_data_type = Combine(data_type + Opt(Word('*')))
        ident = Word(alphas+'_', alphanums+'_')
        number = pyparsing_common.number
        arg = Group(decl_data_type + ident)
        LPAR, RPAR = map(Suppress, "()")

        code_body = nested_expr('{', '}', ignore_expr=(quoted_string | c_style_comment))

        c_function = (decl_data_type("type")
                      + ident("name")
                      + LPAR + Opt(DelimitedList(arg), [])("args") + RPAR
                      + code_body("body"))
        c_function.ignore(c_style_comment)

        source_code = '''
            int is_odd(int x) {
                return (x%2);
            }

            int dec_to_hex(char hchar) {
                if (hchar >= '0' && hchar <= '9') {
                    return (ord(hchar)-ord('0'));
                } else {
                    return (10+ord(hchar)-ord('A'));
                }
            }
        '''
        for func in c_function.search_string(source_code):
            print("%(name)s (%(type)s) args: %(args)s" % func)


    prints::

        is_odd (int) args: [['int', 'x']]
        dec_to_hex (int) args: [['char', 'hchar']]
    """
    if ignoreExpr != ignore_expr:
        ignoreExpr = ignore_expr if ignoreExpr == quoted_string() else ignoreExpr
    if opener == closer:
        raise ValueError("opening and closing strings cannot be the same")
    if content is None:
        if isinstance(opener, str_type) and isinstance(closer, str_type):
            opener = typing.cast(str, opener)
            closer = typing.cast(str, closer)
            if len(opener) == 1 and len(closer) == 1:
                if ignoreExpr is not None:
                    content = Combine(
                        OneOrMore(
                            ~ignoreExpr
                            + CharsNotIn(
                                opener + closer + ParserElement.DEFAULT_WHITE_CHARS,
                                exact=1,
                            )
                        )
                    ).set_parse_action(lambda t: t[0].strip())
                else:
                    content = empty.copy() + CharsNotIn(
                        opener + closer + ParserElement.DEFAULT_WHITE_CHARS
                    ).set_parse_action(lambda t: t[0].strip())
            else:
                if ignoreExpr is not None:
                    content = Combine(
                        OneOrMore(
                            ~ignoreExpr
                            + ~Literal(opener)
                            + ~Literal(closer)
                            + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1)
                        )
                    ).set_parse_action(lambda t: t[0].strip())
                else:
                    content = Combine(
                        OneOrMore(
                            ~Literal(opener)
                            + ~Literal(closer)
                            + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1)
                        )
                    ).set_parse_action(lambda t: t[0].strip())
        else:
            raise ValueError(
                "opening and closing arguments must be strings if no content expression is given"
            )
    ret = Forward()
    if ignoreExpr is not None:
        ret <<= Group(
            Suppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer)
        )
    else:
        ret <<= Group(Suppress(opener) + ZeroOrMore(ret | content) + Suppress(closer))
    ret.set_name("nested %s%s expression" % (opener, closer))
    return ret


def _makeTags(tagStr, xml, suppress_LT=Suppress("<"), suppress_GT=Suppress(">")):
    """Internal helper to construct opening and closing tag expressions, given a tag name"""
    if isinstance(tagStr, str_type):
        resname = tagStr
        tagStr = Keyword(tagStr, caseless=not xml)
    else:
        resname = tagStr.name

    tagAttrName = Word(alphas, alphanums + "_-:")
    if xml:
        tagAttrValue = dbl_quoted_string.copy().set_parse_action(remove_quotes)
        openTag = (
            suppress_LT
            + tagStr("tag")
            + Dict(ZeroOrMore(Group(tagAttrName + Suppress("=") + tagAttrValue)))
            + Opt("/", default=[False])("empty").set_parse_action(
                lambda s, l, t: t[0] == "/"
            )
            + suppress_GT
        )
    else:
        tagAttrValue = quoted_string.copy().set_parse_action(remove_quotes) | Word(
            printables, exclude_chars=">"
        )
        openTag = (
            suppress_LT
            + tagStr("tag")
            + Dict(
                ZeroOrMore(
                    Group(
                        tagAttrName.set_parse_action(lambda t: t[0].lower())
                        + Opt(Suppress("=") + tagAttrValue)
                    )
                )
            )
            + Opt("/", default=[False])("empty").set_parse_action(
                lambda s, l, t: t[0] == "/"
            )
            + suppress_GT
        )
    closeTag = Combine(Literal("</") + tagStr + ">", adjacent=False)

    openTag.set_name("<%s>" % resname)
    # add start<tagname> results name in parse action now that ungrouped names are not reported at two levels
    openTag.add_parse_action(
        lambda t: t.__setitem__(
            "start" + "".join(resname.replace(":", " ").title().split()), t.copy()
        )
    )
    closeTag = closeTag(
        "end" + "".join(resname.replace(":", " ").title().split())
    ).set_name("</%s>" % resname)
    openTag.tag = resname
    closeTag.tag = resname
    openTag.tag_body = SkipTo(closeTag())
    return openTag, closeTag


def make_html_tags(
    tag_str: Union[str, ParserElement]
) -> Tuple[ParserElement, ParserElement]:
    """Helper to construct opening and closing tag expressions for HTML,
    given a tag name. Matches tags in either upper or lower case,
    attributes with namespaces and with quoted or unquoted values.

    Example::

        text = '<td>More info at the <a href="https://github.com/pyparsing/pyparsing/wiki">pyparsing</a> wiki page</td>'
        # make_html_tags returns pyparsing expressions for the opening and
        # closing tags as a 2-tuple
        a, a_end = make_html_tags("A")
        link_expr = a + SkipTo(a_end)("link_text") + a_end

        for link in link_expr.search_string(text):
            # attributes in the <A> tag (like "href" shown here) are
            # also accessible as named results
            print(link.link_text, '->', link.href)

    prints::

        pyparsing -> https://github.com/pyparsing/pyparsing/wiki
    """
    return _makeTags(tag_str, False)


def make_xml_tags(
    tag_str: Union[str, ParserElement]
) -> Tuple[ParserElement, ParserElement]:
    """Helper to construct opening and closing tag expressions for XML,
    given a tag name. Matches tags only in the given upper/lower case.

    Example: similar to :class:`make_html_tags`
    """
    return _makeTags(tag_str, True)


any_open_tag: ParserElement
any_close_tag: ParserElement
any_open_tag, any_close_tag = make_html_tags(
    Word(alphas, alphanums + "_:").set_name("any tag")
)

_htmlEntityMap = {k.rstrip(";"): v for k, v in html.entities.html5.items()}
common_html_entity = Regex("&(?P<entity>" + "|".join(_htmlEntityMap) + ");").set_name(
    "common HTML entity"
)


def replace_html_entity(s, l, t):
    """Helper parser action to replace common HTML entities with their special characters"""
    return _htmlEntityMap.get(t.entity)


class OpAssoc(Enum):
    """Enumeration of operator associativity
    - used in constructing InfixNotationOperatorSpec for :class:`infix_notation`"""

    LEFT = 1
    RIGHT = 2


InfixNotationOperatorArgType = Union[
    ParserElement, str, Tuple[Union[ParserElement, str], Union[ParserElement, str]]
]
InfixNotationOperatorSpec = Union[
    Tuple[
        InfixNotationOperatorArgType,
        int,
        OpAssoc,
        typing.Optional[ParseAction],
    ],
    Tuple[
        InfixNotationOperatorArgType,
        int,
        OpAssoc,
    ],
]


def infix_notation(
    base_expr: ParserElement,
    op_list: List[InfixNotationOperatorSpec],
    lpar: Union[str, ParserElement] = Suppress("("),
    rpar: Union[str, ParserElement] = Suppress(")"),
) -> ParserElement:
    """Helper method for constructing grammars of expressions made up of
    operators working in a precedence hierarchy.  Operators may be unary
    or binary, left- or right-associative.  Parse actions can also be
    attached to operator expressions. The generated parser will also
    recognize the use of parentheses to override operator precedences
    (see example below).

    Note: if you define a deep operator list, you may see performance
    issues when using infix_notation. See
    :class:`ParserElement.enable_packrat` for a mechanism to potentially
    improve your parser performance.

    Parameters:

    - ``base_expr`` - expression representing the most basic operand to
      be used in the expression
    - ``op_list`` - list of tuples, one for each operator precedence level
      in the expression grammar; each tuple is of the form ``(op_expr,
      num_operands, right_left_assoc, (optional)parse_action)``, where:

      - ``op_expr`` is the pyparsing expression for the operator; may also
        be a string, which will be converted to a Literal; if ``num_operands``
        is 3, ``op_expr`` is a tuple of two expressions, for the two
        operators separating the 3 terms
      - ``num_operands`` is the number of terms for this operator (must be 1,
        2, or 3)
      - ``right_left_assoc`` is the indicator whether the operator is right
        or left associative, using the pyparsing-defined constants
        ``OpAssoc.RIGHT`` and ``OpAssoc.LEFT``.
      - ``parse_action`` is the parse action to be associated with
        expressions matching this operator expression (the parse action
        tuple member may be omitted); if the parse action is passed
        a tuple or list of functions, this is equivalent to calling
        ``set_parse_action(*fn)``
        (:class:`ParserElement.set_parse_action`)
    - ``lpar`` - expression for matching left-parentheses; if passed as a
      str, then will be parsed as ``Suppress(lpar)``. If lpar is passed as
      an expression (such as ``Literal('(')``), then it will be kept in
      the parsed results, and grouped with them. (default= ``Suppress('(')``)
    - ``rpar`` - expression for matching right-parentheses; if passed as a
      str, then will be parsed as ``Suppress(rpar)``. If rpar is passed as
      an expression (such as ``Literal(')')``), then it will be kept in
      the parsed results, and grouped with them. (default= ``Suppress(')')``)

    Example::

        # simple example of four-function arithmetic with ints and
        # variable names
        integer = pyparsing_common.signed_integer
        varname = pyparsing_common.identifier

        arith_expr = infix_notation(integer | varname,
            [
            ('-', 1, OpAssoc.RIGHT),
            (one_of('* /'), 2, OpAssoc.LEFT),
            (one_of('+ -'), 2, OpAssoc.LEFT),
            ])

        arith_expr.run_tests('''
            5+3*6
            (5+3)*6
            -2--11
            ''', full_dump=False)

    prints::

        5+3*6
        [[5, '+', [3, '*', 6]]]

        (5+3)*6
        [[[5, '+', 3], '*', 6]]

        (5+x)*y
        [[[5, '+', 'x'], '*', 'y']]

        -2--11
        [[['-', 2], '-', ['-', 11]]]
    """

    # captive version of FollowedBy that does not do parse actions or capture results names
    class _FB(FollowedBy):
        def parseImpl(self, instring, loc, doActions=True):
            self.expr.try_parse(instring, loc)
            return loc, []

    _FB.__name__ = "FollowedBy>"

    ret = Forward()
    if isinstance(lpar, str):
        lpar = Suppress(lpar)
    if isinstance(rpar, str):
        rpar = Suppress(rpar)

    # if lpar and rpar are not suppressed, wrap in group
    if not (isinstance(rpar, Suppress) and isinstance(rpar, Suppress)):
        lastExpr = base_expr | Group(lpar + ret + rpar)
    else:
        lastExpr = base_expr | (lpar + ret + rpar)

    arity: int
    rightLeftAssoc: opAssoc
    pa: typing.Optional[ParseAction]
    opExpr1: ParserElement
    opExpr2: ParserElement
    for i, operDef in enumerate(op_list):
        opExpr, arity, rightLeftAssoc, pa = (operDef + (None,))[:4]  # type: ignore[assignment]
        if isinstance(opExpr, str_type):
            opExpr = ParserElement._literalStringClass(opExpr)
        opExpr = typing.cast(ParserElement, opExpr)
        if arity == 3:
            if not isinstance(opExpr, (tuple, list)) or len(opExpr) != 2:
                raise ValueError(
                    "if numterms=3, opExpr must be a tuple or list of two expressions"
                )
            opExpr1, opExpr2 = opExpr
            term_name = f"{opExpr1}{opExpr2} term"
        else:
            term_name = f"{opExpr} term"

        if not 1 <= arity <= 3:
            raise ValueError("operator must be unary (1), binary (2), or ternary (3)")

        if rightLeftAssoc not in (OpAssoc.LEFT, OpAssoc.RIGHT):
            raise ValueError("operator must indicate right or left associativity")

        thisExpr: ParserElement = Forward().set_name(term_name)
        thisExpr = typing.cast(Forward, thisExpr)
        if rightLeftAssoc is OpAssoc.LEFT:
            if arity == 1:
                matchExpr = _FB(lastExpr + opExpr) + Group(lastExpr + opExpr[1, ...])
            elif arity == 2:
                if opExpr is not None:
                    matchExpr = _FB(lastExpr + opExpr + lastExpr) + Group(
                        lastExpr + (opExpr + lastExpr)[1, ...]
                    )
                else:
                    matchExpr = _FB(lastExpr + lastExpr) + Group(lastExpr[2, ...])
            elif arity == 3:
                matchExpr = _FB(
                    lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr
                ) + Group(lastExpr + OneOrMore(opExpr1 + lastExpr + opExpr2 + lastExpr))
        elif rightLeftAssoc is OpAssoc.RIGHT:
            if arity == 1:
                # try to avoid LR with this extra test
                if not isinstance(opExpr, Opt):
                    opExpr = Opt(opExpr)
                matchExpr = _FB(opExpr.expr + thisExpr) + Group(opExpr + thisExpr)
            elif arity == 2:
                if opExpr is not None:
                    matchExpr = _FB(lastExpr + opExpr + thisExpr) + Group(
                        lastExpr + (opExpr + thisExpr)[1, ...]
                    )
                else:
                    matchExpr = _FB(lastExpr + thisExpr) + Group(
                        lastExpr + thisExpr[1, ...]
                    )
            elif arity == 3:
                matchExpr = _FB(
                    lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr
                ) + Group(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr)
        if pa:
            if isinstance(pa, (tuple, list)):
                matchExpr.set_parse_action(*pa)
            else:
                matchExpr.set_parse_action(pa)
        thisExpr <<= (matchExpr | lastExpr).setName(term_name)
        lastExpr = thisExpr
    ret <<= lastExpr
    return ret


def indentedBlock(blockStatementExpr, indentStack, indent=True, backup_stacks=[]):
    """
    (DEPRECATED - use :class:`IndentedBlock` class instead)
    Helper method for defining space-delimited indentation blocks,
    such as those used to define block statements in Python source code.

    Parameters:

    - ``blockStatementExpr`` - expression defining syntax of statement that
      is repeated within the indented block
    - ``indentStack`` - list created by caller to manage indentation stack
      (multiple ``statementWithIndentedBlock`` expressions within a single
      grammar should share a common ``indentStack``)
    - ``indent`` - boolean indicating whether block must be indented beyond
      the current level; set to ``False`` for block of left-most statements
      (default= ``True``)

    A valid block must contain at least one ``blockStatement``.

    (Note that indentedBlock uses internal parse actions which make it
    incompatible with packrat parsing.)

    Example::

        data = '''
        def A(z):
          A1
          B = 100
          G = A2
          A2
          A3
        B
        def BB(a,b,c):
          BB1
          def BBA():
            bba1
            bba2
            bba3
        C
        D
        def spam(x,y):
             def eggs(z):
                 pass
        '''


        indentStack = [1]
        stmt = Forward()

        identifier = Word(alphas, alphanums)
        funcDecl = ("def" + identifier + Group("(" + Opt(delimitedList(identifier)) + ")") + ":")
        func_body = indentedBlock(stmt, indentStack)
        funcDef = Group(funcDecl + func_body)

        rvalue = Forward()
        funcCall = Group(identifier + "(" + Opt(delimitedList(rvalue)) + ")")
        rvalue << (funcCall | identifier | Word(nums))
        assignment = Group(identifier + "=" + rvalue)
        stmt << (funcDef | assignment | identifier)

        module_body = stmt[1, ...]

        parseTree = module_body.parseString(data)
        parseTree.pprint()

    prints::

        [['def',
          'A',
          ['(', 'z', ')'],
          ':',
          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],
         'B',
         ['def',
          'BB',
          ['(', 'a', 'b', 'c', ')'],
          ':',
          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],
         'C',
         'D',
         ['def',
          'spam',
          ['(', 'x', 'y', ')'],
          ':',
          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]]
    """
    backup_stacks.append(indentStack[:])

    def reset_stack():
        indentStack[:] = backup_stacks[-1]

    def checkPeerIndent(s, l, t):
        if l >= len(s):
            return
        curCol = col(l, s)
        if curCol != indentStack[-1]:
            if curCol > indentStack[-1]:
                raise ParseException(s, l, "illegal nesting")
            raise ParseException(s, l, "not a peer entry")

    def checkSubIndent(s, l, t):
        curCol = col(l, s)
        if curCol > indentStack[-1]:
            indentStack.append(curCol)
        else:
            raise ParseException(s, l, "not a subentry")

    def checkUnindent(s, l, t):
        if l >= len(s):
            return
        curCol = col(l, s)
        if not (indentStack and curCol in indentStack):
            raise ParseException(s, l, "not an unindent")
        if curCol < indentStack[-1]:
            indentStack.pop()

    NL = OneOrMore(LineEnd().set_whitespace_chars("\t ").suppress())
    INDENT = (Empty() + Empty().set_parse_action(checkSubIndent)).set_name("INDENT")
    PEER = Empty().set_parse_action(checkPeerIndent).set_name("")
    UNDENT = Empty().set_parse_action(checkUnindent).set_name("UNINDENT")
    if indent:
        smExpr = Group(
            Opt(NL)
            + INDENT
            + OneOrMore(PEER + Group(blockStatementExpr) + Opt(NL))
            + UNDENT
        )
    else:
        smExpr = Group(
            Opt(NL)
            + OneOrMore(PEER + Group(blockStatementExpr) + Opt(NL))
            + Opt(UNDENT)
        )

    # add a parse action to remove backup_stack from list of backups
    smExpr.add_parse_action(
        lambda: backup_stacks.pop(-1) and None if backup_stacks else None
    )
    smExpr.set_fail_action(lambda a, b, c, d: reset_stack())
    blockStatementExpr.ignore(_bslash + LineEnd())
    return smExpr.set_name("indented block")


# it's easy to get these comment structures wrong - they're very common, so may as well make them available
c_style_comment = Combine(Regex(r"/\*(?:[^*]|\*(?!/))*") + "*/").set_name(
    "C style comment"
)
"Comment of the form ``/* ... */``"

html_comment = Regex(r"<!--[\s\S]*?-->").set_name("HTML comment")
"Comment of the form ``<!-- ... -->``"

rest_of_line = Regex(r".*").leave_whitespace().set_name("rest of line")
dbl_slash_comment = Regex(r"//(?:\\\n|[^\n])*").set_name("// comment")
"Comment of the form ``// ... (to end of line)``"

cpp_style_comment = Combine(
    Regex(r"/\*(?:[^*]|\*(?!/))*") + "*/" | dbl_slash_comment
).set_name("C++ style comment")
"Comment of either form :class:`c_style_comment` or :class:`dbl_slash_comment`"

java_style_comment = cpp_style_comment
"Same as :class:`cpp_style_comment`"

python_style_comment = Regex(r"#.*").set_name("Python style comment")
"Comment of the form ``# ... (to end of line)``"


# build list of built-in expressions, for future reference if a global default value
# gets updated
_builtin_exprs: List[ParserElement] = [
    v for v in vars().values() if isinstance(v, ParserElement)
]


# compatibility function, superseded by DelimitedList class
def delimited_list(
    expr: Union[str, ParserElement],
    delim: Union[str, ParserElement] = ",",
    combine: bool = False,
    min: typing.Optional[int] = None,
    max: typing.Optional[int] = None,
    *,
    allow_trailing_delim: bool = False,
) -> ParserElement:
    """(DEPRECATED - use :class:`DelimitedList` class)"""
    return DelimitedList(
        expr, delim, combine, min, max, allow_trailing_delim=allow_trailing_delim
    )


# pre-PEP8 compatible names
# fmt: off
opAssoc = OpAssoc
anyOpenTag = any_open_tag
anyCloseTag = any_close_tag
commonHTMLEntity = common_html_entity
cStyleComment = c_style_comment
htmlComment = html_comment
restOfLine = rest_of_line
dblSlashComment = dbl_slash_comment
cppStyleComment = cpp_style_comment
javaStyleComment = java_style_comment
pythonStyleComment = python_style_comment

@replaced_by_pep8(DelimitedList)
def delimitedList(): ...

@replaced_by_pep8(DelimitedList)
def delimited_list(): ...

@replaced_by_pep8(counted_array)
def countedArray(): ...

@replaced_by_pep8(match_previous_literal)
def matchPreviousLiteral(): ...

@replaced_by_pep8(match_previous_expr)
def matchPreviousExpr(): ...

@replaced_by_pep8(one_of)
def oneOf(): ...

@replaced_by_pep8(dict_of)
def dictOf(): ...

@replaced_by_pep8(original_text_for)
def originalTextFor(): ...

@replaced_by_pep8(nested_expr)
def nestedExpr(): ...

@replaced_by_pep8(make_html_tags)
def makeHTMLTags(): ...

@replaced_by_pep8(make_xml_tags)
def makeXMLTags(): ...

@replaced_by_pep8(replace_html_entity)
def replaceHTMLEntity(): ...

@replaced_by_pep8(infix_notation)
def infixNotation(): ...
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/core.py
# ========================================================
#
# core.py
#

from collections import deque
import os
import typing
from typing import (
    Any,
    Callable,
    Generator,
    List,
    NamedTuple,
    Sequence,
    Set,
    TextIO,
    Tuple,
    Union,
    cast,
)
from abc import ABC, abstractmethod
from enum import Enum
import string
import copy
import warnings
import re
import sys
from collections.abc import Iterable
import traceback
import types
from operator import itemgetter
from functools import wraps
from threading import RLock
from pathlib import Path

from .util import (
    _FifoCache,
    _UnboundedCache,
    __config_flags,
    _collapse_string_to_ranges,
    _escape_regex_range_chars,
    _bslash,
    _flatten,
    LRUMemo as _LRUMemo,
    UnboundedMemo as _UnboundedMemo,
    replaced_by_pep8,
)
from .exceptions import *
from .actions import *
from .results import ParseResults, _ParseResultsWithOffset
from .unicode import pyparsing_unicode

_MAX_INT = sys.maxsize
str_type: Tuple[type, ...] = (str, bytes)

#
# Copyright (c) 2003-2022  Paul T. McGuire
#
# Permission is hereby granted, free of charge, to any person obtaining
# a copy of this software and associated documentation files (the
# "Software"), to deal in the Software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish,
# distribute, sublicense, and/or sell copies of the Software, and to
# permit persons to whom the Software is furnished to do so, subject to
# the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#


if sys.version_info >= (3, 8):
    from functools import cached_property
else:

    class cached_property:
        def __init__(self, func):
            self._func = func

        def __get__(self, instance, owner=None):
            ret = instance.__dict__[self._func.__name__] = self._func(instance)
            return ret


class __compat__(__config_flags):
    """
    A cross-version compatibility configuration for pyparsing features that will be
    released in a future version. By setting values in this configuration to True,
    those features can be enabled in prior versions for compatibility development
    and testing.

    - ``collect_all_And_tokens`` - flag to enable fix for Issue #63 that fixes erroneous grouping
      of results names when an :class:`And` expression is nested within an :class:`Or` or :class:`MatchFirst`;
      maintained for compatibility, but setting to ``False`` no longer restores pre-2.3.1
      behavior
    """

    _type_desc = "compatibility"

    collect_all_And_tokens = True

    _all_names = [__ for __ in locals() if not __.startswith("_")]
    _fixed_names = """
        collect_all_And_tokens
        """.split()


class __diag__(__config_flags):
    _type_desc = "diagnostic"

    warn_multiple_tokens_in_named_alternation = False
    warn_ungrouped_named_tokens_in_collection = False
    warn_name_set_on_empty_Forward = False
    warn_on_parse_using_empty_Forward = False
    warn_on_assignment_to_Forward = False
    warn_on_multiple_string_args_to_oneof = False
    warn_on_match_first_with_lshift_operator = False
    enable_debug_on_named_expressions = False

    _all_names = [__ for __ in locals() if not __.startswith("_")]
    _warning_names = [name for name in _all_names if name.startswith("warn")]
    _debug_names = [name for name in _all_names if name.startswith("enable_debug")]

    @classmethod
    def enable_all_warnings(cls) -> None:
        for name in cls._warning_names:
            cls.enable(name)


class Diagnostics(Enum):
    """
    Diagnostic configuration (all default to disabled)

    - ``warn_multiple_tokens_in_named_alternation`` - flag to enable warnings when a results
      name is defined on a :class:`MatchFirst` or :class:`Or` expression with one or more :class:`And` subexpressions
    - ``warn_ungrouped_named_tokens_in_collection`` - flag to enable warnings when a results
      name is defined on a containing expression with ungrouped subexpressions that also
      have results names
    - ``warn_name_set_on_empty_Forward`` - flag to enable warnings when a :class:`Forward` is defined
      with a results name, but has no contents defined
    - ``warn_on_parse_using_empty_Forward`` - flag to enable warnings when a :class:`Forward` is
      defined in a grammar but has never had an expression attached to it
    - ``warn_on_assignment_to_Forward`` - flag to enable warnings when a :class:`Forward` is defined
      but is overwritten by assigning using ``'='`` instead of ``'<<='`` or ``'<<'``
    - ``warn_on_multiple_string_args_to_oneof`` - flag to enable warnings when :class:`one_of` is
      incorrectly called with multiple str arguments
    - ``enable_debug_on_named_expressions`` - flag to auto-enable debug on all subsequent
      calls to :class:`ParserElement.set_name`

    Diagnostics are enabled/disabled by calling :class:`enable_diag` and :class:`disable_diag`.
    All warnings can be enabled by calling :class:`enable_all_warnings`.
    """

    warn_multiple_tokens_in_named_alternation = 0
    warn_ungrouped_named_tokens_in_collection = 1
    warn_name_set_on_empty_Forward = 2
    warn_on_parse_using_empty_Forward = 3
    warn_on_assignment_to_Forward = 4
    warn_on_multiple_string_args_to_oneof = 5
    warn_on_match_first_with_lshift_operator = 6
    enable_debug_on_named_expressions = 7


def enable_diag(diag_enum: Diagnostics) -> None:
    """
    Enable a global pyparsing diagnostic flag (see :class:`Diagnostics`).
    """
    __diag__.enable(diag_enum.name)


def disable_diag(diag_enum: Diagnostics) -> None:
    """
    Disable a global pyparsing diagnostic flag (see :class:`Diagnostics`).
    """
    __diag__.disable(diag_enum.name)


def enable_all_warnings() -> None:
    """
    Enable all global pyparsing diagnostic warnings (see :class:`Diagnostics`).
    """
    __diag__.enable_all_warnings()


# hide abstract class
del __config_flags


def _should_enable_warnings(
    cmd_line_warn_options: typing.Iterable[str], warn_env_var: typing.Optional[str]
) -> bool:
    enable = bool(warn_env_var)
    for warn_opt in cmd_line_warn_options:
        w_action, w_message, w_category, w_module, w_line = (warn_opt + "::::").split(
            ":"
        )[:5]
        if not w_action.lower().startswith("i") and (
            not (w_message or w_category or w_module) or w_module == "pyparsing"
        ):
            enable = True
        elif w_action.lower().startswith("i") and w_module in ("pyparsing", ""):
            enable = False
    return enable


if _should_enable_warnings(
    sys.warnoptions, os.environ.get("PYPARSINGENABLEALLWARNINGS")
):
    enable_all_warnings()


# build list of single arg builtins, that can be used as parse actions
_single_arg_builtins = {
    sum,
    len,
    sorted,
    reversed,
    list,
    tuple,
    set,
    any,
    all,
    min,
    max,
}

_generatorType = types.GeneratorType
ParseImplReturnType = Tuple[int, Any]
PostParseReturnType = Union[ParseResults, Sequence[ParseResults]]
ParseAction = Union[
    Callable[[], Any],
    Callable[[ParseResults], Any],
    Callable[[int, ParseResults], Any],
    Callable[[str, int, ParseResults], Any],
]
ParseCondition = Union[
    Callable[[], bool],
    Callable[[ParseResults], bool],
    Callable[[int, ParseResults], bool],
    Callable[[str, int, ParseResults], bool],
]
ParseFailAction = Callable[[str, int, "ParserElement", Exception], None]
DebugStartAction = Callable[[str, int, "ParserElement", bool], None]
DebugSuccessAction = Callable[
    [str, int, int, "ParserElement", ParseResults, bool], None
]
DebugExceptionAction = Callable[[str, int, "ParserElement", Exception, bool], None]


alphas = string.ascii_uppercase + string.ascii_lowercase
identchars = pyparsing_unicode.Latin1.identchars
identbodychars = pyparsing_unicode.Latin1.identbodychars
nums = "0123456789"
hexnums = nums + "ABCDEFabcdef"
alphanums = alphas + nums
printables = "".join([c for c in string.printable if c not in string.whitespace])

_trim_arity_call_line: traceback.StackSummary = None  # type: ignore[assignment]


def _trim_arity(func, max_limit=3):
    """decorator to trim function calls to match the arity of the target"""
    global _trim_arity_call_line

    if func in _single_arg_builtins:
        return lambda s, l, t: func(t)

    limit = 0
    found_arity = False

    # synthesize what would be returned by traceback.extract_stack at the call to
    # user's parse action 'func', so that we don't incur call penalty at parse time

    # fmt: off
    LINE_DIFF = 7
    # IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND
    # THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!
    _trim_arity_call_line = (_trim_arity_call_line or traceback.extract_stack(limit=2)[-1])
    pa_call_line_synth = (_trim_arity_call_line[0], _trim_arity_call_line[1] + LINE_DIFF)

    def wrapper(*args):
        nonlocal found_arity, limit
        while 1:
            try:
                ret = func(*args[limit:])
                found_arity = True
                return ret
            except TypeError as te:
                # re-raise TypeErrors if they did not come from our arity testing
                if found_arity:
                    raise
                else:
                    tb = te.__traceback__
                    frames = traceback.extract_tb(tb, limit=2)
                    frame_summary = frames[-1]
                    trim_arity_type_error = (
                        [frame_summary[:2]][-1][:2] == pa_call_line_synth
                    )
                    del tb

                    if trim_arity_type_error:
                        if limit < max_limit:
                            limit += 1
                            continue

                    raise
    # fmt: on

    # copy func name to wrapper for sensible debug output
    # (can't use functools.wraps, since that messes with function signature)
    func_name = getattr(func, "__name__", getattr(func, "__class__").__name__)
    wrapper.__name__ = func_name
    wrapper.__doc__ = func.__doc__

    return wrapper


def condition_as_parse_action(
    fn: ParseCondition, message: typing.Optional[str] = None, fatal: bool = False
) -> ParseAction:
    """
    Function to convert a simple predicate function that returns ``True`` or ``False``
    into a parse action. Can be used in places when a parse action is required
    and :class:`ParserElement.add_condition` cannot be used (such as when adding a condition
    to an operator level in :class:`infix_notation`).

    Optional keyword arguments:

    - ``message`` - define a custom message to be used in the raised exception
    - ``fatal`` - if True, will raise :class:`ParseFatalException` to stop parsing immediately;
      otherwise will raise :class:`ParseException`

    """
    msg = message if message is not None else "failed user-defined condition"
    exc_type = ParseFatalException if fatal else ParseException
    fn = _trim_arity(fn)

    @wraps(fn)
    def pa(s, l, t):
        if not bool(fn(s, l, t)):
            raise exc_type(s, l, msg)

    return pa


def _default_start_debug_action(
    instring: str, loc: int, expr: "ParserElement", cache_hit: bool = False
):
    cache_hit_str = "*" if cache_hit else ""
    print(
        (
            f"{cache_hit_str}Match {expr} at loc {loc}({lineno(loc, instring)},{col(loc, instring)})\n"
            f"  {line(loc, instring)}\n"
            f"  {' ' * (col(loc, instring) - 1)}^"
        )
    )


def _default_success_debug_action(
    instring: str,
    startloc: int,
    endloc: int,
    expr: "ParserElement",
    toks: ParseResults,
    cache_hit: bool = False,
):
    cache_hit_str = "*" if cache_hit else ""
    print(f"{cache_hit_str}Matched {expr} -> {toks.as_list()}")


def _default_exception_debug_action(
    instring: str,
    loc: int,
    expr: "ParserElement",
    exc: Exception,
    cache_hit: bool = False,
):
    cache_hit_str = "*" if cache_hit else ""
    print(f"{cache_hit_str}Match {expr} failed, {type(exc).__name__} raised: {exc}")


def null_debug_action(*args):
    """'Do-nothing' debug action, to suppress debugging output during parsing."""


class ParserElement(ABC):
    """Abstract base level parser element class."""

    DEFAULT_WHITE_CHARS: str = " \n\t\r"
    verbose_stacktrace: bool = False
    _literalStringClass: type = None  # type: ignore[assignment]

    @staticmethod
    def set_default_whitespace_chars(chars: str) -> None:
        r"""
        Overrides the default whitespace chars

        Example::

            # default whitespace chars are space, <TAB> and newline
            Word(alphas)[1, ...].parse_string("abc def\nghi jkl")  # -> ['abc', 'def', 'ghi', 'jkl']

            # change to just treat newline as significant
            ParserElement.set_default_whitespace_chars(" \t")
            Word(alphas)[1, ...].parse_string("abc def\nghi jkl")  # -> ['abc', 'def']
        """
        ParserElement.DEFAULT_WHITE_CHARS = chars

        # update whitespace all parse expressions defined in this module
        for expr in _builtin_exprs:
            if expr.copyDefaultWhiteChars:
                expr.whiteChars = set(chars)

    @staticmethod
    def inline_literals_using(cls: type) -> None:
        """
        Set class to be used for inclusion of string literals into a parser.

        Example::

            # default literal class used is Literal
            integer = Word(nums)
            date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

            date_str.parse_string("1999/12/31")  # -> ['1999', '/', '12', '/', '31']


            # change to Suppress
            ParserElement.inline_literals_using(Suppress)
            date_str = integer("year") + '/' + integer("month") + '/' + integer("day")

            date_str.parse_string("1999/12/31")  # -> ['1999', '12', '31']
        """
        ParserElement._literalStringClass = cls

    @classmethod
    def using_each(cls, seq, **class_kwargs):
        """
        Yields a sequence of class(obj, **class_kwargs) for obj in seq.

        Example::

            LPAR, RPAR, LBRACE, RBRACE, SEMI = Suppress.using_each("(){};")

        """
        yield from (cls(obj, **class_kwargs) for obj in seq)

    class DebugActions(NamedTuple):
        debug_try: typing.Optional[DebugStartAction]
        debug_match: typing.Optional[DebugSuccessAction]
        debug_fail: typing.Optional[DebugExceptionAction]

    def __init__(self, savelist: bool = False):
        self.parseAction: List[ParseAction] = list()
        self.failAction: typing.Optional[ParseFailAction] = None
        self.customName: str = None  # type: ignore[assignment]
        self._defaultName: typing.Optional[str] = None
        self.resultsName: str = None  # type: ignore[assignment]
        self.saveAsList = savelist
        self.skipWhitespace = True
        self.whiteChars = set(ParserElement.DEFAULT_WHITE_CHARS)
        self.copyDefaultWhiteChars = True
        # used when checking for left-recursion
        self.mayReturnEmpty = False
        self.keepTabs = False
        self.ignoreExprs: List["ParserElement"] = list()
        self.debug = False
        self.streamlined = False
        # optimize exception handling for subclasses that don't advance parse index
        self.mayIndexError = True
        self.errmsg = ""
        # mark results names as modal (report only last) or cumulative (list all)
        self.modalResults = True
        # custom debug actions
        self.debugActions = self.DebugActions(None, None, None)
        # avoid redundant calls to preParse
        self.callPreparse = True
        self.callDuringTry = False
        self.suppress_warnings_: List[Diagnostics] = []

    def suppress_warning(self, warning_type: Diagnostics) -> "ParserElement":
        """
        Suppress warnings emitted for a particular diagnostic on this expression.

        Example::

            base = pp.Forward()
            base.suppress_warning(Diagnostics.warn_on_parse_using_empty_Forward)

            # statement would normally raise a warning, but is now suppressed
            print(base.parse_string("x"))

        """
        self.suppress_warnings_.append(warning_type)
        return self

    def visit_all(self):
        """General-purpose method to yield all expressions and sub-expressions
        in a grammar. Typically just for internal use.
        """
        to_visit = deque([self])
        seen = set()
        while to_visit:
            cur = to_visit.popleft()

            # guard against looping forever through recursive grammars
            if cur in seen:
                continue
            seen.add(cur)

            to_visit.extend(cur.recurse())
            yield cur

    def copy(self) -> "ParserElement":
        """
        Make a copy of this :class:`ParserElement`.  Useful for defining
        different parse actions for the same parsing pattern, using copies of
        the original parse element.

        Example::

            integer = Word(nums).set_parse_action(lambda toks: int(toks[0]))
            integerK = integer.copy().add_parse_action(lambda toks: toks[0] * 1024) + Suppress("K")
            integerM = integer.copy().add_parse_action(lambda toks: toks[0] * 1024 * 1024) + Suppress("M")

            print((integerK | integerM | integer)[1, ...].parse_string("5K 100 640K 256M"))

        prints::

            [5120, 100, 655360, 268435456]

        Equivalent form of ``expr.copy()`` is just ``expr()``::

            integerM = integer().add_parse_action(lambda toks: toks[0] * 1024 * 1024) + Suppress("M")
        """
        cpy = copy.copy(self)
        cpy.parseAction = self.parseAction[:]
        cpy.ignoreExprs = self.ignoreExprs[:]
        if self.copyDefaultWhiteChars:
            cpy.whiteChars = set(ParserElement.DEFAULT_WHITE_CHARS)
        return cpy

    def set_results_name(
        self, name: str, list_all_matches: bool = False, *, listAllMatches: bool = False
    ) -> "ParserElement":
        """
        Define name for referencing matching tokens as a nested attribute
        of the returned parse results.

        Normally, results names are assigned as you would assign keys in a dict:
        any existing value is overwritten by later values. If it is necessary to
        keep all values captured for a particular results name, call ``set_results_name``
        with ``list_all_matches`` = True.

        NOTE: ``set_results_name`` returns a *copy* of the original :class:`ParserElement` object;
        this is so that the client can define a basic element, such as an
        integer, and reference it in multiple places with different names.

        You can also set results names using the abbreviated syntax,
        ``expr("name")`` in place of ``expr.set_results_name("name")``
        - see :class:`__call__`. If ``list_all_matches`` is required, use
        ``expr("name*")``.

        Example::

            date_str = (integer.set_results_name("year") + '/'
                        + integer.set_results_name("month") + '/'
                        + integer.set_results_name("day"))

            # equivalent form:
            date_str = integer("year") + '/' + integer("month") + '/' + integer("day")
        """
        listAllMatches = listAllMatches or list_all_matches
        return self._setResultsName(name, listAllMatches)

    def _setResultsName(self, name, listAllMatches=False):
        if name is None:
            return self
        newself = self.copy()
        if name.endswith("*"):
            name = name[:-1]
            listAllMatches = True
        newself.resultsName = name
        newself.modalResults = not listAllMatches
        return newself

    def set_break(self, break_flag: bool = True) -> "ParserElement":
        """
        Method to invoke the Python pdb debugger when this element is
        about to be parsed. Set ``break_flag`` to ``True`` to enable, ``False`` to
        disable.
        """
        if break_flag:
            _parseMethod = self._parse

            def breaker(instring, loc, doActions=True, callPreParse=True):
                import pdb

                # this call to pdb.set_trace() is intentional, not a checkin error
                pdb.set_trace()
                return _parseMethod(instring, loc, doActions, callPreParse)

            breaker._originalParseMethod = _parseMethod  # type: ignore [attr-defined]
            self._parse = breaker  # type: ignore [assignment]
        else:
            if hasattr(self._parse, "_originalParseMethod"):
                self._parse = self._parse._originalParseMethod  # type: ignore [attr-defined, assignment]
        return self

    def set_parse_action(self, *fns: ParseAction, **kwargs) -> "ParserElement":
        """
        Define one or more actions to perform when successfully matching parse element definition.

        Parse actions can be called to perform data conversions, do extra validation,
        update external data structures, or enhance or replace the parsed tokens.
        Each parse action ``fn`` is a callable method with 0-3 arguments, called as
        ``fn(s, loc, toks)`` , ``fn(loc, toks)`` , ``fn(toks)`` , or just ``fn()`` , where:

        - ``s``    = the original string being parsed (see note below)
        - ``loc``  = the location of the matching substring
        - ``toks`` = a list of the matched tokens, packaged as a :class:`ParseResults` object

        The parsed tokens are passed to the parse action as ParseResults. They can be
        modified in place using list-style append, extend, and pop operations to update
        the parsed list elements; and with dictionary-style item set and del operations
        to add, update, or remove any named results. If the tokens are modified in place,
        it is not necessary to return them with a return statement.

        Parse actions can also completely replace the given tokens, with another ``ParseResults``
        object, or with some entirely different object (common for parse actions that perform data
        conversions). A convenient way to build a new parse result is to define the values
        using a dict, and then create the return value using :class:`ParseResults.from_dict`.

        If None is passed as the ``fn`` parse action, all previously added parse actions for this
        expression are cleared.

        Optional keyword arguments:

        - ``call_during_try`` = (default= ``False``) indicate if parse action should be run during
          lookaheads and alternate testing. For parse actions that have side effects, it is
          important to only call the parse action once it is determined that it is being
          called as part of a successful parse. For parse actions that perform additional
          validation, then call_during_try should be passed as True, so that the validation
          code is included in the preliminary "try" parses.

        Note: the default parsing behavior is to expand tabs in the input string
        before starting the parsing process.  See :class:`parse_string` for more
        information on parsing strings containing ``<TAB>`` s, and suggested
        methods to maintain a consistent view of the parsed string, the parse
        location, and line and column positions within the parsed string.

        Example::

            # parse dates in the form YYYY/MM/DD

            # use parse action to convert toks from str to int at parse time
            def convert_to_int(toks):
                return int(toks[0])

            # use a parse action to verify that the date is a valid date
            def is_valid_date(instring, loc, toks):
                from datetime import date
                year, month, day = toks[::2]
                try:
                    date(year, month, day)
                except ValueError:
                    raise ParseException(instring, loc, "invalid date given")

            integer = Word(nums)
            date_str = integer + '/' + integer + '/' + integer

            # add parse actions
            integer.set_parse_action(convert_to_int)
            date_str.set_parse_action(is_valid_date)

            # note that integer fields are now ints, not strings
            date_str.run_tests('''
                # successful parse - note that integer fields were converted to ints
                1999/12/31

                # fail - invalid date
                1999/13/31
                ''')
        """
        if list(fns) == [None]:
            self.parseAction = []
        else:
            if not all(callable(fn) for fn in fns):
                raise TypeError("parse actions must be callable")
            self.parseAction = [_trim_arity(fn) for fn in fns]
            self.callDuringTry = kwargs.get(
                "call_during_try", kwargs.get("callDuringTry", False)
            )
        return self

    def add_parse_action(self, *fns: ParseAction, **kwargs) -> "ParserElement":
        """
        Add one or more parse actions to expression's list of parse actions. See :class:`set_parse_action`.

        See examples in :class:`copy`.
        """
        self.parseAction += [_trim_arity(fn) for fn in fns]
        self.callDuringTry = self.callDuringTry or kwargs.get(
            "call_during_try", kwargs.get("callDuringTry", False)
        )
        return self

    def add_condition(self, *fns: ParseCondition, **kwargs) -> "ParserElement":
        """Add a boolean predicate function to expression's list of parse actions. See
        :class:`set_parse_action` for function call signatures. Unlike ``set_parse_action``,
        functions passed to ``add_condition`` need to return boolean success/fail of the condition.

        Optional keyword arguments:

        - ``message`` = define a custom message to be used in the raised exception
        - ``fatal`` = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise
          ParseException
        - ``call_during_try`` = boolean to indicate if this method should be called during internal tryParse calls,
          default=False

        Example::

            integer = Word(nums).set_parse_action(lambda toks: int(toks[0]))
            year_int = integer.copy()
            year_int.add_condition(lambda toks: toks[0] >= 2000, message="Only support years 2000 and later")
            date_str = year_int + '/' + integer + '/' + integer

            result = date_str.parse_string("1999/12/31")  # -> Exception: Only support years 2000 and later (at char 0),
                                                                         (line:1, col:1)
        """
        for fn in fns:
            self.parseAction.append(
                condition_as_parse_action(
                    fn,
                    message=str(kwargs.get("message")),
                    fatal=bool(kwargs.get("fatal", False)),
                )
            )

        self.callDuringTry = self.callDuringTry or kwargs.get(
            "call_during_try", kwargs.get("callDuringTry", False)
        )
        return self

    def set_fail_action(self, fn: ParseFailAction) -> "ParserElement":
        """
        Define action to perform if parsing fails at this expression.
        Fail acton fn is a callable function that takes the arguments
        ``fn(s, loc, expr, err)`` where:

        - ``s`` = string being parsed
        - ``loc`` = location where expression match was attempted and failed
        - ``expr`` = the parse expression that failed
        - ``err`` = the exception thrown

        The function returns no value.  It may throw :class:`ParseFatalException`
        if it is desired to stop parsing immediately."""
        self.failAction = fn
        return self

    def _skipIgnorables(self, instring: str, loc: int) -> int:
        if not self.ignoreExprs:
            return loc
        exprsFound = True
        ignore_expr_fns = [e._parse for e in self.ignoreExprs]
        while exprsFound:
            exprsFound = False
            for ignore_fn in ignore_expr_fns:
                try:
                    while 1:
                        loc, dummy = ignore_fn(instring, loc)
                        exprsFound = True
                except ParseException:
                    pass
        return loc

    def preParse(self, instring: str, loc: int) -> int:
        if self.ignoreExprs:
            loc = self._skipIgnorables(instring, loc)

        if self.skipWhitespace:
            instrlen = len(instring)
            white_chars = self.whiteChars
            while loc < instrlen and instring[loc] in white_chars:
                loc += 1

        return loc

    def parseImpl(self, instring, loc, doActions=True):
        return loc, []

    def postParse(self, instring, loc, tokenlist):
        return tokenlist

    # @profile
    def _parseNoCache(
        self, instring, loc, doActions=True, callPreParse=True
    ) -> Tuple[int, ParseResults]:
        TRY, MATCH, FAIL = 0, 1, 2
        debugging = self.debug  # and doActions)
        len_instring = len(instring)

        if debugging or self.failAction:
            # print("Match {} at loc {}({}, {})".format(self, loc, lineno(loc, instring), col(loc, instring)))
            try:
                if callPreParse and self.callPreparse:
                    pre_loc = self.preParse(instring, loc)
                else:
                    pre_loc = loc
                tokens_start = pre_loc
                if self.debugActions.debug_try:
                    self.debugActions.debug_try(instring, tokens_start, self, False)
                if self.mayIndexError or pre_loc >= len_instring:
                    try:
                        loc, tokens = self.parseImpl(instring, pre_loc, doActions)
                    except IndexError:
                        raise ParseException(instring, len_instring, self.errmsg, self)
                else:
                    loc, tokens = self.parseImpl(instring, pre_loc, doActions)
            except Exception as err:
                # print("Exception raised:", err)
                if self.debugActions.debug_fail:
                    self.debugActions.debug_fail(
                        instring, tokens_start, self, err, False
                    )
                if self.failAction:
                    self.failAction(instring, tokens_start, self, err)
                raise
        else:
            if callPreParse and self.callPreparse:
                pre_loc = self.preParse(instring, loc)
            else:
                pre_loc = loc
            tokens_start = pre_loc
            if self.mayIndexError or pre_loc >= len_instring:
                try:
                    loc, tokens = self.parseImpl(instring, pre_loc, doActions)
                except IndexError:
                    raise ParseException(instring, len_instring, self.errmsg, self)
            else:
                loc, tokens = self.parseImpl(instring, pre_loc, doActions)

        tokens = self.postParse(instring, loc, tokens)

        ret_tokens = ParseResults(
            tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults
        )
        if self.parseAction and (doActions or self.callDuringTry):
            if debugging:
                try:
                    for fn in self.parseAction:
                        try:
                            tokens = fn(instring, tokens_start, ret_tokens)  # type: ignore [call-arg, arg-type]
                        except IndexError as parse_action_exc:
                            exc = ParseException("exception raised in parse action")
                            raise exc from parse_action_exc

                        if tokens is not None and tokens is not ret_tokens:
                            ret_tokens = ParseResults(
                                tokens,
                                self.resultsName,
                                asList=self.saveAsList
                                and isinstance(tokens, (ParseResults, list)),
                                modal=self.modalResults,
                            )
                except Exception as err:
                    # print "Exception raised in user parse action:", err
                    if self.debugActions.debug_fail:
                        self.debugActions.debug_fail(
                            instring, tokens_start, self, err, False
                        )
                    raise
            else:
                for fn in self.parseAction:
                    try:
                        tokens = fn(instring, tokens_start, ret_tokens)  # type: ignore [call-arg, arg-type]
                    except IndexError as parse_action_exc:
                        exc = ParseException("exception raised in parse action")
                        raise exc from parse_action_exc

                    if tokens is not None and tokens is not ret_tokens:
                        ret_tokens = ParseResults(
                            tokens,
                            self.resultsName,
                            asList=self.saveAsList
                            and isinstance(tokens, (ParseResults, list)),
                            modal=self.modalResults,
                        )
        if debugging:
            # print("Matched", self, "->", ret_tokens.as_list())
            if self.debugActions.debug_match:
                self.debugActions.debug_match(
                    instring, tokens_start, loc, self, ret_tokens, False
                )

        return loc, ret_tokens

    def try_parse(
        self,
        instring: str,
        loc: int,
        *,
        raise_fatal: bool = False,
        do_actions: bool = False,
    ) -> int:
        try:
            return self._parse(instring, loc, doActions=do_actions)[0]
        except ParseFatalException:
            if raise_fatal:
                raise
            raise ParseException(instring, loc, self.errmsg, self)

    def can_parse_next(self, instring: str, loc: int, do_actions: bool = False) -> bool:
        try:
            self.try_parse(instring, loc, do_actions=do_actions)
        except (ParseException, IndexError):
            return False
        else:
            return True

    # cache for left-recursion in Forward references
    recursion_lock = RLock()
    recursion_memos: typing.Dict[
        Tuple[int, "Forward", bool], Tuple[int, Union[ParseResults, Exception]]
    ] = {}

    class _CacheType(dict):
        """
        class to help type checking
        """

        not_in_cache: bool

        def get(self, *args):
            ...

        def set(self, *args):
            ...

    # argument cache for optimizing repeated calls when backtracking through recursive expressions
    packrat_cache = (
        _CacheType()
    )  # set later by enable_packrat(); this is here so that reset_cache() doesn't fail
    packrat_cache_lock = RLock()
    packrat_cache_stats = [0, 0]

    # this method gets repeatedly called during backtracking with the same arguments -
    # we can cache these arguments and save ourselves the trouble of re-parsing the contained expression
    def _parseCache(
        self, instring, loc, doActions=True, callPreParse=True
    ) -> Tuple[int, ParseResults]:
        HIT, MISS = 0, 1
        TRY, MATCH, FAIL = 0, 1, 2
        lookup = (self, instring, loc, callPreParse, doActions)
        with ParserElement.packrat_cache_lock:
            cache = ParserElement.packrat_cache
            value = cache.get(lookup)
            if value is cache.not_in_cache:
                ParserElement.packrat_cache_stats[MISS] += 1
                try:
                    value = self._parseNoCache(instring, loc, doActions, callPreParse)
                except ParseBaseException as pe:
                    # cache a copy of the exception, without the traceback
                    cache.set(lookup, pe.__class__(*pe.args))
                    raise
                else:
                    cache.set(lookup, (value[0], value[1].copy(), loc))
                    return value
            else:
                ParserElement.packrat_cache_stats[HIT] += 1
                if self.debug and self.debugActions.debug_try:
                    try:
                        self.debugActions.debug_try(instring, loc, self, cache_hit=True)  # type: ignore [call-arg]
                    except TypeError:
                        pass
                if isinstance(value, Exception):
                    if self.debug and self.debugActions.debug_fail:
                        try:
                            self.debugActions.debug_fail(
                                instring, loc, self, value, cache_hit=True  # type: ignore [call-arg]
                            )
                        except TypeError:
                            pass
                    raise value

                value = cast(Tuple[int, ParseResults, int], value)
                loc_, result, endloc = value[0], value[1].copy(), value[2]
                if self.debug and self.debugActions.debug_match:
                    try:
                        self.debugActions.debug_match(
                            instring, loc_, endloc, self, result, cache_hit=True  # type: ignore [call-arg]
                        )
                    except TypeError:
                        pass

                return loc_, result

    _parse = _parseNoCache

    @staticmethod
    def reset_cache() -> None:
        ParserElement.packrat_cache.clear()
        ParserElement.packrat_cache_stats[:] = [0] * len(
            ParserElement.packrat_cache_stats
        )
        ParserElement.recursion_memos.clear()

    _packratEnabled = False
    _left_recursion_enabled = False

    @staticmethod
    def disable_memoization() -> None:
        """
        Disables active Packrat or Left Recursion parsing and their memoization

        This method also works if neither Packrat nor Left Recursion are enabled.
        This makes it safe to call before activating Packrat nor Left Recursion
        to clear any previous settings.
        """
        ParserElement.reset_cache()
        ParserElement._left_recursion_enabled = False
        ParserElement._packratEnabled = False
        ParserElement._parse = ParserElement._parseNoCache

    @staticmethod
    def enable_left_recursion(
        cache_size_limit: typing.Optional[int] = None, *, force=False
    ) -> None:
        """
        Enables "bounded recursion" parsing, which allows for both direct and indirect
        left-recursion. During parsing, left-recursive :class:`Forward` elements are
        repeatedly matched with a fixed recursion depth that is gradually increased
        until finding the longest match.

        Example::

            from pip._vendor import pyparsing as pp
            pp.ParserElement.enable_left_recursion()

            E = pp.Forward("E")
            num = pp.Word(pp.nums)
            # match `num`, or `num '+' num`, or `num '+' num '+' num`, ...
            E <<= E + '+' - num | num

            print(E.parse_string("1+2+3"))

        Recursion search naturally memoizes matches of ``Forward`` elements and may
        thus skip reevaluation of parse actions during backtracking. This may break
        programs with parse actions which rely on strict ordering of side-effects.

        Parameters:

        - ``cache_size_limit`` - (default=``None``) - memoize at most this many
          ``Forward`` elements during matching; if ``None`` (the default),
          memoize all ``Forward`` elements.

        Bounded Recursion parsing works similar but not identical to Packrat parsing,
        thus the two cannot be used together. Use ``force=True`` to disable any
        previous, conflicting settings.
        """
        if force:
            ParserElement.disable_memoization()
        elif ParserElement._packratEnabled:
            raise RuntimeError("Packrat and Bounded Recursion are not compatible")
        if cache_size_limit is None:
            ParserElement.recursion_memos = _UnboundedMemo()  # type: ignore[assignment]
        elif cache_size_limit > 0:
            ParserElement.recursion_memos = _LRUMemo(capacity=cache_size_limit)  # type: ignore[assignment]
        else:
            raise NotImplementedError("Memo size of %s" % cache_size_limit)
        ParserElement._left_recursion_enabled = True

    @staticmethod
    def enable_packrat(cache_size_limit: int = 128, *, force: bool = False) -> None:
        """
        Enables "packrat" parsing, which adds memoizing to the parsing logic.
        Repeated parse attempts at the same string location (which happens
        often in many complex grammars) can immediately return a cached value,
        instead of re-executing parsing/validating code.  Memoizing is done of
        both valid results and parsing exceptions.

        Parameters:

        - ``cache_size_limit`` - (default= ``128``) - if an integer value is provided
          will limit the size of the packrat cache; if None is passed, then
          the cache size will be unbounded; if 0 is passed, the cache will
          be effectively disabled.

        This speedup may break existing programs that use parse actions that
        have side-effects.  For this reason, packrat parsing is disabled when
        you first import pyparsing.  To activate the packrat feature, your
        program must call the class method :class:`ParserElement.enable_packrat`.
        For best results, call ``enable_packrat()`` immediately after
        importing pyparsing.

        Example::

            from pip._vendor import pyparsing
            pyparsing.ParserElement.enable_packrat()

        Packrat parsing works similar but not identical to Bounded Recursion parsing,
        thus the two cannot be used together. Use ``force=True`` to disable any
        previous, conflicting settings.
        """
        if force:
            ParserElement.disable_memoization()
        elif ParserElement._left_recursion_enabled:
            raise RuntimeError("Packrat and Bounded Recursion are not compatible")
        if not ParserElement._packratEnabled:
            ParserElement._packratEnabled = True
            if cache_size_limit is None:
                ParserElement.packrat_cache = _UnboundedCache()
            else:
                ParserElement.packrat_cache = _FifoCache(cache_size_limit)  # type: ignore[assignment]
            ParserElement._parse = ParserElement._parseCache

    def parse_string(
        self, instring: str, parse_all: bool = False, *, parseAll: bool = False
    ) -> ParseResults:
        """
        Parse a string with respect to the parser definition. This function is intended as the primary interface to the
        client code.

        :param instring: The input string to be parsed.
        :param parse_all: If set, the entire input string must match the grammar.
        :param parseAll: retained for pre-PEP8 compatibility, will be removed in a future release.
        :raises ParseException: Raised if ``parse_all`` is set and the input string does not match the whole grammar.
        :returns: the parsed data as a :class:`ParseResults` object, which may be accessed as a `list`, a `dict`, or
          an object with attributes if the given parser includes results names.

        If the input string is required to match the entire grammar, ``parse_all`` flag must be set to ``True``. This
        is also equivalent to ending the grammar with :class:`StringEnd`\\ ().

        To report proper column numbers, ``parse_string`` operates on a copy of the input string where all tabs are
        converted to spaces (8 spaces per tab, as per the default in ``string.expandtabs``). If the input string
        contains tabs and the grammar uses parse actions that use the ``loc`` argument to index into the string
        being parsed, one can ensure a consistent view of the input string by doing one of the following:

        - calling ``parse_with_tabs`` on your grammar before calling ``parse_string`` (see :class:`parse_with_tabs`),
        - define your parse action using the full ``(s,loc,toks)`` signature, and reference the input string using the
          parse action's ``s`` argument, or
        - explicitly expand the tabs in your input string before calling ``parse_string``.

        Examples:

        By default, partial matches are OK.

        >>> res = Word('a').parse_string('aaaaabaaa')
        >>> print(res)
        ['aaaaa']

        The parsing behavior varies by the inheriting class of this abstract class. Please refer to the children
        directly to see more examples.

        It raises an exception if parse_all flag is set and instring does not match the whole grammar.

        >>> res = Word('a').parse_string('aaaaabaaa', parse_all=True)
        Traceback (most recent call last):
        ...
        pyparsing.ParseException: Expected end of text, found 'b'  (at char 5), (line:1, col:6)
        """
        parseAll = parse_all or parseAll

        ParserElement.reset_cache()
        if not self.streamlined:
            self.streamline()
        for e in self.ignoreExprs:
            e.streamline()
        if not self.keepTabs:
            instring = instring.expandtabs()
        try:
            loc, tokens = self._parse(instring, 0)
            if parseAll:
                loc = self.preParse(instring, loc)
                se = Empty() + StringEnd()
                se._parse(instring, loc)
        except ParseBaseException as exc:
            if ParserElement.verbose_stacktrace:
                raise
            else:
                # catch and re-raise exception from here, clearing out pyparsing internal stack trace
                raise exc.with_traceback(None)
        else:
            return tokens

    def scan_string(
        self,
        instring: str,
        max_matches: int = _MAX_INT,
        overlap: bool = False,
        *,
        debug: bool = False,
        maxMatches: int = _MAX_INT,
    ) -> Generator[Tuple[ParseResults, int, int], None, None]:
        """
        Scan the input string for expression matches.  Each match will return the
        matching tokens, start location, and end location.  May be called with optional
        ``max_matches`` argument, to clip scanning after 'n' matches are found.  If
        ``overlap`` is specified, then overlapping matches will be reported.

        Note that the start and end locations are reported relative to the string
        being parsed.  See :class:`parse_string` for more information on parsing
        strings with embedded tabs.

        Example::

            source = "sldjf123lsdjjkf345sldkjf879lkjsfd987"
            print(source)
            for tokens, start, end in Word(alphas).scan_string(source):
                print(' '*start + '^'*(end-start))
                print(' '*start + tokens[0])

        prints::

            sldjf123lsdjjkf345sldkjf879lkjsfd987
            ^^^^^
            sldjf
                    ^^^^^^^
                    lsdjjkf
                              ^^^^^^
                              sldkjf
                                       ^^^^^^
                                       lkjsfd
        """
        maxMatches = min(maxMatches, max_matches)
        if not self.streamlined:
            self.streamline()
        for e in self.ignoreExprs:
            e.streamline()

        if not self.keepTabs:
            instring = str(instring).expandtabs()
        instrlen = len(instring)
        loc = 0
        preparseFn = self.preParse
        parseFn = self._parse
        ParserElement.resetCache()
        matches = 0
        try:
            while loc <= instrlen and matches < maxMatches:
                try:
                    preloc: int = preparseFn(instring, loc)
                    nextLoc: int
                    tokens: ParseResults
                    nextLoc, tokens = parseFn(instring, preloc, callPreParse=False)
                except ParseException:
                    loc = preloc + 1
                else:
                    if nextLoc > loc:
                        matches += 1
                        if debug:
                            print(
                                {
                                    "tokens": tokens.asList(),
                                    "start": preloc,
                                    "end": nextLoc,
                                }
                            )
                        yield tokens, preloc, nextLoc
                        if overlap:
                            nextloc = preparseFn(instring, loc)
                            if nextloc > loc:
                                loc = nextLoc
                            else:
                                loc += 1
                        else:
                            loc = nextLoc
                    else:
                        loc = preloc + 1
        except ParseBaseException as exc:
            if ParserElement.verbose_stacktrace:
                raise
            else:
                # catch and re-raise exception from here, clears out pyparsing internal stack trace
                raise exc.with_traceback(None)

    def transform_string(self, instring: str, *, debug: bool = False) -> str:
        """
        Extension to :class:`scan_string`, to modify matching text with modified tokens that may
        be returned from a parse action.  To use ``transform_string``, define a grammar and
        attach a parse action to it that modifies the returned token list.
        Invoking ``transform_string()`` on a target string will then scan for matches,
        and replace the matched text patterns according to the logic in the parse
        action.  ``transform_string()`` returns the resulting transformed string.

        Example::

            wd = Word(alphas)
            wd.set_parse_action(lambda toks: toks[0].title())

            print(wd.transform_string("now is the winter of our discontent made glorious summer by this sun of york."))

        prints::

            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.
        """
        out: List[str] = []
        lastE = 0
        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to
        # keep string locs straight between transform_string and scan_string
        self.keepTabs = True
        try:
            for t, s, e in self.scan_string(instring, debug=debug):
                out.append(instring[lastE:s])
                if t:
                    if isinstance(t, ParseResults):
                        out += t.as_list()
                    elif isinstance(t, Iterable) and not isinstance(t, str_type):
                        out.extend(t)
                    else:
                        out.append(t)
                lastE = e
            out.append(instring[lastE:])
            out = [o for o in out if o]
            return "".join([str(s) for s in _flatten(out)])
        except ParseBaseException as exc:
            if ParserElement.verbose_stacktrace:
                raise
            else:
                # catch and re-raise exception from here, clears out pyparsing internal stack trace
                raise exc.with_traceback(None)

    def search_string(
        self,
        instring: str,
        max_matches: int = _MAX_INT,
        *,
        debug: bool = False,
        maxMatches: int = _MAX_INT,
    ) -> ParseResults:
        """
        Another extension to :class:`scan_string`, simplifying the access to the tokens found
        to match the given parse expression.  May be called with optional
        ``max_matches`` argument, to clip searching after 'n' matches are found.

        Example::

            # a capitalized word starts with an uppercase letter, followed by zero or more lowercase letters
            cap_word = Word(alphas.upper(), alphas.lower())

            print(cap_word.search_string("More than Iron, more than Lead, more than Gold I need Electricity"))

            # the sum() builtin can be used to merge results into a single ParseResults object
            print(sum(cap_word.search_string("More than Iron, more than Lead, more than Gold I need Electricity")))

        prints::

            [['More'], ['Iron'], ['Lead'], ['Gold'], ['I'], ['Electricity']]
            ['More', 'Iron', 'Lead', 'Gold', 'I', 'Electricity']
        """
        maxMatches = min(maxMatches, max_matches)
        try:
            return ParseResults(
                [t for t, s, e in self.scan_string(instring, maxMatches, debug=debug)]
            )
        except ParseBaseException as exc:
            if ParserElement.verbose_stacktrace:
                raise
            else:
                # catch and re-raise exception from here, clears out pyparsing internal stack trace
                raise exc.with_traceback(None)

    def split(
        self,
        instring: str,
        maxsplit: int = _MAX_INT,
        include_separators: bool = False,
        *,
        includeSeparators=False,
    ) -> Generator[str, None, None]:
        """
        Generator method to split a string using the given expression as a separator.
        May be called with optional ``maxsplit`` argument, to limit the number of splits;
        and the optional ``include_separators`` argument (default= ``False``), if the separating
        matching text should be included in the split results.

        Example::

            punc = one_of(list(".,;:/-!?"))
            print(list(punc.split("This, this?, this sentence, is badly punctuated!")))

        prints::

            ['This', ' this', '', ' this sentence', ' is badly punctuated', '']
        """
        includeSeparators = includeSeparators or include_separators
        last = 0
        for t, s, e in self.scan_string(instring, max_matches=maxsplit):
            yield instring[last:s]
            if includeSeparators:
                yield t[0]
            last = e
        yield instring[last:]

    def __add__(self, other) -> "ParserElement":
        """
        Implementation of ``+`` operator - returns :class:`And`. Adding strings to a :class:`ParserElement`
        converts them to :class:`Literal`\\ s by default.

        Example::

            greet = Word(alphas) + "," + Word(alphas) + "!"
            hello = "Hello, World!"
            print(hello, "->", greet.parse_string(hello))

        prints::

            Hello, World! -> ['Hello', ',', 'World', '!']

        ``...`` may be used as a parse expression as a short form of :class:`SkipTo`::

            Literal('start') + ... + Literal('end')

        is equivalent to::

            Literal('start') + SkipTo('end')("_skipped*") + Literal('end')

        Note that the skipped text is returned with '_skipped' as a results name,
        and to support having multiple skips in the same parser, the value returned is
        a list of all skipped text.
        """
        if other is Ellipsis:
            return _PendingSkip(self)

        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return And([self, other])

    def __radd__(self, other) -> "ParserElement":
        """
        Implementation of ``+`` operator when left operand is not a :class:`ParserElement`
        """
        if other is Ellipsis:
            return SkipTo(self)("_skipped*") + self

        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return other + self

    def __sub__(self, other) -> "ParserElement":
        """
        Implementation of ``-`` operator, returns :class:`And` with error stop
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return self + And._ErrorStop() + other

    def __rsub__(self, other) -> "ParserElement":
        """
        Implementation of ``-`` operator when left operand is not a :class:`ParserElement`
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return other - self

    def __mul__(self, other) -> "ParserElement":
        """
        Implementation of ``*`` operator, allows use of ``expr * 3`` in place of
        ``expr + expr + expr``.  Expressions may also be multiplied by a 2-integer
        tuple, similar to ``{min, max}`` multipliers in regular expressions.  Tuples
        may also include ``None`` as in:

        - ``expr*(n, None)`` or ``expr*(n, )`` is equivalent
          to ``expr*n + ZeroOrMore(expr)``
          (read as "at least n instances of ``expr``")
        - ``expr*(None, n)`` is equivalent to ``expr*(0, n)``
          (read as "0 to n instances of ``expr``")
        - ``expr*(None, None)`` is equivalent to ``ZeroOrMore(expr)``
        - ``expr*(1, None)`` is equivalent to ``OneOrMore(expr)``

        Note that ``expr*(None, n)`` does not raise an exception if
        more than n exprs exist in the input stream; that is,
        ``expr*(None, n)`` does not enforce a maximum number of expr
        occurrences.  If this behavior is desired, then write
        ``expr*(None, n) + ~expr``
        """
        if other is Ellipsis:
            other = (0, None)
        elif isinstance(other, tuple) and other[:1] == (Ellipsis,):
            other = ((0,) + other[1:] + (None,))[:2]

        if isinstance(other, int):
            minElements, optElements = other, 0
        elif isinstance(other, tuple):
            other = tuple(o if o is not Ellipsis else None for o in other)
            other = (other + (None, None))[:2]
            if other[0] is None:
                other = (0, other[1])
            if isinstance(other[0], int) and other[1] is None:
                if other[0] == 0:
                    return ZeroOrMore(self)
                if other[0] == 1:
                    return OneOrMore(self)
                else:
                    return self * other[0] + ZeroOrMore(self)
            elif isinstance(other[0], int) and isinstance(other[1], int):
                minElements, optElements = other
                optElements -= minElements
            else:
                return NotImplemented
        else:
            return NotImplemented

        if minElements < 0:
            raise ValueError("cannot multiply ParserElement by negative value")
        if optElements < 0:
            raise ValueError(
                "second tuple value must be greater or equal to first tuple value"
            )
        if minElements == optElements == 0:
            return And([])

        if optElements:

            def makeOptionalList(n):
                if n > 1:
                    return Opt(self + makeOptionalList(n - 1))
                else:
                    return Opt(self)

            if minElements:
                if minElements == 1:
                    ret = self + makeOptionalList(optElements)
                else:
                    ret = And([self] * minElements) + makeOptionalList(optElements)
            else:
                ret = makeOptionalList(optElements)
        else:
            if minElements == 1:
                ret = self
            else:
                ret = And([self] * minElements)
        return ret

    def __rmul__(self, other) -> "ParserElement":
        return self.__mul__(other)

    def __or__(self, other) -> "ParserElement":
        """
        Implementation of ``|`` operator - returns :class:`MatchFirst`
        """
        if other is Ellipsis:
            return _PendingSkip(self, must_skip=True)

        if isinstance(other, str_type):
            # `expr | ""` is equivalent to `Opt(expr)`
            if other == "":
                return Opt(self)
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return MatchFirst([self, other])

    def __ror__(self, other) -> "ParserElement":
        """
        Implementation of ``|`` operator when left operand is not a :class:`ParserElement`
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return other | self

    def __xor__(self, other) -> "ParserElement":
        """
        Implementation of ``^`` operator - returns :class:`Or`
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return Or([self, other])

    def __rxor__(self, other) -> "ParserElement":
        """
        Implementation of ``^`` operator when left operand is not a :class:`ParserElement`
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return other ^ self

    def __and__(self, other) -> "ParserElement":
        """
        Implementation of ``&`` operator - returns :class:`Each`
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return Each([self, other])

    def __rand__(self, other) -> "ParserElement":
        """
        Implementation of ``&`` operator when left operand is not a :class:`ParserElement`
        """
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return other & self

    def __invert__(self) -> "ParserElement":
        """
        Implementation of ``~`` operator - returns :class:`NotAny`
        """
        return NotAny(self)

    # disable __iter__ to override legacy use of sequential access to __getitem__ to
    # iterate over a sequence
    __iter__ = None

    def __getitem__(self, key):
        """
        use ``[]`` indexing notation as a short form for expression repetition:

        - ``expr[n]`` is equivalent to ``expr*n``
        - ``expr[m, n]`` is equivalent to ``expr*(m, n)``
        - ``expr[n, ...]`` or ``expr[n,]`` is equivalent
             to ``expr*n + ZeroOrMore(expr)``
             (read as "at least n instances of ``expr``")
        - ``expr[..., n]`` is equivalent to ``expr*(0, n)``
             (read as "0 to n instances of ``expr``")
        - ``expr[...]`` and ``expr[0, ...]`` are equivalent to ``ZeroOrMore(expr)``
        - ``expr[1, ...]`` is equivalent to ``OneOrMore(expr)``

        ``None`` may be used in place of ``...``.

        Note that ``expr[..., n]`` and ``expr[m, n]`` do not raise an exception
        if more than ``n`` ``expr``\\ s exist in the input stream.  If this behavior is
        desired, then write ``expr[..., n] + ~expr``.

        For repetition with a stop_on expression, use slice notation:

        - ``expr[...: end_expr]`` and ``expr[0, ...: end_expr]`` are equivalent to ``ZeroOrMore(expr, stop_on=end_expr)``
        - ``expr[1, ...: end_expr]`` is equivalent to ``OneOrMore(expr, stop_on=end_expr)``

        """

        stop_on_defined = False
        stop_on = NoMatch()
        if isinstance(key, slice):
            key, stop_on = key.start, key.stop
            if key is None:
                key = ...
            stop_on_defined = True
        elif isinstance(key, tuple) and isinstance(key[-1], slice):
            key, stop_on = (key[0], key[1].start), key[1].stop
            stop_on_defined = True

        # convert single arg keys to tuples
        if isinstance(key, str_type):
            key = (key,)
        try:
            iter(key)
        except TypeError:
            key = (key, key)

        if len(key) > 2:
            raise TypeError(
                f"only 1 or 2 index arguments supported ({key[:5]}{f'... [{len(key)}]' if len(key) > 5 else ''})"
            )

        # clip to 2 elements
        ret = self * tuple(key[:2])
        ret = typing.cast(_MultipleMatch, ret)

        if stop_on_defined:
            ret.stopOn(stop_on)

        return ret

    def __call__(self, name: typing.Optional[str] = None) -> "ParserElement":
        """
        Shortcut for :class:`set_results_name`, with ``list_all_matches=False``.

        If ``name`` is given with a trailing ``'*'`` character, then ``list_all_matches`` will be
        passed as ``True``.

        If ``name`` is omitted, same as calling :class:`copy`.

        Example::

            # these are equivalent
            userdata = Word(alphas).set_results_name("name") + Word(nums + "-").set_results_name("socsecno")
            userdata = Word(alphas)("name") + Word(nums + "-")("socsecno")
        """
        if name is not None:
            return self._setResultsName(name)
        else:
            return self.copy()

    def suppress(self) -> "ParserElement":
        """
        Suppresses the output of this :class:`ParserElement`; useful to keep punctuation from
        cluttering up returned output.
        """
        return Suppress(self)

    def ignore_whitespace(self, recursive: bool = True) -> "ParserElement":
        """
        Enables the skipping of whitespace before matching the characters in the
        :class:`ParserElement`'s defined pattern.

        :param recursive: If ``True`` (the default), also enable whitespace skipping in child elements (if any)
        """
        self.skipWhitespace = True
        return self

    def leave_whitespace(self, recursive: bool = True) -> "ParserElement":
        """
        Disables the skipping of whitespace before matching the characters in the
        :class:`ParserElement`'s defined pattern.  This is normally only used internally by
        the pyparsing module, but may be needed in some whitespace-sensitive grammars.

        :param recursive: If true (the default), also disable whitespace skipping in child elements (if any)
        """
        self.skipWhitespace = False
        return self

    def set_whitespace_chars(
        self, chars: Union[Set[str], str], copy_defaults: bool = False
    ) -> "ParserElement":
        """
        Overrides the default whitespace chars
        """
        self.skipWhitespace = True
        self.whiteChars = set(chars)
        self.copyDefaultWhiteChars = copy_defaults
        return self

    def parse_with_tabs(self) -> "ParserElement":
        """
        Overrides default behavior to expand ``<TAB>`` s to spaces before parsing the input string.
        Must be called before ``parse_string`` when the input grammar contains elements that
        match ``<TAB>`` characters.
        """
        self.keepTabs = True
        return self

    def ignore(self, other: "ParserElement") -> "ParserElement":
        """
        Define expression to be ignored (e.g., comments) while doing pattern
        matching; may be called repeatedly, to define multiple comment or other
        ignorable patterns.

        Example::

            patt = Word(alphas)[1, ...]
            patt.parse_string('ablaj /* comment */ lskjd')
            # -> ['ablaj']

            patt.ignore(c_style_comment)
            patt.parse_string('ablaj /* comment */ lskjd')
            # -> ['ablaj', 'lskjd']
        """
        import typing

        if isinstance(other, str_type):
            other = Suppress(other)

        if isinstance(other, Suppress):
            if other not in self.ignoreExprs:
                self.ignoreExprs.append(other)
        else:
            self.ignoreExprs.append(Suppress(other.copy()))
        return self

    def set_debug_actions(
        self,
        start_action: DebugStartAction,
        success_action: DebugSuccessAction,
        exception_action: DebugExceptionAction,
    ) -> "ParserElement":
        """
        Customize display of debugging messages while doing pattern matching:

        - ``start_action`` - method to be called when an expression is about to be parsed;
          should have the signature ``fn(input_string: str, location: int, expression: ParserElement, cache_hit: bool)``

        - ``success_action`` - method to be called when an expression has successfully parsed;
          should have the signature ``fn(input_string: str, start_location: int, end_location: int, expression: ParserELement, parsed_tokens: ParseResults, cache_hit: bool)``

        - ``exception_action`` - method to be called when expression fails to parse;
          should have the signature ``fn(input_string: str, location: int, expression: ParserElement, exception: Exception, cache_hit: bool)``
        """
        self.debugActions = self.DebugActions(
            start_action or _default_start_debug_action,  # type: ignore[truthy-function]
            success_action or _default_success_debug_action,  # type: ignore[truthy-function]
            exception_action or _default_exception_debug_action,  # type: ignore[truthy-function]
        )
        self.debug = True
        return self

    def set_debug(self, flag: bool = True, recurse: bool = False) -> "ParserElement":
        """
        Enable display of debugging messages while doing pattern matching.
        Set ``flag`` to ``True`` to enable, ``False`` to disable.
        Set ``recurse`` to ``True`` to set the debug flag on this expression and all sub-expressions.

        Example::

            wd = Word(alphas).set_name("alphaword")
            integer = Word(nums).set_name("numword")
            term = wd | integer

            # turn on debugging for wd
            wd.set_debug()

            term[1, ...].parse_string("abc 123 xyz 890")

        prints::

            Match alphaword at loc 0(1,1)
            Matched alphaword -> ['abc']
            Match alphaword at loc 3(1,4)
            Exception raised:Expected alphaword (at char 4), (line:1, col:5)
            Match alphaword at loc 7(1,8)
            Matched alphaword -> ['xyz']
            Match alphaword at loc 11(1,12)
            Exception raised:Expected alphaword (at char 12), (line:1, col:13)
            Match alphaword at loc 15(1,16)
            Exception raised:Expected alphaword (at char 15), (line:1, col:16)

        The output shown is that produced by the default debug actions - custom debug actions can be
        specified using :class:`set_debug_actions`. Prior to attempting
        to match the ``wd`` expression, the debugging message ``"Match <exprname> at loc <n>(<line>,<col>)"``
        is shown. Then if the parse succeeds, a ``"Matched"`` message is shown, or an ``"Exception raised"``
        message is shown. Also note the use of :class:`set_name` to assign a human-readable name to the expression,
        which makes debugging and exception messages easier to understand - for instance, the default
        name created for the :class:`Word` expression without calling ``set_name`` is ``"W:(A-Za-z)"``.
        """
        if recurse:
            for expr in self.visit_all():
                expr.set_debug(flag, recurse=False)
            return self

        if flag:
            self.set_debug_actions(
                _default_start_debug_action,
                _default_success_debug_action,
                _default_exception_debug_action,
            )
        else:
            self.debug = False
        return self

    @property
    def default_name(self) -> str:
        if self._defaultName is None:
            self._defaultName = self._generateDefaultName()
        return self._defaultName

    @abstractmethod
    def _generateDefaultName(self) -> str:
        """
        Child classes must define this method, which defines how the ``default_name`` is set.
        """

    def set_name(self, name: str) -> "ParserElement":
        """
        Define name for this expression, makes debugging and exception messages clearer.

        Example::

            Word(nums).parse_string("ABC")  # -> Exception: Expected W:(0-9) (at char 0), (line:1, col:1)
            Word(nums).set_name("integer").parse_string("ABC")  # -> Exception: Expected integer (at char 0), (line:1, col:1)
        """
        self.customName = name
        self.errmsg = "Expected " + self.name
        if __diag__.enable_debug_on_named_expressions:
            self.set_debug()
        return self

    @property
    def name(self) -> str:
        # This will use a user-defined name if available, but otherwise defaults back to the auto-generated name
        return self.customName if self.customName is not None else self.default_name

    def __str__(self) -> str:
        return self.name

    def __repr__(self) -> str:
        return str(self)

    def streamline(self) -> "ParserElement":
        self.streamlined = True
        self._defaultName = None
        return self

    def recurse(self) -> List["ParserElement"]:
        return []

    def _checkRecursion(self, parseElementList):
        subRecCheckList = parseElementList[:] + [self]
        for e in self.recurse():
            e._checkRecursion(subRecCheckList)

    def validate(self, validateTrace=None) -> None:
        """
        Check defined expressions for valid structure, check for infinite recursive definitions.
        """
        warnings.warn(
            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
            DeprecationWarning,
            stacklevel=2,
        )
        self._checkRecursion([])

    def parse_file(
        self,
        file_or_filename: Union[str, Path, TextIO],
        encoding: str = "utf-8",
        parse_all: bool = False,
        *,
        parseAll: bool = False,
    ) -> ParseResults:
        """
        Execute the parse expression on the given file or filename.
        If a filename is specified (instead of a file object),
        the entire file is opened, read, and closed before parsing.
        """
        parseAll = parseAll or parse_all
        try:
            file_or_filename = typing.cast(TextIO, file_or_filename)
            file_contents = file_or_filename.read()
        except AttributeError:
            file_or_filename = typing.cast(str, file_or_filename)
            with open(file_or_filename, "r", encoding=encoding) as f:
                file_contents = f.read()
        try:
            return self.parse_string(file_contents, parseAll)
        except ParseBaseException as exc:
            if ParserElement.verbose_stacktrace:
                raise
            else:
                # catch and re-raise exception from here, clears out pyparsing internal stack trace
                raise exc.with_traceback(None)

    def __eq__(self, other):
        if self is other:
            return True
        elif isinstance(other, str_type):
            return self.matches(other, parse_all=True)
        elif isinstance(other, ParserElement):
            return vars(self) == vars(other)
        return False

    def __hash__(self):
        return id(self)

    def matches(
        self, test_string: str, parse_all: bool = True, *, parseAll: bool = True
    ) -> bool:
        """
        Method for quick testing of a parser against a test string. Good for simple
        inline microtests of sub expressions while building up larger parser.

        Parameters:

        - ``test_string`` - to test against this expression for a match
        - ``parse_all`` - (default= ``True``) - flag to pass to :class:`parse_string` when running tests

        Example::

            expr = Word(nums)
            assert expr.matches("100")
        """
        parseAll = parseAll and parse_all
        try:
            self.parse_string(str(test_string), parse_all=parseAll)
            return True
        except ParseBaseException:
            return False

    def run_tests(
        self,
        tests: Union[str, List[str]],
        parse_all: bool = True,
        comment: typing.Optional[Union["ParserElement", str]] = "#",
        full_dump: bool = True,
        print_results: bool = True,
        failure_tests: bool = False,
        post_parse: typing.Optional[Callable[[str, ParseResults], str]] = None,
        file: typing.Optional[TextIO] = None,
        with_line_numbers: bool = False,
        *,
        parseAll: bool = True,
        fullDump: bool = True,
        printResults: bool = True,
        failureTests: bool = False,
        postParse: typing.Optional[Callable[[str, ParseResults], str]] = None,
    ) -> Tuple[bool, List[Tuple[str, Union[ParseResults, Exception]]]]:
        """
        Execute the parse expression on a series of test strings, showing each
        test, the parsed results or where the parse failed. Quick and easy way to
        run a parse expression against a list of sample strings.

        Parameters:

        - ``tests`` - a list of separate test strings, or a multiline string of test strings
        - ``parse_all`` - (default= ``True``) - flag to pass to :class:`parse_string` when running tests
        - ``comment`` - (default= ``'#'``) - expression for indicating embedded comments in the test
          string; pass None to disable comment filtering
        - ``full_dump`` - (default= ``True``) - dump results as list followed by results names in nested outline;
          if False, only dump nested list
        - ``print_results`` - (default= ``True``) prints test output to stdout
        - ``failure_tests`` - (default= ``False``) indicates if these tests are expected to fail parsing
        - ``post_parse`` - (default= ``None``) optional callback for successful parse results; called as
          `fn(test_string, parse_results)` and returns a string to be added to the test output
        - ``file`` - (default= ``None``) optional file-like object to which test output will be written;
          if None, will default to ``sys.stdout``
        - ``with_line_numbers`` - default= ``False``) show test strings with line and column numbers

        Returns: a (success, results) tuple, where success indicates that all tests succeeded
        (or failed if ``failure_tests`` is True), and the results contain a list of lines of each
        test's output

        Example::

            number_expr = pyparsing_common.number.copy()

            result = number_expr.run_tests('''
                # unsigned integer
                100
                # negative integer
                -100
                # float with scientific notation
                6.02e23
                # integer with scientific notation
                1e-12
                ''')
            print("Success" if result[0] else "Failed!")

            result = number_expr.run_tests('''
                # stray character
                100Z
                # missing leading digit before '.'
                -.100
                # too many '.'
                3.14.159
                ''', failure_tests=True)
            print("Success" if result[0] else "Failed!")

        prints::

            # unsigned integer
            100
            [100]

            # negative integer
            -100
            [-100]

            # float with scientific notation
            6.02e23
            [6.02e+23]

            # integer with scientific notation
            1e-12
            [1e-12]

            Success

            # stray character
            100Z
               ^
            FAIL: Expected end of text (at char 3), (line:1, col:4)

            # missing leading digit before '.'
            -.100
            ^
            FAIL: Expected {real number with scientific notation | real number | signed integer} (at char 0), (line:1, col:1)

            # too many '.'
            3.14.159
                ^
            FAIL: Expected end of text (at char 4), (line:1, col:5)

            Success

        Each test string must be on a single line. If you want to test a string that spans multiple
        lines, create a test like this::

            expr.run_tests(r"this is a test\\n of strings that spans \\n 3 lines")

        (Note that this is a raw string literal, you must include the leading ``'r'``.)
        """
        from .testing import pyparsing_test

        parseAll = parseAll and parse_all
        fullDump = fullDump and full_dump
        printResults = printResults and print_results
        failureTests = failureTests or failure_tests
        postParse = postParse or post_parse
        if isinstance(tests, str_type):
            tests = typing.cast(str, tests)
            line_strip = type(tests).strip
            tests = [line_strip(test_line) for test_line in tests.rstrip().splitlines()]
        comment_specified = comment is not None
        if comment_specified:
            if isinstance(comment, str_type):
                comment = typing.cast(str, comment)
                comment = Literal(comment)
        comment = typing.cast(ParserElement, comment)
        if file is None:
            file = sys.stdout
        print_ = file.write

        result: Union[ParseResults, Exception]
        allResults: List[Tuple[str, Union[ParseResults, Exception]]] = []
        comments: List[str] = []
        success = True
        NL = Literal(r"\n").add_parse_action(replace_with("\n")).ignore(quoted_string)
        BOM = "\ufeff"
        for t in tests:
            if comment_specified and comment.matches(t, False) or comments and not t:
                comments.append(
                    pyparsing_test.with_line_numbers(t) if with_line_numbers else t
                )
                continue
            if not t:
                continue
            out = [
                "\n" + "\n".join(comments) if comments else "",
                pyparsing_test.with_line_numbers(t) if with_line_numbers else t,
            ]
            comments = []
            try:
                # convert newline marks to actual newlines, and strip leading BOM if present
                t = NL.transform_string(t.lstrip(BOM))
                result = self.parse_string(t, parse_all=parseAll)
            except ParseBaseException as pe:
                fatal = "(FATAL)" if isinstance(pe, ParseFatalException) else ""
                out.append(pe.explain())
                out.append("FAIL: " + str(pe))
                if ParserElement.verbose_stacktrace:
                    out.extend(traceback.format_tb(pe.__traceback__))
                success = success and failureTests
                result = pe
            except Exception as exc:
                out.append(f"FAIL-EXCEPTION: {type(exc).__name__}: {exc}")
                if ParserElement.verbose_stacktrace:
                    out.extend(traceback.format_tb(exc.__traceback__))
                success = success and failureTests
                result = exc
            else:
                success = success and not failureTests
                if postParse is not None:
                    try:
                        pp_value = postParse(t, result)
                        if pp_value is not None:
                            if isinstance(pp_value, ParseResults):
                                out.append(pp_value.dump())
                            else:
                                out.append(str(pp_value))
                        else:
                            out.append(result.dump())
                    except Exception as e:
                        out.append(result.dump(full=fullDump))
                        out.append(
                            f"{postParse.__name__} failed: {type(e).__name__}: {e}"
                        )
                else:
                    out.append(result.dump(full=fullDump))
            out.append("")

            if printResults:
                print_("\n".join(out))

            allResults.append((t, result))

        return success, allResults

    def create_diagram(
        self,
        output_html: Union[TextIO, Path, str],
        vertical: int = 3,
        show_results_names: bool = False,
        show_groups: bool = False,
        embed: bool = False,
        **kwargs,
    ) -> None:
        """
        Create a railroad diagram for the parser.

        Parameters:

        - ``output_html`` (str or file-like object) - output target for generated
          diagram HTML
        - ``vertical`` (int) - threshold for formatting multiple alternatives vertically
          instead of horizontally (default=3)
        - ``show_results_names`` - bool flag whether diagram should show annotations for
          defined results names
        - ``show_groups`` - bool flag whether groups should be highlighted with an unlabeled surrounding box
        - ``embed`` - bool flag whether generated HTML should omit <HEAD>, <BODY>, and <DOCTYPE> tags to embed
          the resulting HTML in an enclosing HTML source
        - ``head`` - str containing additional HTML to insert into the <HEAD> section of the generated code;
          can be used to insert custom CSS styling
        - ``body`` - str containing additional HTML to insert at the beginning of the <BODY> section of the
          generated code

        Additional diagram-formatting keyword arguments can also be included;
        see railroad.Diagram class.
        """

        try:
            from .diagram import to_railroad, railroad_to_html
        except ImportError as ie:
            raise Exception(
                "must ``pip install pyparsing[diagrams]`` to generate parser railroad diagrams"
            ) from ie

        self.streamline()

        railroad = to_railroad(
            self,
            vertical=vertical,
            show_results_names=show_results_names,
            show_groups=show_groups,
            diagram_kwargs=kwargs,
        )
        if isinstance(output_html, (str, Path)):
            with open(output_html, "w", encoding="utf-8") as diag_file:
                diag_file.write(railroad_to_html(railroad, embed=embed, **kwargs))
        else:
            # we were passed a file-like object, just write to it
            output_html.write(railroad_to_html(railroad, embed=embed, **kwargs))

    # Compatibility synonyms
    # fmt: off
    @staticmethod
    @replaced_by_pep8(inline_literals_using)
    def inlineLiteralsUsing(): ...

    @staticmethod
    @replaced_by_pep8(set_default_whitespace_chars)
    def setDefaultWhitespaceChars(): ...

    @replaced_by_pep8(set_results_name)
    def setResultsName(self): ...

    @replaced_by_pep8(set_break)
    def setBreak(self): ...

    @replaced_by_pep8(set_parse_action)
    def setParseAction(self): ...

    @replaced_by_pep8(add_parse_action)
    def addParseAction(self): ...

    @replaced_by_pep8(add_condition)
    def addCondition(self): ...

    @replaced_by_pep8(set_fail_action)
    def setFailAction(self): ...

    @replaced_by_pep8(try_parse)
    def tryParse(self): ...

    @staticmethod
    @replaced_by_pep8(enable_left_recursion)
    def enableLeftRecursion(): ...

    @staticmethod
    @replaced_by_pep8(enable_packrat)
    def enablePackrat(): ...

    @replaced_by_pep8(parse_string)
    def parseString(self): ...

    @replaced_by_pep8(scan_string)
    def scanString(self): ...

    @replaced_by_pep8(transform_string)
    def transformString(self): ...

    @replaced_by_pep8(search_string)
    def searchString(self): ...

    @replaced_by_pep8(ignore_whitespace)
    def ignoreWhitespace(self): ...

    @replaced_by_pep8(leave_whitespace)
    def leaveWhitespace(self): ...

    @replaced_by_pep8(set_whitespace_chars)
    def setWhitespaceChars(self): ...

    @replaced_by_pep8(parse_with_tabs)
    def parseWithTabs(self): ...

    @replaced_by_pep8(set_debug_actions)
    def setDebugActions(self): ...

    @replaced_by_pep8(set_debug)
    def setDebug(self): ...

    @replaced_by_pep8(set_name)
    def setName(self): ...

    @replaced_by_pep8(parse_file)
    def parseFile(self): ...

    @replaced_by_pep8(run_tests)
    def runTests(self): ...

    canParseNext = can_parse_next
    resetCache = reset_cache
    defaultName = default_name
    # fmt: on


class _PendingSkip(ParserElement):
    # internal placeholder class to hold a place were '...' is added to a parser element,
    # once another ParserElement is added, this placeholder will be replaced with a SkipTo
    def __init__(self, expr: ParserElement, must_skip: bool = False):
        super().__init__()
        self.anchor = expr
        self.must_skip = must_skip

    def _generateDefaultName(self) -> str:
        return str(self.anchor + Empty()).replace("Empty", "...")

    def __add__(self, other) -> "ParserElement":
        skipper = SkipTo(other).set_name("...")("_skipped*")
        if self.must_skip:

            def must_skip(t):
                if not t._skipped or t._skipped.as_list() == [""]:
                    del t[0]
                    t.pop("_skipped", None)

            def show_skip(t):
                if t._skipped.as_list()[-1:] == [""]:
                    t.pop("_skipped")
                    t["_skipped"] = "missing <" + repr(self.anchor) + ">"

            return (
                self.anchor + skipper().add_parse_action(must_skip)
                | skipper().add_parse_action(show_skip)
            ) + other

        return self.anchor + skipper + other

    def __repr__(self):
        return self.defaultName

    def parseImpl(self, *args):
        raise Exception(
            "use of `...` expression without following SkipTo target expression"
        )


class Token(ParserElement):
    """Abstract :class:`ParserElement` subclass, for defining atomic
    matching patterns.
    """

    def __init__(self):
        super().__init__(savelist=False)

    def _generateDefaultName(self) -> str:
        return type(self).__name__


class NoMatch(Token):
    """
    A token that will never match.
    """

    def __init__(self):
        super().__init__()
        self.mayReturnEmpty = True
        self.mayIndexError = False
        self.errmsg = "Unmatchable token"

    def parseImpl(self, instring, loc, doActions=True):
        raise ParseException(instring, loc, self.errmsg, self)


class Literal(Token):
    """
    Token to exactly match a specified string.

    Example::

        Literal('blah').parse_string('blah')  # -> ['blah']
        Literal('blah').parse_string('blahfooblah')  # -> ['blah']
        Literal('blah').parse_string('bla')  # -> Exception: Expected "blah"

    For case-insensitive matching, use :class:`CaselessLiteral`.

    For keyword matching (force word break before and after the matched string),
    use :class:`Keyword` or :class:`CaselessKeyword`.
    """

    def __new__(cls, match_string: str = "", *, matchString: str = ""):
        # Performance tuning: select a subclass with optimized parseImpl
        if cls is Literal:
            match_string = matchString or match_string
            if not match_string:
                return super().__new__(Empty)
            if len(match_string) == 1:
                return super().__new__(_SingleCharLiteral)

        # Default behavior
        return super().__new__(cls)

    # Needed to make copy.copy() work correctly if we customize __new__
    def __getnewargs__(self):
        return (self.match,)

    def __init__(self, match_string: str = "", *, matchString: str = ""):
        super().__init__()
        match_string = matchString or match_string
        self.match = match_string
        self.matchLen = len(match_string)
        self.firstMatchChar = match_string[:1]
        self.errmsg = "Expected " + self.name
        self.mayReturnEmpty = False
        self.mayIndexError = False

    def _generateDefaultName(self) -> str:
        return repr(self.match)

    def parseImpl(self, instring, loc, doActions=True):
        if instring[loc] == self.firstMatchChar and instring.startswith(
            self.match, loc
        ):
            return loc + self.matchLen, self.match
        raise ParseException(instring, loc, self.errmsg, self)


class Empty(Literal):
    """
    An empty token, will always match.
    """

    def __init__(self, match_string="", *, matchString=""):
        super().__init__("")
        self.mayReturnEmpty = True
        self.mayIndexError = False

    def _generateDefaultName(self) -> str:
        return "Empty"

    def parseImpl(self, instring, loc, doActions=True):
        return loc, []


class _SingleCharLiteral(Literal):
    def parseImpl(self, instring, loc, doActions=True):
        if instring[loc] == self.firstMatchChar:
            return loc + 1, self.match
        raise ParseException(instring, loc, self.errmsg, self)


ParserElement._literalStringClass = Literal


class Keyword(Token):
    """
    Token to exactly match a specified string as a keyword, that is,
    it must be immediately preceded and followed by whitespace or
    non-keyword characters. Compare with :class:`Literal`:

    - ``Literal("if")`` will match the leading ``'if'`` in
      ``'ifAndOnlyIf'``.
    - ``Keyword("if")`` will not; it will only match the leading
      ``'if'`` in ``'if x=1'``, or ``'if(y==2)'``

    Accepts two optional constructor arguments in addition to the
    keyword string:

    - ``ident_chars`` is a string of characters that would be valid
      identifier characters, defaulting to all alphanumerics + "_" and
      "$"
    - ``caseless`` allows case-insensitive matching, default is ``False``.

    Example::

        Keyword("start").parse_string("start")  # -> ['start']
        Keyword("start").parse_string("starting")  # -> Exception

    For case-insensitive matching, use :class:`CaselessKeyword`.
    """

    DEFAULT_KEYWORD_CHARS = alphanums + "_$"

    def __init__(
        self,
        match_string: str = "",
        ident_chars: typing.Optional[str] = None,
        caseless: bool = False,
        *,
        matchString: str = "",
        identChars: typing.Optional[str] = None,
    ):
        super().__init__()
        identChars = identChars or ident_chars
        if identChars is None:
            identChars = Keyword.DEFAULT_KEYWORD_CHARS
        match_string = matchString or match_string
        self.match = match_string
        self.matchLen = len(match_string)
        try:
            self.firstMatchChar = match_string[0]
        except IndexError:
            raise ValueError("null string passed to Keyword; use Empty() instead")
        self.errmsg = f"Expected {type(self).__name__} {self.name}"
        self.mayReturnEmpty = False
        self.mayIndexError = False
        self.caseless = caseless
        if caseless:
            self.caselessmatch = match_string.upper()
            identChars = identChars.upper()
        self.identChars = set(identChars)

    def _generateDefaultName(self) -> str:
        return repr(self.match)

    def parseImpl(self, instring, loc, doActions=True):
        errmsg = self.errmsg
        errloc = loc
        if self.caseless:
            if instring[loc : loc + self.matchLen].upper() == self.caselessmatch:
                if loc == 0 or instring[loc - 1].upper() not in self.identChars:
                    if (
                        loc >= len(instring) - self.matchLen
                        or instring[loc + self.matchLen].upper() not in self.identChars
                    ):
                        return loc + self.matchLen, self.match
                    else:
                        # followed by keyword char
                        errmsg += ", was immediately followed by keyword character"
                        errloc = loc + self.matchLen
                else:
                    # preceded by keyword char
                    errmsg += ", keyword was immediately preceded by keyword character"
                    errloc = loc - 1
            # else no match just raise plain exception

        else:
            if (
                instring[loc] == self.firstMatchChar
                and self.matchLen == 1
                or instring.startswith(self.match, loc)
            ):
                if loc == 0 or instring[loc - 1] not in self.identChars:
                    if (
                        loc >= len(instring) - self.matchLen
                        or instring[loc + self.matchLen] not in self.identChars
                    ):
                        return loc + self.matchLen, self.match
                    else:
                        # followed by keyword char
                        errmsg += (
                            ", keyword was immediately followed by keyword character"
                        )
                        errloc = loc + self.matchLen
                else:
                    # preceded by keyword char
                    errmsg += ", keyword was immediately preceded by keyword character"
                    errloc = loc - 1
            # else no match just raise plain exception

        raise ParseException(instring, errloc, errmsg, self)

    @staticmethod
    def set_default_keyword_chars(chars) -> None:
        """
        Overrides the default characters used by :class:`Keyword` expressions.
        """
        Keyword.DEFAULT_KEYWORD_CHARS = chars

    setDefaultKeywordChars = set_default_keyword_chars


class CaselessLiteral(Literal):
    """
    Token to match a specified string, ignoring case of letters.
    Note: the matched results will always be in the case of the given
    match string, NOT the case of the input text.

    Example::

        CaselessLiteral("CMD")[1, ...].parse_string("cmd CMD Cmd10")
        # -> ['CMD', 'CMD', 'CMD']

    (Contrast with example for :class:`CaselessKeyword`.)
    """

    def __init__(self, match_string: str = "", *, matchString: str = ""):
        match_string = matchString or match_string
        super().__init__(match_string.upper())
        # Preserve the defining literal.
        self.returnString = match_string
        self.errmsg = "Expected " + self.name

    def parseImpl(self, instring, loc, doActions=True):
        if instring[loc : loc + self.matchLen].upper() == self.match:
            return loc + self.matchLen, self.returnString
        raise ParseException(instring, loc, self.errmsg, self)


class CaselessKeyword(Keyword):
    """
    Caseless version of :class:`Keyword`.

    Example::

        CaselessKeyword("CMD")[1, ...].parse_string("cmd CMD Cmd10")
        # -> ['CMD', 'CMD']

    (Contrast with example for :class:`CaselessLiteral`.)
    """

    def __init__(
        self,
        match_string: str = "",
        ident_chars: typing.Optional[str] = None,
        *,
        matchString: str = "",
        identChars: typing.Optional[str] = None,
    ):
        identChars = identChars or ident_chars
        match_string = matchString or match_string
        super().__init__(match_string, identChars, caseless=True)


class CloseMatch(Token):
    """A variation on :class:`Literal` which matches "close" matches,
    that is, strings with at most 'n' mismatching characters.
    :class:`CloseMatch` takes parameters:

    - ``match_string`` - string to be matched
    - ``caseless`` - a boolean indicating whether to ignore casing when comparing characters
    - ``max_mismatches`` - (``default=1``) maximum number of
      mismatches allowed to count as a match

    The results from a successful parse will contain the matched text
    from the input string and the following named results:

    - ``mismatches`` - a list of the positions within the
      match_string where mismatches were found
    - ``original`` - the original match_string used to compare
      against the input string

    If ``mismatches`` is an empty list, then the match was an exact
    match.

    Example::

        patt = CloseMatch("ATCATCGAATGGA")
        patt.parse_string("ATCATCGAAXGGA") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})
        patt.parse_string("ATCAXCGAAXGGA") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)

        # exact match
        patt.parse_string("ATCATCGAATGGA") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})

        # close match allowing up to 2 mismatches
        patt = CloseMatch("ATCATCGAATGGA", max_mismatches=2)
        patt.parse_string("ATCAXCGAAXGGA") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})
    """

    def __init__(
        self,
        match_string: str,
        max_mismatches: typing.Optional[int] = None,
        *,
        maxMismatches: int = 1,
        caseless=False,
    ):
        maxMismatches = max_mismatches if max_mismatches is not None else maxMismatches
        super().__init__()
        self.match_string = match_string
        self.maxMismatches = maxMismatches
        self.errmsg = f"Expected {self.match_string!r} (with up to {self.maxMismatches} mismatches)"
        self.caseless = caseless
        self.mayIndexError = False
        self.mayReturnEmpty = False

    def _generateDefaultName(self) -> str:
        return f"{type(self).__name__}:{self.match_string!r}"

    def parseImpl(self, instring, loc, doActions=True):
        start = loc
        instrlen = len(instring)
        maxloc = start + len(self.match_string)

        if maxloc <= instrlen:
            match_string = self.match_string
            match_stringloc = 0
            mismatches = []
            maxMismatches = self.maxMismatches

            for match_stringloc, s_m in enumerate(
                zip(instring[loc:maxloc], match_string)
            ):
                src, mat = s_m
                if self.caseless:
                    src, mat = src.lower(), mat.lower()

                if src != mat:
                    mismatches.append(match_stringloc)
                    if len(mismatches) > maxMismatches:
                        break
            else:
                loc = start + match_stringloc + 1
                results = ParseResults([instring[start:loc]])
                results["original"] = match_string
                results["mismatches"] = mismatches
                return loc, results

        raise ParseException(instring, loc, self.errmsg, self)


class Word(Token):
    """Token for matching words composed of allowed character sets.

    Parameters:

    - ``init_chars`` - string of all characters that should be used to
      match as a word; "ABC" will match "AAA", "ABAB", "CBAC", etc.;
      if ``body_chars`` is also specified, then this is the string of
      initial characters
    - ``body_chars`` - string of characters that
      can be used for matching after a matched initial character as
      given in ``init_chars``; if omitted, same as the initial characters
      (default=``None``)
    - ``min`` - minimum number of characters to match (default=1)
    - ``max`` - maximum number of characters to match (default=0)
    - ``exact`` - exact number of characters to match (default=0)
    - ``as_keyword`` - match as a keyword (default=``False``)
    - ``exclude_chars`` - characters that might be
      found in the input ``body_chars`` string but which should not be
      accepted for matching ;useful to define a word of all
      printables except for one or two characters, for instance
      (default=``None``)

    :class:`srange` is useful for defining custom character set strings
    for defining :class:`Word` expressions, using range notation from
    regular expression character sets.

    A common mistake is to use :class:`Word` to match a specific literal
    string, as in ``Word("Address")``. Remember that :class:`Word`
    uses the string argument to define *sets* of matchable characters.
    This expression would match "Add", "AAA", "dAred", or any other word
    made up of the characters 'A', 'd', 'r', 'e', and 's'. To match an
    exact literal string, use :class:`Literal` or :class:`Keyword`.

    pyparsing includes helper strings for building Words:

    - :class:`alphas`
    - :class:`nums`
    - :class:`alphanums`
    - :class:`hexnums`
    - :class:`alphas8bit` (alphabetic characters in ASCII range 128-255
      - accented, tilded, umlauted, etc.)
    - :class:`punc8bit` (non-alphabetic characters in ASCII range
      128-255 - currency, symbols, superscripts, diacriticals, etc.)
    - :class:`printables` (any non-whitespace character)

    ``alphas``, ``nums``, and ``printables`` are also defined in several
    Unicode sets - see :class:`pyparsing_unicode``.

    Example::

        # a word composed of digits
        integer = Word(nums) # equivalent to Word("0123456789") or Word(srange("0-9"))

        # a word with a leading capital, and zero or more lowercase
        capital_word = Word(alphas.upper(), alphas.lower())

        # hostnames are alphanumeric, with leading alpha, and '-'
        hostname = Word(alphas, alphanums + '-')

        # roman numeral (not a strict parser, accepts invalid mix of characters)
        roman = Word("IVXLCDM")

        # any string of non-whitespace characters, except for ','
        csv_value = Word(printables, exclude_chars=",")
    """

    def __init__(
        self,
        init_chars: str = "",
        body_chars: typing.Optional[str] = None,
        min: int = 1,
        max: int = 0,
        exact: int = 0,
        as_keyword: bool = False,
        exclude_chars: typing.Optional[str] = None,
        *,
        initChars: typing.Optional[str] = None,
        bodyChars: typing.Optional[str] = None,
        asKeyword: bool = False,
        excludeChars: typing.Optional[str] = None,
    ):
        initChars = initChars or init_chars
        bodyChars = bodyChars or body_chars
        asKeyword = asKeyword or as_keyword
        excludeChars = excludeChars or exclude_chars
        super().__init__()
        if not initChars:
            raise ValueError(
                f"invalid {type(self).__name__}, initChars cannot be empty string"
            )

        initChars_set = set(initChars)
        if excludeChars:
            excludeChars_set = set(excludeChars)
            initChars_set -= excludeChars_set
            if bodyChars:
                bodyChars = "".join(set(bodyChars) - excludeChars_set)
        self.initChars = initChars_set
        self.initCharsOrig = "".join(sorted(initChars_set))

        if bodyChars:
            self.bodyChars = set(bodyChars)
            self.bodyCharsOrig = "".join(sorted(bodyChars))
        else:
            self.bodyChars = initChars_set
            self.bodyCharsOrig = self.initCharsOrig

        self.maxSpecified = max > 0

        if min < 1:
            raise ValueError(
                "cannot specify a minimum length < 1; use Opt(Word()) if zero-length word is permitted"
            )

        if self.maxSpecified and min > max:
            raise ValueError(
                f"invalid args, if min and max both specified min must be <= max (min={min}, max={max})"
            )

        self.minLen = min

        if max > 0:
            self.maxLen = max
        else:
            self.maxLen = _MAX_INT

        if exact > 0:
            min = max = exact
            self.maxLen = exact
            self.minLen = exact

        self.errmsg = "Expected " + self.name
        self.mayIndexError = False
        self.asKeyword = asKeyword
        if self.asKeyword:
            self.errmsg += " as a keyword"

        # see if we can make a regex for this Word
        if " " not in (self.initChars | self.bodyChars):
            if len(self.initChars) == 1:
                re_leading_fragment = re.escape(self.initCharsOrig)
            else:
                re_leading_fragment = f"[{_collapse_string_to_ranges(self.initChars)}]"

            if self.bodyChars == self.initChars:
                if max == 0:
                    repeat = "+"
                elif max == 1:
                    repeat = ""
                else:
                    if self.minLen != self.maxLen:
                        repeat = f"{{{self.minLen},{'' if self.maxLen == _MAX_INT else self.maxLen}}}"
                    else:
                        repeat = f"{{{self.minLen}}}"
                self.reString = f"{re_leading_fragment}{repeat}"
            else:
                if max == 1:
                    re_body_fragment = ""
                    repeat = ""
                else:
                    re_body_fragment = f"[{_collapse_string_to_ranges(self.bodyChars)}]"
                    if max == 0:
                        repeat = "*"
                    elif max == 2:
                        repeat = "?" if min <= 1 else ""
                    else:
                        if min != max:
                            repeat = f"{{{min - 1 if min > 0 else 0},{max - 1}}}"
                        else:
                            repeat = f"{{{min - 1 if min > 0 else 0}}}"

                self.reString = (
                    f"{re_leading_fragment}" f"{re_body_fragment}" f"{repeat}"
                )

            if self.asKeyword:
                self.reString = rf"\b{self.reString}\b"

            try:
                self.re = re.compile(self.reString)
            except re.error:
                self.re = None  # type: ignore[assignment]
            else:
                self.re_match = self.re.match
                self.parseImpl = self.parseImpl_regex  # type: ignore[assignment]

    def _generateDefaultName(self) -> str:
        def charsAsStr(s):
            max_repr_len = 16
            s = _collapse_string_to_ranges(s, re_escape=False)
            if len(s) > max_repr_len:
                return s[: max_repr_len - 3] + "..."
            else:
                return s

        if self.initChars != self.bodyChars:
            base = f"W:({charsAsStr(self.initChars)}, {charsAsStr(self.bodyChars)})"
        else:
            base = f"W:({charsAsStr(self.initChars)})"

        # add length specification
        if self.minLen > 1 or self.maxLen != _MAX_INT:
            if self.minLen == self.maxLen:
                if self.minLen == 1:
                    return base[2:]
                else:
                    return base + f"{{{self.minLen}}}"
            elif self.maxLen == _MAX_INT:
                return base + f"{{{self.minLen},...}}"
            else:
                return base + f"{{{self.minLen},{self.maxLen}}}"
        return base

    def parseImpl(self, instring, loc, doActions=True):
        if instring[loc] not in self.initChars:
            raise ParseException(instring, loc, self.errmsg, self)

        start = loc
        loc += 1
        instrlen = len(instring)
        bodychars = self.bodyChars
        maxloc = start + self.maxLen
        maxloc = min(maxloc, instrlen)
        while loc < maxloc and instring[loc] in bodychars:
            loc += 1

        throwException = False
        if loc - start < self.minLen:
            throwException = True
        elif self.maxSpecified and loc < instrlen and instring[loc] in bodychars:
            throwException = True
        elif self.asKeyword:
            if (
                start > 0
                and instring[start - 1] in bodychars
                or loc < instrlen
                and instring[loc] in bodychars
            ):
                throwException = True

        if throwException:
            raise ParseException(instring, loc, self.errmsg, self)

        return loc, instring[start:loc]

    def parseImpl_regex(self, instring, loc, doActions=True):
        result = self.re_match(instring, loc)
        if not result:
            raise ParseException(instring, loc, self.errmsg, self)

        loc = result.end()
        return loc, result.group()


class Char(Word):
    """A short-cut class for defining :class:`Word` ``(characters, exact=1)``,
    when defining a match of any single character in a string of
    characters.
    """

    def __init__(
        self,
        charset: str,
        as_keyword: bool = False,
        exclude_chars: typing.Optional[str] = None,
        *,
        asKeyword: bool = False,
        excludeChars: typing.Optional[str] = None,
    ):
        asKeyword = asKeyword or as_keyword
        excludeChars = excludeChars or exclude_chars
        super().__init__(
            charset, exact=1, as_keyword=asKeyword, exclude_chars=excludeChars
        )


class Regex(Token):
    r"""Token for matching strings that match a given regular
    expression. Defined with string specifying the regular expression in
    a form recognized by the stdlib Python  `re module <https://docs.python.org/3/library/re.html>`_.
    If the given regex contains named groups (defined using ``(?P<name>...)``),
    these will be preserved as named :class:`ParseResults`.

    If instead of the Python stdlib ``re`` module you wish to use a different RE module
    (such as the ``regex`` module), you can do so by building your ``Regex`` object with
    a compiled RE that was compiled using ``regex``.

    Example::

        realnum = Regex(r"[+-]?\d+\.\d*")
        # ref: https://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression
        roman = Regex(r"M{0,4}(CM|CD|D?{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})")

        # named fields in a regex will be returned as named results
        date = Regex(r'(?P<year>\d{4})-(?P<month>\d\d?)-(?P<day>\d\d?)')

        # the Regex class will accept re's compiled using the regex module
        import regex
        parser = pp.Regex(regex.compile(r'[0-9]'))
    """

    def __init__(
        self,
        pattern: Any,
        flags: Union[re.RegexFlag, int] = 0,
        as_group_list: bool = False,
        as_match: bool = False,
        *,
        asGroupList: bool = False,
        asMatch: bool = False,
    ):
        """The parameters ``pattern`` and ``flags`` are passed
        to the ``re.compile()`` function as-is. See the Python
        `re module <https://docs.python.org/3/library/re.html>`_ module for an
        explanation of the acceptable patterns and flags.
        """
        super().__init__()
        asGroupList = asGroupList or as_group_list
        asMatch = asMatch or as_match

        if isinstance(pattern, str_type):
            if not pattern:
                raise ValueError("null string passed to Regex; use Empty() instead")

            self._re = None
            self.reString = self.pattern = pattern
            self.flags = flags

        elif hasattr(pattern, "pattern") and hasattr(pattern, "match"):
            self._re = pattern
            self.pattern = self.reString = pattern.pattern
            self.flags = flags

        else:
            raise TypeError(
                "Regex may only be constructed with a string or a compiled RE object"
            )

        self.errmsg = "Expected " + self.name
        self.mayIndexError = False
        self.asGroupList = asGroupList
        self.asMatch = asMatch
        if self.asGroupList:
            self.parseImpl = self.parseImplAsGroupList  # type: ignore [assignment]
        if self.asMatch:
            self.parseImpl = self.parseImplAsMatch  # type: ignore [assignment]

    @cached_property
    def re(self):
        if self._re:
            return self._re
        else:
            try:
                return re.compile(self.pattern, self.flags)
            except re.error:
                raise ValueError(f"invalid pattern ({self.pattern!r}) passed to Regex")

    @cached_property
    def re_match(self):
        return self.re.match

    @cached_property
    def mayReturnEmpty(self):
        return self.re_match("") is not None

    def _generateDefaultName(self) -> str:
        return "Re:({})".format(repr(self.pattern).replace("\\\\", "\\"))

    def parseImpl(self, instring, loc, doActions=True):
        result = self.re_match(instring, loc)
        if not result:
            raise ParseException(instring, loc, self.errmsg, self)

        loc = result.end()
        ret = ParseResults(result.group())
        d = result.groupdict()
        if d:
            for k, v in d.items():
                ret[k] = v
        return loc, ret

    def parseImplAsGroupList(self, instring, loc, doActions=True):
        result = self.re_match(instring, loc)
        if not result:
            raise ParseException(instring, loc, self.errmsg, self)

        loc = result.end()
        ret = result.groups()
        return loc, ret

    def parseImplAsMatch(self, instring, loc, doActions=True):
        result = self.re_match(instring, loc)
        if not result:
            raise ParseException(instring, loc, self.errmsg, self)

        loc = result.end()
        ret = result
        return loc, ret

    def sub(self, repl: str) -> ParserElement:
        r"""
        Return :class:`Regex` with an attached parse action to transform the parsed
        result as if called using `re.sub(expr, repl, string) <https://docs.python.org/3/library/re.html#re.sub>`_.

        Example::

            make_html = Regex(r"(\w+):(.*?):").sub(r"<\1>\2</\1>")
            print(make_html.transform_string("h1:main title:"))
            # prints "<h1>main title</h1>"
        """
        if self.asGroupList:
            raise TypeError("cannot use sub() with Regex(as_group_list=True)")

        if self.asMatch and callable(repl):
            raise TypeError(
                "cannot use sub() with a callable with Regex(as_match=True)"
            )

        if self.asMatch:

            def pa(tokens):
                return tokens[0].expand(repl)

        else:

            def pa(tokens):
                return self.re.sub(repl, tokens[0])

        return self.add_parse_action(pa)


class QuotedString(Token):
    r"""
    Token for matching strings that are delimited by quoting characters.

    Defined with the following parameters:

    - ``quote_char`` - string of one or more characters defining the
      quote delimiting string
    - ``esc_char`` - character to re_escape quotes, typically backslash
      (default= ``None``)
    - ``esc_quote`` - special quote sequence to re_escape an embedded quote
      string (such as SQL's ``""`` to re_escape an embedded ``"``)
      (default= ``None``)
    - ``multiline`` - boolean indicating whether quotes can span
      multiple lines (default= ``False``)
    - ``unquote_results`` - boolean indicating whether the matched text
      should be unquoted (default= ``True``)
    - ``end_quote_char`` - string of one or more characters defining the
      end of the quote delimited string (default= ``None``  => same as
      quote_char)
    - ``convert_whitespace_escapes`` - convert escaped whitespace
      (``'\t'``, ``'\n'``, etc.) to actual whitespace
      (default= ``True``)

    Example::

        qs = QuotedString('"')
        print(qs.search_string('lsjdf "This is the quote" sldjf'))
        complex_qs = QuotedString('{{', end_quote_char='}}')
        print(complex_qs.search_string('lsjdf {{This is the "quote"}} sldjf'))
        sql_qs = QuotedString('"', esc_quote='""')
        print(sql_qs.search_string('lsjdf "This is the quote with ""embedded"" quotes" sldjf'))

    prints::

        [['This is the quote']]
        [['This is the "quote"']]
        [['This is the quote with "embedded" quotes']]
    """
    ws_map = dict(((r"\t", "\t"), (r"\n", "\n"), (r"\f", "\f"), (r"\r", "\r")))

    def __init__(
        self,
        quote_char: str = "",
        esc_char: typing.Optional[str] = None,
        esc_quote: typing.Optional[str] = None,
        multiline: bool = False,
        unquote_results: bool = True,
        end_quote_char: typing.Optional[str] = None,
        convert_whitespace_escapes: bool = True,
        *,
        quoteChar: str = "",
        escChar: typing.Optional[str] = None,
        escQuote: typing.Optional[str] = None,
        unquoteResults: bool = True,
        endQuoteChar: typing.Optional[str] = None,
        convertWhitespaceEscapes: bool = True,
    ):
        super().__init__()
        escChar = escChar or esc_char
        escQuote = escQuote or esc_quote
        unquoteResults = unquoteResults and unquote_results
        endQuoteChar = endQuoteChar or end_quote_char
        convertWhitespaceEscapes = (
            convertWhitespaceEscapes and convert_whitespace_escapes
        )
        quote_char = quoteChar or quote_char

        # remove white space from quote chars - wont work anyway
        quote_char = quote_char.strip()
        if not quote_char:
            raise ValueError("quote_char cannot be the empty string")

        if endQuoteChar is None:
            endQuoteChar = quote_char
        else:
            endQuoteChar = endQuoteChar.strip()
            if not endQuoteChar:
                raise ValueError("end_quote_char cannot be the empty string")

        self.quoteChar: str = quote_char
        self.quoteCharLen: int = len(quote_char)
        self.firstQuoteChar: str = quote_char[0]
        self.endQuoteChar: str = endQuoteChar
        self.endQuoteCharLen: int = len(endQuoteChar)
        self.escChar: str = escChar or ""
        self.escQuote: str = escQuote or ""
        self.unquoteResults: bool = unquoteResults
        self.convertWhitespaceEscapes: bool = convertWhitespaceEscapes
        self.multiline = multiline

        sep = ""
        inner_pattern = ""

        if escQuote:
            inner_pattern += rf"{sep}(?:{re.escape(escQuote)})"
            sep = "|"

        if escChar:
            inner_pattern += rf"{sep}(?:{re.escape(escChar)}.)"
            sep = "|"
            self.escCharReplacePattern = re.escape(escChar) + "(.)"

        if len(self.endQuoteChar) > 1:
            inner_pattern += (
                f"{sep}(?:"
                + "|".join(
                    f"(?:{re.escape(self.endQuoteChar[:i])}(?!{re.escape(self.endQuoteChar[i:])}))"
                    for i in range(len(self.endQuoteChar) - 1, 0, -1)
                )
                + ")"
            )
            sep = "|"

        self.flags = re.RegexFlag(0)

        if multiline:
            self.flags = re.MULTILINE | re.DOTALL
            inner_pattern += (
                rf"{sep}(?:[^{_escape_regex_range_chars(self.endQuoteChar[0])}"
                rf"{(_escape_regex_range_chars(escChar) if escChar is not None else '')}])"
            )
        else:
            inner_pattern += (
                rf"{sep}(?:[^{_escape_regex_range_chars(self.endQuoteChar[0])}\n\r"
                rf"{(_escape_regex_range_chars(escChar) if escChar is not None else '')}])"
            )

        self.pattern = "".join(
            [
                re.escape(self.quoteChar),
                "(?:",
                inner_pattern,
                ")*",
                re.escape(self.endQuoteChar),
            ]
        )

        if self.unquoteResults:
            if self.convertWhitespaceEscapes:
                self.unquote_scan_re = re.compile(
                    rf"({'|'.join(re.escape(k) for k in self.ws_map)})|({re.escape(self.escChar)}.)|(\n|.)",
                    flags=self.flags,
                )
            else:
                self.unquote_scan_re = re.compile(
                    rf"({re.escape(self.escChar)}.)|(\n|.)", flags=self.flags
                )

        try:
            self.re = re.compile(self.pattern, self.flags)
            self.reString = self.pattern
            self.re_match = self.re.match
        except re.error:
            raise ValueError(f"invalid pattern {self.pattern!r} passed to Regex")

        self.errmsg = "Expected " + self.name
        self.mayIndexError = False
        self.mayReturnEmpty = True

    def _generateDefaultName(self) -> str:
        if self.quoteChar == self.endQuoteChar and isinstance(self.quoteChar, str_type):
            return f"string enclosed in {self.quoteChar!r}"

        return f"quoted string, starting with {self.quoteChar} ending with {self.endQuoteChar}"

    def parseImpl(self, instring, loc, doActions=True):
        result = (
            instring[loc] == self.firstQuoteChar
            and self.re_match(instring, loc)
            or None
        )
        if not result:
            raise ParseException(instring, loc, self.errmsg, self)

        loc = result.end()
        ret = result.group()

        if self.unquoteResults:
            # strip off quotes
            ret = ret[self.quoteCharLen : -self.endQuoteCharLen]

            if isinstance(ret, str_type):
                if self.convertWhitespaceEscapes:
                    ret = "".join(
                        self.ws_map[match.group(1)]
                        if match.group(1)
                        else match.group(2)[-1]
                        if match.group(2)
                        else match.group(3)
                        for match in self.unquote_scan_re.finditer(ret)
                    )
                else:
                    ret = "".join(
                        match.group(1)[-1] if match.group(1) else match.group(2)
                        for match in self.unquote_scan_re.finditer(ret)
                    )

                # replace escaped quotes
                if self.escQuote:
                    ret = ret.replace(self.escQuote, self.endQuoteChar)

        return loc, ret


class CharsNotIn(Token):
    """Token for matching words composed of characters *not* in a given
    set (will include whitespace in matched characters if not listed in
    the provided exclusion set - see example). Defined with string
    containing all disallowed characters, and an optional minimum,
    maximum, and/or exact length.  The default value for ``min`` is
    1 (a minimum value < 1 is not valid); the default values for
    ``max`` and ``exact`` are 0, meaning no maximum or exact
    length restriction.

    Example::

        # define a comma-separated-value as anything that is not a ','
        csv_value = CharsNotIn(',')
        print(DelimitedList(csv_value).parse_string("dkls,lsdkjf,s12 34,@!#,213"))

    prints::

        ['dkls', 'lsdkjf', 's12 34', '@!#', '213']
    """

    def __init__(
        self,
        not_chars: str = "",
        min: int = 1,
        max: int = 0,
        exact: int = 0,
        *,
        notChars: str = "",
    ):
        super().__init__()
        self.skipWhitespace = False
        self.notChars = not_chars or notChars
        self.notCharsSet = set(self.notChars)

        if min < 1:
            raise ValueError(
                "cannot specify a minimum length < 1; use "
                "Opt(CharsNotIn()) if zero-length char group is permitted"
            )

        self.minLen = min

        if max > 0:
            self.maxLen = max
        else:
            self.maxLen = _MAX_INT

        if exact > 0:
            self.maxLen = exact
            self.minLen = exact

        self.errmsg = "Expected " + self.name
        self.mayReturnEmpty = self.minLen == 0
        self.mayIndexError = False

    def _generateDefaultName(self) -> str:
        not_chars_str = _collapse_string_to_ranges(self.notChars)
        if len(not_chars_str) > 16:
            return f"!W:({self.notChars[: 16 - 3]}...)"
        else:
            return f"!W:({self.notChars})"

    def parseImpl(self, instring, loc, doActions=True):
        notchars = self.notCharsSet
        if instring[loc] in notchars:
            raise ParseException(instring, loc, self.errmsg, self)

        start = loc
        loc += 1
        maxlen = min(start + self.maxLen, len(instring))
        while loc < maxlen and instring[loc] not in notchars:
            loc += 1

        if loc - start < self.minLen:
            raise ParseException(instring, loc, self.errmsg, self)

        return loc, instring[start:loc]


class White(Token):
    """Special matching class for matching whitespace.  Normally,
    whitespace is ignored by pyparsing grammars.  This class is included
    when some whitespace structures are significant.  Define with
    a string containing the whitespace characters to be matched; default
    is ``" \\t\\r\\n"``.  Also takes optional ``min``,
    ``max``, and ``exact`` arguments, as defined for the
    :class:`Word` class.
    """

    whiteStrs = {
        " ": "<SP>",
        "\t": "<TAB>",
        "\n": "<LF>",
        "\r": "<CR>",
        "\f": "<FF>",
        "\u00A0": "<NBSP>",
        "\u1680": "<OGHAM_SPACE_MARK>",
        "\u180E": "<MONGOLIAN_VOWEL_SEPARATOR>",
        "\u2000": "<EN_QUAD>",
        "\u2001": "<EM_QUAD>",
        "\u2002": "<EN_SPACE>",
        "\u2003": "<EM_SPACE>",
        "\u2004": "<THREE-PER-EM_SPACE>",
        "\u2005": "<FOUR-PER-EM_SPACE>",
        "\u2006": "<SIX-PER-EM_SPACE>",
        "\u2007": "<FIGURE_SPACE>",
        "\u2008": "<PUNCTUATION_SPACE>",
        "\u2009": "<THIN_SPACE>",
        "\u200A": "<HAIR_SPACE>",
        "\u200B": "<ZERO_WIDTH_SPACE>",
        "\u202F": "<NNBSP>",
        "\u205F": "<MMSP>",
        "\u3000": "<IDEOGRAPHIC_SPACE>",
    }

    def __init__(self, ws: str = " \t\r\n", min: int = 1, max: int = 0, exact: int = 0):
        super().__init__()
        self.matchWhite = ws
        self.set_whitespace_chars(
            "".join(c for c in self.whiteStrs if c not in self.matchWhite),
            copy_defaults=True,
        )
        # self.leave_whitespace()
        self.mayReturnEmpty = True
        self.errmsg = "Expected " + self.name

        self.minLen = min

        if max > 0:
            self.maxLen = max
        else:
            self.maxLen = _MAX_INT

        if exact > 0:
            self.maxLen = exact
            self.minLen = exact

    def _generateDefaultName(self) -> str:
        return "".join(White.whiteStrs[c] for c in self.matchWhite)

    def parseImpl(self, instring, loc, doActions=True):
        if instring[loc] not in self.matchWhite:
            raise ParseException(instring, loc, self.errmsg, self)
        start = loc
        loc += 1
        maxloc = start + self.maxLen
        maxloc = min(maxloc, len(instring))
        while loc < maxloc and instring[loc] in self.matchWhite:
            loc += 1

        if loc - start < self.minLen:
            raise ParseException(instring, loc, self.errmsg, self)

        return loc, instring[start:loc]


class PositionToken(Token):
    def __init__(self):
        super().__init__()
        self.mayReturnEmpty = True
        self.mayIndexError = False


class GoToColumn(PositionToken):
    """Token to advance to a specific column of input text; useful for
    tabular report scraping.
    """

    def __init__(self, colno: int):
        super().__init__()
        self.col = colno

    def preParse(self, instring: str, loc: int) -> int:
        if col(loc, instring) != self.col:
            instrlen = len(instring)
            if self.ignoreExprs:
                loc = self._skipIgnorables(instring, loc)
            while (
                loc < instrlen
                and instring[loc].isspace()
                and col(loc, instring) != self.col
            ):
                loc += 1
        return loc

    def parseImpl(self, instring, loc, doActions=True):
        thiscol = col(loc, instring)
        if thiscol > self.col:
            raise ParseException(instring, loc, "Text not in expected column", self)
        newloc = loc + self.col - thiscol
        ret = instring[loc:newloc]
        return newloc, ret


class LineStart(PositionToken):
    r"""Matches if current position is at the beginning of a line within
    the parse string

    Example::

        test = '''\
        AAA this line
        AAA and this line
          AAA but not this one
        B AAA and definitely not this one
        '''

        for t in (LineStart() + 'AAA' + rest_of_line).search_string(test):
            print(t)

    prints::

        ['AAA', ' this line']
        ['AAA', ' and this line']

    """

    def __init__(self):
        super().__init__()
        self.leave_whitespace()
        self.orig_whiteChars = set() | self.whiteChars
        self.whiteChars.discard("\n")
        self.skipper = Empty().set_whitespace_chars(self.whiteChars)
        self.errmsg = "Expected start of line"

    def preParse(self, instring: str, loc: int) -> int:
        if loc == 0:
            return loc
        else:
            ret = self.skipper.preParse(instring, loc)
            if "\n" in self.orig_whiteChars:
                while instring[ret : ret + 1] == "\n":
                    ret = self.skipper.preParse(instring, ret + 1)
            return ret

    def parseImpl(self, instring, loc, doActions=True):
        if col(loc, instring) == 1:
            return loc, []
        raise ParseException(instring, loc, self.errmsg, self)


class LineEnd(PositionToken):
    """Matches if current position is at the end of a line within the
    parse string
    """

    def __init__(self):
        super().__init__()
        self.whiteChars.discard("\n")
        self.set_whitespace_chars(self.whiteChars, copy_defaults=False)
        self.errmsg = "Expected end of line"

    def parseImpl(self, instring, loc, doActions=True):
        if loc < len(instring):
            if instring[loc] == "\n":
                return loc + 1, "\n"
            else:
                raise ParseException(instring, loc, self.errmsg, self)
        elif loc == len(instring):
            return loc + 1, []
        else:
            raise ParseException(instring, loc, self.errmsg, self)


class StringStart(PositionToken):
    """Matches if current position is at the beginning of the parse
    string
    """

    def __init__(self):
        super().__init__()
        self.errmsg = "Expected start of text"

    def parseImpl(self, instring, loc, doActions=True):
        if loc != 0:
            # see if entire string up to here is just whitespace and ignoreables
            if loc != self.preParse(instring, 0):
                raise ParseException(instring, loc, self.errmsg, self)
        return loc, []


class StringEnd(PositionToken):
    """
    Matches if current position is at the end of the parse string
    """

    def __init__(self):
        super().__init__()
        self.errmsg = "Expected end of text"

    def parseImpl(self, instring, loc, doActions=True):
        if loc < len(instring):
            raise ParseException(instring, loc, self.errmsg, self)
        elif loc == len(instring):
            return loc + 1, []
        elif loc > len(instring):
            return loc, []
        else:
            raise ParseException(instring, loc, self.errmsg, self)


class WordStart(PositionToken):
    """Matches if the current position is at the beginning of a
    :class:`Word`, and is not preceded by any character in a given
    set of ``word_chars`` (default= ``printables``). To emulate the
    ``\b`` behavior of regular expressions, use
    ``WordStart(alphanums)``. ``WordStart`` will also match at
    the beginning of the string being parsed, or at the beginning of
    a line.
    """

    def __init__(self, word_chars: str = printables, *, wordChars: str = printables):
        wordChars = word_chars if wordChars == printables else wordChars
        super().__init__()
        self.wordChars = set(wordChars)
        self.errmsg = "Not at the start of a word"

    def parseImpl(self, instring, loc, doActions=True):
        if loc != 0:
            if (
                instring[loc - 1] in self.wordChars
                or instring[loc] not in self.wordChars
            ):
                raise ParseException(instring, loc, self.errmsg, self)
        return loc, []


class WordEnd(PositionToken):
    """Matches if the current position is at the end of a :class:`Word`,
    and is not followed by any character in a given set of ``word_chars``
    (default= ``printables``). To emulate the ``\b`` behavior of
    regular expressions, use ``WordEnd(alphanums)``. ``WordEnd``
    will also match at the end of the string being parsed, or at the end
    of a line.
    """

    def __init__(self, word_chars: str = printables, *, wordChars: str = printables):
        wordChars = word_chars if wordChars == printables else wordChars
        super().__init__()
        self.wordChars = set(wordChars)
        self.skipWhitespace = False
        self.errmsg = "Not at the end of a word"

    def parseImpl(self, instring, loc, doActions=True):
        instrlen = len(instring)
        if instrlen > 0 and loc < instrlen:
            if (
                instring[loc] in self.wordChars
                or instring[loc - 1] not in self.wordChars
            ):
                raise ParseException(instring, loc, self.errmsg, self)
        return loc, []


class ParseExpression(ParserElement):
    """Abstract subclass of ParserElement, for combining and
    post-processing parsed tokens.
    """

    def __init__(self, exprs: typing.Iterable[ParserElement], savelist: bool = False):
        super().__init__(savelist)
        self.exprs: List[ParserElement]
        if isinstance(exprs, _generatorType):
            exprs = list(exprs)

        if isinstance(exprs, str_type):
            self.exprs = [self._literalStringClass(exprs)]
        elif isinstance(exprs, ParserElement):
            self.exprs = [exprs]
        elif isinstance(exprs, Iterable):
            exprs = list(exprs)
            # if sequence of strings provided, wrap with Literal
            if any(isinstance(expr, str_type) for expr in exprs):
                exprs = (
                    self._literalStringClass(e) if isinstance(e, str_type) else e
                    for e in exprs
                )
            self.exprs = list(exprs)
        else:
            try:
                self.exprs = list(exprs)
            except TypeError:
                self.exprs = [exprs]
        self.callPreparse = False

    def recurse(self) -> List[ParserElement]:
        return self.exprs[:]

    def append(self, other) -> ParserElement:
        self.exprs.append(other)
        self._defaultName = None
        return self

    def leave_whitespace(self, recursive: bool = True) -> ParserElement:
        """
        Extends ``leave_whitespace`` defined in base class, and also invokes ``leave_whitespace`` on
           all contained expressions.
        """
        super().leave_whitespace(recursive)

        if recursive:
            self.exprs = [e.copy() for e in self.exprs]
            for e in self.exprs:
                e.leave_whitespace(recursive)
        return self

    def ignore_whitespace(self, recursive: bool = True) -> ParserElement:
        """
        Extends ``ignore_whitespace`` defined in base class, and also invokes ``leave_whitespace`` on
           all contained expressions.
        """
        super().ignore_whitespace(recursive)
        if recursive:
            self.exprs = [e.copy() for e in self.exprs]
            for e in self.exprs:
                e.ignore_whitespace(recursive)
        return self

    def ignore(self, other) -> ParserElement:
        if isinstance(other, Suppress):
            if other not in self.ignoreExprs:
                super().ignore(other)
                for e in self.exprs:
                    e.ignore(self.ignoreExprs[-1])
        else:
            super().ignore(other)
            for e in self.exprs:
                e.ignore(self.ignoreExprs[-1])
        return self

    def _generateDefaultName(self) -> str:
        return f"{self.__class__.__name__}:({str(self.exprs)})"

    def streamline(self) -> ParserElement:
        if self.streamlined:
            return self

        super().streamline()

        for e in self.exprs:
            e.streamline()

        # collapse nested :class:`And`'s of the form ``And(And(And(a, b), c), d)`` to ``And(a, b, c, d)``
        # but only if there are no parse actions or resultsNames on the nested And's
        # (likewise for :class:`Or`'s and :class:`MatchFirst`'s)
        if len(self.exprs) == 2:
            other = self.exprs[0]
            if (
                isinstance(other, self.__class__)
                and not other.parseAction
                and other.resultsName is None
                and not other.debug
            ):
                self.exprs = other.exprs[:] + [self.exprs[1]]
                self._defaultName = None
                self.mayReturnEmpty |= other.mayReturnEmpty
                self.mayIndexError |= other.mayIndexError

            other = self.exprs[-1]
            if (
                isinstance(other, self.__class__)
                and not other.parseAction
                and other.resultsName is None
                and not other.debug
            ):
                self.exprs = self.exprs[:-1] + other.exprs[:]
                self._defaultName = None
                self.mayReturnEmpty |= other.mayReturnEmpty
                self.mayIndexError |= other.mayIndexError

        self.errmsg = "Expected " + str(self)

        return self

    def validate(self, validateTrace=None) -> None:
        warnings.warn(
            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
            DeprecationWarning,
            stacklevel=2,
        )
        tmp = (validateTrace if validateTrace is not None else [])[:] + [self]
        for e in self.exprs:
            e.validate(tmp)
        self._checkRecursion([])

    def copy(self) -> ParserElement:
        ret = super().copy()
        ret = typing.cast(ParseExpression, ret)
        ret.exprs = [e.copy() for e in self.exprs]
        return ret

    def _setResultsName(self, name, listAllMatches=False):
        if (
            __diag__.warn_ungrouped_named_tokens_in_collection
            and Diagnostics.warn_ungrouped_named_tokens_in_collection
            not in self.suppress_warnings_
        ):
            for e in self.exprs:
                if (
                    isinstance(e, ParserElement)
                    and e.resultsName
                    and Diagnostics.warn_ungrouped_named_tokens_in_collection
                    not in e.suppress_warnings_
                ):
                    warnings.warn(
                        "{}: setting results name {!r} on {} expression "
                        "collides with {!r} on contained expression".format(
                            "warn_ungrouped_named_tokens_in_collection",
                            name,
                            type(self).__name__,
                            e.resultsName,
                        ),
                        stacklevel=3,
                    )

        return super()._setResultsName(name, listAllMatches)

    # Compatibility synonyms
    # fmt: off
    @replaced_by_pep8(leave_whitespace)
    def leaveWhitespace(self): ...

    @replaced_by_pep8(ignore_whitespace)
    def ignoreWhitespace(self): ...
    # fmt: on


class And(ParseExpression):
    """
    Requires all given :class:`ParseExpression` s to be found in the given order.
    Expressions may be separated by whitespace.
    May be constructed using the ``'+'`` operator.
    May also be constructed using the ``'-'`` operator, which will
    suppress backtracking.

    Example::

        integer = Word(nums)
        name_expr = Word(alphas)[1, ...]

        expr = And([integer("id"), name_expr("name"), integer("age")])
        # more easily written as:
        expr = integer("id") + name_expr("name") + integer("age")
    """

    class _ErrorStop(Empty):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.leave_whitespace()

        def _generateDefaultName(self) -> str:
            return "-"

    def __init__(
        self, exprs_arg: typing.Iterable[ParserElement], savelist: bool = True
    ):
        exprs: List[ParserElement] = list(exprs_arg)
        if exprs and Ellipsis in exprs:
            tmp = []
            for i, expr in enumerate(exprs):
                if expr is Ellipsis:
                    if i < len(exprs) - 1:
                        skipto_arg: ParserElement = typing.cast(
                            ParseExpression, (Empty() + exprs[i + 1])
                        ).exprs[-1]
                        tmp.append(SkipTo(skipto_arg)("_skipped*"))
                    else:
                        raise Exception(
                            "cannot construct And with sequence ending in ..."
                        )
                else:
                    tmp.append(expr)
            exprs[:] = tmp
        super().__init__(exprs, savelist)
        if self.exprs:
            self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)
            if not isinstance(self.exprs[0], White):
                self.set_whitespace_chars(
                    self.exprs[0].whiteChars,
                    copy_defaults=self.exprs[0].copyDefaultWhiteChars,
                )
                self.skipWhitespace = self.exprs[0].skipWhitespace
            else:
                self.skipWhitespace = False
        else:
            self.mayReturnEmpty = True
        self.callPreparse = True

    def streamline(self) -> ParserElement:
        # collapse any _PendingSkip's
        if self.exprs:
            if any(
                isinstance(e, ParseExpression)
                and e.exprs
                and isinstance(e.exprs[-1], _PendingSkip)
                for e in self.exprs[:-1]
            ):
                deleted_expr_marker = NoMatch()
                for i, e in enumerate(self.exprs[:-1]):
                    if e is deleted_expr_marker:
                        continue
                    if (
                        isinstance(e, ParseExpression)
                        and e.exprs
                        and isinstance(e.exprs[-1], _PendingSkip)
                    ):
                        e.exprs[-1] = e.exprs[-1] + self.exprs[i + 1]
                        self.exprs[i + 1] = deleted_expr_marker
                self.exprs = [e for e in self.exprs if e is not deleted_expr_marker]

        super().streamline()

        # link any IndentedBlocks to the prior expression
        prev: ParserElement
        cur: ParserElement
        for prev, cur in zip(self.exprs, self.exprs[1:]):
            # traverse cur or any first embedded expr of cur looking for an IndentedBlock
            # (but watch out for recursive grammar)
            seen = set()
            while True:
                if id(cur) in seen:
                    break
                seen.add(id(cur))
                if isinstance(cur, IndentedBlock):
                    prev.add_parse_action(
                        lambda s, l, t, cur_=cur: setattr(
                            cur_, "parent_anchor", col(l, s)
                        )
                    )
                    break
                subs = cur.recurse()
                next_first = next(iter(subs), None)
                if next_first is None:
                    break
                cur = typing.cast(ParserElement, next_first)

        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)
        return self

    def parseImpl(self, instring, loc, doActions=True):
        # pass False as callPreParse arg to _parse for first element, since we already
        # pre-parsed the string as part of our And pre-parsing
        loc, resultlist = self.exprs[0]._parse(
            instring, loc, doActions, callPreParse=False
        )
        errorStop = False
        for e in self.exprs[1:]:
            # if isinstance(e, And._ErrorStop):
            if type(e) is And._ErrorStop:
                errorStop = True
                continue
            if errorStop:
                try:
                    loc, exprtokens = e._parse(instring, loc, doActions)
                except ParseSyntaxException:
                    raise
                except ParseBaseException as pe:
                    pe.__traceback__ = None
                    raise ParseSyntaxException._from_exception(pe)
                except IndexError:
                    raise ParseSyntaxException(
                        instring, len(instring), self.errmsg, self
                    )
            else:
                loc, exprtokens = e._parse(instring, loc, doActions)
            resultlist += exprtokens
        return loc, resultlist

    def __iadd__(self, other):
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return self.append(other)  # And([self, other])

    def _checkRecursion(self, parseElementList):
        subRecCheckList = parseElementList[:] + [self]
        for e in self.exprs:
            e._checkRecursion(subRecCheckList)
            if not e.mayReturnEmpty:
                break

    def _generateDefaultName(self) -> str:
        inner = " ".join(str(e) for e in self.exprs)
        # strip off redundant inner {}'s
        while len(inner) > 1 and inner[0 :: len(inner) - 1] == "{}":
            inner = inner[1:-1]
        return "{" + inner + "}"


class Or(ParseExpression):
    """Requires that at least one :class:`ParseExpression` is found. If
    two expressions match, the expression that matches the longest
    string will be used. May be constructed using the ``'^'``
    operator.

    Example::

        # construct Or using '^' operator

        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))
        print(number.search_string("123 3.1416 789"))

    prints::

        [['123'], ['3.1416'], ['789']]
    """

    def __init__(self, exprs: typing.Iterable[ParserElement], savelist: bool = False):
        super().__init__(exprs, savelist)
        if self.exprs:
            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)
            self.skipWhitespace = all(e.skipWhitespace for e in self.exprs)
        else:
            self.mayReturnEmpty = True

    def streamline(self) -> ParserElement:
        super().streamline()
        if self.exprs:
            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)
            self.saveAsList = any(e.saveAsList for e in self.exprs)
            self.skipWhitespace = all(
                e.skipWhitespace and not isinstance(e, White) for e in self.exprs
            )
        else:
            self.saveAsList = False
        return self

    def parseImpl(self, instring, loc, doActions=True):
        maxExcLoc = -1
        maxException = None
        matches = []
        fatals = []
        if all(e.callPreparse for e in self.exprs):
            loc = self.preParse(instring, loc)
        for e in self.exprs:
            try:
                loc2 = e.try_parse(instring, loc, raise_fatal=True)
            except ParseFatalException as pfe:
                pfe.__traceback__ = None
                pfe.parser_element = e
                fatals.append(pfe)
                maxException = None
                maxExcLoc = -1
            except ParseException as err:
                if not fatals:
                    err.__traceback__ = None
                    if err.loc > maxExcLoc:
                        maxException = err
                        maxExcLoc = err.loc
            except IndexError:
                if len(instring) > maxExcLoc:
                    maxException = ParseException(
                        instring, len(instring), e.errmsg, self
                    )
                    maxExcLoc = len(instring)
            else:
                # save match among all matches, to retry longest to shortest
                matches.append((loc2, e))

        if matches:
            # re-evaluate all matches in descending order of length of match, in case attached actions
            # might change whether or how much they match of the input.
            matches.sort(key=itemgetter(0), reverse=True)

            if not doActions:
                # no further conditions or parse actions to change the selection of
                # alternative, so the first match will be the best match
                best_expr = matches[0][1]
                return best_expr._parse(instring, loc, doActions)

            longest = -1, None
            for loc1, expr1 in matches:
                if loc1 <= longest[0]:
                    # already have a longer match than this one will deliver, we are done
                    return longest

                try:
                    loc2, toks = expr1._parse(instring, loc, doActions)
                except ParseException as err:
                    err.__traceback__ = None
                    if err.loc > maxExcLoc:
                        maxException = err
                        maxExcLoc = err.loc
                else:
                    if loc2 >= loc1:
                        return loc2, toks
                    # didn't match as much as before
                    elif loc2 > longest[0]:
                        longest = loc2, toks

            if longest != (-1, None):
                return longest

        if fatals:
            if len(fatals) > 1:
                fatals.sort(key=lambda e: -e.loc)
                if fatals[0].loc == fatals[1].loc:
                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parser_element))))
            max_fatal = fatals[0]
            raise max_fatal

        if maxException is not None:
            # infer from this check that all alternatives failed at the current position
            # so emit this collective error message instead of any single error message
            if maxExcLoc == loc:
                maxException.msg = self.errmsg
            raise maxException
        else:
            raise ParseException(
                instring, loc, "no defined alternatives to match", self
            )

    def __ixor__(self, other):
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return self.append(other)  # Or([self, other])

    def _generateDefaultName(self) -> str:
        return "{" + " ^ ".join(str(e) for e in self.exprs) + "}"

    def _setResultsName(self, name, listAllMatches=False):
        if (
            __diag__.warn_multiple_tokens_in_named_alternation
            and Diagnostics.warn_multiple_tokens_in_named_alternation
            not in self.suppress_warnings_
        ):
            if any(
                isinstance(e, And)
                and Diagnostics.warn_multiple_tokens_in_named_alternation
                not in e.suppress_warnings_
                for e in self.exprs
            ):
                warnings.warn(
                    "{}: setting results name {!r} on {} expression "
                    "will return a list of all parsed tokens in an And alternative, "
                    "in prior versions only the first token was returned; enclose "
                    "contained argument in Group".format(
                        "warn_multiple_tokens_in_named_alternation",
                        name,
                        type(self).__name__,
                    ),
                    stacklevel=3,
                )

        return super()._setResultsName(name, listAllMatches)


class MatchFirst(ParseExpression):
    """Requires that at least one :class:`ParseExpression` is found. If
    more than one expression matches, the first one listed is the one that will
    match. May be constructed using the ``'|'`` operator.

    Example::

        # construct MatchFirst using '|' operator

        # watch the order of expressions to match
        number = Word(nums) | Combine(Word(nums) + '.' + Word(nums))
        print(number.search_string("123 3.1416 789")) #  Fail! -> [['123'], ['3'], ['1416'], ['789']]

        # put more selective expression first
        number = Combine(Word(nums) + '.' + Word(nums)) | Word(nums)
        print(number.search_string("123 3.1416 789")) #  Better -> [['123'], ['3.1416'], ['789']]
    """

    def __init__(self, exprs: typing.Iterable[ParserElement], savelist: bool = False):
        super().__init__(exprs, savelist)
        if self.exprs:
            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)
            self.skipWhitespace = all(e.skipWhitespace for e in self.exprs)
        else:
            self.mayReturnEmpty = True

    def streamline(self) -> ParserElement:
        if self.streamlined:
            return self

        super().streamline()
        if self.exprs:
            self.saveAsList = any(e.saveAsList for e in self.exprs)
            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)
            self.skipWhitespace = all(
                e.skipWhitespace and not isinstance(e, White) for e in self.exprs
            )
        else:
            self.saveAsList = False
            self.mayReturnEmpty = True
        return self

    def parseImpl(self, instring, loc, doActions=True):
        maxExcLoc = -1
        maxException = None

        for e in self.exprs:
            try:
                return e._parse(
                    instring,
                    loc,
                    doActions,
                )
            except ParseFatalException as pfe:
                pfe.__traceback__ = None
                pfe.parser_element = e
                raise
            except ParseException as err:
                if err.loc > maxExcLoc:
                    maxException = err
                    maxExcLoc = err.loc
            except IndexError:
                if len(instring) > maxExcLoc:
                    maxException = ParseException(
                        instring, len(instring), e.errmsg, self
                    )
                    maxExcLoc = len(instring)

        if maxException is not None:
            # infer from this check that all alternatives failed at the current position
            # so emit this collective error message instead of any individual error message
            if maxExcLoc == loc:
                maxException.msg = self.errmsg
            raise maxException
        else:
            raise ParseException(
                instring, loc, "no defined alternatives to match", self
            )

    def __ior__(self, other):
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return self.append(other)  # MatchFirst([self, other])

    def _generateDefaultName(self) -> str:
        return "{" + " | ".join(str(e) for e in self.exprs) + "}"

    def _setResultsName(self, name, listAllMatches=False):
        if (
            __diag__.warn_multiple_tokens_in_named_alternation
            and Diagnostics.warn_multiple_tokens_in_named_alternation
            not in self.suppress_warnings_
        ):
            if any(
                isinstance(e, And)
                and Diagnostics.warn_multiple_tokens_in_named_alternation
                not in e.suppress_warnings_
                for e in self.exprs
            ):
                warnings.warn(
                    "{}: setting results name {!r} on {} expression "
                    "will return a list of all parsed tokens in an And alternative, "
                    "in prior versions only the first token was returned; enclose "
                    "contained argument in Group".format(
                        "warn_multiple_tokens_in_named_alternation",
                        name,
                        type(self).__name__,
                    ),
                    stacklevel=3,
                )

        return super()._setResultsName(name, listAllMatches)


class Each(ParseExpression):
    """Requires all given :class:`ParseExpression` s to be found, but in
    any order. Expressions may be separated by whitespace.

    May be constructed using the ``'&'`` operator.

    Example::

        color = one_of("RED ORANGE YELLOW GREEN BLUE PURPLE BLACK WHITE BROWN")
        shape_type = one_of("SQUARE CIRCLE TRIANGLE STAR HEXAGON OCTAGON")
        integer = Word(nums)
        shape_attr = "shape:" + shape_type("shape")
        posn_attr = "posn:" + Group(integer("x") + ',' + integer("y"))("posn")
        color_attr = "color:" + color("color")
        size_attr = "size:" + integer("size")

        # use Each (using operator '&') to accept attributes in any order
        # (shape and posn are required, color and size are optional)
        shape_spec = shape_attr & posn_attr & Opt(color_attr) & Opt(size_attr)

        shape_spec.run_tests('''
            shape: SQUARE color: BLACK posn: 100, 120
            shape: CIRCLE size: 50 color: BLUE posn: 50,80
            color:GREEN size:20 shape:TRIANGLE posn:20,40
            '''
            )

    prints::

        shape: SQUARE color: BLACK posn: 100, 120
        ['shape:', 'SQUARE', 'color:', 'BLACK', 'posn:', ['100', ',', '120']]
        - color: BLACK
        - posn: ['100', ',', '120']
          - x: 100
          - y: 120
        - shape: SQUARE


        shape: CIRCLE size: 50 color: BLUE posn: 50,80
        ['shape:', 'CIRCLE', 'size:', '50', 'color:', 'BLUE', 'posn:', ['50', ',', '80']]
        - color: BLUE
        - posn: ['50', ',', '80']
          - x: 50
          - y: 80
        - shape: CIRCLE
        - size: 50


        color: GREEN size: 20 shape: TRIANGLE posn: 20,40
        ['color:', 'GREEN', 'size:', '20', 'shape:', 'TRIANGLE', 'posn:', ['20', ',', '40']]
        - color: GREEN
        - posn: ['20', ',', '40']
          - x: 20
          - y: 40
        - shape: TRIANGLE
        - size: 20
    """

    def __init__(self, exprs: typing.Iterable[ParserElement], savelist: bool = True):
        super().__init__(exprs, savelist)
        if self.exprs:
            self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)
        else:
            self.mayReturnEmpty = True
        self.skipWhitespace = True
        self.initExprGroups = True
        self.saveAsList = True

    def __iand__(self, other):
        if isinstance(other, str_type):
            other = self._literalStringClass(other)
        if not isinstance(other, ParserElement):
            return NotImplemented
        return self.append(other)  # Each([self, other])

    def streamline(self) -> ParserElement:
        super().streamline()
        if self.exprs:
            self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)
        else:
            self.mayReturnEmpty = True
        return self

    def parseImpl(self, instring, loc, doActions=True):
        if self.initExprGroups:
            self.opt1map = dict(
                (id(e.expr), e) for e in self.exprs if isinstance(e, Opt)
            )
            opt1 = [e.expr for e in self.exprs if isinstance(e, Opt)]
            opt2 = [
                e
                for e in self.exprs
                if e.mayReturnEmpty and not isinstance(e, (Opt, Regex, ZeroOrMore))
            ]
            self.optionals = opt1 + opt2
            self.multioptionals = [
                e.expr.set_results_name(e.resultsName, list_all_matches=True)
                for e in self.exprs
                if isinstance(e, _MultipleMatch)
            ]
            self.multirequired = [
                e.expr.set_results_name(e.resultsName, list_all_matches=True)
                for e in self.exprs
                if isinstance(e, OneOrMore)
            ]
            self.required = [
                e for e in self.exprs if not isinstance(e, (Opt, ZeroOrMore, OneOrMore))
            ]
            self.required += self.multirequired
            self.initExprGroups = False

        tmpLoc = loc
        tmpReqd = self.required[:]
        tmpOpt = self.optionals[:]
        multis = self.multioptionals[:]
        matchOrder = []

        keepMatching = True
        failed = []
        fatals = []
        while keepMatching:
            tmpExprs = tmpReqd + tmpOpt + multis
            failed.clear()
            fatals.clear()
            for e in tmpExprs:
                try:
                    tmpLoc = e.try_parse(instring, tmpLoc, raise_fatal=True)
                except ParseFatalException as pfe:
                    pfe.__traceback__ = None
                    pfe.parser_element = e
                    fatals.append(pfe)
                    failed.append(e)
                except ParseException:
                    failed.append(e)
                else:
                    matchOrder.append(self.opt1map.get(id(e), e))
                    if e in tmpReqd:
                        tmpReqd.remove(e)
                    elif e in tmpOpt:
                        tmpOpt.remove(e)
            if len(failed) == len(tmpExprs):
                keepMatching = False

        # look for any ParseFatalExceptions
        if fatals:
            if len(fatals) > 1:
                fatals.sort(key=lambda e: -e.loc)
                if fatals[0].loc == fatals[1].loc:
                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parser_element))))
            max_fatal = fatals[0]
            raise max_fatal

        if tmpReqd:
            missing = ", ".join([str(e) for e in tmpReqd])
            raise ParseException(
                instring,
                loc,
                f"Missing one or more required elements ({missing})",
            )

        # add any unmatched Opts, in case they have default values defined
        matchOrder += [e for e in self.exprs if isinstance(e, Opt) and e.expr in tmpOpt]

        total_results = ParseResults([])
        for e in matchOrder:
            loc, results = e._parse(instring, loc, doActions)
            total_results += results

        return loc, total_results

    def _generateDefaultName(self) -> str:
        return "{" + " & ".join(str(e) for e in self.exprs) + "}"


class ParseElementEnhance(ParserElement):
    """Abstract subclass of :class:`ParserElement`, for combining and
    post-processing parsed tokens.
    """

    def __init__(self, expr: Union[ParserElement, str], savelist: bool = False):
        super().__init__(savelist)
        if isinstance(expr, str_type):
            expr_str = typing.cast(str, expr)
            if issubclass(self._literalStringClass, Token):
                expr = self._literalStringClass(expr_str)  # type: ignore[call-arg]
            elif issubclass(type(self), self._literalStringClass):
                expr = Literal(expr_str)
            else:
                expr = self._literalStringClass(Literal(expr_str))  # type: ignore[assignment, call-arg]
        expr = typing.cast(ParserElement, expr)
        self.expr = expr
        if expr is not None:
            self.mayIndexError = expr.mayIndexError
            self.mayReturnEmpty = expr.mayReturnEmpty
            self.set_whitespace_chars(
                expr.whiteChars, copy_defaults=expr.copyDefaultWhiteChars
            )
            self.skipWhitespace = expr.skipWhitespace
            self.saveAsList = expr.saveAsList
            self.callPreparse = expr.callPreparse
            self.ignoreExprs.extend(expr.ignoreExprs)

    def recurse(self) -> List[ParserElement]:
        return [self.expr] if self.expr is not None else []

    def parseImpl(self, instring, loc, doActions=True):
        if self.expr is not None:
            try:
                return self.expr._parse(instring, loc, doActions, callPreParse=False)
            except ParseBaseException as pbe:
                pbe.msg = self.errmsg
                raise
        else:
            raise ParseException(instring, loc, "No expression defined", self)

    def leave_whitespace(self, recursive: bool = True) -> ParserElement:
        super().leave_whitespace(recursive)

        if recursive:
            if self.expr is not None:
                self.expr = self.expr.copy()
                self.expr.leave_whitespace(recursive)
        return self

    def ignore_whitespace(self, recursive: bool = True) -> ParserElement:
        super().ignore_whitespace(recursive)

        if recursive:
            if self.expr is not None:
                self.expr = self.expr.copy()
                self.expr.ignore_whitespace(recursive)
        return self

    def ignore(self, other) -> ParserElement:
        if isinstance(other, Suppress):
            if other not in self.ignoreExprs:
                super().ignore(other)
                if self.expr is not None:
                    self.expr.ignore(self.ignoreExprs[-1])
        else:
            super().ignore(other)
            if self.expr is not None:
                self.expr.ignore(self.ignoreExprs[-1])
        return self

    def streamline(self) -> ParserElement:
        super().streamline()
        if self.expr is not None:
            self.expr.streamline()
        return self

    def _checkRecursion(self, parseElementList):
        if self in parseElementList:
            raise RecursiveGrammarException(parseElementList + [self])
        subRecCheckList = parseElementList[:] + [self]
        if self.expr is not None:
            self.expr._checkRecursion(subRecCheckList)

    def validate(self, validateTrace=None) -> None:
        warnings.warn(
            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
            DeprecationWarning,
            stacklevel=2,
        )
        if validateTrace is None:
            validateTrace = []
        tmp = validateTrace[:] + [self]
        if self.expr is not None:
            self.expr.validate(tmp)
        self._checkRecursion([])

    def _generateDefaultName(self) -> str:
        return f"{self.__class__.__name__}:({str(self.expr)})"

    # Compatibility synonyms
    # fmt: off
    @replaced_by_pep8(leave_whitespace)
    def leaveWhitespace(self): ...

    @replaced_by_pep8(ignore_whitespace)
    def ignoreWhitespace(self): ...
    # fmt: on


class IndentedBlock(ParseElementEnhance):
    """
    Expression to match one or more expressions at a given indentation level.
    Useful for parsing text where structure is implied by indentation (like Python source code).
    """

    class _Indent(Empty):
        def __init__(self, ref_col: int):
            super().__init__()
            self.errmsg = f"expected indent at column {ref_col}"
            self.add_condition(lambda s, l, t: col(l, s) == ref_col)

    class _IndentGreater(Empty):
        def __init__(self, ref_col: int):
            super().__init__()
            self.errmsg = f"expected indent at column greater than {ref_col}"
            self.add_condition(lambda s, l, t: col(l, s) > ref_col)

    def __init__(
        self, expr: ParserElement, *, recursive: bool = False, grouped: bool = True
    ):
        super().__init__(expr, savelist=True)
        # if recursive:
        #     raise NotImplementedError("IndentedBlock with recursive is not implemented")
        self._recursive = recursive
        self._grouped = grouped
        self.parent_anchor = 1

    def parseImpl(self, instring, loc, doActions=True):
        # advance parse position to non-whitespace by using an Empty()
        # this should be the column to be used for all subsequent indented lines
        anchor_loc = Empty().preParse(instring, loc)

        # see if self.expr matches at the current location - if not it will raise an exception
        # and no further work is necessary
        self.expr.try_parse(instring, anchor_loc, do_actions=doActions)

        indent_col = col(anchor_loc, instring)
        peer_detect_expr = self._Indent(indent_col)

        inner_expr = Empty() + peer_detect_expr + self.expr
        if self._recursive:
            sub_indent = self._IndentGreater(indent_col)
            nested_block = IndentedBlock(
                self.expr, recursive=self._recursive, grouped=self._grouped
            )
            nested_block.set_debug(self.debug)
            nested_block.parent_anchor = indent_col
            inner_expr += Opt(sub_indent + nested_block)

        inner_expr.set_name(f"inner {hex(id(inner_expr))[-4:].upper()}@{indent_col}")
        block = OneOrMore(inner_expr)

        trailing_undent = self._Indent(self.parent_anchor) | StringEnd()

        if self._grouped:
            wrapper = Group
        else:
            wrapper = lambda expr: expr
        return (wrapper(block) + Optional(trailing_undent)).parseImpl(
            instring, anchor_loc, doActions
        )


class AtStringStart(ParseElementEnhance):
    """Matches if expression matches at the beginning of the parse
    string::

        AtStringStart(Word(nums)).parse_string("123")
        # prints ["123"]

        AtStringStart(Word(nums)).parse_string("    123")
        # raises ParseException
    """

    def __init__(self, expr: Union[ParserElement, str]):
        super().__init__(expr)
        self.callPreparse = False

    def parseImpl(self, instring, loc, doActions=True):
        if loc != 0:
            raise ParseException(instring, loc, "not found at string start")
        return super().parseImpl(instring, loc, doActions)


class AtLineStart(ParseElementEnhance):
    r"""Matches if an expression matches at the beginning of a line within
    the parse string

    Example::

        test = '''\
        AAA this line
        AAA and this line
          AAA but not this one
        B AAA and definitely not this one
        '''

        for t in (AtLineStart('AAA') + rest_of_line).search_string(test):
            print(t)

    prints::

        ['AAA', ' this line']
        ['AAA', ' and this line']

    """

    def __init__(self, expr: Union[ParserElement, str]):
        super().__init__(expr)
        self.callPreparse = False

    def parseImpl(self, instring, loc, doActions=True):
        if col(loc, instring) != 1:
            raise ParseException(instring, loc, "not found at line start")
        return super().parseImpl(instring, loc, doActions)


class FollowedBy(ParseElementEnhance):
    """Lookahead matching of the given parse expression.
    ``FollowedBy`` does *not* advance the parsing position within
    the input string, it only verifies that the specified parse
    expression matches at the current position.  ``FollowedBy``
    always returns a null token list. If any results names are defined
    in the lookahead expression, those *will* be returned for access by
    name.

    Example::

        # use FollowedBy to match a label only if it is followed by a ':'
        data_word = Word(alphas)
        label = data_word + FollowedBy(':')
        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join))

        attr_expr[1, ...].parse_string("shape: SQUARE color: BLACK posn: upper left").pprint()

    prints::

        [['shape', 'SQUARE'], ['color', 'BLACK'], ['posn', 'upper left']]
    """

    def __init__(self, expr: Union[ParserElement, str]):
        super().__init__(expr)
        self.mayReturnEmpty = True

    def parseImpl(self, instring, loc, doActions=True):
        # by using self._expr.parse and deleting the contents of the returned ParseResults list
        # we keep any named results that were defined in the FollowedBy expression
        _, ret = self.expr._parse(instring, loc, doActions=doActions)
        del ret[:]

        return loc, ret


class PrecededBy(ParseElementEnhance):
    """Lookbehind matching of the given parse expression.
    ``PrecededBy`` does not advance the parsing position within the
    input string, it only verifies that the specified parse expression
    matches prior to the current position.  ``PrecededBy`` always
    returns a null token list, but if a results name is defined on the
    given expression, it is returned.

    Parameters:

    - ``expr`` - expression that must match prior to the current parse
      location
    - ``retreat`` - (default= ``None``) - (int) maximum number of characters
      to lookbehind prior to the current parse location

    If the lookbehind expression is a string, :class:`Literal`,
    :class:`Keyword`, or a :class:`Word` or :class:`CharsNotIn`
    with a specified exact or maximum length, then the retreat
    parameter is not required. Otherwise, retreat must be specified to
    give a maximum number of characters to look back from
    the current parse position for a lookbehind match.

    Example::

        # VB-style variable names with type prefixes
        int_var = PrecededBy("#") + pyparsing_common.identifier
        str_var = PrecededBy("$") + pyparsing_common.identifier

    """

    def __init__(
        self, expr: Union[ParserElement, str], retreat: typing.Optional[int] = None
    ):
        super().__init__(expr)
        self.expr = self.expr().leave_whitespace()
        self.mayReturnEmpty = True
        self.mayIndexError = False
        self.exact = False
        if isinstance(expr, str_type):
            expr = typing.cast(str, expr)
            retreat = len(expr)
            self.exact = True
        elif isinstance(expr, (Literal, Keyword)):
            retreat = expr.matchLen
            self.exact = True
        elif isinstance(expr, (Word, CharsNotIn)) and expr.maxLen != _MAX_INT:
            retreat = expr.maxLen
            self.exact = True
        elif isinstance(expr, PositionToken):
            retreat = 0
            self.exact = True
        self.retreat = retreat
        self.errmsg = "not preceded by " + str(expr)
        self.skipWhitespace = False
        self.parseAction.append(lambda s, l, t: t.__delitem__(slice(None, None)))

    def parseImpl(self, instring, loc=0, doActions=True):
        if self.exact:
            if loc < self.retreat:
                raise ParseException(instring, loc, self.errmsg)
            start = loc - self.retreat
            _, ret = self.expr._parse(instring, start)
        else:
            # retreat specified a maximum lookbehind window, iterate
            test_expr = self.expr + StringEnd()
            instring_slice = instring[max(0, loc - self.retreat) : loc]
            last_expr = ParseException(instring, loc, self.errmsg)
            for offset in range(1, min(loc, self.retreat + 1) + 1):
                try:
                    # print('trying', offset, instring_slice, repr(instring_slice[loc - offset:]))
                    _, ret = test_expr._parse(
                        instring_slice, len(instring_slice) - offset
                    )
                except ParseBaseException as pbe:
                    last_expr = pbe
                else:
                    break
            else:
                raise last_expr
        return loc, ret


class Located(ParseElementEnhance):
    """
    Decorates a returned token with its starting and ending
    locations in the input string.

    This helper adds the following results names:

    - ``locn_start`` - location where matched expression begins
    - ``locn_end`` - location where matched expression ends
    - ``value`` - the actual parsed results

    Be careful if the input text contains ``<TAB>`` characters, you
    may want to call :class:`ParserElement.parse_with_tabs`

    Example::

        wd = Word(alphas)
        for match in Located(wd).search_string("ljsdf123lksdjjf123lkkjj1222"):
            print(match)

    prints::

        [0, ['ljsdf'], 5]
        [8, ['lksdjjf'], 15]
        [18, ['lkkjj'], 23]

    """

    def parseImpl(self, instring, loc, doActions=True):
        start = loc
        loc, tokens = self.expr._parse(instring, start, doActions, callPreParse=False)
        ret_tokens = ParseResults([start, tokens, loc])
        ret_tokens["locn_start"] = start
        ret_tokens["value"] = tokens
        ret_tokens["locn_end"] = loc
        if self.resultsName:
            # must return as a list, so that the name will be attached to the complete group
            return loc, [ret_tokens]
        else:
            return loc, ret_tokens


class NotAny(ParseElementEnhance):
    """
    Lookahead to disallow matching with the given parse expression.
    ``NotAny`` does *not* advance the parsing position within the
    input string, it only verifies that the specified parse expression
    does *not* match at the current position.  Also, ``NotAny`` does
    *not* skip over leading whitespace. ``NotAny`` always returns
    a null token list.  May be constructed using the ``'~'`` operator.

    Example::

        AND, OR, NOT = map(CaselessKeyword, "AND OR NOT".split())

        # take care not to mistake keywords for identifiers
        ident = ~(AND | OR | NOT) + Word(alphas)
        boolean_term = Opt(NOT) + ident

        # very crude boolean expression - to support parenthesis groups and
        # operation hierarchy, use infix_notation
        boolean_expr = boolean_term + ((AND | OR) + boolean_term)[...]

        # integers that are followed by "." are actually floats
        integer = Word(nums) + ~Char(".")
    """

    def __init__(self, expr: Union[ParserElement, str]):
        super().__init__(expr)
        # do NOT use self.leave_whitespace(), don't want to propagate to exprs
        # self.leave_whitespace()
        self.skipWhitespace = False

        self.mayReturnEmpty = True
        self.errmsg = "Found unwanted token, " + str(self.expr)

    def parseImpl(self, instring, loc, doActions=True):
        if self.expr.can_parse_next(instring, loc, do_actions=doActions):
            raise ParseException(instring, loc, self.errmsg, self)
        return loc, []

    def _generateDefaultName(self) -> str:
        return "~{" + str(self.expr) + "}"


class _MultipleMatch(ParseElementEnhance):
    def __init__(
        self,
        expr: Union[str, ParserElement],
        stop_on: typing.Optional[Union[ParserElement, str]] = None,
        *,
        stopOn: typing.Optional[Union[ParserElement, str]] = None,
    ):
        super().__init__(expr)
        stopOn = stopOn or stop_on
        self.saveAsList = True
        ender = stopOn
        if isinstance(ender, str_type):
            ender = self._literalStringClass(ender)
        self.stopOn(ender)

    def stopOn(self, ender) -> ParserElement:
        if isinstance(ender, str_type):
            ender = self._literalStringClass(ender)
        self.not_ender = ~ender if ender is not None else None
        return self

    def parseImpl(self, instring, loc, doActions=True):
        self_expr_parse = self.expr._parse
        self_skip_ignorables = self._skipIgnorables
        check_ender = self.not_ender is not None
        if check_ender:
            try_not_ender = self.not_ender.try_parse

        # must be at least one (but first see if we are the stopOn sentinel;
        # if so, fail)
        if check_ender:
            try_not_ender(instring, loc)
        loc, tokens = self_expr_parse(instring, loc, doActions)
        try:
            hasIgnoreExprs = not not self.ignoreExprs
            while 1:
                if check_ender:
                    try_not_ender(instring, loc)
                if hasIgnoreExprs:
                    preloc = self_skip_ignorables(instring, loc)
                else:
                    preloc = loc
                loc, tmptokens = self_expr_parse(instring, preloc, doActions)
                tokens += tmptokens
        except (ParseException, IndexError):
            pass

        return loc, tokens

    def _setResultsName(self, name, listAllMatches=False):
        if (
            __diag__.warn_ungrouped_named_tokens_in_collection
            and Diagnostics.warn_ungrouped_named_tokens_in_collection
            not in self.suppress_warnings_
        ):
            for e in [self.expr] + self.expr.recurse():
                if (
                    isinstance(e, ParserElement)
                    and e.resultsName
                    and Diagnostics.warn_ungrouped_named_tokens_in_collection
                    not in e.suppress_warnings_
                ):
                    warnings.warn(
                        "{}: setting results name {!r} on {} expression "
                        "collides with {!r} on contained expression".format(
                            "warn_ungrouped_named_tokens_in_collection",
                            name,
                            type(self).__name__,
                            e.resultsName,
                        ),
                        stacklevel=3,
                    )

        return super()._setResultsName(name, listAllMatches)


class OneOrMore(_MultipleMatch):
    """
    Repetition of one or more of the given expression.

    Parameters:

    - ``expr`` - expression that must match one or more times
    - ``stop_on`` - (default= ``None``) - expression for a terminating sentinel
      (only required if the sentinel would ordinarily match the repetition
      expression)

    Example::

        data_word = Word(alphas)
        label = data_word + FollowedBy(':')
        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).set_parse_action(' '.join))

        text = "shape: SQUARE posn: upper left color: BLACK"
        attr_expr[1, ...].parse_string(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]

        # use stop_on attribute for OneOrMore to avoid reading label string as part of the data
        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join))
        OneOrMore(attr_expr).parse_string(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]

        # could also be written as
        (attr_expr * (1,)).parse_string(text).pprint()
    """

    def _generateDefaultName(self) -> str:
        return "{" + str(self.expr) + "}..."


class ZeroOrMore(_MultipleMatch):
    """
    Optional repetition of zero or more of the given expression.

    Parameters:

    - ``expr`` - expression that must match zero or more times
    - ``stop_on`` - expression for a terminating sentinel
      (only required if the sentinel would ordinarily match the repetition
      expression) - (default= ``None``)

    Example: similar to :class:`OneOrMore`
    """

    def __init__(
        self,
        expr: Union[str, ParserElement],
        stop_on: typing.Optional[Union[ParserElement, str]] = None,
        *,
        stopOn: typing.Optional[Union[ParserElement, str]] = None,
    ):
        super().__init__(expr, stopOn=stopOn or stop_on)
        self.mayReturnEmpty = True

    def parseImpl(self, instring, loc, doActions=True):
        try:
            return super().parseImpl(instring, loc, doActions)
        except (ParseException, IndexError):
            return loc, ParseResults([], name=self.resultsName)

    def _generateDefaultName(self) -> str:
        return "[" + str(self.expr) + "]..."


class DelimitedList(ParseElementEnhance):
    def __init__(
        self,
        expr: Union[str, ParserElement],
        delim: Union[str, ParserElement] = ",",
        combine: bool = False,
        min: typing.Optional[int] = None,
        max: typing.Optional[int] = None,
        *,
        allow_trailing_delim: bool = False,
    ):
        """Helper to define a delimited list of expressions - the delimiter
        defaults to ','. By default, the list elements and delimiters can
        have intervening whitespace, and comments, but this can be
        overridden by passing ``combine=True`` in the constructor. If
        ``combine`` is set to ``True``, the matching tokens are
        returned as a single token string, with the delimiters included;
        otherwise, the matching tokens are returned as a list of tokens,
        with the delimiters suppressed.

        If ``allow_trailing_delim`` is set to True, then the list may end with
        a delimiter.

        Example::

            DelimitedList(Word(alphas)).parse_string("aa,bb,cc") # -> ['aa', 'bb', 'cc']
            DelimitedList(Word(hexnums), delim=':', combine=True).parse_string("AA:BB:CC:DD:EE") # -> ['AA:BB:CC:DD:EE']
        """
        if isinstance(expr, str_type):
            expr = ParserElement._literalStringClass(expr)
        expr = typing.cast(ParserElement, expr)

        if min is not None:
            if min < 1:
                raise ValueError("min must be greater than 0")
        if max is not None:
            if min is not None and max < min:
                raise ValueError("max must be greater than, or equal to min")

        self.content = expr
        self.raw_delim = str(delim)
        self.delim = delim
        self.combine = combine
        if not combine:
            self.delim = Suppress(delim)
        self.min = min or 1
        self.max = max
        self.allow_trailing_delim = allow_trailing_delim

        delim_list_expr = self.content + (self.delim + self.content) * (
            self.min - 1,
            None if self.max is None else self.max - 1,
        )
        if self.allow_trailing_delim:
            delim_list_expr += Opt(self.delim)

        if self.combine:
            delim_list_expr = Combine(delim_list_expr)

        super().__init__(delim_list_expr, savelist=True)

    def _generateDefaultName(self) -> str:
        return "{0} [{1} {0}]...".format(self.content.streamline(), self.raw_delim)


class _NullToken:
    def __bool__(self):
        return False

    def __str__(self):
        return ""


class Opt(ParseElementEnhance):
    """
    Optional matching of the given expression.

    Parameters:

    - ``expr`` - expression that must match zero or more times
    - ``default`` (optional) - value to be returned if the optional expression is not found.

    Example::

        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier
        zip = Combine(Word(nums, exact=5) + Opt('-' + Word(nums, exact=4)))
        zip.run_tests('''
            # traditional ZIP code
            12345

            # ZIP+4 form
            12101-0001

            # invalid ZIP
            98765-
            ''')

    prints::

        # traditional ZIP code
        12345
        ['12345']

        # ZIP+4 form
        12101-0001
        ['12101-0001']

        # invalid ZIP
        98765-
             ^
        FAIL: Expected end of text (at char 5), (line:1, col:6)
    """

    __optionalNotMatched = _NullToken()

    def __init__(
        self, expr: Union[ParserElement, str], default: Any = __optionalNotMatched
    ):
        super().__init__(expr, savelist=False)
        self.saveAsList = self.expr.saveAsList
        self.defaultValue = default
        self.mayReturnEmpty = True

    def parseImpl(self, instring, loc, doActions=True):
        self_expr = self.expr
        try:
            loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)
        except (ParseException, IndexError):
            default_value = self.defaultValue
            if default_value is not self.__optionalNotMatched:
                if self_expr.resultsName:
                    tokens = ParseResults([default_value])
                    tokens[self_expr.resultsName] = default_value
                else:
                    tokens = [default_value]
            else:
                tokens = []
        return loc, tokens

    def _generateDefaultName(self) -> str:
        inner = str(self.expr)
        # strip off redundant inner {}'s
        while len(inner) > 1 and inner[0 :: len(inner) - 1] == "{}":
            inner = inner[1:-1]
        return "[" + inner + "]"


Optional = Opt


class SkipTo(ParseElementEnhance):
    """
    Token for skipping over all undefined text until the matched
    expression is found.

    Parameters:

    - ``expr`` - target expression marking the end of the data to be skipped
    - ``include`` - if ``True``, the target expression is also parsed
      (the skipped text and target expression are returned as a 2-element
      list) (default= ``False``).
    - ``ignore`` - (default= ``None``) used to define grammars (typically quoted strings and
      comments) that might contain false matches to the target expression
    - ``fail_on`` - (default= ``None``) define expressions that are not allowed to be
      included in the skipped test; if found before the target expression is found,
      the :class:`SkipTo` is not a match

    Example::

        report = '''
            Outstanding Issues Report - 1 Jan 2000

               # | Severity | Description                               |  Days Open
            -----+----------+-------------------------------------------+-----------
             101 | Critical | Intermittent system crash                 |          6
              94 | Cosmetic | Spelling error on Login ('log|n')         |         14
              79 | Minor    | System slow when running too many reports |         47
            '''
        integer = Word(nums)
        SEP = Suppress('|')
        # use SkipTo to simply match everything up until the next SEP
        # - ignore quoted strings, so that a '|' character inside a quoted string does not match
        # - parse action will call token.strip() for each matched token, i.e., the description body
        string_data = SkipTo(SEP, ignore=quoted_string)
        string_data.set_parse_action(token_map(str.strip))
        ticket_expr = (integer("issue_num") + SEP
                      + string_data("sev") + SEP
                      + string_data("desc") + SEP
                      + integer("days_open"))

        for tkt in ticket_expr.search_string(report):
            print tkt.dump()

    prints::

        ['101', 'Critical', 'Intermittent system crash', '6']
        - days_open: '6'
        - desc: 'Intermittent system crash'
        - issue_num: '101'
        - sev: 'Critical'
        ['94', 'Cosmetic', "Spelling error on Login ('log|n')", '14']
        - days_open: '14'
        - desc: "Spelling error on Login ('log|n')"
        - issue_num: '94'
        - sev: 'Cosmetic'
        ['79', 'Minor', 'System slow when running too many reports', '47']
        - days_open: '47'
        - desc: 'System slow when running too many reports'
        - issue_num: '79'
        - sev: 'Minor'
    """

    def __init__(
        self,
        other: Union[ParserElement, str],
        include: bool = False,
        ignore: typing.Optional[Union[ParserElement, str]] = None,
        fail_on: typing.Optional[Union[ParserElement, str]] = None,
        *,
        failOn: typing.Optional[Union[ParserElement, str]] = None,
    ):
        super().__init__(other)
        failOn = failOn or fail_on
        if ignore is not None:
            self.ignore(ignore)
        self.mayReturnEmpty = True
        self.mayIndexError = False
        self.includeMatch = include
        self.saveAsList = False
        if isinstance(failOn, str_type):
            self.failOn = self._literalStringClass(failOn)
        else:
            self.failOn = failOn
        self.errmsg = "No match found for " + str(self.expr)

    def parseImpl(self, instring, loc, doActions=True):
        startloc = loc
        instrlen = len(instring)
        self_expr_parse = self.expr._parse
        self_failOn_canParseNext = (
            self.failOn.canParseNext if self.failOn is not None else None
        )
        self_preParse = self.preParse if self.callPreparse else None

        tmploc = loc
        while tmploc <= instrlen:
            if self_failOn_canParseNext is not None:
                # break if failOn expression matches
                if self_failOn_canParseNext(instring, tmploc):
                    break

            if self_preParse is not None:
                # skip grammar-ignored expressions
                tmploc = self_preParse(instring, tmploc)

            try:
                self_expr_parse(instring, tmploc, doActions=False, callPreParse=False)
            except (ParseException, IndexError):
                # no match, advance loc in string
                tmploc += 1
            else:
                # matched skipto expr, done
                break

        else:
            # ran off the end of the input string without matching skipto expr, fail
            raise ParseException(instring, loc, self.errmsg, self)

        # build up return values
        loc = tmploc
        skiptext = instring[startloc:loc]
        skipresult = ParseResults(skiptext)

        if self.includeMatch:
            loc, mat = self_expr_parse(instring, loc, doActions, callPreParse=False)
            skipresult += mat

        return loc, skipresult


class Forward(ParseElementEnhance):
    """
    Forward declaration of an expression to be defined later -
    used for recursive grammars, such as algebraic infix notation.
    When the expression is known, it is assigned to the ``Forward``
    variable using the ``'<<'`` operator.

    Note: take care when assigning to ``Forward`` not to overlook
    precedence of operators.

    Specifically, ``'|'`` has a lower precedence than ``'<<'``, so that::

        fwd_expr << a | b | c

    will actually be evaluated as::

        (fwd_expr << a) | b | c

    thereby leaving b and c out as parseable alternatives.  It is recommended that you
    explicitly group the values inserted into the ``Forward``::

        fwd_expr << (a | b | c)

    Converting to use the ``'<<='`` operator instead will avoid this problem.

    See :class:`ParseResults.pprint` for an example of a recursive
    parser created using ``Forward``.
    """

    def __init__(self, other: typing.Optional[Union[ParserElement, str]] = None):
        self.caller_frame = traceback.extract_stack(limit=2)[0]
        super().__init__(other, savelist=False)  # type: ignore[arg-type]
        self.lshift_line = None

    def __lshift__(self, other) -> "Forward":
        if hasattr(self, "caller_frame"):
            del self.caller_frame
        if isinstance(other, str_type):
            other = self._literalStringClass(other)

        if not isinstance(other, ParserElement):
            return NotImplemented

        self.expr = other
        self.streamlined = other.streamlined
        self.mayIndexError = self.expr.mayIndexError
        self.mayReturnEmpty = self.expr.mayReturnEmpty
        self.set_whitespace_chars(
            self.expr.whiteChars, copy_defaults=self.expr.copyDefaultWhiteChars
        )
        self.skipWhitespace = self.expr.skipWhitespace
        self.saveAsList = self.expr.saveAsList
        self.ignoreExprs.extend(self.expr.ignoreExprs)
        self.lshift_line = traceback.extract_stack(limit=2)[-2]  # type: ignore[assignment]
        return self

    def __ilshift__(self, other) -> "Forward":
        if not isinstance(other, ParserElement):
            return NotImplemented

        return self << other

    def __or__(self, other) -> "ParserElement":
        caller_line = traceback.extract_stack(limit=2)[-2]
        if (
            __diag__.warn_on_match_first_with_lshift_operator
            and caller_line == self.lshift_line
            and Diagnostics.warn_on_match_first_with_lshift_operator
            not in self.suppress_warnings_
        ):
            warnings.warn(
                "using '<<' operator with '|' is probably an error, use '<<='",
                stacklevel=2,
            )
        ret = super().__or__(other)
        return ret

    def __del__(self):
        # see if we are getting dropped because of '=' reassignment of var instead of '<<=' or '<<'
        if (
            self.expr is None
            and __diag__.warn_on_assignment_to_Forward
            and Diagnostics.warn_on_assignment_to_Forward not in self.suppress_warnings_
        ):
            warnings.warn_explicit(
                "Forward defined here but no expression attached later using '<<=' or '<<'",
                UserWarning,
                filename=self.caller_frame.filename,
                lineno=self.caller_frame.lineno,
            )

    def parseImpl(self, instring, loc, doActions=True):
        if (
            self.expr is None
            and __diag__.warn_on_parse_using_empty_Forward
            and Diagnostics.warn_on_parse_using_empty_Forward
            not in self.suppress_warnings_
        ):
            # walk stack until parse_string, scan_string, search_string, or transform_string is found
            parse_fns = (
                "parse_string",
                "scan_string",
                "search_string",
                "transform_string",
            )
            tb = traceback.extract_stack(limit=200)
            for i, frm in enumerate(reversed(tb), start=1):
                if frm.name in parse_fns:
                    stacklevel = i + 1
                    break
            else:
                stacklevel = 2
            warnings.warn(
                "Forward expression was never assigned a value, will not parse any input",
                stacklevel=stacklevel,
            )
        if not ParserElement._left_recursion_enabled:
            return super().parseImpl(instring, loc, doActions)
        # ## Bounded Recursion algorithm ##
        # Recursion only needs to be processed at ``Forward`` elements, since they are
        # the only ones that can actually refer to themselves. The general idea is
        # to handle recursion stepwise: We start at no recursion, then recurse once,
        # recurse twice, ..., until more recursion offers no benefit (we hit the bound).
        #
        # The "trick" here is that each ``Forward`` gets evaluated in two contexts
        # - to *match* a specific recursion level, and
        # - to *search* the bounded recursion level
        # and the two run concurrently. The *search* must *match* each recursion level
        # to find the best possible match. This is handled by a memo table, which
        # provides the previous match to the next level match attempt.
        #
        # See also "Left Recursion in Parsing Expression Grammars", Medeiros et al.
        #
        # There is a complication since we not only *parse* but also *transform* via
        # actions: We do not want to run the actions too often while expanding. Thus,
        # we expand using `doActions=False` and only run `doActions=True` if the next
        # recursion level is acceptable.
        with ParserElement.recursion_lock:
            memo = ParserElement.recursion_memos
            try:
                # we are parsing at a specific recursion expansion - use it as-is
                prev_loc, prev_result = memo[loc, self, doActions]
                if isinstance(prev_result, Exception):
                    raise prev_result
                return prev_loc, prev_result.copy()
            except KeyError:
                act_key = (loc, self, True)
                peek_key = (loc, self, False)
                # we are searching for the best recursion expansion - keep on improving
                # both `doActions` cases must be tracked separately here!
                prev_loc, prev_peek = memo[peek_key] = (
                    loc - 1,
                    ParseException(
                        instring, loc, "Forward recursion without base case", self
                    ),
                )
                if doActions:
                    memo[act_key] = memo[peek_key]
                while True:
                    try:
                        new_loc, new_peek = super().parseImpl(instring, loc, False)
                    except ParseException:
                        # we failed before getting any match  do not hide the error
                        if isinstance(prev_peek, Exception):
                            raise
                        new_loc, new_peek = prev_loc, prev_peek
                    # the match did not get better: we are done
                    if new_loc <= prev_loc:
                        if doActions:
                            # replace the match for doActions=False as well,
                            # in case the action did backtrack
                            prev_loc, prev_result = memo[peek_key] = memo[act_key]
                            del memo[peek_key], memo[act_key]
                            return prev_loc, prev_result.copy()
                        del memo[peek_key]
                        return prev_loc, prev_peek.copy()
                    # the match did get better: see if we can improve further
                    else:
                        if doActions:
                            try:
                                memo[act_key] = super().parseImpl(instring, loc, True)
                            except ParseException as e:
                                memo[peek_key] = memo[act_key] = (new_loc, e)
                                raise
                        prev_loc, prev_peek = memo[peek_key] = new_loc, new_peek

    def leave_whitespace(self, recursive: bool = True) -> ParserElement:
        self.skipWhitespace = False
        return self

    def ignore_whitespace(self, recursive: bool = True) -> ParserElement:
        self.skipWhitespace = True
        return self

    def streamline(self) -> ParserElement:
        if not self.streamlined:
            self.streamlined = True
            if self.expr is not None:
                self.expr.streamline()
        return self

    def validate(self, validateTrace=None) -> None:
        warnings.warn(
            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
            DeprecationWarning,
            stacklevel=2,
        )
        if validateTrace is None:
            validateTrace = []

        if self not in validateTrace:
            tmp = validateTrace[:] + [self]
            if self.expr is not None:
                self.expr.validate(tmp)
        self._checkRecursion([])

    def _generateDefaultName(self) -> str:
        # Avoid infinite recursion by setting a temporary _defaultName
        self._defaultName = ": ..."

        # Use the string representation of main expression.
        retString = "..."
        try:
            if self.expr is not None:
                retString = str(self.expr)[:1000]
            else:
                retString = "None"
        finally:
            return self.__class__.__name__ + ": " + retString

    def copy(self) -> ParserElement:
        if self.expr is not None:
            return super().copy()
        else:
            ret = Forward()
            ret <<= self
            return ret

    def _setResultsName(self, name, list_all_matches=False):
        if (
            __diag__.warn_name_set_on_empty_Forward
            and Diagnostics.warn_name_set_on_empty_Forward
            not in self.suppress_warnings_
        ):
            if self.expr is None:
                warnings.warn(
                    "{}: setting results name {!r} on {} expression "
                    "that has no contained expression".format(
                        "warn_name_set_on_empty_Forward", name, type(self).__name__
                    ),
                    stacklevel=3,
                )

        return super()._setResultsName(name, list_all_matches)

    # Compatibility synonyms
    # fmt: off
    @replaced_by_pep8(leave_whitespace)
    def leaveWhitespace(self): ...

    @replaced_by_pep8(ignore_whitespace)
    def ignoreWhitespace(self): ...
    # fmt: on


class TokenConverter(ParseElementEnhance):
    """
    Abstract subclass of :class:`ParseExpression`, for converting parsed results.
    """

    def __init__(self, expr: Union[ParserElement, str], savelist=False):
        super().__init__(expr)  # , savelist)
        self.saveAsList = False


class Combine(TokenConverter):
    """Converter to concatenate all matching tokens to a single string.
    By default, the matching patterns must also be contiguous in the
    input string; this can be disabled by specifying
    ``'adjacent=False'`` in the constructor.

    Example::

        real = Word(nums) + '.' + Word(nums)
        print(real.parse_string('3.1416')) # -> ['3', '.', '1416']
        # will also erroneously match the following
        print(real.parse_string('3. 1416')) # -> ['3', '.', '1416']

        real = Combine(Word(nums) + '.' + Word(nums))
        print(real.parse_string('3.1416')) # -> ['3.1416']
        # no match when there are internal spaces
        print(real.parse_string('3. 1416')) # -> Exception: Expected W:(0123...)
    """

    def __init__(
        self,
        expr: ParserElement,
        join_string: str = "",
        adjacent: bool = True,
        *,
        joinString: typing.Optional[str] = None,
    ):
        super().__init__(expr)
        joinString = joinString if joinString is not None else join_string
        # suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself
        if adjacent:
            self.leave_whitespace()
        self.adjacent = adjacent
        self.skipWhitespace = True
        self.joinString = joinString
        self.callPreparse = True

    def ignore(self, other) -> ParserElement:
        if self.adjacent:
            ParserElement.ignore(self, other)
        else:
            super().ignore(other)
        return self

    def postParse(self, instring, loc, tokenlist):
        retToks = tokenlist.copy()
        del retToks[:]
        retToks += ParseResults(
            ["".join(tokenlist._asStringList(self.joinString))], modal=self.modalResults
        )

        if self.resultsName and retToks.haskeys():
            return [retToks]
        else:
            return retToks


class Group(TokenConverter):
    """Converter to return the matched tokens as a list - useful for
    returning tokens of :class:`ZeroOrMore` and :class:`OneOrMore` expressions.

    The optional ``aslist`` argument when set to True will return the
    parsed tokens as a Python list instead of a pyparsing ParseResults.

    Example::

        ident = Word(alphas)
        num = Word(nums)
        term = ident | num
        func = ident + Opt(DelimitedList(term))
        print(func.parse_string("fn a, b, 100"))
        # -> ['fn', 'a', 'b', '100']

        func = ident + Group(Opt(DelimitedList(term)))
        print(func.parse_string("fn a, b, 100"))
        # -> ['fn', ['a', 'b', '100']]
    """

    def __init__(self, expr: ParserElement, aslist: bool = False):
        super().__init__(expr)
        self.saveAsList = True
        self._asPythonList = aslist

    def postParse(self, instring, loc, tokenlist):
        if self._asPythonList:
            return ParseResults.List(
                tokenlist.asList()
                if isinstance(tokenlist, ParseResults)
                else list(tokenlist)
            )
        else:
            return [tokenlist]


class Dict(TokenConverter):
    """Converter to return a repetitive expression as a list, but also
    as a dictionary. Each element can also be referenced using the first
    token in the expression as its key. Useful for tabular report
    scraping when the first column can be used as a item key.

    The optional ``asdict`` argument when set to True will return the
    parsed tokens as a Python dict instead of a pyparsing ParseResults.

    Example::

        data_word = Word(alphas)
        label = data_word + FollowedBy(':')

        text = "shape: SQUARE posn: upper left color: light blue texture: burlap"
        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join))

        # print attributes as plain groups
        print(attr_expr[1, ...].parse_string(text).dump())

        # instead of OneOrMore(expr), parse using Dict(Group(expr)[1, ...]) - Dict will auto-assign names
        result = Dict(Group(attr_expr)[1, ...]).parse_string(text)
        print(result.dump())

        # access named fields as dict entries, or output as dict
        print(result['shape'])
        print(result.as_dict())

    prints::

        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']
        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]
        - color: 'light blue'
        - posn: 'upper left'
        - shape: 'SQUARE'
        - texture: 'burlap'
        SQUARE
        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}

    See more examples at :class:`ParseResults` of accessing fields by results name.
    """

    def __init__(self, expr: ParserElement, asdict: bool = False):
        super().__init__(expr)
        self.saveAsList = True
        self._asPythonDict = asdict

    def postParse(self, instring, loc, tokenlist):
        for i, tok in enumerate(tokenlist):
            if len(tok) == 0:
                continue

            ikey = tok[0]
            if isinstance(ikey, int):
                ikey = str(ikey).strip()

            if len(tok) == 1:
                tokenlist[ikey] = _ParseResultsWithOffset("", i)

            elif len(tok) == 2 and not isinstance(tok[1], ParseResults):
                tokenlist[ikey] = _ParseResultsWithOffset(tok[1], i)

            else:
                try:
                    dictvalue = tok.copy()  # ParseResults(i)
                except Exception:
                    exc = TypeError(
                        "could not extract dict values from parsed results"
                        " - Dict expression must contain Grouped expressions"
                    )
                    raise exc from None

                del dictvalue[0]

                if len(dictvalue) != 1 or (
                    isinstance(dictvalue, ParseResults) and dictvalue.haskeys()
                ):
                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue, i)
                else:
                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0], i)

        if self._asPythonDict:
            return [tokenlist.as_dict()] if self.resultsName else tokenlist.as_dict()
        else:
            return [tokenlist] if self.resultsName else tokenlist


class Suppress(TokenConverter):
    """Converter for ignoring the results of a parsed expression.

    Example::

        source = "a, b, c,d"
        wd = Word(alphas)
        wd_list1 = wd + (',' + wd)[...]
        print(wd_list1.parse_string(source))

        # often, delimiters that are useful during parsing are just in the
        # way afterward - use Suppress to keep them out of the parsed output
        wd_list2 = wd + (Suppress(',') + wd)[...]
        print(wd_list2.parse_string(source))

        # Skipped text (using '...') can be suppressed as well
        source = "lead in START relevant text END trailing text"
        start_marker = Keyword("START")
        end_marker = Keyword("END")
        find_body = Suppress(...) + start_marker + ... + end_marker
        print(find_body.parse_string(source)

    prints::

        ['a', ',', 'b', ',', 'c', ',', 'd']
        ['a', 'b', 'c', 'd']
        ['START', 'relevant text ', 'END']

    (See also :class:`DelimitedList`.)
    """

    def __init__(self, expr: Union[ParserElement, str], savelist: bool = False):
        if expr is ...:
            expr = _PendingSkip(NoMatch())
        super().__init__(expr)

    def __add__(self, other) -> "ParserElement":
        if isinstance(self.expr, _PendingSkip):
            return Suppress(SkipTo(other)) + other
        else:
            return super().__add__(other)

    def __sub__(self, other) -> "ParserElement":
        if isinstance(self.expr, _PendingSkip):
            return Suppress(SkipTo(other)) - other
        else:
            return super().__sub__(other)

    def postParse(self, instring, loc, tokenlist):
        return []

    def suppress(self) -> ParserElement:
        return self


def trace_parse_action(f: ParseAction) -> ParseAction:
    """Decorator for debugging parse actions.

    When the parse action is called, this decorator will print
    ``">> entering method-name(line:<current_source_line>, <parse_location>, <matched_tokens>)"``.
    When the parse action completes, the decorator will print
    ``"<<"`` followed by the returned value, or any exception that the parse action raised.

    Example::

        wd = Word(alphas)

        @trace_parse_action
        def remove_duplicate_chars(tokens):
            return ''.join(sorted(set(''.join(tokens))))

        wds = wd[1, ...].set_parse_action(remove_duplicate_chars)
        print(wds.parse_string("slkdjs sld sldd sdlf sdljf"))

    prints::

        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))
        <<leaving remove_duplicate_chars (ret: 'dfjkls')
        ['dfjkls']
    """
    f = _trim_arity(f)

    def z(*paArgs):
        thisFunc = f.__name__
        s, l, t = paArgs[-3:]
        if len(paArgs) > 3:
            thisFunc = paArgs[0].__class__.__name__ + "." + thisFunc
        sys.stderr.write(f">>entering {thisFunc}(line: {line(l, s)!r}, {l}, {t!r})\n")
        try:
            ret = f(*paArgs)
        except Exception as exc:
            sys.stderr.write(f"<<leaving {thisFunc} (exception: {exc})\n")
            raise
        sys.stderr.write(f"<<leaving {thisFunc} (ret: {ret!r})\n")
        return ret

    z.__name__ = f.__name__
    return z


# convenience constants for positional expressions
empty = Empty().set_name("empty")
line_start = LineStart().set_name("line_start")
line_end = LineEnd().set_name("line_end")
string_start = StringStart().set_name("string_start")
string_end = StringEnd().set_name("string_end")

_escapedPunc = Regex(r"\\[\\[\]\/\-\*\.\$\+\^\?()~ ]").set_parse_action(
    lambda s, l, t: t[0][1]
)
_escapedHexChar = Regex(r"\\0?[xX][0-9a-fA-F]+").set_parse_action(
    lambda s, l, t: chr(int(t[0].lstrip(r"\0x"), 16))
)
_escapedOctChar = Regex(r"\\0[0-7]+").set_parse_action(
    lambda s, l, t: chr(int(t[0][1:], 8))
)
_singleChar = (
    _escapedPunc | _escapedHexChar | _escapedOctChar | CharsNotIn(r"\]", exact=1)
)
_charRange = Group(_singleChar + Suppress("-") + _singleChar)
_reBracketExpr = (
    Literal("[")
    + Opt("^").set_results_name("negate")
    + Group(OneOrMore(_charRange | _singleChar)).set_results_name("body")
    + Literal("]")
)


def srange(s: str) -> str:
    r"""Helper to easily define string ranges for use in :class:`Word`
    construction. Borrows syntax from regexp ``'[]'`` string range
    definitions::

        srange("[0-9]")   -> "0123456789"
        srange("[a-z]")   -> "abcdefghijklmnopqrstuvwxyz"
        srange("[a-z$_]") -> "abcdefghijklmnopqrstuvwxyz$_"

    The input string must be enclosed in []'s, and the returned string
    is the expanded character set joined into a single string. The
    values enclosed in the []'s may be:

    - a single character
    - an escaped character with a leading backslash (such as ``\-``
      or ``\]``)
    - an escaped hex character with a leading ``'\x'``
      (``\x21``, which is a ``'!'`` character) (``\0x##``
      is also supported for backwards compatibility)
    - an escaped octal character with a leading ``'\0'``
      (``\041``, which is a ``'!'`` character)
    - a range of any of the above, separated by a dash (``'a-z'``,
      etc.)
    - any combination of the above (``'aeiouy'``,
      ``'a-zA-Z0-9_$'``, etc.)
    """
    _expanded = (
        lambda p: p
        if not isinstance(p, ParseResults)
        else "".join(chr(c) for c in range(ord(p[0]), ord(p[1]) + 1))
    )
    try:
        return "".join(_expanded(part) for part in _reBracketExpr.parse_string(s).body)
    except Exception as e:
        return ""


def token_map(func, *args) -> ParseAction:
    """Helper to define a parse action by mapping a function to all
    elements of a :class:`ParseResults` list. If any additional args are passed,
    they are forwarded to the given function as additional arguments
    after the token, as in
    ``hex_integer = Word(hexnums).set_parse_action(token_map(int, 16))``,
    which will convert the parsed data to an integer using base 16.

    Example (compare the last to example in :class:`ParserElement.transform_string`::

        hex_ints = Word(hexnums)[1, ...].set_parse_action(token_map(int, 16))
        hex_ints.run_tests('''
            00 11 22 aa FF 0a 0d 1a
            ''')

        upperword = Word(alphas).set_parse_action(token_map(str.upper))
        upperword[1, ...].run_tests('''
            my kingdom for a horse
            ''')

        wd = Word(alphas).set_parse_action(token_map(str.title))
        wd[1, ...].set_parse_action(' '.join).run_tests('''
            now is the winter of our discontent made glorious summer by this sun of york
            ''')

    prints::

        00 11 22 aa FF 0a 0d 1a
        [0, 17, 34, 170, 255, 10, 13, 26]

        my kingdom for a horse
        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']

        now is the winter of our discontent made glorious summer by this sun of york
        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']
    """

    def pa(s, l, t):
        return [func(tokn, *args) for tokn in t]

    func_name = getattr(func, "__name__", getattr(func, "__class__").__name__)
    pa.__name__ = func_name

    return pa


def autoname_elements() -> None:
    """
    Utility to simplify mass-naming of parser elements, for
    generating railroad diagram with named subdiagrams.
    """
    calling_frame = sys._getframe().f_back
    if calling_frame is None:
        return
    calling_frame = typing.cast(types.FrameType, calling_frame)
    for name, var in calling_frame.f_locals.items():
        if isinstance(var, ParserElement) and not var.customName:
            var.set_name(name)


dbl_quoted_string = Combine(
    Regex(r'"(?:[^"\n\r\\]|(?:"")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*') + '"'
).set_name("string enclosed in double quotes")

sgl_quoted_string = Combine(
    Regex(r"'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*") + "'"
).set_name("string enclosed in single quotes")

quoted_string = Combine(
    (Regex(r'"(?:[^"\n\r\\]|(?:"")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*') + '"').set_name(
        "double quoted string"
    )
    | (Regex(r"'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*") + "'").set_name(
        "single quoted string"
    )
).set_name("quoted string using single or double quotes")

python_quoted_string = Combine(
    (Regex(r'"""(?:[^"\\]|""(?!")|"(?!"")|\\.)*', flags=re.MULTILINE) + '"""').set_name(
        "multiline double quoted string"
    )
    ^ (
        Regex(r"'''(?:[^'\\]|''(?!')|'(?!'')|\\.)*", flags=re.MULTILINE) + "'''"
    ).set_name("multiline single quoted string")
    ^ (Regex(r'"(?:[^"\n\r\\]|(?:\\")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*') + '"').set_name(
        "double quoted string"
    )
    ^ (Regex(r"'(?:[^'\n\r\\]|(?:\\')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*") + "'").set_name(
        "single quoted string"
    )
).set_name("Python quoted string")

unicode_string = Combine("u" + quoted_string.copy()).set_name("unicode string literal")


alphas8bit = srange(r"[\0xc0-\0xd6\0xd8-\0xf6\0xf8-\0xff]")
punc8bit = srange(r"[\0xa1-\0xbf\0xd7\0xf7]")

# build list of built-in expressions, for future reference if a global default value
# gets updated
_builtin_exprs: List[ParserElement] = [
    v for v in vars().values() if isinstance(v, ParserElement)
]

# backward compatibility names
# fmt: off
sglQuotedString = sgl_quoted_string
dblQuotedString = dbl_quoted_string
quotedString = quoted_string
unicodeString = unicode_string
lineStart = line_start
lineEnd = line_end
stringStart = string_start
stringEnd = string_end

@replaced_by_pep8(null_debug_action)
def nullDebugAction(): ...

@replaced_by_pep8(trace_parse_action)
def traceParseAction(): ...

@replaced_by_pep8(condition_as_parse_action)
def conditionAsParseAction(): ...

@replaced_by_pep8(token_map)
def tokenMap(): ...
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/common.py
# ========================================================
# common.py
from .core import *
from .helpers import DelimitedList, any_open_tag, any_close_tag
from datetime import datetime


# some other useful expressions - using lower-case class name since we are really using this as a namespace
class pyparsing_common:
    """Here are some common low-level expressions that may be useful in
    jump-starting parser development:

    - numeric forms (:class:`integers<integer>`, :class:`reals<real>`,
      :class:`scientific notation<sci_real>`)
    - common :class:`programming identifiers<identifier>`
    - network addresses (:class:`MAC<mac_address>`,
      :class:`IPv4<ipv4_address>`, :class:`IPv6<ipv6_address>`)
    - ISO8601 :class:`dates<iso8601_date>` and
      :class:`datetime<iso8601_datetime>`
    - :class:`UUID<uuid>`
    - :class:`comma-separated list<comma_separated_list>`
    - :class:`url`

    Parse actions:

    - :class:`convert_to_integer`
    - :class:`convert_to_float`
    - :class:`convert_to_date`
    - :class:`convert_to_datetime`
    - :class:`strip_html_tags`
    - :class:`upcase_tokens`
    - :class:`downcase_tokens`

    Example::

        pyparsing_common.number.run_tests('''
            # any int or real number, returned as the appropriate type
            100
            -100
            +100
            3.14159
            6.02e23
            1e-12
            ''')

        pyparsing_common.fnumber.run_tests('''
            # any int or real number, returned as float
            100
            -100
            +100
            3.14159
            6.02e23
            1e-12
            ''')

        pyparsing_common.hex_integer.run_tests('''
            # hex numbers
            100
            FF
            ''')

        pyparsing_common.fraction.run_tests('''
            # fractions
            1/2
            -3/4
            ''')

        pyparsing_common.mixed_integer.run_tests('''
            # mixed fractions
            1
            1/2
            -3/4
            1-3/4
            ''')

        import uuid
        pyparsing_common.uuid.set_parse_action(token_map(uuid.UUID))
        pyparsing_common.uuid.run_tests('''
            # uuid
            12345678-1234-5678-1234-567812345678
            ''')

    prints::

        # any int or real number, returned as the appropriate type
        100
        [100]

        -100
        [-100]

        +100
        [100]

        3.14159
        [3.14159]

        6.02e23
        [6.02e+23]

        1e-12
        [1e-12]

        # any int or real number, returned as float
        100
        [100.0]

        -100
        [-100.0]

        +100
        [100.0]

        3.14159
        [3.14159]

        6.02e23
        [6.02e+23]

        1e-12
        [1e-12]

        # hex numbers
        100
        [256]

        FF
        [255]

        # fractions
        1/2
        [0.5]

        -3/4
        [-0.75]

        # mixed fractions
        1
        [1]

        1/2
        [0.5]

        -3/4
        [-0.75]

        1-3/4
        [1.75]

        # uuid
        12345678-1234-5678-1234-567812345678
        [UUID('12345678-1234-5678-1234-567812345678')]
    """

    convert_to_integer = token_map(int)
    """
    Parse action for converting parsed integers to Python int
    """

    convert_to_float = token_map(float)
    """
    Parse action for converting parsed numbers to Python float
    """

    integer = Word(nums).set_name("integer").set_parse_action(convert_to_integer)
    """expression that parses an unsigned integer, returns an int"""

    hex_integer = (
        Word(hexnums).set_name("hex integer").set_parse_action(token_map(int, 16))
    )
    """expression that parses a hexadecimal integer, returns an int"""

    signed_integer = (
        Regex(r"[+-]?\d+")
        .set_name("signed integer")
        .set_parse_action(convert_to_integer)
    )
    """expression that parses an integer with optional leading sign, returns an int"""

    fraction = (
        signed_integer().set_parse_action(convert_to_float)
        + "/"
        + signed_integer().set_parse_action(convert_to_float)
    ).set_name("fraction")
    """fractional expression of an integer divided by an integer, returns a float"""
    fraction.add_parse_action(lambda tt: tt[0] / tt[-1])

    mixed_integer = (
        fraction | signed_integer + Opt(Opt("-").suppress() + fraction)
    ).set_name("fraction or mixed integer-fraction")
    """mixed integer of the form 'integer - fraction', with optional leading integer, returns float"""
    mixed_integer.add_parse_action(sum)

    real = (
        Regex(r"[+-]?(?:\d+\.\d*|\.\d+)")
        .set_name("real number")
        .set_parse_action(convert_to_float)
    )
    """expression that parses a floating point number and returns a float"""

    sci_real = (
        Regex(r"[+-]?(?:\d+(?:[eE][+-]?\d+)|(?:\d+\.\d*|\.\d+)(?:[eE][+-]?\d+)?)")
        .set_name("real number with scientific notation")
        .set_parse_action(convert_to_float)
    )
    """expression that parses a floating point number with optional
    scientific notation and returns a float"""

    # streamlining this expression makes the docs nicer-looking
    number = (sci_real | real | signed_integer).setName("number").streamline()
    """any numeric expression, returns the corresponding Python type"""

    fnumber = (
        Regex(r"[+-]?\d+\.?\d*([eE][+-]?\d+)?")
        .set_name("fnumber")
        .set_parse_action(convert_to_float)
    )
    """any int or real number, returned as float"""

    identifier = Word(identchars, identbodychars).set_name("identifier")
    """typical code identifier (leading alpha or '_', followed by 0 or more alphas, nums, or '_')"""

    ipv4_address = Regex(
        r"(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})(\.(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})){3}"
    ).set_name("IPv4 address")
    "IPv4 address (``0.0.0.0 - 255.255.255.255``)"

    _ipv6_part = Regex(r"[0-9a-fA-F]{1,4}").set_name("hex_integer")
    _full_ipv6_address = (_ipv6_part + (":" + _ipv6_part) * 7).set_name(
        "full IPv6 address"
    )
    _short_ipv6_address = (
        Opt(_ipv6_part + (":" + _ipv6_part) * (0, 6))
        + "::"
        + Opt(_ipv6_part + (":" + _ipv6_part) * (0, 6))
    ).set_name("short IPv6 address")
    _short_ipv6_address.add_condition(
        lambda t: sum(1 for tt in t if pyparsing_common._ipv6_part.matches(tt)) < 8
    )
    _mixed_ipv6_address = ("::ffff:" + ipv4_address).set_name("mixed IPv6 address")
    ipv6_address = Combine(
        (_full_ipv6_address | _mixed_ipv6_address | _short_ipv6_address).set_name(
            "IPv6 address"
        )
    ).set_name("IPv6 address")
    "IPv6 address (long, short, or mixed form)"

    mac_address = Regex(
        r"[0-9a-fA-F]{2}([:.-])[0-9a-fA-F]{2}(?:\1[0-9a-fA-F]{2}){4}"
    ).set_name("MAC address")
    "MAC address xx:xx:xx:xx:xx (may also have '-' or '.' delimiters)"

    @staticmethod
    def convert_to_date(fmt: str = "%Y-%m-%d"):
        """
        Helper to create a parse action for converting parsed date string to Python datetime.date

        Params -
        - fmt - format to be passed to datetime.strptime (default= ``"%Y-%m-%d"``)

        Example::

            date_expr = pyparsing_common.iso8601_date.copy()
            date_expr.set_parse_action(pyparsing_common.convert_to_date())
            print(date_expr.parse_string("1999-12-31"))

        prints::

            [datetime.date(1999, 12, 31)]
        """

        def cvt_fn(ss, ll, tt):
            try:
                return datetime.strptime(tt[0], fmt).date()
            except ValueError as ve:
                raise ParseException(ss, ll, str(ve))

        return cvt_fn

    @staticmethod
    def convert_to_datetime(fmt: str = "%Y-%m-%dT%H:%M:%S.%f"):
        """Helper to create a parse action for converting parsed
        datetime string to Python datetime.datetime

        Params -
        - fmt - format to be passed to datetime.strptime (default= ``"%Y-%m-%dT%H:%M:%S.%f"``)

        Example::

            dt_expr = pyparsing_common.iso8601_datetime.copy()
            dt_expr.set_parse_action(pyparsing_common.convert_to_datetime())
            print(dt_expr.parse_string("1999-12-31T23:59:59.999"))

        prints::

            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]
        """

        def cvt_fn(s, l, t):
            try:
                return datetime.strptime(t[0], fmt)
            except ValueError as ve:
                raise ParseException(s, l, str(ve))

        return cvt_fn

    iso8601_date = Regex(
        r"(?P<year>\d{4})(?:-(?P<month>\d\d)(?:-(?P<day>\d\d))?)?"
    ).set_name("ISO8601 date")
    "ISO8601 date (``yyyy-mm-dd``)"

    iso8601_datetime = Regex(
        r"(?P<year>\d{4})-(?P<month>\d\d)-(?P<day>\d\d)[T ](?P<hour>\d\d):(?P<minute>\d\d)(:(?P<second>\d\d(\.\d*)?)?)?(?P<tz>Z|[+-]\d\d:?\d\d)?"
    ).set_name("ISO8601 datetime")
    "ISO8601 datetime (``yyyy-mm-ddThh:mm:ss.s(Z|+-00:00)``) - trailing seconds, milliseconds, and timezone optional; accepts separating ``'T'`` or ``' '``"

    uuid = Regex(r"[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}").set_name("UUID")
    "UUID (``xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx``)"

    _html_stripper = any_open_tag.suppress() | any_close_tag.suppress()

    @staticmethod
    def strip_html_tags(s: str, l: int, tokens: ParseResults):
        """Parse action to remove HTML tags from web page HTML source

        Example::

            # strip HTML links from normal text
            text = '<td>More info at the <a href="https://github.com/pyparsing/pyparsing/wiki">pyparsing</a> wiki page</td>'
            td, td_end = make_html_tags("TD")
            table_text = td + SkipTo(td_end).set_parse_action(pyparsing_common.strip_html_tags)("body") + td_end
            print(table_text.parse_string(text).body)

        Prints::

            More info at the pyparsing wiki page
        """
        return pyparsing_common._html_stripper.transform_string(tokens[0])

    _commasepitem = (
        Combine(
            OneOrMore(
                ~Literal(",")
                + ~LineEnd()
                + Word(printables, exclude_chars=",")
                + Opt(White(" \t") + ~FollowedBy(LineEnd() | ","))
            )
        )
        .streamline()
        .set_name("commaItem")
    )
    comma_separated_list = DelimitedList(
        Opt(quoted_string.copy() | _commasepitem, default="")
    ).set_name("comma separated list")
    """Predefined expression of 1 or more printable words or quoted strings, separated by commas."""

    upcase_tokens = staticmethod(token_map(lambda t: t.upper()))
    """Parse action to convert tokens to upper case."""

    downcase_tokens = staticmethod(token_map(lambda t: t.lower()))
    """Parse action to convert tokens to lower case."""

    # fmt: off
    url = Regex(
        # https://mathiasbynens.be/demo/url-regex
        # https://gist.github.com/dperini/729294
        r"(?P<url>" +
        # protocol identifier (optional)
        # short syntax // still required
        r"(?:(?:(?P<scheme>https?|ftp):)?\/\/)" +
        # user:pass BasicAuth (optional)
        r"(?:(?P<auth>\S+(?::\S*)?)@)?" +
        r"(?P<host>" +
        # IP address exclusion
        # private & local networks
        r"(?!(?:10|127)(?:\.\d{1,3}){3})" +
        r"(?!(?:169\.254|192\.168)(?:\.\d{1,3}){2})" +
        r"(?!172\.(?:1[6-9]|2\d|3[0-1])(?:\.\d{1,3}){2})" +
        # IP address dotted notation octets
        # excludes loopback network 0.0.0.0
        # excludes reserved space >= 224.0.0.0
        # excludes network & broadcast addresses
        # (first & last IP address of each class)
        r"(?:[1-9]\d?|1\d\d|2[01]\d|22[0-3])" +
        r"(?:\.(?:1?\d{1,2}|2[0-4]\d|25[0-5])){2}" +
        r"(?:\.(?:[1-9]\d?|1\d\d|2[0-4]\d|25[0-4]))" +
        r"|" +
        # host & domain names, may end with dot
        # can be replaced by a shortest alternative
        # (?![-_])(?:[-\w\u00a1-\uffff]{0,63}[^-_]\.)+
        r"(?:" +
        r"(?:" +
        r"[a-z0-9\u00a1-\uffff]" +
        r"[a-z0-9\u00a1-\uffff_-]{0,62}" +
        r")?" +
        r"[a-z0-9\u00a1-\uffff]\." +
        r")+" +
        # TLD identifier name, may end with dot
        r"(?:[a-z\u00a1-\uffff]{2,}\.?)" +
        r")" +
        # port number (optional)
        r"(:(?P<port>\d{2,5}))?" +
        # resource path (optional)
        r"(?P<path>\/[^?# ]*)?" +
        # query string (optional)
        r"(\?(?P<query>[^#]*))?" +
        # fragment (optional)
        r"(#(?P<fragment>\S*))?" +
        r")"
    ).set_name("url")
    """URL (http/https/ftp scheme)"""
    # fmt: on

    # pre-PEP8 compatibility names
    convertToInteger = convert_to_integer
    """Deprecated - use :class:`convert_to_integer`"""
    convertToFloat = convert_to_float
    """Deprecated - use :class:`convert_to_float`"""
    convertToDate = convert_to_date
    """Deprecated - use :class:`convert_to_date`"""
    convertToDatetime = convert_to_datetime
    """Deprecated - use :class:`convert_to_datetime`"""
    stripHTMLTags = strip_html_tags
    """Deprecated - use :class:`strip_html_tags`"""
    upcaseTokens = upcase_tokens
    """Deprecated - use :class:`upcase_tokens`"""
    downcaseTokens = downcase_tokens
    """Deprecated - use :class:`downcase_tokens`"""


_builtin_exprs = [
    v for v in vars(pyparsing_common).values() if isinstance(v, ParserElement)
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/actions.py
# ========================================================
# actions.py

from .exceptions import ParseException
from .util import col, replaced_by_pep8


class OnlyOnce:
    """
    Wrapper for parse actions, to ensure they are only called once.
    """

    def __init__(self, method_call):
        from .core import _trim_arity

        self.callable = _trim_arity(method_call)
        self.called = False

    def __call__(self, s, l, t):
        if not self.called:
            results = self.callable(s, l, t)
            self.called = True
            return results
        raise ParseException(s, l, "OnlyOnce obj called multiple times w/out reset")

    def reset(self):
        """
        Allow the associated parse action to be called once more.
        """

        self.called = False


def match_only_at_col(n):
    """
    Helper method for defining parse actions that require matching at
    a specific column in the input text.
    """

    def verify_col(strg, locn, toks):
        if col(locn, strg) != n:
            raise ParseException(strg, locn, f"matched token not at column {n}")

    return verify_col


def replace_with(repl_str):
    """
    Helper method for common parse actions that simply return
    a literal value.  Especially useful when used with
    :class:`transform_string<ParserElement.transform_string>` ().

    Example::

        num = Word(nums).set_parse_action(lambda toks: int(toks[0]))
        na = one_of("N/A NA").set_parse_action(replace_with(math.nan))
        term = na | num

        term[1, ...].parse_string("324 234 N/A 234") # -> [324, 234, nan, 234]
    """
    return lambda s, l, t: [repl_str]


def remove_quotes(s, l, t):
    """
    Helper parse action for removing quotation marks from parsed
    quoted strings.

    Example::

        # by default, quotation marks are included in parsed results
        quoted_string.parse_string("'Now is the Winter of our Discontent'") # -> ["'Now is the Winter of our Discontent'"]

        # use remove_quotes to strip quotation marks from parsed results
        quoted_string.set_parse_action(remove_quotes)
        quoted_string.parse_string("'Now is the Winter of our Discontent'") # -> ["Now is the Winter of our Discontent"]
    """
    return t[0][1:-1]


def with_attribute(*args, **attr_dict):
    """
    Helper to create a validating parse action to be used with start
    tags created with :class:`make_xml_tags` or
    :class:`make_html_tags`. Use ``with_attribute`` to qualify
    a starting tag with a required attribute value, to avoid false
    matches on common tags such as ``<TD>`` or ``<DIV>``.

    Call ``with_attribute`` with a series of attribute names and
    values. Specify the list of filter attributes names and values as:

    - keyword arguments, as in ``(align="right")``, or
    - as an explicit dict with ``**`` operator, when an attribute
      name is also a Python reserved word, as in ``**{"class":"Customer", "align":"right"}``
    - a list of name-value tuples, as in ``(("ns1:class", "Customer"), ("ns2:align", "right"))``

    For attribute names with a namespace prefix, you must use the second
    form.  Attribute names are matched insensitive to upper/lower case.

    If just testing for ``class`` (with or without a namespace), use
    :class:`with_class`.

    To verify that the attribute exists, but without specifying a value,
    pass ``with_attribute.ANY_VALUE`` as the value.

    Example::

        html = '''
            <div>
            Some text
            <div type="grid">1 4 0 1 0</div>
            <div type="graph">1,3 2,3 1,1</div>
            <div>this has no type</div>
            </div>

        '''
        div,div_end = make_html_tags("div")

        # only match div tag having a type attribute with value "grid"
        div_grid = div().set_parse_action(with_attribute(type="grid"))
        grid_expr = div_grid + SkipTo(div | div_end)("body")
        for grid_header in grid_expr.search_string(html):
            print(grid_header.body)

        # construct a match with any div tag having a type attribute, regardless of the value
        div_any_type = div().set_parse_action(with_attribute(type=with_attribute.ANY_VALUE))
        div_expr = div_any_type + SkipTo(div | div_end)("body")
        for div_header in div_expr.search_string(html):
            print(div_header.body)

    prints::

        1 4 0 1 0

        1 4 0 1 0
        1,3 2,3 1,1
    """
    if args:
        attrs = args[:]
    else:
        attrs = attr_dict.items()
    attrs = [(k, v) for k, v in attrs]

    def pa(s, l, tokens):
        for attrName, attrValue in attrs:
            if attrName not in tokens:
                raise ParseException(s, l, "no matching attribute " + attrName)
            if attrValue != with_attribute.ANY_VALUE and tokens[attrName] != attrValue:
                raise ParseException(
                    s,
                    l,
                    f"attribute {attrName!r} has value {tokens[attrName]!r}, must be {attrValue!r}",
                )

    return pa


with_attribute.ANY_VALUE = object()  # type: ignore [attr-defined]


def with_class(classname, namespace=""):
    """
    Simplified version of :class:`with_attribute` when
    matching on a div class - made difficult because ``class`` is
    a reserved word in Python.

    Example::

        html = '''
            <div>
            Some text
            <div class="grid">1 4 0 1 0</div>
            <div class="graph">1,3 2,3 1,1</div>
            <div>this &lt;div&gt; has no class</div>
            </div>

        '''
        div,div_end = make_html_tags("div")
        div_grid = div().set_parse_action(with_class("grid"))

        grid_expr = div_grid + SkipTo(div | div_end)("body")
        for grid_header in grid_expr.search_string(html):
            print(grid_header.body)

        div_any_type = div().set_parse_action(with_class(withAttribute.ANY_VALUE))
        div_expr = div_any_type + SkipTo(div | div_end)("body")
        for div_header in div_expr.search_string(html):
            print(div_header.body)

    prints::

        1 4 0 1 0

        1 4 0 1 0
        1,3 2,3 1,1
    """
    classattr = f"{namespace}:class" if namespace else "class"
    return with_attribute(**{classattr: classname})


# pre-PEP8 compatibility symbols
# fmt: off
@replaced_by_pep8(replace_with)
def replaceWith(): ...

@replaced_by_pep8(remove_quotes)
def removeQuotes(): ...

@replaced_by_pep8(with_attribute)
def withAttribute(): ...

@replaced_by_pep8(with_class)
def withClass(): ...

@replaced_by_pep8(match_only_at_col)
def matchOnlyAtCol(): ...

# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/util.py
# ========================================================
# util.py
import inspect
import warnings
import types
import collections
import itertools
from functools import lru_cache, wraps
from typing import Callable, List, Union, Iterable, TypeVar, cast

_bslash = chr(92)
C = TypeVar("C", bound=Callable)


class __config_flags:
    """Internal class for defining compatibility and debugging flags"""

    _all_names: List[str] = []
    _fixed_names: List[str] = []
    _type_desc = "configuration"

    @classmethod
    def _set(cls, dname, value):
        if dname in cls._fixed_names:
            warnings.warn(
                f"{cls.__name__}.{dname} {cls._type_desc} is {str(getattr(cls, dname)).upper()}"
                f" and cannot be overridden",
                stacklevel=3,
            )
            return
        if dname in cls._all_names:
            setattr(cls, dname, value)
        else:
            raise ValueError(f"no such {cls._type_desc} {dname!r}")

    enable = classmethod(lambda cls, name: cls._set(name, True))
    disable = classmethod(lambda cls, name: cls._set(name, False))


@lru_cache(maxsize=128)
def col(loc: int, strg: str) -> int:
    """
    Returns current column within a string, counting newlines as line separators.
    The first column is number 1.

    Note: the default parsing behavior is to expand tabs in the input string
    before starting the parsing process.  See
    :class:`ParserElement.parse_string` for more
    information on parsing strings containing ``<TAB>`` s, and suggested
    methods to maintain a consistent view of the parsed string, the parse
    location, and line and column positions within the parsed string.
    """
    s = strg
    return 1 if 0 < loc < len(s) and s[loc - 1] == "\n" else loc - s.rfind("\n", 0, loc)


@lru_cache(maxsize=128)
def lineno(loc: int, strg: str) -> int:
    """Returns current line number within a string, counting newlines as line separators.
    The first line is number 1.

    Note - the default parsing behavior is to expand tabs in the input string
    before starting the parsing process.  See :class:`ParserElement.parse_string`
    for more information on parsing strings containing ``<TAB>`` s, and
    suggested methods to maintain a consistent view of the parsed string, the
    parse location, and line and column positions within the parsed string.
    """
    return strg.count("\n", 0, loc) + 1


@lru_cache(maxsize=128)
def line(loc: int, strg: str) -> str:
    """
    Returns the line of text containing loc within a string, counting newlines as line separators.
    """
    last_cr = strg.rfind("\n", 0, loc)
    next_cr = strg.find("\n", loc)
    return strg[last_cr + 1 : next_cr] if next_cr >= 0 else strg[last_cr + 1 :]


class _UnboundedCache:
    def __init__(self):
        cache = {}
        cache_get = cache.get
        self.not_in_cache = not_in_cache = object()

        def get(_, key):
            return cache_get(key, not_in_cache)

        def set_(_, key, value):
            cache[key] = value

        def clear(_):
            cache.clear()

        self.size = None
        self.get = types.MethodType(get, self)
        self.set = types.MethodType(set_, self)
        self.clear = types.MethodType(clear, self)


class _FifoCache:
    def __init__(self, size):
        self.not_in_cache = not_in_cache = object()
        cache = {}
        keyring = [object()] * size
        cache_get = cache.get
        cache_pop = cache.pop
        keyiter = itertools.cycle(range(size))

        def get(_, key):
            return cache_get(key, not_in_cache)

        def set_(_, key, value):
            cache[key] = value
            i = next(keyiter)
            cache_pop(keyring[i], None)
            keyring[i] = key

        def clear(_):
            cache.clear()
            keyring[:] = [object()] * size

        self.size = size
        self.get = types.MethodType(get, self)
        self.set = types.MethodType(set_, self)
        self.clear = types.MethodType(clear, self)


class LRUMemo:
    """
    A memoizing mapping that retains `capacity` deleted items

    The memo tracks retained items by their access order; once `capacity` items
    are retained, the least recently used item is discarded.
    """

    def __init__(self, capacity):
        self._capacity = capacity
        self._active = {}
        self._memory = collections.OrderedDict()

    def __getitem__(self, key):
        try:
            return self._active[key]
        except KeyError:
            self._memory.move_to_end(key)
            return self._memory[key]

    def __setitem__(self, key, value):
        self._memory.pop(key, None)
        self._active[key] = value

    def __delitem__(self, key):
        try:
            value = self._active.pop(key)
        except KeyError:
            pass
        else:
            while len(self._memory) >= self._capacity:
                self._memory.popitem(last=False)
            self._memory[key] = value

    def clear(self):
        self._active.clear()
        self._memory.clear()


class UnboundedMemo(dict):
    """
    A memoizing mapping that retains all deleted items
    """

    def __delitem__(self, key):
        pass


def _escape_regex_range_chars(s: str) -> str:
    # escape these chars: ^-[]
    for c in r"\^-[]":
        s = s.replace(c, _bslash + c)
    s = s.replace("\n", r"\n")
    s = s.replace("\t", r"\t")
    return str(s)


def _collapse_string_to_ranges(
    s: Union[str, Iterable[str]], re_escape: bool = True
) -> str:
    def is_consecutive(c):
        c_int = ord(c)
        is_consecutive.prev, prev = c_int, is_consecutive.prev
        if c_int - prev > 1:
            is_consecutive.value = next(is_consecutive.counter)
        return is_consecutive.value

    is_consecutive.prev = 0  # type: ignore [attr-defined]
    is_consecutive.counter = itertools.count()  # type: ignore [attr-defined]
    is_consecutive.value = -1  # type: ignore [attr-defined]

    def escape_re_range_char(c):
        return "\\" + c if c in r"\^-][" else c

    def no_escape_re_range_char(c):
        return c

    if not re_escape:
        escape_re_range_char = no_escape_re_range_char

    ret = []
    s = "".join(sorted(set(s)))
    if len(s) > 3:
        for _, chars in itertools.groupby(s, key=is_consecutive):
            first = last = next(chars)
            last = collections.deque(
                itertools.chain(iter([last]), chars), maxlen=1
            ).pop()
            if first == last:
                ret.append(escape_re_range_char(first))
            else:
                sep = "" if ord(last) == ord(first) + 1 else "-"
                ret.append(
                    f"{escape_re_range_char(first)}{sep}{escape_re_range_char(last)}"
                )
    else:
        ret = [escape_re_range_char(c) for c in s]

    return "".join(ret)


def _flatten(ll: list) -> list:
    ret = []
    for i in ll:
        if isinstance(i, list):
            ret.extend(_flatten(i))
        else:
            ret.append(i)
    return ret


def _make_synonym_function(compat_name: str, fn: C) -> C:
    # In a future version, uncomment the code in the internal _inner() functions
    # to begin emitting DeprecationWarnings.

    # Unwrap staticmethod/classmethod
    fn = getattr(fn, "__func__", fn)

    # (Presence of 'self' arg in signature is used by explain_exception() methods, so we take
    # some extra steps to add it if present in decorated function.)
    if "self" == list(inspect.signature(fn).parameters)[0]:

        @wraps(fn)
        def _inner(self, *args, **kwargs):
            # warnings.warn(
            #     f"Deprecated - use {fn.__name__}", DeprecationWarning, stacklevel=3
            # )
            return fn(self, *args, **kwargs)

    else:

        @wraps(fn)
        def _inner(*args, **kwargs):
            # warnings.warn(
            #     f"Deprecated - use {fn.__name__}", DeprecationWarning, stacklevel=3
            # )
            return fn(*args, **kwargs)

    _inner.__doc__ = f"""Deprecated - use :class:`{fn.__name__}`"""
    _inner.__name__ = compat_name
    _inner.__annotations__ = fn.__annotations__
    if isinstance(fn, types.FunctionType):
        _inner.__kwdefaults__ = fn.__kwdefaults__
    elif isinstance(fn, type) and hasattr(fn, "__init__"):
        _inner.__kwdefaults__ = fn.__init__.__kwdefaults__
    else:
        _inner.__kwdefaults__ = None
    _inner.__qualname__ = fn.__qualname__
    return cast(C, _inner)


def replaced_by_pep8(fn: C) -> Callable[[Callable], C]:
    """
    Decorator for pre-PEP8 compatibility synonyms, to link them to the new function.
    """
    return lambda other: _make_synonym_function(other.__name__, fn)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/unicode.py
# ========================================================
# unicode.py

import sys
from itertools import filterfalse
from typing import List, Tuple, Union


class _lazyclassproperty:
    def __init__(self, fn):
        self.fn = fn
        self.__doc__ = fn.__doc__
        self.__name__ = fn.__name__

    def __get__(self, obj, cls):
        if cls is None:
            cls = type(obj)
        if not hasattr(cls, "_intern") or any(
            cls._intern is getattr(superclass, "_intern", [])
            for superclass in cls.__mro__[1:]
        ):
            cls._intern = {}
        attrname = self.fn.__name__
        if attrname not in cls._intern:
            cls._intern[attrname] = self.fn(cls)
        return cls._intern[attrname]


UnicodeRangeList = List[Union[Tuple[int, int], Tuple[int]]]


class unicode_set:
    """
    A set of Unicode characters, for language-specific strings for
    ``alphas``, ``nums``, ``alphanums``, and ``printables``.
    A unicode_set is defined by a list of ranges in the Unicode character
    set, in a class attribute ``_ranges``. Ranges can be specified using
    2-tuples or a 1-tuple, such as::

        _ranges = [
            (0x0020, 0x007e),
            (0x00a0, 0x00ff),
            (0x0100,),
            ]

    Ranges are left- and right-inclusive. A 1-tuple of (x,) is treated as (x, x).

    A unicode set can also be defined using multiple inheritance of other unicode sets::

        class CJK(Chinese, Japanese, Korean):
            pass
    """

    _ranges: UnicodeRangeList = []

    @_lazyclassproperty
    def _chars_for_ranges(cls):
        ret = []
        for cc in cls.__mro__:
            if cc is unicode_set:
                break
            for rr in getattr(cc, "_ranges", ()):
                ret.extend(range(rr[0], rr[-1] + 1))
        return [chr(c) for c in sorted(set(ret))]

    @_lazyclassproperty
    def printables(cls):
        """all non-whitespace characters in this range"""
        return "".join(filterfalse(str.isspace, cls._chars_for_ranges))

    @_lazyclassproperty
    def alphas(cls):
        """all alphabetic characters in this range"""
        return "".join(filter(str.isalpha, cls._chars_for_ranges))

    @_lazyclassproperty
    def nums(cls):
        """all numeric digit characters in this range"""
        return "".join(filter(str.isdigit, cls._chars_for_ranges))

    @_lazyclassproperty
    def alphanums(cls):
        """all alphanumeric characters in this range"""
        return cls.alphas + cls.nums

    @_lazyclassproperty
    def identchars(cls):
        """all characters in this range that are valid identifier characters, plus underscore '_'"""
        return "".join(
            sorted(
                set(
                    "".join(filter(str.isidentifier, cls._chars_for_ranges))
                    + "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
                    + ""
                    + "_"
                )
            )
        )

    @_lazyclassproperty
    def identbodychars(cls):
        """
        all characters in this range that are valid identifier body characters,
        plus the digits 0-9, and  (Unicode MIDDLE DOT)
        """
        return "".join(
            sorted(
                set(
                    cls.identchars
                    + "0123456789"
                    + "".join(
                        [c for c in cls._chars_for_ranges if ("_" + c).isidentifier()]
                    )
                )
            )
        )

    @_lazyclassproperty
    def identifier(cls):
        """
        a pyparsing Word expression for an identifier using this range's definitions for
        identchars and identbodychars
        """
        from pip._vendor.pyparsing import Word

        return Word(cls.identchars, cls.identbodychars)


class pyparsing_unicode(unicode_set):
    """
    A namespace class for defining common language unicode_sets.
    """

    # fmt: off

    # define ranges in language character sets
    _ranges: UnicodeRangeList = [
        (0x0020, sys.maxunicode),
    ]

    class BasicMultilingualPlane(unicode_set):
        """Unicode set for the Basic Multilingual Plane"""
        _ranges: UnicodeRangeList = [
            (0x0020, 0xFFFF),
        ]

    class Latin1(unicode_set):
        """Unicode set for Latin-1 Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0020, 0x007E),
            (0x00A0, 0x00FF),
        ]

    class LatinA(unicode_set):
        """Unicode set for Latin-A Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0100, 0x017F),
        ]

    class LatinB(unicode_set):
        """Unicode set for Latin-B Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0180, 0x024F),
        ]

    class Greek(unicode_set):
        """Unicode set for Greek Unicode Character Ranges"""
        _ranges: UnicodeRangeList = [
            (0x0342, 0x0345),
            (0x0370, 0x0377),
            (0x037A, 0x037F),
            (0x0384, 0x038A),
            (0x038C,),
            (0x038E, 0x03A1),
            (0x03A3, 0x03E1),
            (0x03F0, 0x03FF),
            (0x1D26, 0x1D2A),
            (0x1D5E,),
            (0x1D60,),
            (0x1D66, 0x1D6A),
            (0x1F00, 0x1F15),
            (0x1F18, 0x1F1D),
            (0x1F20, 0x1F45),
            (0x1F48, 0x1F4D),
            (0x1F50, 0x1F57),
            (0x1F59,),
            (0x1F5B,),
            (0x1F5D,),
            (0x1F5F, 0x1F7D),
            (0x1F80, 0x1FB4),
            (0x1FB6, 0x1FC4),
            (0x1FC6, 0x1FD3),
            (0x1FD6, 0x1FDB),
            (0x1FDD, 0x1FEF),
            (0x1FF2, 0x1FF4),
            (0x1FF6, 0x1FFE),
            (0x2129,),
            (0x2719, 0x271A),
            (0xAB65,),
            (0x10140, 0x1018D),
            (0x101A0,),
            (0x1D200, 0x1D245),
            (0x1F7A1, 0x1F7A7),
        ]

    class Cyrillic(unicode_set):
        """Unicode set for Cyrillic Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0400, 0x052F),
            (0x1C80, 0x1C88),
            (0x1D2B,),
            (0x1D78,),
            (0x2DE0, 0x2DFF),
            (0xA640, 0xA672),
            (0xA674, 0xA69F),
            (0xFE2E, 0xFE2F),
        ]

    class Chinese(unicode_set):
        """Unicode set for Chinese Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x2E80, 0x2E99),
            (0x2E9B, 0x2EF3),
            (0x31C0, 0x31E3),
            (0x3400, 0x4DB5),
            (0x4E00, 0x9FEF),
            (0xA700, 0xA707),
            (0xF900, 0xFA6D),
            (0xFA70, 0xFAD9),
            (0x16FE2, 0x16FE3),
            (0x1F210, 0x1F212),
            (0x1F214, 0x1F23B),
            (0x1F240, 0x1F248),
            (0x20000, 0x2A6D6),
            (0x2A700, 0x2B734),
            (0x2B740, 0x2B81D),
            (0x2B820, 0x2CEA1),
            (0x2CEB0, 0x2EBE0),
            (0x2F800, 0x2FA1D),
        ]

    class Japanese(unicode_set):
        """Unicode set for Japanese Unicode Character Range, combining Kanji, Hiragana, and Katakana ranges"""

        class Kanji(unicode_set):
            "Unicode set for Kanji Unicode Character Range"
            _ranges: UnicodeRangeList = [
                (0x4E00, 0x9FBF),
                (0x3000, 0x303F),
            ]

        class Hiragana(unicode_set):
            """Unicode set for Hiragana Unicode Character Range"""
            _ranges: UnicodeRangeList = [
                (0x3041, 0x3096),
                (0x3099, 0x30A0),
                (0x30FC,),
                (0xFF70,),
                (0x1B001,),
                (0x1B150, 0x1B152),
                (0x1F200,),
            ]

        class Katakana(unicode_set):
            """Unicode set for Katakana  Unicode Character Range"""
            _ranges: UnicodeRangeList = [
                (0x3099, 0x309C),
                (0x30A0, 0x30FF),
                (0x31F0, 0x31FF),
                (0x32D0, 0x32FE),
                (0xFF65, 0xFF9F),
                (0x1B000,),
                (0x1B164, 0x1B167),
                (0x1F201, 0x1F202),
                (0x1F213,),
            ]

         = Kanji
         = Katakana
         = Hiragana

        _ranges = (
            Kanji._ranges
            + Hiragana._ranges
            + Katakana._ranges
        )

    class Hangul(unicode_set):
        """Unicode set for Hangul (Korean) Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x1100, 0x11FF),
            (0x302E, 0x302F),
            (0x3131, 0x318E),
            (0x3200, 0x321C),
            (0x3260, 0x327B),
            (0x327E,),
            (0xA960, 0xA97C),
            (0xAC00, 0xD7A3),
            (0xD7B0, 0xD7C6),
            (0xD7CB, 0xD7FB),
            (0xFFA0, 0xFFBE),
            (0xFFC2, 0xFFC7),
            (0xFFCA, 0xFFCF),
            (0xFFD2, 0xFFD7),
            (0xFFDA, 0xFFDC),
        ]

    Korean = Hangul

    class CJK(Chinese, Japanese, Hangul):
        """Unicode set for combined Chinese, Japanese, and Korean (CJK) Unicode Character Range"""

    class Thai(unicode_set):
        """Unicode set for Thai Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0E01, 0x0E3A),
            (0x0E3F, 0x0E5B)
        ]

    class Arabic(unicode_set):
        """Unicode set for Arabic Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0600, 0x061B),
            (0x061E, 0x06FF),
            (0x0700, 0x077F),
        ]

    class Hebrew(unicode_set):
        """Unicode set for Hebrew Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0591, 0x05C7),
            (0x05D0, 0x05EA),
            (0x05EF, 0x05F4),
            (0xFB1D, 0xFB36),
            (0xFB38, 0xFB3C),
            (0xFB3E,),
            (0xFB40, 0xFB41),
            (0xFB43, 0xFB44),
            (0xFB46, 0xFB4F),
        ]

    class Devanagari(unicode_set):
        """Unicode set for Devanagari Unicode Character Range"""
        _ranges: UnicodeRangeList = [
            (0x0900, 0x097F),
            (0xA8E0, 0xA8FF)
        ]

    BMP = BasicMultilingualPlane

    # add language identifiers using language Unicode
     = Arabic
     = Chinese
     = Cyrillic
     = Greek
     = Hebrew
     = Japanese
     = Korean
     = Thai
     = Devanagari

    # fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/ansi.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
'''
This module generates ANSI character codes to printing colors to terminals.
See: http://en.wikipedia.org/wiki/ANSI_escape_code
'''

CSI = '\033['
OSC = '\033]'
BEL = '\a'


def code_to_chars(code):
    return CSI + str(code) + 'm'

def set_title(title):
    return OSC + '2;' + title + BEL

def clear_screen(mode=2):
    return CSI + str(mode) + 'J'

def clear_line(mode=2):
    return CSI + str(mode) + 'K'


class AnsiCodes(object):
    def __init__(self):
        # the subclasses declare class attributes which are numbers.
        # Upon instantiation we define instance attributes, which are the same
        # as the class attributes but wrapped with the ANSI escape sequence
        for name in dir(self):
            if not name.startswith('_'):
                value = getattr(self, name)
                setattr(self, name, code_to_chars(value))


class AnsiCursor(object):
    def UP(self, n=1):
        return CSI + str(n) + 'A'
    def DOWN(self, n=1):
        return CSI + str(n) + 'B'
    def FORWARD(self, n=1):
        return CSI + str(n) + 'C'
    def BACK(self, n=1):
        return CSI + str(n) + 'D'
    def POS(self, x=1, y=1):
        return CSI + str(y) + ';' + str(x) + 'H'


class AnsiFore(AnsiCodes):
    BLACK           = 30
    RED             = 31
    GREEN           = 32
    YELLOW          = 33
    BLUE            = 34
    MAGENTA         = 35
    CYAN            = 36
    WHITE           = 37
    RESET           = 39

    # These are fairly well supported, but not part of the standard.
    LIGHTBLACK_EX   = 90
    LIGHTRED_EX     = 91
    LIGHTGREEN_EX   = 92
    LIGHTYELLOW_EX  = 93
    LIGHTBLUE_EX    = 94
    LIGHTMAGENTA_EX = 95
    LIGHTCYAN_EX    = 96
    LIGHTWHITE_EX   = 97


class AnsiBack(AnsiCodes):
    BLACK           = 40
    RED             = 41
    GREEN           = 42
    YELLOW          = 43
    BLUE            = 44
    MAGENTA         = 45
    CYAN            = 46
    WHITE           = 47
    RESET           = 49

    # These are fairly well supported, but not part of the standard.
    LIGHTBLACK_EX   = 100
    LIGHTRED_EX     = 101
    LIGHTGREEN_EX   = 102
    LIGHTYELLOW_EX  = 103
    LIGHTBLUE_EX    = 104
    LIGHTMAGENTA_EX = 105
    LIGHTCYAN_EX    = 106
    LIGHTWHITE_EX   = 107


class AnsiStyle(AnsiCodes):
    BRIGHT    = 1
    DIM       = 2
    NORMAL    = 22
    RESET_ALL = 0

Fore   = AnsiFore()
Back   = AnsiBack()
Style  = AnsiStyle()
Cursor = AnsiCursor()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/win32.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.

# from winbase.h
STDOUT = -11
STDERR = -12

ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004

try:
    import ctypes
    from ctypes import LibraryLoader
    windll = LibraryLoader(ctypes.WinDLL)
    from ctypes import wintypes
except (AttributeError, ImportError):
    windll = None
    SetConsoleTextAttribute = lambda *_: None
    winapi_test = lambda *_: None
else:
    from ctypes import byref, Structure, c_char, POINTER

    COORD = wintypes._COORD

    class CONSOLE_SCREEN_BUFFER_INFO(Structure):
        """struct in wincon.h."""
        _fields_ = [
            ("dwSize", COORD),
            ("dwCursorPosition", COORD),
            ("wAttributes", wintypes.WORD),
            ("srWindow", wintypes.SMALL_RECT),
            ("dwMaximumWindowSize", COORD),
        ]
        def __str__(self):
            return '(%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d)' % (
                self.dwSize.Y, self.dwSize.X
                , self.dwCursorPosition.Y, self.dwCursorPosition.X
                , self.wAttributes
                , self.srWindow.Top, self.srWindow.Left, self.srWindow.Bottom, self.srWindow.Right
                , self.dwMaximumWindowSize.Y, self.dwMaximumWindowSize.X
            )

    _GetStdHandle = windll.kernel32.GetStdHandle
    _GetStdHandle.argtypes = [
        wintypes.DWORD,
    ]
    _GetStdHandle.restype = wintypes.HANDLE

    _GetConsoleScreenBufferInfo = windll.kernel32.GetConsoleScreenBufferInfo
    _GetConsoleScreenBufferInfo.argtypes = [
        wintypes.HANDLE,
        POINTER(CONSOLE_SCREEN_BUFFER_INFO),
    ]
    _GetConsoleScreenBufferInfo.restype = wintypes.BOOL

    _SetConsoleTextAttribute = windll.kernel32.SetConsoleTextAttribute
    _SetConsoleTextAttribute.argtypes = [
        wintypes.HANDLE,
        wintypes.WORD,
    ]
    _SetConsoleTextAttribute.restype = wintypes.BOOL

    _SetConsoleCursorPosition = windll.kernel32.SetConsoleCursorPosition
    _SetConsoleCursorPosition.argtypes = [
        wintypes.HANDLE,
        COORD,
    ]
    _SetConsoleCursorPosition.restype = wintypes.BOOL

    _FillConsoleOutputCharacterA = windll.kernel32.FillConsoleOutputCharacterA
    _FillConsoleOutputCharacterA.argtypes = [
        wintypes.HANDLE,
        c_char,
        wintypes.DWORD,
        COORD,
        POINTER(wintypes.DWORD),
    ]
    _FillConsoleOutputCharacterA.restype = wintypes.BOOL

    _FillConsoleOutputAttribute = windll.kernel32.FillConsoleOutputAttribute
    _FillConsoleOutputAttribute.argtypes = [
        wintypes.HANDLE,
        wintypes.WORD,
        wintypes.DWORD,
        COORD,
        POINTER(wintypes.DWORD),
    ]
    _FillConsoleOutputAttribute.restype = wintypes.BOOL

    _SetConsoleTitleW = windll.kernel32.SetConsoleTitleW
    _SetConsoleTitleW.argtypes = [
        wintypes.LPCWSTR
    ]
    _SetConsoleTitleW.restype = wintypes.BOOL

    _GetConsoleMode = windll.kernel32.GetConsoleMode
    _GetConsoleMode.argtypes = [
        wintypes.HANDLE,
        POINTER(wintypes.DWORD)
    ]
    _GetConsoleMode.restype = wintypes.BOOL

    _SetConsoleMode = windll.kernel32.SetConsoleMode
    _SetConsoleMode.argtypes = [
        wintypes.HANDLE,
        wintypes.DWORD
    ]
    _SetConsoleMode.restype = wintypes.BOOL

    def _winapi_test(handle):
        csbi = CONSOLE_SCREEN_BUFFER_INFO()
        success = _GetConsoleScreenBufferInfo(
            handle, byref(csbi))
        return bool(success)

    def winapi_test():
        return any(_winapi_test(h) for h in
                   (_GetStdHandle(STDOUT), _GetStdHandle(STDERR)))

    def GetConsoleScreenBufferInfo(stream_id=STDOUT):
        handle = _GetStdHandle(stream_id)
        csbi = CONSOLE_SCREEN_BUFFER_INFO()
        success = _GetConsoleScreenBufferInfo(
            handle, byref(csbi))
        return csbi

    def SetConsoleTextAttribute(stream_id, attrs):
        handle = _GetStdHandle(stream_id)
        return _SetConsoleTextAttribute(handle, attrs)

    def SetConsoleCursorPosition(stream_id, position, adjust=True):
        position = COORD(*position)
        # If the position is out of range, do nothing.
        if position.Y <= 0 or position.X <= 0:
            return
        # Adjust for Windows' SetConsoleCursorPosition:
        #    1. being 0-based, while ANSI is 1-based.
        #    2. expecting (x,y), while ANSI uses (y,x).
        adjusted_position = COORD(position.Y - 1, position.X - 1)
        if adjust:
            # Adjust for viewport's scroll position
            sr = GetConsoleScreenBufferInfo(STDOUT).srWindow
            adjusted_position.Y += sr.Top
            adjusted_position.X += sr.Left
        # Resume normal processing
        handle = _GetStdHandle(stream_id)
        return _SetConsoleCursorPosition(handle, adjusted_position)

    def FillConsoleOutputCharacter(stream_id, char, length, start):
        handle = _GetStdHandle(stream_id)
        char = c_char(char.encode())
        length = wintypes.DWORD(length)
        num_written = wintypes.DWORD(0)
        # Note that this is hard-coded for ANSI (vs wide) bytes.
        success = _FillConsoleOutputCharacterA(
            handle, char, length, start, byref(num_written))
        return num_written.value

    def FillConsoleOutputAttribute(stream_id, attr, length, start):
        ''' FillConsoleOutputAttribute( hConsole, csbi.wAttributes, dwConSize, coordScreen, &cCharsWritten )'''
        handle = _GetStdHandle(stream_id)
        attribute = wintypes.WORD(attr)
        length = wintypes.DWORD(length)
        num_written = wintypes.DWORD(0)
        # Note that this is hard-coded for ANSI (vs wide) bytes.
        return _FillConsoleOutputAttribute(
            handle, attribute, length, start, byref(num_written))

    def SetConsoleTitle(title):
        return _SetConsoleTitleW(title)

    def GetConsoleMode(handle):
        mode = wintypes.DWORD()
        success = _GetConsoleMode(handle, byref(mode))
        if not success:
            raise ctypes.WinError()
        return mode.value

    def SetConsoleMode(handle, mode):
        success = _SetConsoleMode(handle, mode)
        if not success:
            raise ctypes.WinError()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/ansitowin32.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import re
import sys
import os

from .ansi import AnsiFore, AnsiBack, AnsiStyle, Style, BEL
from .winterm import enable_vt_processing, WinTerm, WinColor, WinStyle
from .win32 import windll, winapi_test


winterm = None
if windll is not None:
    winterm = WinTerm()


class StreamWrapper(object):
    '''
    Wraps a stream (such as stdout), acting as a transparent proxy for all
    attribute access apart from method 'write()', which is delegated to our
    Converter instance.
    '''
    def __init__(self, wrapped, converter):
        # double-underscore everything to prevent clashes with names of
        # attributes on the wrapped stream object.
        self.__wrapped = wrapped
        self.__convertor = converter

    def __getattr__(self, name):
        return getattr(self.__wrapped, name)

    def __enter__(self, *args, **kwargs):
        # special method lookup bypasses __getattr__/__getattribute__, see
        # https://stackoverflow.com/questions/12632894/why-doesnt-getattr-work-with-exit
        # thus, contextlib magic methods are not proxied via __getattr__
        return self.__wrapped.__enter__(*args, **kwargs)

    def __exit__(self, *args, **kwargs):
        return self.__wrapped.__exit__(*args, **kwargs)

    def __setstate__(self, state):
        self.__dict__ = state

    def __getstate__(self):
        return self.__dict__

    def write(self, text):
        self.__convertor.write(text)

    def isatty(self):
        stream = self.__wrapped
        if 'PYCHARM_HOSTED' in os.environ:
            if stream is not None and (stream is sys.__stdout__ or stream is sys.__stderr__):
                return True
        try:
            stream_isatty = stream.isatty
        except AttributeError:
            return False
        else:
            return stream_isatty()

    @property
    def closed(self):
        stream = self.__wrapped
        try:
            return stream.closed
        # AttributeError in the case that the stream doesn't support being closed
        # ValueError for the case that the stream has already been detached when atexit runs
        except (AttributeError, ValueError):
            return True


class AnsiToWin32(object):
    '''
    Implements a 'write()' method which, on Windows, will strip ANSI character
    sequences from the text, and if outputting to a tty, will convert them into
    win32 function calls.
    '''
    ANSI_CSI_RE = re.compile('\001?\033\\[((?:\\d|;)*)([a-zA-Z])\002?')   # Control Sequence Introducer
    ANSI_OSC_RE = re.compile('\001?\033\\]([^\a]*)(\a)\002?')             # Operating System Command

    def __init__(self, wrapped, convert=None, strip=None, autoreset=False):
        # The wrapped stream (normally sys.stdout or sys.stderr)
        self.wrapped = wrapped

        # should we reset colors to defaults after every .write()
        self.autoreset = autoreset

        # create the proxy wrapping our output stream
        self.stream = StreamWrapper(wrapped, self)

        on_windows = os.name == 'nt'
        # We test if the WinAPI works, because even if we are on Windows
        # we may be using a terminal that doesn't support the WinAPI
        # (e.g. Cygwin Terminal). In this case it's up to the terminal
        # to support the ANSI codes.
        conversion_supported = on_windows and winapi_test()
        try:
            fd = wrapped.fileno()
        except Exception:
            fd = -1
        system_has_native_ansi = not on_windows or enable_vt_processing(fd)
        have_tty = not self.stream.closed and self.stream.isatty()
        need_conversion = conversion_supported and not system_has_native_ansi

        # should we strip ANSI sequences from our output?
        if strip is None:
            strip = need_conversion or not have_tty
        self.strip = strip

        # should we should convert ANSI sequences into win32 calls?
        if convert is None:
            convert = need_conversion and have_tty
        self.convert = convert

        # dict of ansi codes to win32 functions and parameters
        self.win32_calls = self.get_win32_calls()

        # are we wrapping stderr?
        self.on_stderr = self.wrapped is sys.stderr

    def should_wrap(self):
        '''
        True if this class is actually needed. If false, then the output
        stream will not be affected, nor will win32 calls be issued, so
        wrapping stdout is not actually required. This will generally be
        False on non-Windows platforms, unless optional functionality like
        autoreset has been requested using kwargs to init()
        '''
        return self.convert or self.strip or self.autoreset

    def get_win32_calls(self):
        if self.convert and winterm:
            return {
                AnsiStyle.RESET_ALL: (winterm.reset_all, ),
                AnsiStyle.BRIGHT: (winterm.style, WinStyle.BRIGHT),
                AnsiStyle.DIM: (winterm.style, WinStyle.NORMAL),
                AnsiStyle.NORMAL: (winterm.style, WinStyle.NORMAL),
                AnsiFore.BLACK: (winterm.fore, WinColor.BLACK),
                AnsiFore.RED: (winterm.fore, WinColor.RED),
                AnsiFore.GREEN: (winterm.fore, WinColor.GREEN),
                AnsiFore.YELLOW: (winterm.fore, WinColor.YELLOW),
                AnsiFore.BLUE: (winterm.fore, WinColor.BLUE),
                AnsiFore.MAGENTA: (winterm.fore, WinColor.MAGENTA),
                AnsiFore.CYAN: (winterm.fore, WinColor.CYAN),
                AnsiFore.WHITE: (winterm.fore, WinColor.GREY),
                AnsiFore.RESET: (winterm.fore, ),
                AnsiFore.LIGHTBLACK_EX: (winterm.fore, WinColor.BLACK, True),
                AnsiFore.LIGHTRED_EX: (winterm.fore, WinColor.RED, True),
                AnsiFore.LIGHTGREEN_EX: (winterm.fore, WinColor.GREEN, True),
                AnsiFore.LIGHTYELLOW_EX: (winterm.fore, WinColor.YELLOW, True),
                AnsiFore.LIGHTBLUE_EX: (winterm.fore, WinColor.BLUE, True),
                AnsiFore.LIGHTMAGENTA_EX: (winterm.fore, WinColor.MAGENTA, True),
                AnsiFore.LIGHTCYAN_EX: (winterm.fore, WinColor.CYAN, True),
                AnsiFore.LIGHTWHITE_EX: (winterm.fore, WinColor.GREY, True),
                AnsiBack.BLACK: (winterm.back, WinColor.BLACK),
                AnsiBack.RED: (winterm.back, WinColor.RED),
                AnsiBack.GREEN: (winterm.back, WinColor.GREEN),
                AnsiBack.YELLOW: (winterm.back, WinColor.YELLOW),
                AnsiBack.BLUE: (winterm.back, WinColor.BLUE),
                AnsiBack.MAGENTA: (winterm.back, WinColor.MAGENTA),
                AnsiBack.CYAN: (winterm.back, WinColor.CYAN),
                AnsiBack.WHITE: (winterm.back, WinColor.GREY),
                AnsiBack.RESET: (winterm.back, ),
                AnsiBack.LIGHTBLACK_EX: (winterm.back, WinColor.BLACK, True),
                AnsiBack.LIGHTRED_EX: (winterm.back, WinColor.RED, True),
                AnsiBack.LIGHTGREEN_EX: (winterm.back, WinColor.GREEN, True),
                AnsiBack.LIGHTYELLOW_EX: (winterm.back, WinColor.YELLOW, True),
                AnsiBack.LIGHTBLUE_EX: (winterm.back, WinColor.BLUE, True),
                AnsiBack.LIGHTMAGENTA_EX: (winterm.back, WinColor.MAGENTA, True),
                AnsiBack.LIGHTCYAN_EX: (winterm.back, WinColor.CYAN, True),
                AnsiBack.LIGHTWHITE_EX: (winterm.back, WinColor.GREY, True),
            }
        return dict()

    def write(self, text):
        if self.strip or self.convert:
            self.write_and_convert(text)
        else:
            self.wrapped.write(text)
            self.wrapped.flush()
        if self.autoreset:
            self.reset_all()


    def reset_all(self):
        if self.convert:
            self.call_win32('m', (0,))
        elif not self.strip and not self.stream.closed:
            self.wrapped.write(Style.RESET_ALL)


    def write_and_convert(self, text):
        '''
        Write the given text to our wrapped stream, stripping any ANSI
        sequences from the text, and optionally converting them into win32
        calls.
        '''
        cursor = 0
        text = self.convert_osc(text)
        for match in self.ANSI_CSI_RE.finditer(text):
            start, end = match.span()
            self.write_plain_text(text, cursor, start)
            self.convert_ansi(*match.groups())
            cursor = end
        self.write_plain_text(text, cursor, len(text))


    def write_plain_text(self, text, start, end):
        if start < end:
            self.wrapped.write(text[start:end])
            self.wrapped.flush()


    def convert_ansi(self, paramstring, command):
        if self.convert:
            params = self.extract_params(command, paramstring)
            self.call_win32(command, params)


    def extract_params(self, command, paramstring):
        if command in 'Hf':
            params = tuple(int(p) if len(p) != 0 else 1 for p in paramstring.split(';'))
            while len(params) < 2:
                # defaults:
                params = params + (1,)
        else:
            params = tuple(int(p) for p in paramstring.split(';') if len(p) != 0)
            if len(params) == 0:
                # defaults:
                if command in 'JKm':
                    params = (0,)
                elif command in 'ABCD':
                    params = (1,)

        return params


    def call_win32(self, command, params):
        if command == 'm':
            for param in params:
                if param in self.win32_calls:
                    func_args = self.win32_calls[param]
                    func = func_args[0]
                    args = func_args[1:]
                    kwargs = dict(on_stderr=self.on_stderr)
                    func(*args, **kwargs)
        elif command in 'J':
            winterm.erase_screen(params[0], on_stderr=self.on_stderr)
        elif command in 'K':
            winterm.erase_line(params[0], on_stderr=self.on_stderr)
        elif command in 'Hf':     # cursor position - absolute
            winterm.set_cursor_position(params, on_stderr=self.on_stderr)
        elif command in 'ABCD':   # cursor position - relative
            n = params[0]
            # A - up, B - down, C - forward, D - back
            x, y = {'A': (0, -n), 'B': (0, n), 'C': (n, 0), 'D': (-n, 0)}[command]
            winterm.cursor_adjust(x, y, on_stderr=self.on_stderr)


    def convert_osc(self, text):
        for match in self.ANSI_OSC_RE.finditer(text):
            start, end = match.span()
            text = text[:start] + text[end:]
            paramstring, command = match.groups()
            if command == BEL:
                if paramstring.count(";") == 1:
                    params = paramstring.split(";")
                    # 0 - change title and icon (we will only change title)
                    # 1 - change icon (we don't support this)
                    # 2 - change title
                    if params[0] in '02':
                        winterm.set_title(params[1])
        return text


    def flush(self):
        self.wrapped.flush()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/__init__.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
from .initialise import init, deinit, reinit, colorama_text, just_fix_windows_console
from .ansi import Fore, Back, Style, Cursor
from .ansitowin32 import AnsiToWin32

__version__ = '0.4.6'



# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/winterm.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
try:
    from msvcrt import get_osfhandle
except ImportError:
    def get_osfhandle(_):
        raise OSError("This isn't windows!")


from . import win32

# from wincon.h
class WinColor(object):
    BLACK   = 0
    BLUE    = 1
    GREEN   = 2
    CYAN    = 3
    RED     = 4
    MAGENTA = 5
    YELLOW  = 6
    GREY    = 7

# from wincon.h
class WinStyle(object):
    NORMAL              = 0x00 # dim text, dim background
    BRIGHT              = 0x08 # bright text, dim background
    BRIGHT_BACKGROUND   = 0x80 # dim text, bright background

class WinTerm(object):

    def __init__(self):
        self._default = win32.GetConsoleScreenBufferInfo(win32.STDOUT).wAttributes
        self.set_attrs(self._default)
        self._default_fore = self._fore
        self._default_back = self._back
        self._default_style = self._style
        # In order to emulate LIGHT_EX in windows, we borrow the BRIGHT style.
        # So that LIGHT_EX colors and BRIGHT style do not clobber each other,
        # we track them separately, since LIGHT_EX is overwritten by Fore/Back
        # and BRIGHT is overwritten by Style codes.
        self._light = 0

    def get_attrs(self):
        return self._fore + self._back * 16 + (self._style | self._light)

    def set_attrs(self, value):
        self._fore = value & 7
        self._back = (value >> 4) & 7
        self._style = value & (WinStyle.BRIGHT | WinStyle.BRIGHT_BACKGROUND)

    def reset_all(self, on_stderr=None):
        self.set_attrs(self._default)
        self.set_console(attrs=self._default)
        self._light = 0

    def fore(self, fore=None, light=False, on_stderr=False):
        if fore is None:
            fore = self._default_fore
        self._fore = fore
        # Emulate LIGHT_EX with BRIGHT Style
        if light:
            self._light |= WinStyle.BRIGHT
        else:
            self._light &= ~WinStyle.BRIGHT
        self.set_console(on_stderr=on_stderr)

    def back(self, back=None, light=False, on_stderr=False):
        if back is None:
            back = self._default_back
        self._back = back
        # Emulate LIGHT_EX with BRIGHT_BACKGROUND Style
        if light:
            self._light |= WinStyle.BRIGHT_BACKGROUND
        else:
            self._light &= ~WinStyle.BRIGHT_BACKGROUND
        self.set_console(on_stderr=on_stderr)

    def style(self, style=None, on_stderr=False):
        if style is None:
            style = self._default_style
        self._style = style
        self.set_console(on_stderr=on_stderr)

    def set_console(self, attrs=None, on_stderr=False):
        if attrs is None:
            attrs = self.get_attrs()
        handle = win32.STDOUT
        if on_stderr:
            handle = win32.STDERR
        win32.SetConsoleTextAttribute(handle, attrs)

    def get_position(self, handle):
        position = win32.GetConsoleScreenBufferInfo(handle).dwCursorPosition
        # Because Windows coordinates are 0-based,
        # and win32.SetConsoleCursorPosition expects 1-based.
        position.X += 1
        position.Y += 1
        return position

    def set_cursor_position(self, position=None, on_stderr=False):
        if position is None:
            # I'm not currently tracking the position, so there is no default.
            # position = self.get_position()
            return
        handle = win32.STDOUT
        if on_stderr:
            handle = win32.STDERR
        win32.SetConsoleCursorPosition(handle, position)

    def cursor_adjust(self, x, y, on_stderr=False):
        handle = win32.STDOUT
        if on_stderr:
            handle = win32.STDERR
        position = self.get_position(handle)
        adjusted_position = (position.Y + y, position.X + x)
        win32.SetConsoleCursorPosition(handle, adjusted_position, adjust=False)

    def erase_screen(self, mode=0, on_stderr=False):
        # 0 should clear from the cursor to the end of the screen.
        # 1 should clear from the cursor to the beginning of the screen.
        # 2 should clear the entire screen, and move cursor to (1,1)
        handle = win32.STDOUT
        if on_stderr:
            handle = win32.STDERR
        csbi = win32.GetConsoleScreenBufferInfo(handle)
        # get the number of character cells in the current buffer
        cells_in_screen = csbi.dwSize.X * csbi.dwSize.Y
        # get number of character cells before current cursor position
        cells_before_cursor = csbi.dwSize.X * csbi.dwCursorPosition.Y + csbi.dwCursorPosition.X
        if mode == 0:
            from_coord = csbi.dwCursorPosition
            cells_to_erase = cells_in_screen - cells_before_cursor
        elif mode == 1:
            from_coord = win32.COORD(0, 0)
            cells_to_erase = cells_before_cursor
        elif mode == 2:
            from_coord = win32.COORD(0, 0)
            cells_to_erase = cells_in_screen
        else:
            # invalid mode
            return
        # fill the entire screen with blanks
        win32.FillConsoleOutputCharacter(handle, ' ', cells_to_erase, from_coord)
        # now set the buffer's attributes accordingly
        win32.FillConsoleOutputAttribute(handle, self.get_attrs(), cells_to_erase, from_coord)
        if mode == 2:
            # put the cursor where needed
            win32.SetConsoleCursorPosition(handle, (1, 1))

    def erase_line(self, mode=0, on_stderr=False):
        # 0 should clear from the cursor to the end of the line.
        # 1 should clear from the cursor to the beginning of the line.
        # 2 should clear the entire line.
        handle = win32.STDOUT
        if on_stderr:
            handle = win32.STDERR
        csbi = win32.GetConsoleScreenBufferInfo(handle)
        if mode == 0:
            from_coord = csbi.dwCursorPosition
            cells_to_erase = csbi.dwSize.X - csbi.dwCursorPosition.X
        elif mode == 1:
            from_coord = win32.COORD(0, csbi.dwCursorPosition.Y)
            cells_to_erase = csbi.dwCursorPosition.X
        elif mode == 2:
            from_coord = win32.COORD(0, csbi.dwCursorPosition.Y)
            cells_to_erase = csbi.dwSize.X
        else:
            # invalid mode
            return
        # fill the entire screen with blanks
        win32.FillConsoleOutputCharacter(handle, ' ', cells_to_erase, from_coord)
        # now set the buffer's attributes accordingly
        win32.FillConsoleOutputAttribute(handle, self.get_attrs(), cells_to_erase, from_coord)

    def set_title(self, title):
        win32.SetConsoleTitle(title)


def enable_vt_processing(fd):
    if win32.windll is None or not win32.winapi_test():
        return False

    try:
        handle = get_osfhandle(fd)
        mode = win32.GetConsoleMode(handle)
        win32.SetConsoleMode(
            handle,
            mode | win32.ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        )

        mode = win32.GetConsoleMode(handle)
        if mode & win32.ENABLE_VIRTUAL_TERMINAL_PROCESSING:
            return True
    # Can get TypeError in testsuite where 'fd' is a Mock()
    except (OSError, TypeError):
        return False


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/initialise.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


def _wipe_internal_state_for_tests():
    global orig_stdout, orig_stderr
    orig_stdout = None
    orig_stderr = None

    global wrapped_stdout, wrapped_stderr
    wrapped_stdout = None
    wrapped_stderr = None

    global atexit_done
    atexit_done = False

    global fixed_windows_console
    fixed_windows_console = False

    try:
        # no-op if it wasn't registered
        atexit.unregister(reset_all)
    except AttributeError:
        # python 2: no atexit.unregister. Oh well, we did our best.
        pass


def reset_all():
    if AnsiToWin32 is not None:    # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):

    if not wrap and any([autoreset, convert, strip]):
        raise ValueError('wrap=False conflicts with any other arg=True')

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = \
            wrap_stream(orig_stdout, convert, strip, autoreset, wrap)
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = \
            wrap_stream(orig_stderr, convert, strip, autoreset, wrap)

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


def just_fix_windows_console():
    global fixed_windows_console

    if sys.platform != "win32":
        return
    if fixed_windows_console:
        return
    if wrapped_stdout is not None or wrapped_stderr is not None:
        # Someone already ran init() and it did stuff, so we won't second-guess them
        return

    # On newer versions of Windows, AnsiToWin32.__init__ will implicitly enable the
    # native ANSI support in the console as a side-effect. We only need to actually
    # replace sys.stdout/stderr if we're in the old-style conversion mode.
    new_stdout = AnsiToWin32(sys.stdout, convert=None, strip=None, autoreset=False)
    if new_stdout.convert:
        sys.stdout = new_stdout
    new_stderr = AnsiToWin32(sys.stderr, convert=None, strip=None, autoreset=False)
    if new_stderr.convert:
        sys.stderr = new_stderr

    fixed_windows_console = True

@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream,
            convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream


# Use this for initial setup as well, to reduce code duplication
_wipe_internal_state_for_tests()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/initialise_test.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import sys
from unittest import TestCase, main, skipUnless

try:
    from unittest.mock import patch, Mock
except ImportError:
    from mock import patch, Mock

from ..ansitowin32 import StreamWrapper
from ..initialise import init, just_fix_windows_console, _wipe_internal_state_for_tests
from .utils import osname, replace_by

orig_stdout = sys.stdout
orig_stderr = sys.stderr


class InitTest(TestCase):

    @skipUnless(sys.stdout.isatty(), "sys.stdout is not a tty")
    def setUp(self):
        # sanity check
        self.assertNotWrapped()

    def tearDown(self):
        _wipe_internal_state_for_tests()
        sys.stdout = orig_stdout
        sys.stderr = orig_stderr

    def assertWrapped(self):
        self.assertIsNot(sys.stdout, orig_stdout, 'stdout should be wrapped')
        self.assertIsNot(sys.stderr, orig_stderr, 'stderr should be wrapped')
        self.assertTrue(isinstance(sys.stdout, StreamWrapper),
            'bad stdout wrapper')
        self.assertTrue(isinstance(sys.stderr, StreamWrapper),
            'bad stderr wrapper')

    def assertNotWrapped(self):
        self.assertIs(sys.stdout, orig_stdout, 'stdout should not be wrapped')
        self.assertIs(sys.stderr, orig_stderr, 'stderr should not be wrapped')

    @patch('colorama.initialise.reset_all')
    @patch('colorama.ansitowin32.winapi_test', lambda *_: True)
    @patch('colorama.ansitowin32.enable_vt_processing', lambda *_: False)
    def testInitWrapsOnWindows(self, _):
        with osname("nt"):
            init()
            self.assertWrapped()

    @patch('colorama.initialise.reset_all')
    @patch('colorama.ansitowin32.winapi_test', lambda *_: False)
    def testInitDoesntWrapOnEmulatedWindows(self, _):
        with osname("nt"):
            init()
            self.assertNotWrapped()

    def testInitDoesntWrapOnNonWindows(self):
        with osname("posix"):
            init()
            self.assertNotWrapped()

    def testInitDoesntWrapIfNone(self):
        with replace_by(None):
            init()
            # We can't use assertNotWrapped here because replace_by(None)
            # changes stdout/stderr already.
            self.assertIsNone(sys.stdout)
            self.assertIsNone(sys.stderr)

    def testInitAutoresetOnWrapsOnAllPlatforms(self):
        with osname("posix"):
            init(autoreset=True)
            self.assertWrapped()

    def testInitWrapOffDoesntWrapOnWindows(self):
        with osname("nt"):
            init(wrap=False)
            self.assertNotWrapped()

    def testInitWrapOffIncompatibleWithAutoresetOn(self):
        self.assertRaises(ValueError, lambda: init(autoreset=True, wrap=False))

    @patch('colorama.win32.SetConsoleTextAttribute')
    @patch('colorama.initialise.AnsiToWin32')
    def testAutoResetPassedOn(self, mockATW32, _):
        with osname("nt"):
            init(autoreset=True)
            self.assertEqual(len(mockATW32.call_args_list), 2)
            self.assertEqual(mockATW32.call_args_list[1][1]['autoreset'], True)
            self.assertEqual(mockATW32.call_args_list[0][1]['autoreset'], True)

    @patch('colorama.initialise.AnsiToWin32')
    def testAutoResetChangeable(self, mockATW32):
        with osname("nt"):
            init()

            init(autoreset=True)
            self.assertEqual(len(mockATW32.call_args_list), 4)
            self.assertEqual(mockATW32.call_args_list[2][1]['autoreset'], True)
            self.assertEqual(mockATW32.call_args_list[3][1]['autoreset'], True)

            init()
            self.assertEqual(len(mockATW32.call_args_list), 6)
            self.assertEqual(
                mockATW32.call_args_list[4][1]['autoreset'], False)
            self.assertEqual(
                mockATW32.call_args_list[5][1]['autoreset'], False)


    @patch('colorama.initialise.atexit.register')
    def testAtexitRegisteredOnlyOnce(self, mockRegister):
        init()
        self.assertTrue(mockRegister.called)
        mockRegister.reset_mock()
        init()
        self.assertFalse(mockRegister.called)


class JustFixWindowsConsoleTest(TestCase):
    def _reset(self):
        _wipe_internal_state_for_tests()
        sys.stdout = orig_stdout
        sys.stderr = orig_stderr

    def tearDown(self):
        self._reset()

    @patch("colorama.ansitowin32.winapi_test", lambda: True)
    def testJustFixWindowsConsole(self):
        if sys.platform != "win32":
            # just_fix_windows_console should be a no-op
            just_fix_windows_console()
            self.assertIs(sys.stdout, orig_stdout)
            self.assertIs(sys.stderr, orig_stderr)
        else:
            def fake_std():
                # Emulate stdout=not a tty, stderr=tty
                # to check that we handle both cases correctly
                stdout = Mock()
                stdout.closed = False
                stdout.isatty.return_value = False
                stdout.fileno.return_value = 1
                sys.stdout = stdout

                stderr = Mock()
                stderr.closed = False
                stderr.isatty.return_value = True
                stderr.fileno.return_value = 2
                sys.stderr = stderr

            for native_ansi in [False, True]:
                with patch(
                    'colorama.ansitowin32.enable_vt_processing',
                    lambda *_: native_ansi
                ):
                    self._reset()
                    fake_std()

                    # Regular single-call test
                    prev_stdout = sys.stdout
                    prev_stderr = sys.stderr
                    just_fix_windows_console()
                    self.assertIs(sys.stdout, prev_stdout)
                    if native_ansi:
                        self.assertIs(sys.stderr, prev_stderr)
                    else:
                        self.assertIsNot(sys.stderr, prev_stderr)

                    # second call without resetting is always a no-op
                    prev_stdout = sys.stdout
                    prev_stderr = sys.stderr
                    just_fix_windows_console()
                    self.assertIs(sys.stdout, prev_stdout)
                    self.assertIs(sys.stderr, prev_stderr)

                    self._reset()
                    fake_std()

                    # If init() runs first, just_fix_windows_console should be a no-op
                    init()
                    prev_stdout = sys.stdout
                    prev_stderr = sys.stderr
                    just_fix_windows_console()
                    self.assertIs(prev_stdout, sys.stdout)
                    self.assertIs(prev_stderr, sys.stderr)


if __name__ == '__main__':
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/utils.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
from contextlib import contextmanager
from io import StringIO
import sys
import os


class StreamTTY(StringIO):
    def isatty(self):
        return True

class StreamNonTTY(StringIO):
    def isatty(self):
        return False

@contextmanager
def osname(name):
    orig = os.name
    os.name = name
    yield
    os.name = orig

@contextmanager
def replace_by(stream):
    orig_stdout = sys.stdout
    orig_stderr = sys.stderr
    sys.stdout = stream
    sys.stderr = stream
    yield
    sys.stdout = orig_stdout
    sys.stderr = orig_stderr

@contextmanager
def replace_original_by(stream):
    orig_stdout = sys.__stdout__
    orig_stderr = sys.__stderr__
    sys.__stdout__ = stream
    sys.__stderr__ = stream
    yield
    sys.__stdout__ = orig_stdout
    sys.__stderr__ = orig_stderr

@contextmanager
def pycharm():
    os.environ["PYCHARM_HOSTED"] = "1"
    non_tty = StreamNonTTY()
    with replace_by(non_tty), replace_original_by(non_tty):
        yield
    del os.environ["PYCHARM_HOSTED"]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/ansi_test.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import sys
from unittest import TestCase, main

from ..ansi import Back, Fore, Style
from ..ansitowin32 import AnsiToWin32

stdout_orig = sys.stdout
stderr_orig = sys.stderr


class AnsiTest(TestCase):

    def setUp(self):
        # sanity check: stdout should be a file or StringIO object.
        # It will only be AnsiToWin32 if init() has previously wrapped it
        self.assertNotEqual(type(sys.stdout), AnsiToWin32)
        self.assertNotEqual(type(sys.stderr), AnsiToWin32)

    def tearDown(self):
        sys.stdout = stdout_orig
        sys.stderr = stderr_orig


    def testForeAttributes(self):
        self.assertEqual(Fore.BLACK, '\033[30m')
        self.assertEqual(Fore.RED, '\033[31m')
        self.assertEqual(Fore.GREEN, '\033[32m')
        self.assertEqual(Fore.YELLOW, '\033[33m')
        self.assertEqual(Fore.BLUE, '\033[34m')
        self.assertEqual(Fore.MAGENTA, '\033[35m')
        self.assertEqual(Fore.CYAN, '\033[36m')
        self.assertEqual(Fore.WHITE, '\033[37m')
        self.assertEqual(Fore.RESET, '\033[39m')

        # Check the light, extended versions.
        self.assertEqual(Fore.LIGHTBLACK_EX, '\033[90m')
        self.assertEqual(Fore.LIGHTRED_EX, '\033[91m')
        self.assertEqual(Fore.LIGHTGREEN_EX, '\033[92m')
        self.assertEqual(Fore.LIGHTYELLOW_EX, '\033[93m')
        self.assertEqual(Fore.LIGHTBLUE_EX, '\033[94m')
        self.assertEqual(Fore.LIGHTMAGENTA_EX, '\033[95m')
        self.assertEqual(Fore.LIGHTCYAN_EX, '\033[96m')
        self.assertEqual(Fore.LIGHTWHITE_EX, '\033[97m')


    def testBackAttributes(self):
        self.assertEqual(Back.BLACK, '\033[40m')
        self.assertEqual(Back.RED, '\033[41m')
        self.assertEqual(Back.GREEN, '\033[42m')
        self.assertEqual(Back.YELLOW, '\033[43m')
        self.assertEqual(Back.BLUE, '\033[44m')
        self.assertEqual(Back.MAGENTA, '\033[45m')
        self.assertEqual(Back.CYAN, '\033[46m')
        self.assertEqual(Back.WHITE, '\033[47m')
        self.assertEqual(Back.RESET, '\033[49m')

        # Check the light, extended versions.
        self.assertEqual(Back.LIGHTBLACK_EX, '\033[100m')
        self.assertEqual(Back.LIGHTRED_EX, '\033[101m')
        self.assertEqual(Back.LIGHTGREEN_EX, '\033[102m')
        self.assertEqual(Back.LIGHTYELLOW_EX, '\033[103m')
        self.assertEqual(Back.LIGHTBLUE_EX, '\033[104m')
        self.assertEqual(Back.LIGHTMAGENTA_EX, '\033[105m')
        self.assertEqual(Back.LIGHTCYAN_EX, '\033[106m')
        self.assertEqual(Back.LIGHTWHITE_EX, '\033[107m')


    def testStyleAttributes(self):
        self.assertEqual(Style.DIM, '\033[2m')
        self.assertEqual(Style.NORMAL, '\033[22m')
        self.assertEqual(Style.BRIGHT, '\033[1m')


if __name__ == '__main__':
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/isatty_test.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import sys
from unittest import TestCase, main

from ..ansitowin32 import StreamWrapper, AnsiToWin32
from .utils import pycharm, replace_by, replace_original_by, StreamTTY, StreamNonTTY


def is_a_tty(stream):
    return StreamWrapper(stream, None).isatty()

class IsattyTest(TestCase):

    def test_TTY(self):
        tty = StreamTTY()
        self.assertTrue(is_a_tty(tty))
        with pycharm():
            self.assertTrue(is_a_tty(tty))

    def test_nonTTY(self):
        non_tty = StreamNonTTY()
        self.assertFalse(is_a_tty(non_tty))
        with pycharm():
            self.assertFalse(is_a_tty(non_tty))

    def test_withPycharm(self):
        with pycharm():
            self.assertTrue(is_a_tty(sys.stderr))
            self.assertTrue(is_a_tty(sys.stdout))

    def test_withPycharmTTYOverride(self):
        tty = StreamTTY()
        with pycharm(), replace_by(tty):
            self.assertTrue(is_a_tty(tty))

    def test_withPycharmNonTTYOverride(self):
        non_tty = StreamNonTTY()
        with pycharm(), replace_by(non_tty):
            self.assertFalse(is_a_tty(non_tty))

    def test_withPycharmNoneOverride(self):
        with pycharm():
            with replace_by(None), replace_original_by(None):
                self.assertFalse(is_a_tty(None))
                self.assertFalse(is_a_tty(StreamNonTTY()))
                self.assertTrue(is_a_tty(StreamTTY()))

    def test_withPycharmStreamWrapped(self):
        with pycharm():
            self.assertTrue(AnsiToWin32(StreamTTY()).stream.isatty())
            self.assertFalse(AnsiToWin32(StreamNonTTY()).stream.isatty())
            self.assertTrue(AnsiToWin32(sys.stdout).stream.isatty())
            self.assertTrue(AnsiToWin32(sys.stderr).stream.isatty())


if __name__ == '__main__':
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/winterm_test.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import sys
from unittest import TestCase, main, skipUnless

try:
    from unittest.mock import Mock, patch
except ImportError:
    from mock import Mock, patch

from ..winterm import WinColor, WinStyle, WinTerm


class WinTermTest(TestCase):

    @patch('colorama.winterm.win32')
    def testInit(self, mockWin32):
        mockAttr = Mock()
        mockAttr.wAttributes = 7 + 6 * 16 + 8
        mockWin32.GetConsoleScreenBufferInfo.return_value = mockAttr
        term = WinTerm()
        self.assertEqual(term._fore, 7)
        self.assertEqual(term._back, 6)
        self.assertEqual(term._style, 8)

    @skipUnless(sys.platform.startswith("win"), "requires Windows")
    def testGetAttrs(self):
        term = WinTerm()

        term._fore = 0
        term._back = 0
        term._style = 0
        self.assertEqual(term.get_attrs(), 0)

        term._fore = WinColor.YELLOW
        self.assertEqual(term.get_attrs(), WinColor.YELLOW)

        term._back = WinColor.MAGENTA
        self.assertEqual(
            term.get_attrs(),
            WinColor.YELLOW + WinColor.MAGENTA * 16)

        term._style = WinStyle.BRIGHT
        self.assertEqual(
            term.get_attrs(),
            WinColor.YELLOW + WinColor.MAGENTA * 16 + WinStyle.BRIGHT)

    @patch('colorama.winterm.win32')
    def testResetAll(self, mockWin32):
        mockAttr = Mock()
        mockAttr.wAttributes = 1 + 2 * 16 + 8
        mockWin32.GetConsoleScreenBufferInfo.return_value = mockAttr
        term = WinTerm()

        term.set_console = Mock()
        term._fore = -1
        term._back = -1
        term._style = -1

        term.reset_all()

        self.assertEqual(term._fore, 1)
        self.assertEqual(term._back, 2)
        self.assertEqual(term._style, 8)
        self.assertEqual(term.set_console.called, True)

    @skipUnless(sys.platform.startswith("win"), "requires Windows")
    def testFore(self):
        term = WinTerm()
        term.set_console = Mock()
        term._fore = 0

        term.fore(5)

        self.assertEqual(term._fore, 5)
        self.assertEqual(term.set_console.called, True)

    @skipUnless(sys.platform.startswith("win"), "requires Windows")
    def testBack(self):
        term = WinTerm()
        term.set_console = Mock()
        term._back = 0

        term.back(5)

        self.assertEqual(term._back, 5)
        self.assertEqual(term.set_console.called, True)

    @skipUnless(sys.platform.startswith("win"), "requires Windows")
    def testStyle(self):
        term = WinTerm()
        term.set_console = Mock()
        term._style = 0

        term.style(22)

        self.assertEqual(term._style, 22)
        self.assertEqual(term.set_console.called, True)

    @patch('colorama.winterm.win32')
    def testSetConsole(self, mockWin32):
        mockAttr = Mock()
        mockAttr.wAttributes = 0
        mockWin32.GetConsoleScreenBufferInfo.return_value = mockAttr
        term = WinTerm()
        term.windll = Mock()

        term.set_console()

        self.assertEqual(
            mockWin32.SetConsoleTextAttribute.call_args,
            ((mockWin32.STDOUT, term.get_attrs()), {})
        )

    @patch('colorama.winterm.win32')
    def testSetConsoleOnStderr(self, mockWin32):
        mockAttr = Mock()
        mockAttr.wAttributes = 0
        mockWin32.GetConsoleScreenBufferInfo.return_value = mockAttr
        term = WinTerm()
        term.windll = Mock()

        term.set_console(on_stderr=True)

        self.assertEqual(
            mockWin32.SetConsoleTextAttribute.call_args,
            ((mockWin32.STDERR, term.get_attrs()), {})
        )


if __name__ == '__main__':
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/__init__.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/ansitowin32_test.py
# ========================================================
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
from io import StringIO, TextIOWrapper
from unittest import TestCase, main
try:
    from contextlib import ExitStack
except ImportError:
    # python 2
    from contextlib2 import ExitStack

try:
    from unittest.mock import MagicMock, Mock, patch
except ImportError:
    from mock import MagicMock, Mock, patch

from ..ansitowin32 import AnsiToWin32, StreamWrapper
from ..win32 import ENABLE_VIRTUAL_TERMINAL_PROCESSING
from .utils import osname


class StreamWrapperTest(TestCase):

    def testIsAProxy(self):
        mockStream = Mock()
        wrapper = StreamWrapper(mockStream, None)
        self.assertTrue( wrapper.random_attr is mockStream.random_attr )

    def testDelegatesWrite(self):
        mockStream = Mock()
        mockConverter = Mock()
        wrapper = StreamWrapper(mockStream, mockConverter)
        wrapper.write('hello')
        self.assertTrue(mockConverter.write.call_args, (('hello',), {}))

    def testDelegatesContext(self):
        mockConverter = Mock()
        s = StringIO()
        with StreamWrapper(s, mockConverter) as fp:
            fp.write(u'hello')
        self.assertTrue(s.closed)

    def testProxyNoContextManager(self):
        mockStream = MagicMock()
        mockStream.__enter__.side_effect = AttributeError()
        mockConverter = Mock()
        with self.assertRaises(AttributeError) as excinfo:
            with StreamWrapper(mockStream, mockConverter) as wrapper:
                wrapper.write('hello')

    def test_closed_shouldnt_raise_on_closed_stream(self):
        stream = StringIO()
        stream.close()
        wrapper = StreamWrapper(stream, None)
        self.assertEqual(wrapper.closed, True)

    def test_closed_shouldnt_raise_on_detached_stream(self):
        stream = TextIOWrapper(StringIO())
        stream.detach()
        wrapper = StreamWrapper(stream, None)
        self.assertEqual(wrapper.closed, True)

class AnsiToWin32Test(TestCase):

    def testInit(self):
        mockStdout = Mock()
        auto = Mock()
        stream = AnsiToWin32(mockStdout, autoreset=auto)
        self.assertEqual(stream.wrapped, mockStdout)
        self.assertEqual(stream.autoreset, auto)

    @patch('colorama.ansitowin32.winterm', None)
    @patch('colorama.ansitowin32.winapi_test', lambda *_: True)
    def testStripIsTrueOnWindows(self):
        with osname('nt'):
            mockStdout = Mock()
            stream = AnsiToWin32(mockStdout)
            self.assertTrue(stream.strip)

    def testStripIsFalseOffWindows(self):
        with osname('posix'):
            mockStdout = Mock(closed=False)
            stream = AnsiToWin32(mockStdout)
            self.assertFalse(stream.strip)

    def testWriteStripsAnsi(self):
        mockStdout = Mock()
        stream = AnsiToWin32(mockStdout)
        stream.wrapped = Mock()
        stream.write_and_convert = Mock()
        stream.strip = True

        stream.write('abc')

        self.assertFalse(stream.wrapped.write.called)
        self.assertEqual(stream.write_and_convert.call_args, (('abc',), {}))

    def testWriteDoesNotStripAnsi(self):
        mockStdout = Mock()
        stream = AnsiToWin32(mockStdout)
        stream.wrapped = Mock()
        stream.write_and_convert = Mock()
        stream.strip = False
        stream.convert = False

        stream.write('abc')

        self.assertFalse(stream.write_and_convert.called)
        self.assertEqual(stream.wrapped.write.call_args, (('abc',), {}))

    def assert_autoresets(self, convert, autoreset=True):
        stream = AnsiToWin32(Mock())
        stream.convert = convert
        stream.reset_all = Mock()
        stream.autoreset = autoreset
        stream.winterm = Mock()

        stream.write('abc')

        self.assertEqual(stream.reset_all.called, autoreset)

    def testWriteAutoresets(self):
        self.assert_autoresets(convert=True)
        self.assert_autoresets(convert=False)
        self.assert_autoresets(convert=True, autoreset=False)
        self.assert_autoresets(convert=False, autoreset=False)

    def testWriteAndConvertWritesPlainText(self):
        stream = AnsiToWin32(Mock())
        stream.write_and_convert( 'abc' )
        self.assertEqual( stream.wrapped.write.call_args, (('abc',), {}) )

    def testWriteAndConvertStripsAllValidAnsi(self):
        stream = AnsiToWin32(Mock())
        stream.call_win32 = Mock()
        data = [
            'abc\033[mdef',
            'abc\033[0mdef',
            'abc\033[2mdef',
            'abc\033[02mdef',
            'abc\033[002mdef',
            'abc\033[40mdef',
            'abc\033[040mdef',
            'abc\033[0;1mdef',
            'abc\033[40;50mdef',
            'abc\033[50;30;40mdef',
            'abc\033[Adef',
            'abc\033[0Gdef',
            'abc\033[1;20;128Hdef',
        ]
        for datum in data:
            stream.wrapped.write.reset_mock()
            stream.write_and_convert( datum )
            self.assertEqual(
               [args[0] for args in stream.wrapped.write.call_args_list],
               [ ('abc',), ('def',) ]
            )

    def testWriteAndConvertSkipsEmptySnippets(self):
        stream = AnsiToWin32(Mock())
        stream.call_win32 = Mock()
        stream.write_and_convert( '\033[40m\033[41m' )
        self.assertFalse( stream.wrapped.write.called )

    def testWriteAndConvertCallsWin32WithParamsAndCommand(self):
        stream = AnsiToWin32(Mock())
        stream.convert = True
        stream.call_win32 = Mock()
        stream.extract_params = Mock(return_value='params')
        data = {
            'abc\033[adef':         ('a', 'params'),
            'abc\033[;;bdef':       ('b', 'params'),
            'abc\033[0cdef':        ('c', 'params'),
            'abc\033[;;0;;Gdef':    ('G', 'params'),
            'abc\033[1;20;128Hdef': ('H', 'params'),
        }
        for datum, expected in data.items():
            stream.call_win32.reset_mock()
            stream.write_and_convert( datum )
            self.assertEqual( stream.call_win32.call_args[0], expected )

    def test_reset_all_shouldnt_raise_on_closed_orig_stdout(self):
        stream = StringIO()
        converter = AnsiToWin32(stream)
        stream.close()

        converter.reset_all()

    def test_wrap_shouldnt_raise_on_closed_orig_stdout(self):
        stream = StringIO()
        stream.close()
        with \
            patch("colorama.ansitowin32.os.name", "nt"), \
            patch("colorama.ansitowin32.winapi_test", lambda: True):
                converter = AnsiToWin32(stream)
        self.assertTrue(converter.strip)
        self.assertFalse(converter.convert)

    def test_wrap_shouldnt_raise_on_missing_closed_attr(self):
        with \
            patch("colorama.ansitowin32.os.name", "nt"), \
            patch("colorama.ansitowin32.winapi_test", lambda: True):
                converter = AnsiToWin32(object())
        self.assertTrue(converter.strip)
        self.assertFalse(converter.convert)

    def testExtractParams(self):
        stream = AnsiToWin32(Mock())
        data = {
            '':               (0,),
            ';;':             (0,),
            '2':              (2,),
            ';;002;;':        (2,),
            '0;1':            (0, 1),
            ';;003;;456;;':   (3, 456),
            '11;22;33;44;55': (11, 22, 33, 44, 55),
        }
        for datum, expected in data.items():
            self.assertEqual(stream.extract_params('m', datum), expected)

    def testCallWin32UsesLookup(self):
        listener = Mock()
        stream = AnsiToWin32(listener)
        stream.win32_calls = {
            1: (lambda *_, **__: listener(11),),
            2: (lambda *_, **__: listener(22),),
            3: (lambda *_, **__: listener(33),),
        }
        stream.call_win32('m', (3, 1, 99, 2))
        self.assertEqual(
            [a[0][0] for a in listener.call_args_list],
            [33, 11, 22] )

    def test_osc_codes(self):
        mockStdout = Mock()
        stream = AnsiToWin32(mockStdout, convert=True)
        with patch('colorama.ansitowin32.winterm') as winterm:
            data = [
                '\033]0\x07',                      # missing arguments
                '\033]0;foo\x08',                  # wrong OSC command
                '\033]0;colorama_test_title\x07',  # should work
                '\033]1;colorama_test_title\x07',  # wrong set command
                '\033]2;colorama_test_title\x07',  # should work
                '\033]' + ';' * 64 + '\x08',       # see issue #247
            ]
            for code in data:
                stream.write(code)
            self.assertEqual(winterm.set_title.call_count, 2)

    def test_native_windows_ansi(self):
        with ExitStack() as stack:
            def p(a, b):
                stack.enter_context(patch(a, b, create=True))
            # Pretend to be on Windows
            p("colorama.ansitowin32.os.name", "nt")
            p("colorama.ansitowin32.winapi_test", lambda: True)
            p("colorama.win32.winapi_test", lambda: True)
            p("colorama.winterm.win32.windll", "non-None")
            p("colorama.winterm.get_osfhandle", lambda _: 1234)

            # Pretend that our mock stream has native ANSI support
            p(
                "colorama.winterm.win32.GetConsoleMode",
                lambda _: ENABLE_VIRTUAL_TERMINAL_PROCESSING,
            )
            SetConsoleMode = Mock()
            p("colorama.winterm.win32.SetConsoleMode", SetConsoleMode)

            stdout = Mock()
            stdout.closed = False
            stdout.isatty.return_value = True
            stdout.fileno.return_value = 1

            # Our fake console says it has native vt support, so AnsiToWin32 should
            # enable that support and do nothing else.
            stream = AnsiToWin32(stdout)
            SetConsoleMode.assert_called_with(1234, ENABLE_VIRTUAL_TERMINAL_PROCESSING)
            self.assertFalse(stream.strip)
            self.assertFalse(stream.convert)
            self.assertFalse(stream.should_wrap())

            # Now let's pretend we're on an old Windows console, that doesn't have
            # native ANSI support.
            p("colorama.winterm.win32.GetConsoleMode", lambda _: 0)
            SetConsoleMode = Mock()
            p("colorama.winterm.win32.SetConsoleMode", SetConsoleMode)

            stream = AnsiToWin32(stdout)
            SetConsoleMode.assert_called_with(1234, ENABLE_VIRTUAL_TERMINAL_PROCESSING)
            self.assertTrue(stream.strip)
            self.assertTrue(stream.convert)
            self.assertTrue(stream.should_wrap())


if __name__ == '__main__':
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/chardistribution.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Tuple, Union

from .big5freq import (
    BIG5_CHAR_TO_FREQ_ORDER,
    BIG5_TABLE_SIZE,
    BIG5_TYPICAL_DISTRIBUTION_RATIO,
)
from .euckrfreq import (
    EUCKR_CHAR_TO_FREQ_ORDER,
    EUCKR_TABLE_SIZE,
    EUCKR_TYPICAL_DISTRIBUTION_RATIO,
)
from .euctwfreq import (
    EUCTW_CHAR_TO_FREQ_ORDER,
    EUCTW_TABLE_SIZE,
    EUCTW_TYPICAL_DISTRIBUTION_RATIO,
)
from .gb2312freq import (
    GB2312_CHAR_TO_FREQ_ORDER,
    GB2312_TABLE_SIZE,
    GB2312_TYPICAL_DISTRIBUTION_RATIO,
)
from .jisfreq import (
    JIS_CHAR_TO_FREQ_ORDER,
    JIS_TABLE_SIZE,
    JIS_TYPICAL_DISTRIBUTION_RATIO,
)
from .johabfreq import JOHAB_TO_EUCKR_ORDER_TABLE


class CharDistributionAnalysis:
    ENOUGH_DATA_THRESHOLD = 1024
    SURE_YES = 0.99
    SURE_NO = 0.01
    MINIMUM_DATA_THRESHOLD = 3

    def __init__(self) -> None:
        # Mapping table to get frequency order from char order (get from
        # GetOrder())
        self._char_to_freq_order: Tuple[int, ...] = tuple()
        self._table_size = 0  # Size of above table
        # This is a constant value which varies from language to language,
        # used in calculating confidence.  See
        # http://www.mozilla.org/projects/intl/UniversalCharsetDetection.html
        # for further detail.
        self.typical_distribution_ratio = 0.0
        self._done = False
        self._total_chars = 0
        self._freq_chars = 0
        self.reset()

    def reset(self) -> None:
        """reset analyser, clear any state"""
        # If this flag is set to True, detection is done and conclusion has
        # been made
        self._done = False
        self._total_chars = 0  # Total characters encountered
        # The number of characters whose frequency order is less than 512
        self._freq_chars = 0

    def feed(self, char: Union[bytes, bytearray], char_len: int) -> None:
        """feed a character with known length"""
        if char_len == 2:
            # we only care about 2-bytes character in our distribution analysis
            order = self.get_order(char)
        else:
            order = -1
        if order >= 0:
            self._total_chars += 1
            # order is valid
            if order < self._table_size:
                if 512 > self._char_to_freq_order[order]:
                    self._freq_chars += 1

    def get_confidence(self) -> float:
        """return confidence based on existing data"""
        # if we didn't receive any character in our consideration range,
        # return negative answer
        if self._total_chars <= 0 or self._freq_chars <= self.MINIMUM_DATA_THRESHOLD:
            return self.SURE_NO

        if self._total_chars != self._freq_chars:
            r = self._freq_chars / (
                (self._total_chars - self._freq_chars) * self.typical_distribution_ratio
            )
            if r < self.SURE_YES:
                return r

        # normalize confidence (we don't want to be 100% sure)
        return self.SURE_YES

    def got_enough_data(self) -> bool:
        # It is not necessary to receive all data to draw conclusion.
        # For charset detection, certain amount of data is enough
        return self._total_chars > self.ENOUGH_DATA_THRESHOLD

    def get_order(self, _: Union[bytes, bytearray]) -> int:
        # We do not handle characters based on the original encoding string,
        # but convert this encoding string to a number, here called order.
        # This allows multiple encodings of a language to share one frequency
        # table.
        return -1


class EUCTWDistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = EUCTW_CHAR_TO_FREQ_ORDER
        self._table_size = EUCTW_TABLE_SIZE
        self.typical_distribution_ratio = EUCTW_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        # for euc-TW encoding, we are interested
        #   first  byte range: 0xc4 -- 0xfe
        #   second byte range: 0xa1 -- 0xfe
        # no validation needed here. State machine has done that
        first_char = byte_str[0]
        if first_char >= 0xC4:
            return 94 * (first_char - 0xC4) + byte_str[1] - 0xA1
        return -1


class EUCKRDistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = EUCKR_CHAR_TO_FREQ_ORDER
        self._table_size = EUCKR_TABLE_SIZE
        self.typical_distribution_ratio = EUCKR_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        # for euc-KR encoding, we are interested
        #   first  byte range: 0xb0 -- 0xfe
        #   second byte range: 0xa1 -- 0xfe
        # no validation needed here. State machine has done that
        first_char = byte_str[0]
        if first_char >= 0xB0:
            return 94 * (first_char - 0xB0) + byte_str[1] - 0xA1
        return -1


class JOHABDistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = EUCKR_CHAR_TO_FREQ_ORDER
        self._table_size = EUCKR_TABLE_SIZE
        self.typical_distribution_ratio = EUCKR_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        first_char = byte_str[0]
        if 0x88 <= first_char < 0xD4:
            code = first_char * 256 + byte_str[1]
            return JOHAB_TO_EUCKR_ORDER_TABLE.get(code, -1)
        return -1


class GB2312DistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = GB2312_CHAR_TO_FREQ_ORDER
        self._table_size = GB2312_TABLE_SIZE
        self.typical_distribution_ratio = GB2312_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        # for GB2312 encoding, we are interested
        #  first  byte range: 0xb0 -- 0xfe
        #  second byte range: 0xa1 -- 0xfe
        # no validation needed here. State machine has done that
        first_char, second_char = byte_str[0], byte_str[1]
        if (first_char >= 0xB0) and (second_char >= 0xA1):
            return 94 * (first_char - 0xB0) + second_char - 0xA1
        return -1


class Big5DistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = BIG5_CHAR_TO_FREQ_ORDER
        self._table_size = BIG5_TABLE_SIZE
        self.typical_distribution_ratio = BIG5_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        # for big5 encoding, we are interested
        #   first  byte range: 0xa4 -- 0xfe
        #   second byte range: 0x40 -- 0x7e , 0xa1 -- 0xfe
        # no validation needed here. State machine has done that
        first_char, second_char = byte_str[0], byte_str[1]
        if first_char >= 0xA4:
            if second_char >= 0xA1:
                return 157 * (first_char - 0xA4) + second_char - 0xA1 + 63
            return 157 * (first_char - 0xA4) + second_char - 0x40
        return -1


class SJISDistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = JIS_CHAR_TO_FREQ_ORDER
        self._table_size = JIS_TABLE_SIZE
        self.typical_distribution_ratio = JIS_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        # for sjis encoding, we are interested
        #   first  byte range: 0x81 -- 0x9f , 0xe0 -- 0xfe
        #   second byte range: 0x40 -- 0x7e,  0x81 -- oxfe
        # no validation needed here. State machine has done that
        first_char, second_char = byte_str[0], byte_str[1]
        if 0x81 <= first_char <= 0x9F:
            order = 188 * (first_char - 0x81)
        elif 0xE0 <= first_char <= 0xEF:
            order = 188 * (first_char - 0xE0 + 31)
        else:
            return -1
        order = order + second_char - 0x40
        if second_char > 0x7F:
            order = -1
        return order


class EUCJPDistributionAnalysis(CharDistributionAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._char_to_freq_order = JIS_CHAR_TO_FREQ_ORDER
        self._table_size = JIS_TABLE_SIZE
        self.typical_distribution_ratio = JIS_TYPICAL_DISTRIBUTION_RATIO

    def get_order(self, byte_str: Union[bytes, bytearray]) -> int:
        # for euc-JP encoding, we are interested
        #   first  byte range: 0xa0 -- 0xfe
        #   second byte range: 0xa1 -- 0xfe
        # no validation needed here. State machine has done that
        char = byte_str[0]
        if char >= 0xA0:
            return 94 * (char - 0xA1) + byte_str[1] - 0xA1
        return -1


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/eucjpprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Union

from .chardistribution import EUCJPDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .enums import MachineState, ProbingState
from .jpcntx import EUCJPContextAnalysis
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCJP_SM_MODEL


class EUCJPProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCJP_SM_MODEL)
        self.distribution_analyzer = EUCJPDistributionAnalysis()
        self.context_analyzer = EUCJPContextAnalysis()
        self.reset()

    def reset(self) -> None:
        super().reset()
        self.context_analyzer.reset()

    @property
    def charset_name(self) -> str:
        return "EUC-JP"

    @property
    def language(self) -> str:
        return "Japanese"

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        assert self.coding_sm is not None
        assert self.distribution_analyzer is not None

        for i, byte in enumerate(byte_str):
            # PY3K: byte_str is a byte array, so byte is an int, not a byte
            coding_state = self.coding_sm.next_state(byte)
            if coding_state == MachineState.ERROR:
                self.logger.debug(
                    "%s %s prober hit error at byte %s",
                    self.charset_name,
                    self.language,
                    i,
                )
                self._state = ProbingState.NOT_ME
                break
            if coding_state == MachineState.ITS_ME:
                self._state = ProbingState.FOUND_IT
                break
            if coding_state == MachineState.START:
                char_len = self.coding_sm.get_current_charlen()
                if i == 0:
                    self._last_char[1] = byte
                    self.context_analyzer.feed(self._last_char, char_len)
                    self.distribution_analyzer.feed(self._last_char, char_len)
                else:
                    self.context_analyzer.feed(byte_str[i - 1 : i + 1], char_len)
                    self.distribution_analyzer.feed(byte_str[i - 1 : i + 1], char_len)

        self._last_char[0] = byte_str[-1]

        if self.state == ProbingState.DETECTING:
            if self.context_analyzer.got_enough_data() and (
                self.get_confidence() > self.SHORTCUT_THRESHOLD
            ):
                self._state = ProbingState.FOUND_IT

        return self.state

    def get_confidence(self) -> float:
        assert self.distribution_analyzer is not None

        context_conf = self.context_analyzer.get_confidence()
        distrib_conf = self.distribution_analyzer.get_confidence()
        return max(context_conf, distrib_conf)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/big5prober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import Big5DistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import BIG5_SM_MODEL


class Big5Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(BIG5_SM_MODEL)
        self.distribution_analyzer = Big5DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return "Big5"

    @property
    def language(self) -> str:
        return "Chinese"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/utf8prober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Union

from .charsetprober import CharSetProber
from .codingstatemachine import CodingStateMachine
from .enums import MachineState, ProbingState
from .mbcssm import UTF8_SM_MODEL


class UTF8Prober(CharSetProber):
    ONE_CHAR_PROB = 0.5

    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(UTF8_SM_MODEL)
        self._num_mb_chars = 0
        self.reset()

    def reset(self) -> None:
        super().reset()
        self.coding_sm.reset()
        self._num_mb_chars = 0

    @property
    def charset_name(self) -> str:
        return "utf-8"

    @property
    def language(self) -> str:
        return ""

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        for c in byte_str:
            coding_state = self.coding_sm.next_state(c)
            if coding_state == MachineState.ERROR:
                self._state = ProbingState.NOT_ME
                break
            if coding_state == MachineState.ITS_ME:
                self._state = ProbingState.FOUND_IT
                break
            if coding_state == MachineState.START:
                if self.coding_sm.get_current_charlen() >= 2:
                    self._num_mb_chars += 1

        if self.state == ProbingState.DETECTING:
            if self.get_confidence() > self.SHORTCUT_THRESHOLD:
                self._state = ProbingState.FOUND_IT

        return self.state

    def get_confidence(self) -> float:
        unlike = 0.99
        if self._num_mb_chars < 6:
            unlike *= self.ONE_CHAR_PROB**self._num_mb_chars
            return 1.0 - unlike
        return unlike


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/euckrfreq.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# Sampling from about 20M text materials include literature and computer technology

# 128  --> 0.79
# 256  --> 0.92
# 512  --> 0.986
# 1024 --> 0.99944
# 2048 --> 0.99999
#
# Idea Distribution Ratio = 0.98653 / (1-0.98653) = 73.24
# Random Distribution Ration = 512 / (2350-512) = 0.279.
#
# Typical Distribution Ratio

EUCKR_TYPICAL_DISTRIBUTION_RATIO = 6.0

EUCKR_TABLE_SIZE = 2352

# Char to FreqOrder table ,
# fmt: off
EUCKR_CHAR_TO_FREQ_ORDER = (
  13, 130, 120,1396, 481,1719,1720, 328, 609, 212,1721, 707, 400, 299,1722,  87,
1397,1723, 104, 536,1117,1203,1724,1267, 685,1268, 508,1725,1726,1727,1728,1398,
1399,1729,1730,1731, 141, 621, 326,1057, 368,1732, 267, 488,  20,1733,1269,1734,
 945,1400,1735,  47, 904,1270,1736,1737, 773, 248,1738, 409, 313, 786, 429,1739,
 116, 987, 813,1401, 683,  75,1204, 145,1740,1741,1742,1743,  16, 847, 667, 622,
 708,1744,1745,1746, 966, 787, 304, 129,1747,  60, 820, 123, 676,1748,1749,1750,
1751, 617,1752, 626,1753,1754,1755,1756, 653,1757,1758,1759,1760,1761,1762, 856,
 344,1763,1764,1765,1766,  89, 401, 418, 806, 905, 848,1767,1768,1769, 946,1205,
 709,1770,1118,1771, 241,1772,1773,1774,1271,1775, 569,1776, 999,1777,1778,1779,
1780, 337, 751,1058,  28, 628, 254,1781, 177, 906, 270, 349, 891,1079,1782,  19,
1783, 379,1784, 315,1785, 629, 754,1402, 559,1786, 636, 203,1206,1787, 710, 567,
1788, 935, 814,1789,1790,1207, 766, 528,1791,1792,1208,1793,1794,1795,1796,1797,
1403,1798,1799, 533,1059,1404,1405,1156,1406, 936, 884,1080,1800, 351,1801,1802,
1803,1804,1805, 801,1806,1807,1808,1119,1809,1157, 714, 474,1407,1810, 298, 899,
 885,1811,1120, 802,1158,1812, 892,1813,1814,1408, 659,1815,1816,1121,1817,1818,
1819,1820,1821,1822, 319,1823, 594, 545,1824, 815, 937,1209,1825,1826, 573,1409,
1022,1827,1210,1828,1829,1830,1831,1832,1833, 556, 722, 807,1122,1060,1834, 697,
1835, 900, 557, 715,1836,1410, 540,1411, 752,1159, 294, 597,1211, 976, 803, 770,
1412,1837,1838,  39, 794,1413, 358,1839, 371, 925,1840, 453, 661, 788, 531, 723,
 544,1023,1081, 869,  91,1841, 392, 430, 790, 602,1414, 677,1082, 457,1415,1416,
1842,1843, 475, 327,1024,1417, 795, 121,1844, 733, 403,1418,1845,1846,1847, 300,
 119, 711,1212, 627,1848,1272, 207,1849,1850, 796,1213, 382,1851, 519,1852,1083,
 893,1853,1854,1855, 367, 809, 487, 671,1856, 663,1857,1858, 956, 471, 306, 857,
1859,1860,1160,1084,1861,1862,1863,1864,1865,1061,1866,1867,1868,1869,1870,1871,
 282,  96, 574,1872, 502,1085,1873,1214,1874, 907,1875,1876, 827, 977,1419,1420,
1421, 268,1877,1422,1878,1879,1880, 308,1881,   2, 537,1882,1883,1215,1884,1885,
 127, 791,1886,1273,1423,1887,  34, 336, 404, 643,1888, 571, 654, 894, 840,1889,
   0, 886,1274, 122, 575, 260, 908, 938,1890,1275, 410, 316,1891,1892, 100,1893,
1894,1123,  48,1161,1124,1025,1895, 633, 901,1276,1896,1897, 115, 816,1898, 317,
1899, 694,1900, 909, 734,1424, 572, 866,1425, 691,  85, 524,1010, 543, 394, 841,
1901,1902,1903,1026,1904,1905,1906,1907,1908,1909,  30, 451, 651, 988, 310,1910,
1911,1426, 810,1216,  93,1912,1913,1277,1217,1914, 858, 759,  45,  58, 181, 610,
 269,1915,1916, 131,1062, 551, 443,1000, 821,1427, 957, 895,1086,1917,1918, 375,
1919, 359,1920, 687,1921, 822,1922, 293,1923,1924,  40, 662, 118, 692,  29, 939,
 887, 640, 482, 174,1925,  69,1162, 728,1428, 910,1926,1278,1218,1279, 386, 870,
 217, 854,1163, 823,1927,1928,1929,1930, 834,1931,  78,1932, 859,1933,1063,1934,
1935,1936,1937, 438,1164, 208, 595,1938,1939,1940,1941,1219,1125,1942, 280, 888,
1429,1430,1220,1431,1943,1944,1945,1946,1947,1280, 150, 510,1432,1948,1949,1950,
1951,1952,1953,1954,1011,1087,1955,1433,1043,1956, 881,1957, 614, 958,1064,1065,
1221,1958, 638,1001, 860, 967, 896,1434, 989, 492, 553,1281,1165,1959,1282,1002,
1283,1222,1960,1961,1962,1963,  36, 383, 228, 753, 247, 454,1964, 876, 678,1965,
1966,1284, 126, 464, 490, 835, 136, 672, 529, 940,1088,1435, 473,1967,1968, 467,
  50, 390, 227, 587, 279, 378, 598, 792, 968, 240, 151, 160, 849, 882,1126,1285,
 639,1044, 133, 140, 288, 360, 811, 563,1027, 561, 142, 523,1969,1970,1971,   7,
 103, 296, 439, 407, 506, 634, 990,1972,1973,1974,1975, 645,1976,1977,1978,1979,
1980,1981, 236,1982,1436,1983,1984,1089, 192, 828, 618, 518,1166, 333,1127,1985,
 818,1223,1986,1987,1988,1989,1990,1991,1992,1993, 342,1128,1286, 746, 842,1994,
1995, 560, 223,1287,  98,   8, 189, 650, 978,1288,1996,1437,1997,  17, 345, 250,
 423, 277, 234, 512, 226,  97, 289,  42, 167,1998, 201,1999,2000, 843, 836, 824,
 532, 338, 783,1090, 182, 576, 436,1438,1439, 527, 500,2001, 947, 889,2002,2003,
2004,2005, 262, 600, 314, 447,2006, 547,2007, 693, 738,1129,2008,  71,1440, 745,
 619, 688,2009, 829,2010,2011, 147,2012,  33, 948,2013,2014,  74, 224,2015,  61,
 191, 918, 399, 637,2016,1028,1130, 257, 902,2017,2018,2019,2020,2021,2022,2023,
2024,2025,2026, 837,2027,2028,2029,2030, 179, 874, 591,  52, 724, 246,2031,2032,
2033,2034,1167, 969,2035,1289, 630, 605, 911,1091,1168,2036,2037,2038,1441, 912,
2039, 623,2040,2041, 253,1169,1290,2042,1442, 146, 620, 611, 577, 433,2043,1224,
 719,1170, 959, 440, 437, 534,  84, 388, 480,1131, 159, 220, 198, 679,2044,1012,
 819,1066,1443, 113,1225, 194, 318,1003,1029,2045,2046,2047,2048,1067,2049,2050,
2051,2052,2053,  59, 913, 112,2054, 632,2055, 455, 144, 739,1291,2056, 273, 681,
 499,2057, 448,2058,2059, 760,2060,2061, 970, 384, 169, 245,1132,2062,2063, 414,
1444,2064,2065,  41, 235,2066, 157, 252, 877, 568, 919, 789, 580,2067, 725,2068,
2069,1292,2070,2071,1445,2072,1446,2073,2074,  55, 588,  66,1447, 271,1092,2075,
1226,2076, 960,1013, 372,2077,2078,2079,2080,2081,1293,2082,2083,2084,2085, 850,
2086,2087,2088,2089,2090, 186,2091,1068, 180,2092,2093,2094, 109,1227, 522, 606,
2095, 867,1448,1093, 991,1171, 926, 353,1133,2096, 581,2097,2098,2099,1294,1449,
1450,2100, 596,1172,1014,1228,2101,1451,1295,1173,1229,2102,2103,1296,1134,1452,
 949,1135,2104,2105,1094,1453,1454,1455,2106,1095,2107,2108,2109,2110,2111,2112,
2113,2114,2115,2116,2117, 804,2118,2119,1230,1231, 805,1456, 405,1136,2120,2121,
2122,2123,2124, 720, 701,1297, 992,1457, 927,1004,2125,2126,2127,2128,2129,2130,
  22, 417,2131, 303,2132, 385,2133, 971, 520, 513,2134,1174,  73,1096, 231, 274,
 962,1458, 673,2135,1459,2136, 152,1137,2137,2138,2139,2140,1005,1138,1460,1139,
2141,2142,2143,2144,  11, 374, 844,2145, 154,1232,  46,1461,2146, 838, 830, 721,
1233, 106,2147,  90, 428, 462, 578, 566,1175, 352,2148,2149, 538,1234, 124,1298,
2150,1462, 761, 565,2151, 686,2152, 649,2153,  72, 173,2154, 460, 415,2155,1463,
2156,1235, 305,2157,2158,2159,2160,2161,2162, 579,2163,2164,2165,2166,2167, 747,
2168,2169,2170,2171,1464, 669,2172,2173,2174,2175,2176,1465,2177,  23, 530, 285,
2178, 335, 729,2179, 397,2180,2181,2182,1030,2183,2184, 698,2185,2186, 325,2187,
2188, 369,2189, 799,1097,1015, 348,2190,1069, 680,2191, 851,1466,2192,2193,  10,
2194, 613, 424,2195, 979, 108, 449, 589,  27, 172,  81,1031,  80, 774, 281, 350,
1032, 525, 301, 582,1176,2196, 674,1045,2197,2198,1467, 730, 762,2199,2200,2201,
2202,1468,2203, 993,2204,2205, 266,1070, 963,1140,2206,2207,2208, 664,1098, 972,
2209,2210,2211,1177,1469,1470, 871,2212,2213,2214,2215,2216,1471,2217,2218,2219,
2220,2221,2222,2223,2224,2225,2226,2227,1472,1236,2228,2229,2230,2231,2232,2233,
2234,2235,1299,2236,2237, 200,2238, 477, 373,2239,2240, 731, 825, 777,2241,2242,
2243, 521, 486, 548,2244,2245,2246,1473,1300,  53, 549, 137, 875,  76, 158,2247,
1301,1474, 469, 396,1016, 278, 712,2248, 321, 442, 503, 767, 744, 941,1237,1178,
1475,2249,  82, 178,1141,1179, 973,2250,1302,2251, 297,2252,2253, 570,2254,2255,
2256,  18, 450, 206,2257, 290, 292,1142,2258, 511, 162,  99, 346, 164, 735,2259,
1476,1477,   4, 554, 343, 798,1099,2260,1100,2261,  43, 171,1303, 139, 215,2262,
2263, 717, 775,2264,1033, 322, 216,2265, 831,2266, 149,2267,1304,2268,2269, 702,
1238, 135, 845, 347, 309,2270, 484,2271, 878, 655, 238,1006,1478,2272,  67,2273,
 295,2274,2275, 461,2276, 478, 942, 412,2277,1034,2278,2279,2280, 265,2281, 541,
2282,2283,2284,2285,2286,  70, 852,1071,2287,2288,2289,2290,  21,  56, 509, 117,
 432,2291,2292, 331, 980, 552,1101, 148, 284, 105, 393,1180,1239, 755,2293, 187,
2294,1046,1479,2295, 340,2296,  63,1047, 230,2297,2298,1305, 763,1306, 101, 800,
 808, 494,2299,2300,2301, 903,2302,  37,1072,  14,   5,2303,  79, 675,2304, 312,
2305,2306,2307,2308,2309,1480,   6,1307,2310,2311,2312,   1, 470,  35,  24, 229,
2313, 695, 210,  86, 778,  15, 784, 592, 779,  32,  77, 855, 964,2314, 259,2315,
 501, 380,2316,2317,  83, 981, 153, 689,1308,1481,1482,1483,2318,2319, 716,1484,
2320,2321,2322,2323,2324,2325,1485,2326,2327, 128,  57,  68, 261,1048, 211, 170,
1240,  31,2328,  51, 435, 742,2329,2330,2331, 635,2332, 264, 456,2333,2334,2335,
 425,2336,1486, 143, 507, 263, 943,2337, 363, 920,1487, 256,1488,1102, 243, 601,
1489,2338,2339,2340,2341,2342,2343,2344, 861,2345,2346,2347,2348,2349,2350, 395,
2351,1490,1491,  62, 535, 166, 225,2352,2353, 668, 419,1241, 138, 604, 928,2354,
1181,2355,1492,1493,2356,2357,2358,1143,2359, 696,2360, 387, 307,1309, 682, 476,
2361,2362, 332,  12, 222, 156,2363, 232,2364, 641, 276, 656, 517,1494,1495,1035,
 416, 736,1496,2365,1017, 586,2366,2367,2368,1497,2369, 242,2370,2371,2372,1498,
2373, 965, 713,2374,2375,2376,2377, 740, 982,1499, 944,1500,1007,2378,2379,1310,
1501,2380,2381,2382, 785, 329,2383,2384,1502,2385,2386,2387, 932,2388,1503,2389,
2390,2391,2392,1242,2393,2394,2395,2396,2397, 994, 950,2398,2399,2400,2401,1504,
1311,2402,2403,2404,2405,1049, 749,2406,2407, 853, 718,1144,1312,2408,1182,1505,
2409,2410, 255, 516, 479, 564, 550, 214,1506,1507,1313, 413, 239, 444, 339,1145,
1036,1508,1509,1314,1037,1510,1315,2411,1511,2412,2413,2414, 176, 703, 497, 624,
 593, 921, 302,2415, 341, 165,1103,1512,2416,1513,2417,2418,2419, 376,2420, 700,
2421,2422,2423, 258, 768,1316,2424,1183,2425, 995, 608,2426,2427,2428,2429, 221,
2430,2431,2432,2433,2434,2435,2436,2437, 195, 323, 726, 188, 897, 983,1317, 377,
 644,1050, 879,2438, 452,2439,2440,2441,2442,2443,2444, 914,2445,2446,2447,2448,
 915, 489,2449,1514,1184,2450,2451, 515,  64, 427, 495,2452, 583,2453, 483, 485,
1038, 562, 213,1515, 748, 666,2454,2455,2456,2457, 334,2458, 780, 996,1008, 705,
1243,2459,2460,2461,2462,2463, 114,2464, 493,1146, 366, 163,1516, 961,1104,2465,
 291,2466,1318,1105,2467,1517, 365,2468, 355, 951,1244,2469,1319,2470, 631,2471,
2472, 218,1320, 364, 320, 756,1518,1519,1321,1520,1322,2473,2474,2475,2476, 997,
2477,2478,2479,2480, 665,1185,2481, 916,1521,2482,2483,2484, 584, 684,2485,2486,
 797,2487,1051,1186,2488,2489,2490,1522,2491,2492, 370,2493,1039,1187,  65,2494,
 434, 205, 463,1188,2495, 125, 812, 391, 402, 826, 699, 286, 398, 155, 781, 771,
 585,2496, 590, 505,1073,2497, 599, 244, 219, 917,1018, 952, 646,1523,2498,1323,
2499,2500,  49, 984, 354, 741,2501, 625,2502,1324,2503,1019, 190, 357, 757, 491,
  95, 782, 868,2504,2505,2506,2507,2508,2509, 134,1524,1074, 422,1525, 898,2510,
 161,2511,2512,2513,2514, 769,2515,1526,2516,2517, 411,1325,2518, 472,1527,2519,
2520,2521,2522,2523,2524, 985,2525,2526,2527,2528,2529,2530, 764,2531,1245,2532,
2533,  25, 204, 311,2534, 496,2535,1052,2536,2537,2538,2539,2540,2541,2542, 199,
 704, 504, 468, 758, 657,1528, 196,  44, 839,1246, 272, 750,2543, 765, 862,2544,
2545,1326,2546, 132, 615, 933,2547, 732,2548,2549,2550,1189,1529,2551, 283,1247,
1053, 607, 929,2552,2553,2554, 930, 183, 872, 616,1040,1147,2555,1148,1020, 441,
 249,1075,2556,2557,2558, 466, 743,2559,2560,2561,  92, 514, 426, 420, 526,2562,
2563,2564,2565,2566,2567,2568, 185,2569,2570,2571,2572, 776,1530, 658,2573, 362,
2574, 361, 922,1076, 793,2575,2576,2577,2578,2579,2580,1531, 251,2581,2582,2583,
2584,1532,  54, 612, 237,1327,2585,2586, 275, 408, 647, 111,2587,1533,1106, 465,
   3, 458,   9,  38,2588, 107, 110, 890, 209,  26, 737, 498,2589,1534,2590, 431,
 202,  88,1535, 356, 287,1107, 660,1149,2591, 381,1536, 986,1150, 445,1248,1151,
 974,2592,2593, 846,2594, 446, 953, 184,1249,1250, 727,2595, 923, 193, 883,2596,
2597,2598, 102, 324, 539, 817,2599, 421,1041,2600, 832,2601,  94, 175, 197, 406,
2602, 459,2603,2604,2605,2606,2607, 330, 555,2608,2609,2610, 706,1108, 389,2611,
2612,2613,2614, 233,2615, 833, 558, 931, 954,1251,2616,2617,1537, 546,2618,2619,
1009,2620,2621,2622,1538, 690,1328,2623, 955,2624,1539,2625,2626, 772,2627,2628,
2629,2630,2631, 924, 648, 863, 603,2632,2633, 934,1540, 864, 865,2634, 642,1042,
 670,1190,2635,2636,2637,2638, 168,2639, 652, 873, 542,1054,1541,2640,2641,2642,  # 512, 256
)
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/universaldetector.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################
"""
Module containing the UniversalDetector detector class, which is the primary
class a user of ``chardet`` should use.

:author: Mark Pilgrim (initial port to Python)
:author: Shy Shalom (original C code)
:author: Dan Blanchard (major refactoring for 3.0)
:author: Ian Cordasco
"""


import codecs
import logging
import re
from typing import List, Optional, Union

from .charsetgroupprober import CharSetGroupProber
from .charsetprober import CharSetProber
from .enums import InputState, LanguageFilter, ProbingState
from .escprober import EscCharSetProber
from .latin1prober import Latin1Prober
from .macromanprober import MacRomanProber
from .mbcsgroupprober import MBCSGroupProber
from .resultdict import ResultDict
from .sbcsgroupprober import SBCSGroupProber
from .utf1632prober import UTF1632Prober


class UniversalDetector:
    """
    The ``UniversalDetector`` class underlies the ``chardet.detect`` function
    and coordinates all of the different charset probers.

    To get a ``dict`` containing an encoding and its confidence, you can simply
    run:

    .. code::

            u = UniversalDetector()
            u.feed(some_bytes)
            u.close()
            detected = u.result

    """

    MINIMUM_THRESHOLD = 0.20
    HIGH_BYTE_DETECTOR = re.compile(b"[\x80-\xFF]")
    ESC_DETECTOR = re.compile(b"(\033|~{)")
    WIN_BYTE_DETECTOR = re.compile(b"[\x80-\x9F]")
    ISO_WIN_MAP = {
        "iso-8859-1": "Windows-1252",
        "iso-8859-2": "Windows-1250",
        "iso-8859-5": "Windows-1251",
        "iso-8859-6": "Windows-1256",
        "iso-8859-7": "Windows-1253",
        "iso-8859-8": "Windows-1255",
        "iso-8859-9": "Windows-1254",
        "iso-8859-13": "Windows-1257",
    }
    # Based on https://encoding.spec.whatwg.org/#names-and-labels
    # but altered to match Python names for encodings and remove mappings
    # that break tests.
    LEGACY_MAP = {
        "ascii": "Windows-1252",
        "iso-8859-1": "Windows-1252",
        "tis-620": "ISO-8859-11",
        "iso-8859-9": "Windows-1254",
        "gb2312": "GB18030",
        "euc-kr": "CP949",
        "utf-16le": "UTF-16",
    }

    def __init__(
        self,
        lang_filter: LanguageFilter = LanguageFilter.ALL,
        should_rename_legacy: bool = False,
    ) -> None:
        self._esc_charset_prober: Optional[EscCharSetProber] = None
        self._utf1632_prober: Optional[UTF1632Prober] = None
        self._charset_probers: List[CharSetProber] = []
        self.result: ResultDict = {
            "encoding": None,
            "confidence": 0.0,
            "language": None,
        }
        self.done = False
        self._got_data = False
        self._input_state = InputState.PURE_ASCII
        self._last_char = b""
        self.lang_filter = lang_filter
        self.logger = logging.getLogger(__name__)
        self._has_win_bytes = False
        self.should_rename_legacy = should_rename_legacy
        self.reset()

    @property
    def input_state(self) -> int:
        return self._input_state

    @property
    def has_win_bytes(self) -> bool:
        return self._has_win_bytes

    @property
    def charset_probers(self) -> List[CharSetProber]:
        return self._charset_probers

    def reset(self) -> None:
        """
        Reset the UniversalDetector and all of its probers back to their
        initial states.  This is called by ``__init__``, so you only need to
        call this directly in between analyses of different documents.
        """
        self.result = {"encoding": None, "confidence": 0.0, "language": None}
        self.done = False
        self._got_data = False
        self._has_win_bytes = False
        self._input_state = InputState.PURE_ASCII
        self._last_char = b""
        if self._esc_charset_prober:
            self._esc_charset_prober.reset()
        if self._utf1632_prober:
            self._utf1632_prober.reset()
        for prober in self._charset_probers:
            prober.reset()

    def feed(self, byte_str: Union[bytes, bytearray]) -> None:
        """
        Takes a chunk of a document and feeds it through all of the relevant
        charset probers.

        After calling ``feed``, you can check the value of the ``done``
        attribute to see if you need to continue feeding the
        ``UniversalDetector`` more data, or if it has made a prediction
        (in the ``result`` attribute).

        .. note::
           You should always call ``close`` when you're done feeding in your
           document if ``done`` is not already ``True``.
        """
        if self.done:
            return

        if not byte_str:
            return

        if not isinstance(byte_str, bytearray):
            byte_str = bytearray(byte_str)

        # First check for known BOMs, since these are guaranteed to be correct
        if not self._got_data:
            # If the data starts with BOM, we know it is UTF
            if byte_str.startswith(codecs.BOM_UTF8):
                # EF BB BF  UTF-8 with BOM
                self.result = {
                    "encoding": "UTF-8-SIG",
                    "confidence": 1.0,
                    "language": "",
                }
            elif byte_str.startswith((codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE)):
                # FF FE 00 00  UTF-32, little-endian BOM
                # 00 00 FE FF  UTF-32, big-endian BOM
                self.result = {"encoding": "UTF-32", "confidence": 1.0, "language": ""}
            elif byte_str.startswith(b"\xFE\xFF\x00\x00"):
                # FE FF 00 00  UCS-4, unusual octet order BOM (3412)
                self.result = {
                    # TODO: This encoding is not supported by Python. Should remove?
                    "encoding": "X-ISO-10646-UCS-4-3412",
                    "confidence": 1.0,
                    "language": "",
                }
            elif byte_str.startswith(b"\x00\x00\xFF\xFE"):
                # 00 00 FF FE  UCS-4, unusual octet order BOM (2143)
                self.result = {
                    # TODO: This encoding is not supported by Python. Should remove?
                    "encoding": "X-ISO-10646-UCS-4-2143",
                    "confidence": 1.0,
                    "language": "",
                }
            elif byte_str.startswith((codecs.BOM_LE, codecs.BOM_BE)):
                # FF FE  UTF-16, little endian BOM
                # FE FF  UTF-16, big endian BOM
                self.result = {"encoding": "UTF-16", "confidence": 1.0, "language": ""}

            self._got_data = True
            if self.result["encoding"] is not None:
                self.done = True
                return

        # If none of those matched and we've only see ASCII so far, check
        # for high bytes and escape sequences
        if self._input_state == InputState.PURE_ASCII:
            if self.HIGH_BYTE_DETECTOR.search(byte_str):
                self._input_state = InputState.HIGH_BYTE
            elif (
                self._input_state == InputState.PURE_ASCII
                and self.ESC_DETECTOR.search(self._last_char + byte_str)
            ):
                self._input_state = InputState.ESC_ASCII

        self._last_char = byte_str[-1:]

        # next we will look to see if it is appears to be either a UTF-16 or
        # UTF-32 encoding
        if not self._utf1632_prober:
            self._utf1632_prober = UTF1632Prober()

        if self._utf1632_prober.state == ProbingState.DETECTING:
            if self._utf1632_prober.feed(byte_str) == ProbingState.FOUND_IT:
                self.result = {
                    "encoding": self._utf1632_prober.charset_name,
                    "confidence": self._utf1632_prober.get_confidence(),
                    "language": "",
                }
                self.done = True
                return

        # If we've seen escape sequences, use the EscCharSetProber, which
        # uses a simple state machine to check for known escape sequences in
        # HZ and ISO-2022 encodings, since those are the only encodings that
        # use such sequences.
        if self._input_state == InputState.ESC_ASCII:
            if not self._esc_charset_prober:
                self._esc_charset_prober = EscCharSetProber(self.lang_filter)
            if self._esc_charset_prober.feed(byte_str) == ProbingState.FOUND_IT:
                self.result = {
                    "encoding": self._esc_charset_prober.charset_name,
                    "confidence": self._esc_charset_prober.get_confidence(),
                    "language": self._esc_charset_prober.language,
                }
                self.done = True
        # If we've seen high bytes (i.e., those with values greater than 127),
        # we need to do more complicated checks using all our multi-byte and
        # single-byte probers that are left.  The single-byte probers
        # use character bigram distributions to determine the encoding, whereas
        # the multi-byte probers use a combination of character unigram and
        # bigram distributions.
        elif self._input_state == InputState.HIGH_BYTE:
            if not self._charset_probers:
                self._charset_probers = [MBCSGroupProber(self.lang_filter)]
                # If we're checking non-CJK encodings, use single-byte prober
                if self.lang_filter & LanguageFilter.NON_CJK:
                    self._charset_probers.append(SBCSGroupProber())
                self._charset_probers.append(Latin1Prober())
                self._charset_probers.append(MacRomanProber())
            for prober in self._charset_probers:
                if prober.feed(byte_str) == ProbingState.FOUND_IT:
                    self.result = {
                        "encoding": prober.charset_name,
                        "confidence": prober.get_confidence(),
                        "language": prober.language,
                    }
                    self.done = True
                    break
            if self.WIN_BYTE_DETECTOR.search(byte_str):
                self._has_win_bytes = True

    def close(self) -> ResultDict:
        """
        Stop analyzing the current document and come up with a final
        prediction.

        :returns:  The ``result`` attribute, a ``dict`` with the keys
                   `encoding`, `confidence`, and `language`.
        """
        # Don't bother with checks if we're already done
        if self.done:
            return self.result
        self.done = True

        if not self._got_data:
            self.logger.debug("no data received!")

        # Default to ASCII if it is all we've seen so far
        elif self._input_state == InputState.PURE_ASCII:
            self.result = {"encoding": "ascii", "confidence": 1.0, "language": ""}

        # If we have seen non-ASCII, return the best that met MINIMUM_THRESHOLD
        elif self._input_state == InputState.HIGH_BYTE:
            prober_confidence = None
            max_prober_confidence = 0.0
            max_prober = None
            for prober in self._charset_probers:
                if not prober:
                    continue
                prober_confidence = prober.get_confidence()
                if prober_confidence > max_prober_confidence:
                    max_prober_confidence = prober_confidence
                    max_prober = prober
            if max_prober and (max_prober_confidence > self.MINIMUM_THRESHOLD):
                charset_name = max_prober.charset_name
                assert charset_name is not None
                lower_charset_name = charset_name.lower()
                confidence = max_prober.get_confidence()
                # Use Windows encoding name instead of ISO-8859 if we saw any
                # extra Windows-specific bytes
                if lower_charset_name.startswith("iso-8859"):
                    if self._has_win_bytes:
                        charset_name = self.ISO_WIN_MAP.get(
                            lower_charset_name, charset_name
                        )
                # Rename legacy encodings with superset encodings if asked
                if self.should_rename_legacy:
                    charset_name = self.LEGACY_MAP.get(
                        (charset_name or "").lower(), charset_name
                    )
                self.result = {
                    "encoding": charset_name,
                    "confidence": confidence,
                    "language": max_prober.language,
                }

        # Log all prober confidences if none met MINIMUM_THRESHOLD
        if self.logger.getEffectiveLevel() <= logging.DEBUG:
            if self.result["encoding"] is None:
                self.logger.debug("no probers hit minimum threshold")
                for group_prober in self._charset_probers:
                    if not group_prober:
                        continue
                    if isinstance(group_prober, CharSetGroupProber):
                        for prober in group_prober.probers:
                            self.logger.debug(
                                "%s %s confidence = %s",
                                prober.charset_name,
                                prober.language,
                                prober.get_confidence(),
                            )
                    else:
                        self.logger.debug(
                            "%s %s confidence = %s",
                            group_prober.charset_name,
                            group_prober.language,
                            group_prober.get_confidence(),
                        )
        return self.result


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/hebrewprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
#          Shy Shalom
# Portions created by the Initial Developer are Copyright (C) 2005
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Optional, Union

from .charsetprober import CharSetProber
from .enums import ProbingState
from .sbcharsetprober import SingleByteCharSetProber

# This prober doesn't actually recognize a language or a charset.
# It is a helper prober for the use of the Hebrew model probers

### General ideas of the Hebrew charset recognition ###
#
# Four main charsets exist in Hebrew:
# "ISO-8859-8" - Visual Hebrew
# "windows-1255" - Logical Hebrew
# "ISO-8859-8-I" - Logical Hebrew
# "x-mac-hebrew" - ?? Logical Hebrew ??
#
# Both "ISO" charsets use a completely identical set of code points, whereas
# "windows-1255" and "x-mac-hebrew" are two different proper supersets of
# these code points. windows-1255 defines additional characters in the range
# 0x80-0x9F as some misc punctuation marks as well as some Hebrew-specific
# diacritics and additional 'Yiddish' ligature letters in the range 0xc0-0xd6.
# x-mac-hebrew defines similar additional code points but with a different
# mapping.
#
# As far as an average Hebrew text with no diacritics is concerned, all four
# charsets are identical with respect to code points. Meaning that for the
# main Hebrew alphabet, all four map the same values to all 27 Hebrew letters
# (including final letters).
#
# The dominant difference between these charsets is their directionality.
# "Visual" directionality means that the text is ordered as if the renderer is
# not aware of a BIDI rendering algorithm. The renderer sees the text and
# draws it from left to right. The text itself when ordered naturally is read
# backwards. A buffer of Visual Hebrew generally looks like so:
# "[last word of first line spelled backwards] [whole line ordered backwards
# and spelled backwards] [first word of first line spelled backwards]
# [end of line] [last word of second line] ... etc' "
# adding punctuation marks, numbers and English text to visual text is
# naturally also "visual" and from left to right.
#
# "Logical" directionality means the text is ordered "naturally" according to
# the order it is read. It is the responsibility of the renderer to display
# the text from right to left. A BIDI algorithm is used to place general
# punctuation marks, numbers and English text in the text.
#
# Texts in x-mac-hebrew are almost impossible to find on the Internet. From
# what little evidence I could find, it seems that its general directionality
# is Logical.
#
# To sum up all of the above, the Hebrew probing mechanism knows about two
# charsets:
# Visual Hebrew - "ISO-8859-8" - backwards text - Words and sentences are
#    backwards while line order is natural. For charset recognition purposes
#    the line order is unimportant (In fact, for this implementation, even
#    word order is unimportant).
# Logical Hebrew - "windows-1255" - normal, naturally ordered text.
#
# "ISO-8859-8-I" is a subset of windows-1255 and doesn't need to be
#    specifically identified.
# "x-mac-hebrew" is also identified as windows-1255. A text in x-mac-hebrew
#    that contain special punctuation marks or diacritics is displayed with
#    some unconverted characters showing as question marks. This problem might
#    be corrected using another model prober for x-mac-hebrew. Due to the fact
#    that x-mac-hebrew texts are so rare, writing another model prober isn't
#    worth the effort and performance hit.
#
#### The Prober ####
#
# The prober is divided between two SBCharSetProbers and a HebrewProber,
# all of which are managed, created, fed data, inquired and deleted by the
# SBCSGroupProber. The two SBCharSetProbers identify that the text is in
# fact some kind of Hebrew, Logical or Visual. The final decision about which
# one is it is made by the HebrewProber by combining final-letter scores
# with the scores of the two SBCharSetProbers to produce a final answer.
#
# The SBCSGroupProber is responsible for stripping the original text of HTML
# tags, English characters, numbers, low-ASCII punctuation characters, spaces
# and new lines. It reduces any sequence of such characters to a single space.
# The buffer fed to each prober in the SBCS group prober is pure text in
# high-ASCII.
# The two SBCharSetProbers (model probers) share the same language model:
# Win1255Model.
# The first SBCharSetProber uses the model normally as any other
# SBCharSetProber does, to recognize windows-1255, upon which this model was
# built. The second SBCharSetProber is told to make the pair-of-letter
# lookup in the language model backwards. This in practice exactly simulates
# a visual Hebrew model using the windows-1255 logical Hebrew model.
#
# The HebrewProber is not using any language model. All it does is look for
# final-letter evidence suggesting the text is either logical Hebrew or visual
# Hebrew. Disjointed from the model probers, the results of the HebrewProber
# alone are meaningless. HebrewProber always returns 0.00 as confidence
# since it never identifies a charset by itself. Instead, the pointer to the
# HebrewProber is passed to the model probers as a helper "Name Prober".
# When the Group prober receives a positive identification from any prober,
# it asks for the name of the charset identified. If the prober queried is a
# Hebrew model prober, the model prober forwards the call to the
# HebrewProber to make the final decision. In the HebrewProber, the
# decision is made according to the final-letters scores maintained and Both
# model probers scores. The answer is returned in the form of the name of the
# charset identified, either "windows-1255" or "ISO-8859-8".


class HebrewProber(CharSetProber):
    SPACE = 0x20
    # windows-1255 / ISO-8859-8 code points of interest
    FINAL_KAF = 0xEA
    NORMAL_KAF = 0xEB
    FINAL_MEM = 0xED
    NORMAL_MEM = 0xEE
    FINAL_NUN = 0xEF
    NORMAL_NUN = 0xF0
    FINAL_PE = 0xF3
    NORMAL_PE = 0xF4
    FINAL_TSADI = 0xF5
    NORMAL_TSADI = 0xF6

    # Minimum Visual vs Logical final letter score difference.
    # If the difference is below this, don't rely solely on the final letter score
    # distance.
    MIN_FINAL_CHAR_DISTANCE = 5

    # Minimum Visual vs Logical model score difference.
    # If the difference is below this, don't rely at all on the model score
    # distance.
    MIN_MODEL_DISTANCE = 0.01

    VISUAL_HEBREW_NAME = "ISO-8859-8"
    LOGICAL_HEBREW_NAME = "windows-1255"

    def __init__(self) -> None:
        super().__init__()
        self._final_char_logical_score = 0
        self._final_char_visual_score = 0
        self._prev = self.SPACE
        self._before_prev = self.SPACE
        self._logical_prober: Optional[SingleByteCharSetProber] = None
        self._visual_prober: Optional[SingleByteCharSetProber] = None
        self.reset()

    def reset(self) -> None:
        self._final_char_logical_score = 0
        self._final_char_visual_score = 0
        # The two last characters seen in the previous buffer,
        # mPrev and mBeforePrev are initialized to space in order to simulate
        # a word delimiter at the beginning of the data
        self._prev = self.SPACE
        self._before_prev = self.SPACE
        # These probers are owned by the group prober.

    def set_model_probers(
        self,
        logical_prober: SingleByteCharSetProber,
        visual_prober: SingleByteCharSetProber,
    ) -> None:
        self._logical_prober = logical_prober
        self._visual_prober = visual_prober

    def is_final(self, c: int) -> bool:
        return c in [
            self.FINAL_KAF,
            self.FINAL_MEM,
            self.FINAL_NUN,
            self.FINAL_PE,
            self.FINAL_TSADI,
        ]

    def is_non_final(self, c: int) -> bool:
        # The normal Tsadi is not a good Non-Final letter due to words like
        # 'lechotet' (to chat) containing an apostrophe after the tsadi. This
        # apostrophe is converted to a space in FilterWithoutEnglishLetters
        # causing the Non-Final tsadi to appear at an end of a word even
        # though this is not the case in the original text.
        # The letters Pe and Kaf rarely display a related behavior of not being
        # a good Non-Final letter. Words like 'Pop', 'Winamp' and 'Mubarak'
        # for example legally end with a Non-Final Pe or Kaf. However, the
        # benefit of these letters as Non-Final letters outweighs the damage
        # since these words are quite rare.
        return c in [self.NORMAL_KAF, self.NORMAL_MEM, self.NORMAL_NUN, self.NORMAL_PE]

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        # Final letter analysis for logical-visual decision.
        # Look for evidence that the received buffer is either logical Hebrew
        # or visual Hebrew.
        # The following cases are checked:
        # 1) A word longer than 1 letter, ending with a final letter. This is
        #    an indication that the text is laid out "naturally" since the
        #    final letter really appears at the end. +1 for logical score.
        # 2) A word longer than 1 letter, ending with a Non-Final letter. In
        #    normal Hebrew, words ending with Kaf, Mem, Nun, Pe or Tsadi,
        #    should not end with the Non-Final form of that letter. Exceptions
        #    to this rule are mentioned above in isNonFinal(). This is an
        #    indication that the text is laid out backwards. +1 for visual
        #    score
        # 3) A word longer than 1 letter, starting with a final letter. Final
        #    letters should not appear at the beginning of a word. This is an
        #    indication that the text is laid out backwards. +1 for visual
        #    score.
        #
        # The visual score and logical score are accumulated throughout the
        # text and are finally checked against each other in GetCharSetName().
        # No checking for final letters in the middle of words is done since
        # that case is not an indication for either Logical or Visual text.
        #
        # We automatically filter out all 7-bit characters (replace them with
        # spaces) so the word boundary detection works properly. [MAP]

        if self.state == ProbingState.NOT_ME:
            # Both model probers say it's not them. No reason to continue.
            return ProbingState.NOT_ME

        byte_str = self.filter_high_byte_only(byte_str)

        for cur in byte_str:
            if cur == self.SPACE:
                # We stand on a space - a word just ended
                if self._before_prev != self.SPACE:
                    # next-to-last char was not a space so self._prev is not a
                    # 1 letter word
                    if self.is_final(self._prev):
                        # case (1) [-2:not space][-1:final letter][cur:space]
                        self._final_char_logical_score += 1
                    elif self.is_non_final(self._prev):
                        # case (2) [-2:not space][-1:Non-Final letter][
                        #  cur:space]
                        self._final_char_visual_score += 1
            else:
                # Not standing on a space
                if (
                    (self._before_prev == self.SPACE)
                    and (self.is_final(self._prev))
                    and (cur != self.SPACE)
                ):
                    # case (3) [-2:space][-1:final letter][cur:not space]
                    self._final_char_visual_score += 1
            self._before_prev = self._prev
            self._prev = cur

        # Forever detecting, till the end or until both model probers return
        # ProbingState.NOT_ME (handled above)
        return ProbingState.DETECTING

    @property
    def charset_name(self) -> str:
        assert self._logical_prober is not None
        assert self._visual_prober is not None

        # Make the decision: is it Logical or Visual?
        # If the final letter score distance is dominant enough, rely on it.
        finalsub = self._final_char_logical_score - self._final_char_visual_score
        if finalsub >= self.MIN_FINAL_CHAR_DISTANCE:
            return self.LOGICAL_HEBREW_NAME
        if finalsub <= -self.MIN_FINAL_CHAR_DISTANCE:
            return self.VISUAL_HEBREW_NAME

        # It's not dominant enough, try to rely on the model scores instead.
        modelsub = (
            self._logical_prober.get_confidence() - self._visual_prober.get_confidence()
        )
        if modelsub > self.MIN_MODEL_DISTANCE:
            return self.LOGICAL_HEBREW_NAME
        if modelsub < -self.MIN_MODEL_DISTANCE:
            return self.VISUAL_HEBREW_NAME

        # Still no good, back to final letter distance, maybe it'll save the
        # day.
        if finalsub < 0.0:
            return self.VISUAL_HEBREW_NAME

        # (finalsub > 0 - Logical) or (don't know what to do) default to
        # Logical.
        return self.LOGICAL_HEBREW_NAME

    @property
    def language(self) -> str:
        return "Hebrew"

    @property
    def state(self) -> ProbingState:
        assert self._logical_prober is not None
        assert self._visual_prober is not None

        # Remain active as long as any of the model probers are active.
        if (self._logical_prober.state == ProbingState.NOT_ME) and (
            self._visual_prober.state == ProbingState.NOT_ME
        ):
            return ProbingState.NOT_ME
        return ProbingState.DETECTING


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/sbcsgroupprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .hebrewprober import HebrewProber
from .langbulgarianmodel import ISO_8859_5_BULGARIAN_MODEL, WINDOWS_1251_BULGARIAN_MODEL
from .langgreekmodel import ISO_8859_7_GREEK_MODEL, WINDOWS_1253_GREEK_MODEL
from .langhebrewmodel import WINDOWS_1255_HEBREW_MODEL

# from .langhungarianmodel import (ISO_8859_2_HUNGARIAN_MODEL,
#                                  WINDOWS_1250_HUNGARIAN_MODEL)
from .langrussianmodel import (
    IBM855_RUSSIAN_MODEL,
    IBM866_RUSSIAN_MODEL,
    ISO_8859_5_RUSSIAN_MODEL,
    KOI8_R_RUSSIAN_MODEL,
    MACCYRILLIC_RUSSIAN_MODEL,
    WINDOWS_1251_RUSSIAN_MODEL,
)
from .langthaimodel import TIS_620_THAI_MODEL
from .langturkishmodel import ISO_8859_9_TURKISH_MODEL
from .sbcharsetprober import SingleByteCharSetProber


class SBCSGroupProber(CharSetGroupProber):
    def __init__(self) -> None:
        super().__init__()
        hebrew_prober = HebrewProber()
        logical_hebrew_prober = SingleByteCharSetProber(
            WINDOWS_1255_HEBREW_MODEL, is_reversed=False, name_prober=hebrew_prober
        )
        # TODO: See if using ISO-8859-8 Hebrew model works better here, since
        #       it's actually the visual one
        visual_hebrew_prober = SingleByteCharSetProber(
            WINDOWS_1255_HEBREW_MODEL, is_reversed=True, name_prober=hebrew_prober
        )
        hebrew_prober.set_model_probers(logical_hebrew_prober, visual_hebrew_prober)
        # TODO: ORDER MATTERS HERE. I changed the order vs what was in master
        #       and several tests failed that did not before. Some thought
        #       should be put into the ordering, and we should consider making
        #       order not matter here, because that is very counter-intuitive.
        self.probers = [
            SingleByteCharSetProber(WINDOWS_1251_RUSSIAN_MODEL),
            SingleByteCharSetProber(KOI8_R_RUSSIAN_MODEL),
            SingleByteCharSetProber(ISO_8859_5_RUSSIAN_MODEL),
            SingleByteCharSetProber(MACCYRILLIC_RUSSIAN_MODEL),
            SingleByteCharSetProber(IBM866_RUSSIAN_MODEL),
            SingleByteCharSetProber(IBM855_RUSSIAN_MODEL),
            SingleByteCharSetProber(ISO_8859_7_GREEK_MODEL),
            SingleByteCharSetProber(WINDOWS_1253_GREEK_MODEL),
            SingleByteCharSetProber(ISO_8859_5_BULGARIAN_MODEL),
            SingleByteCharSetProber(WINDOWS_1251_BULGARIAN_MODEL),
            # TODO: Restore Hungarian encodings (iso-8859-2 and windows-1250)
            #       after we retrain model.
            # SingleByteCharSetProber(ISO_8859_2_HUNGARIAN_MODEL),
            # SingleByteCharSetProber(WINDOWS_1250_HUNGARIAN_MODEL),
            SingleByteCharSetProber(TIS_620_THAI_MODEL),
            SingleByteCharSetProber(ISO_8859_9_TURKISH_MODEL),
            hebrew_prober,
            logical_hebrew_prober,
            visual_hebrew_prober,
        ]
        self.reset()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/charsetprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

import logging
import re
from typing import Optional, Union

from .enums import LanguageFilter, ProbingState

INTERNATIONAL_WORDS_PATTERN = re.compile(
    b"[a-zA-Z]*[\x80-\xFF]+[a-zA-Z]*[^a-zA-Z\x80-\xFF]?"
)


class CharSetProber:

    SHORTCUT_THRESHOLD = 0.95

    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        self._state = ProbingState.DETECTING
        self.active = True
        self.lang_filter = lang_filter
        self.logger = logging.getLogger(__name__)

    def reset(self) -> None:
        self._state = ProbingState.DETECTING

    @property
    def charset_name(self) -> Optional[str]:
        return None

    @property
    def language(self) -> Optional[str]:
        raise NotImplementedError

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        raise NotImplementedError

    @property
    def state(self) -> ProbingState:
        return self._state

    def get_confidence(self) -> float:
        return 0.0

    @staticmethod
    def filter_high_byte_only(buf: Union[bytes, bytearray]) -> bytes:
        buf = re.sub(b"([\x00-\x7F])+", b" ", buf)
        return buf

    @staticmethod
    def filter_international_words(buf: Union[bytes, bytearray]) -> bytearray:
        """
        We define three types of bytes:
        alphabet: english alphabets [a-zA-Z]
        international: international characters [\x80-\xFF]
        marker: everything else [^a-zA-Z\x80-\xFF]
        The input buffer can be thought to contain a series of words delimited
        by markers. This function works to filter all words that contain at
        least one international character. All contiguous sequences of markers
        are replaced by a single space ascii character.
        This filter applies to all scripts which do not use English characters.
        """
        filtered = bytearray()

        # This regex expression filters out only words that have at-least one
        # international character. The word may include one marker character at
        # the end.
        words = INTERNATIONAL_WORDS_PATTERN.findall(buf)

        for word in words:
            filtered.extend(word[:-1])

            # If the last character in the word is a marker, replace it with a
            # space as markers shouldn't affect our analysis (they are used
            # similarly across all languages and may thus have similar
            # frequencies).
            last_char = word[-1:]
            if not last_char.isalpha() and last_char < b"\x80":
                last_char = b" "
            filtered.extend(last_char)

        return filtered

    @staticmethod
    def remove_xml_tags(buf: Union[bytes, bytearray]) -> bytes:
        """
        Returns a copy of ``buf`` that retains only the sequences of English
        alphabet and high byte characters that are not between <> characters.
        This filter can be applied to all scripts which contain both English
        characters and extended ASCII characters, but is currently only used by
        ``Latin1Prober``.
        """
        filtered = bytearray()
        in_tag = False
        prev = 0
        buf = memoryview(buf).cast("c")

        for curr, buf_char in enumerate(buf):
            # Check if we're coming out of or entering an XML tag

            # https://github.com/python/typeshed/issues/8182
            if buf_char == b">":  # type: ignore[comparison-overlap]
                prev = curr + 1
                in_tag = False
            # https://github.com/python/typeshed/issues/8182
            elif buf_char == b"<":  # type: ignore[comparison-overlap]
                if curr > prev and not in_tag:
                    # Keep everything after last non-extended-ASCII,
                    # non-alphabetic character
                    filtered.extend(buf[prev:curr])
                    # Output a space to delimit stretch we kept
                    filtered.extend(b" ")
                in_tag = True

        # If we're not in a tag...
        if not in_tag:
            # Keep everything after last non-extended-ASCII, non-alphabetic
            # character
            filtered.extend(buf[prev:])

        return filtered


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/escprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Optional, Union

from .charsetprober import CharSetProber
from .codingstatemachine import CodingStateMachine
from .enums import LanguageFilter, MachineState, ProbingState
from .escsm import (
    HZ_SM_MODEL,
    ISO2022CN_SM_MODEL,
    ISO2022JP_SM_MODEL,
    ISO2022KR_SM_MODEL,
)


class EscCharSetProber(CharSetProber):
    """
    This CharSetProber uses a "code scheme" approach for detecting encodings,
    whereby easily recognizable escape or shift sequences are relied on to
    identify these encodings.
    """

    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.coding_sm = []
        if self.lang_filter & LanguageFilter.CHINESE_SIMPLIFIED:
            self.coding_sm.append(CodingStateMachine(HZ_SM_MODEL))
            self.coding_sm.append(CodingStateMachine(ISO2022CN_SM_MODEL))
        if self.lang_filter & LanguageFilter.JAPANESE:
            self.coding_sm.append(CodingStateMachine(ISO2022JP_SM_MODEL))
        if self.lang_filter & LanguageFilter.KOREAN:
            self.coding_sm.append(CodingStateMachine(ISO2022KR_SM_MODEL))
        self.active_sm_count = 0
        self._detected_charset: Optional[str] = None
        self._detected_language: Optional[str] = None
        self._state = ProbingState.DETECTING
        self.reset()

    def reset(self) -> None:
        super().reset()
        for coding_sm in self.coding_sm:
            coding_sm.active = True
            coding_sm.reset()
        self.active_sm_count = len(self.coding_sm)
        self._detected_charset = None
        self._detected_language = None

    @property
    def charset_name(self) -> Optional[str]:
        return self._detected_charset

    @property
    def language(self) -> Optional[str]:
        return self._detected_language

    def get_confidence(self) -> float:
        return 0.99 if self._detected_charset else 0.00

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        for c in byte_str:
            for coding_sm in self.coding_sm:
                if not coding_sm.active:
                    continue
                coding_state = coding_sm.next_state(c)
                if coding_state == MachineState.ERROR:
                    coding_sm.active = False
                    self.active_sm_count -= 1
                    if self.active_sm_count <= 0:
                        self._state = ProbingState.NOT_ME
                        return self.state
                elif coding_state == MachineState.ITS_ME:
                    self._state = ProbingState.FOUND_IT
                    self._detected_charset = coding_sm.get_coding_state_machine()
                    self._detected_language = coding_sm.language
                    return self.state

        return self.state


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/euctwfreq.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# EUCTW frequency table
# Converted from big5 work
# by Taiwan's Mandarin Promotion Council
# <http:#www.edu.tw:81/mandr/>

# 128  --> 0.42261
# 256  --> 0.57851
# 512  --> 0.74851
# 1024 --> 0.89384
# 2048 --> 0.97583
#
# Idea Distribution Ratio = 0.74851/(1-0.74851) =2.98
# Random Distribution Ration = 512/(5401-512)=0.105
#
# Typical Distribution Ratio about 25% of Ideal one, still much higher than RDR

EUCTW_TYPICAL_DISTRIBUTION_RATIO = 0.75

# Char to FreqOrder table
EUCTW_TABLE_SIZE = 5376

# fmt: off
EUCTW_CHAR_TO_FREQ_ORDER = (
    1, 1800, 1506, 255, 1431, 198, 9, 82, 6, 7310, 177, 202, 3615, 1256, 2808, 110,  # 2742
    3735, 33, 3241, 261, 76, 44, 2113, 16, 2931, 2184, 1176, 659, 3868, 26, 3404, 2643,  # 2758
    1198, 3869, 3313, 4060, 410, 2211, 302, 590, 361, 1963, 8, 204, 58, 4296, 7311, 1931,  # 2774
    63, 7312, 7313, 317, 1614, 75, 222, 159, 4061, 2412, 1480, 7314, 3500, 3068, 224, 2809,  # 2790
    3616, 3, 10, 3870, 1471, 29, 2774, 1135, 2852, 1939, 873, 130, 3242, 1123, 312, 7315,  # 2806
    4297, 2051, 507, 252, 682, 7316, 142, 1914, 124, 206, 2932, 34, 3501, 3173, 64, 604,  # 2822
    7317, 2494, 1976, 1977, 155, 1990, 645, 641, 1606, 7318, 3405, 337, 72, 406, 7319, 80,  # 2838
    630, 238, 3174, 1509, 263, 939, 1092, 2644, 756, 1440, 1094, 3406, 449, 69, 2969, 591,  # 2854
    179, 2095, 471, 115, 2034, 1843, 60, 50, 2970, 134, 806, 1868, 734, 2035, 3407, 180,  # 2870
    995, 1607, 156, 537, 2893, 688, 7320, 319, 1305, 779, 2144, 514, 2374, 298, 4298, 359,  # 2886
    2495, 90, 2707, 1338, 663, 11, 906, 1099, 2545, 20, 2436, 182, 532, 1716, 7321, 732,  # 2902
    1376, 4062, 1311, 1420, 3175, 25, 2312, 1056, 113, 399, 382, 1949, 242, 3408, 2467, 529,  # 2918
    3243, 475, 1447, 3617, 7322, 117, 21, 656, 810, 1297, 2295, 2329, 3502, 7323, 126, 4063,  # 2934
    706, 456, 150, 613, 4299, 71, 1118, 2036, 4064, 145, 3069, 85, 835, 486, 2114, 1246,  # 2950
    1426, 428, 727, 1285, 1015, 800, 106, 623, 303, 1281, 7324, 2127, 2354, 347, 3736, 221,  # 2966
    3503, 3110, 7325, 1955, 1153, 4065, 83, 296, 1199, 3070, 192, 624, 93, 7326, 822, 1897,  # 2982
    2810, 3111, 795, 2064, 991, 1554, 1542, 1592, 27, 43, 2853, 859, 139, 1456, 860, 4300,  # 2998
    437, 712, 3871, 164, 2392, 3112, 695, 211, 3017, 2096, 195, 3872, 1608, 3504, 3505, 3618,  # 3014
    3873, 234, 811, 2971, 2097, 3874, 2229, 1441, 3506, 1615, 2375, 668, 2076, 1638, 305, 228,  # 3030
    1664, 4301, 467, 415, 7327, 262, 2098, 1593, 239, 108, 300, 200, 1033, 512, 1247, 2077,  # 3046
    7328, 7329, 2173, 3176, 3619, 2673, 593, 845, 1062, 3244, 88, 1723, 2037, 3875, 1950, 212,  # 3062
    266, 152, 149, 468, 1898, 4066, 4302, 77, 187, 7330, 3018, 37, 5, 2972, 7331, 3876,  # 3078
    7332, 7333, 39, 2517, 4303, 2894, 3177, 2078, 55, 148, 74, 4304, 545, 483, 1474, 1029,  # 3094
    1665, 217, 1869, 1531, 3113, 1104, 2645, 4067, 24, 172, 3507, 900, 3877, 3508, 3509, 4305,  # 3110
    32, 1408, 2811, 1312, 329, 487, 2355, 2247, 2708, 784, 2674, 4, 3019, 3314, 1427, 1788,  # 3126
    188, 109, 499, 7334, 3620, 1717, 1789, 888, 1217, 3020, 4306, 7335, 3510, 7336, 3315, 1520,  # 3142
    3621, 3878, 196, 1034, 775, 7337, 7338, 929, 1815, 249, 439, 38, 7339, 1063, 7340, 794,  # 3158
    3879, 1435, 2296, 46, 178, 3245, 2065, 7341, 2376, 7342, 214, 1709, 4307, 804, 35, 707,  # 3174
    324, 3622, 1601, 2546, 140, 459, 4068, 7343, 7344, 1365, 839, 272, 978, 2257, 2572, 3409,  # 3190
    2128, 1363, 3623, 1423, 697, 100, 3071, 48, 70, 1231, 495, 3114, 2193, 7345, 1294, 7346,  # 3206
    2079, 462, 586, 1042, 3246, 853, 256, 988, 185, 2377, 3410, 1698, 434, 1084, 7347, 3411,  # 3222
    314, 2615, 2775, 4308, 2330, 2331, 569, 2280, 637, 1816, 2518, 757, 1162, 1878, 1616, 3412,  # 3238
    287, 1577, 2115, 768, 4309, 1671, 2854, 3511, 2519, 1321, 3737, 909, 2413, 7348, 4069, 933,  # 3254
    3738, 7349, 2052, 2356, 1222, 4310, 765, 2414, 1322, 786, 4311, 7350, 1919, 1462, 1677, 2895,  # 3270
    1699, 7351, 4312, 1424, 2437, 3115, 3624, 2590, 3316, 1774, 1940, 3413, 3880, 4070, 309, 1369,  # 3286
    1130, 2812, 364, 2230, 1653, 1299, 3881, 3512, 3882, 3883, 2646, 525, 1085, 3021, 902, 2000,  # 3302
    1475, 964, 4313, 421, 1844, 1415, 1057, 2281, 940, 1364, 3116, 376, 4314, 4315, 1381, 7,  # 3318
    2520, 983, 2378, 336, 1710, 2675, 1845, 321, 3414, 559, 1131, 3022, 2742, 1808, 1132, 1313,  # 3334
    265, 1481, 1857, 7352, 352, 1203, 2813, 3247, 167, 1089, 420, 2814, 776, 792, 1724, 3513,  # 3350
    4071, 2438, 3248, 7353, 4072, 7354, 446, 229, 333, 2743, 901, 3739, 1200, 1557, 4316, 2647,  # 3366
    1920, 395, 2744, 2676, 3740, 4073, 1835, 125, 916, 3178, 2616, 4317, 7355, 7356, 3741, 7357,  # 3382
    7358, 7359, 4318, 3117, 3625, 1133, 2547, 1757, 3415, 1510, 2313, 1409, 3514, 7360, 2145, 438,  # 3398
    2591, 2896, 2379, 3317, 1068, 958, 3023, 461, 311, 2855, 2677, 4074, 1915, 3179, 4075, 1978,  # 3414
    383, 750, 2745, 2617, 4076, 274, 539, 385, 1278, 1442, 7361, 1154, 1964, 384, 561, 210,  # 3430
    98, 1295, 2548, 3515, 7362, 1711, 2415, 1482, 3416, 3884, 2897, 1257, 129, 7363, 3742, 642,  # 3446
    523, 2776, 2777, 2648, 7364, 141, 2231, 1333, 68, 176, 441, 876, 907, 4077, 603, 2592,  # 3462
    710, 171, 3417, 404, 549, 18, 3118, 2393, 1410, 3626, 1666, 7365, 3516, 4319, 2898, 4320,  # 3478
    7366, 2973, 368, 7367, 146, 366, 99, 871, 3627, 1543, 748, 807, 1586, 1185, 22, 2258,  # 3494
    379, 3743, 3180, 7368, 3181, 505, 1941, 2618, 1991, 1382, 2314, 7369, 380, 2357, 218, 702,  # 3510
    1817, 1248, 3418, 3024, 3517, 3318, 3249, 7370, 2974, 3628, 930, 3250, 3744, 7371, 59, 7372,  # 3526
    585, 601, 4078, 497, 3419, 1112, 1314, 4321, 1801, 7373, 1223, 1472, 2174, 7374, 749, 1836,  # 3542
    690, 1899, 3745, 1772, 3885, 1476, 429, 1043, 1790, 2232, 2116, 917, 4079, 447, 1086, 1629,  # 3558
    7375, 556, 7376, 7377, 2020, 1654, 844, 1090, 105, 550, 966, 1758, 2815, 1008, 1782, 686,  # 3574
    1095, 7378, 2282, 793, 1602, 7379, 3518, 2593, 4322, 4080, 2933, 2297, 4323, 3746, 980, 2496,  # 3590
    544, 353, 527, 4324, 908, 2678, 2899, 7380, 381, 2619, 1942, 1348, 7381, 1341, 1252, 560,  # 3606
    3072, 7382, 3420, 2856, 7383, 2053, 973, 886, 2080, 143, 4325, 7384, 7385, 157, 3886, 496,  # 3622
    4081, 57, 840, 540, 2038, 4326, 4327, 3421, 2117, 1445, 970, 2259, 1748, 1965, 2081, 4082,  # 3638
    3119, 1234, 1775, 3251, 2816, 3629, 773, 1206, 2129, 1066, 2039, 1326, 3887, 1738, 1725, 4083,  # 3654
    279, 3120, 51, 1544, 2594, 423, 1578, 2130, 2066, 173, 4328, 1879, 7386, 7387, 1583, 264,  # 3670
    610, 3630, 4329, 2439, 280, 154, 7388, 7389, 7390, 1739, 338, 1282, 3073, 693, 2857, 1411,  # 3686
    1074, 3747, 2440, 7391, 4330, 7392, 7393, 1240, 952, 2394, 7394, 2900, 1538, 2679, 685, 1483,  # 3702
    4084, 2468, 1436, 953, 4085, 2054, 4331, 671, 2395, 79, 4086, 2441, 3252, 608, 567, 2680,  # 3718
    3422, 4087, 4088, 1691, 393, 1261, 1791, 2396, 7395, 4332, 7396, 7397, 7398, 7399, 1383, 1672,  # 3734
    3748, 3182, 1464, 522, 1119, 661, 1150, 216, 675, 4333, 3888, 1432, 3519, 609, 4334, 2681,  # 3750
    2397, 7400, 7401, 7402, 4089, 3025, 0, 7403, 2469, 315, 231, 2442, 301, 3319, 4335, 2380,  # 3766
    7404, 233, 4090, 3631, 1818, 4336, 4337, 7405, 96, 1776, 1315, 2082, 7406, 257, 7407, 1809,  # 3782
    3632, 2709, 1139, 1819, 4091, 2021, 1124, 2163, 2778, 1777, 2649, 7408, 3074, 363, 1655, 3183,  # 3798
    7409, 2975, 7410, 7411, 7412, 3889, 1567, 3890, 718, 103, 3184, 849, 1443, 341, 3320, 2934,  # 3814
    1484, 7413, 1712, 127, 67, 339, 4092, 2398, 679, 1412, 821, 7414, 7415, 834, 738, 351,  # 3830
    2976, 2146, 846, 235, 1497, 1880, 418, 1992, 3749, 2710, 186, 1100, 2147, 2746, 3520, 1545,  # 3846
    1355, 2935, 2858, 1377, 583, 3891, 4093, 2573, 2977, 7416, 1298, 3633, 1078, 2549, 3634, 2358,  # 3862
    78, 3750, 3751, 267, 1289, 2099, 2001, 1594, 4094, 348, 369, 1274, 2194, 2175, 1837, 4338,  # 3878
    1820, 2817, 3635, 2747, 2283, 2002, 4339, 2936, 2748, 144, 3321, 882, 4340, 3892, 2749, 3423,  # 3894
    4341, 2901, 7417, 4095, 1726, 320, 7418, 3893, 3026, 788, 2978, 7419, 2818, 1773, 1327, 2859,  # 3910
    3894, 2819, 7420, 1306, 4342, 2003, 1700, 3752, 3521, 2359, 2650, 787, 2022, 506, 824, 3636,  # 3926
    534, 323, 4343, 1044, 3322, 2023, 1900, 946, 3424, 7421, 1778, 1500, 1678, 7422, 1881, 4344,  # 3942
    165, 243, 4345, 3637, 2521, 123, 683, 4096, 764, 4346, 36, 3895, 1792, 589, 2902, 816,  # 3958
    626, 1667, 3027, 2233, 1639, 1555, 1622, 3753, 3896, 7423, 3897, 2860, 1370, 1228, 1932, 891,  # 3974
    2083, 2903, 304, 4097, 7424, 292, 2979, 2711, 3522, 691, 2100, 4098, 1115, 4347, 118, 662,  # 3990
    7425, 611, 1156, 854, 2381, 1316, 2861, 2, 386, 515, 2904, 7426, 7427, 3253, 868, 2234,  # 4006
    1486, 855, 2651, 785, 2212, 3028, 7428, 1040, 3185, 3523, 7429, 3121, 448, 7430, 1525, 7431,  # 4022
    2164, 4348, 7432, 3754, 7433, 4099, 2820, 3524, 3122, 503, 818, 3898, 3123, 1568, 814, 676,  # 4038
    1444, 306, 1749, 7434, 3755, 1416, 1030, 197, 1428, 805, 2821, 1501, 4349, 7435, 7436, 7437,  # 4054
    1993, 7438, 4350, 7439, 7440, 2195, 13, 2779, 3638, 2980, 3124, 1229, 1916, 7441, 3756, 2131,  # 4070
    7442, 4100, 4351, 2399, 3525, 7443, 2213, 1511, 1727, 1120, 7444, 7445, 646, 3757, 2443, 307,  # 4086
    7446, 7447, 1595, 3186, 7448, 7449, 7450, 3639, 1113, 1356, 3899, 1465, 2522, 2523, 7451, 519,  # 4102
    7452, 128, 2132, 92, 2284, 1979, 7453, 3900, 1512, 342, 3125, 2196, 7454, 2780, 2214, 1980,  # 4118
    3323, 7455, 290, 1656, 1317, 789, 827, 2360, 7456, 3758, 4352, 562, 581, 3901, 7457, 401,  # 4134
    4353, 2248, 94, 4354, 1399, 2781, 7458, 1463, 2024, 4355, 3187, 1943, 7459, 828, 1105, 4101,  # 4150
    1262, 1394, 7460, 4102, 605, 4356, 7461, 1783, 2862, 7462, 2822, 819, 2101, 578, 2197, 2937,  # 4166
    7463, 1502, 436, 3254, 4103, 3255, 2823, 3902, 2905, 3425, 3426, 7464, 2712, 2315, 7465, 7466,  # 4182
    2332, 2067, 23, 4357, 193, 826, 3759, 2102, 699, 1630, 4104, 3075, 390, 1793, 1064, 3526,  # 4198
    7467, 1579, 3076, 3077, 1400, 7468, 4105, 1838, 1640, 2863, 7469, 4358, 4359, 137, 4106, 598,  # 4214
    3078, 1966, 780, 104, 974, 2938, 7470, 278, 899, 253, 402, 572, 504, 493, 1339, 7471,  # 4230
    3903, 1275, 4360, 2574, 2550, 7472, 3640, 3029, 3079, 2249, 565, 1334, 2713, 863, 41, 7473,  # 4246
    7474, 4361, 7475, 1657, 2333, 19, 463, 2750, 4107, 606, 7476, 2981, 3256, 1087, 2084, 1323,  # 4262
    2652, 2982, 7477, 1631, 1623, 1750, 4108, 2682, 7478, 2864, 791, 2714, 2653, 2334, 232, 2416,  # 4278
    7479, 2983, 1498, 7480, 2654, 2620, 755, 1366, 3641, 3257, 3126, 2025, 1609, 119, 1917, 3427,  # 4294
    862, 1026, 4109, 7481, 3904, 3760, 4362, 3905, 4363, 2260, 1951, 2470, 7482, 1125, 817, 4110,  # 4310
    4111, 3906, 1513, 1766, 2040, 1487, 4112, 3030, 3258, 2824, 3761, 3127, 7483, 7484, 1507, 7485,  # 4326
    2683, 733, 40, 1632, 1106, 2865, 345, 4113, 841, 2524, 230, 4364, 2984, 1846, 3259, 3428,  # 4342
    7486, 1263, 986, 3429, 7487, 735, 879, 254, 1137, 857, 622, 1300, 1180, 1388, 1562, 3907,  # 4358
    3908, 2939, 967, 2751, 2655, 1349, 592, 2133, 1692, 3324, 2985, 1994, 4114, 1679, 3909, 1901,  # 4374
    2185, 7488, 739, 3642, 2715, 1296, 1290, 7489, 4115, 2198, 2199, 1921, 1563, 2595, 2551, 1870,  # 4390
    2752, 2986, 7490, 435, 7491, 343, 1108, 596, 17, 1751, 4365, 2235, 3430, 3643, 7492, 4366,  # 4406
    294, 3527, 2940, 1693, 477, 979, 281, 2041, 3528, 643, 2042, 3644, 2621, 2782, 2261, 1031,  # 4422
    2335, 2134, 2298, 3529, 4367, 367, 1249, 2552, 7493, 3530, 7494, 4368, 1283, 3325, 2004, 240,  # 4438
    1762, 3326, 4369, 4370, 836, 1069, 3128, 474, 7495, 2148, 2525, 268, 3531, 7496, 3188, 1521,  # 4454
    1284, 7497, 1658, 1546, 4116, 7498, 3532, 3533, 7499, 4117, 3327, 2684, 1685, 4118, 961, 1673,  # 4470
    2622, 190, 2005, 2200, 3762, 4371, 4372, 7500, 570, 2497, 3645, 1490, 7501, 4373, 2623, 3260,  # 4486
    1956, 4374, 584, 1514, 396, 1045, 1944, 7502, 4375, 1967, 2444, 7503, 7504, 4376, 3910, 619,  # 4502
    7505, 3129, 3261, 215, 2006, 2783, 2553, 3189, 4377, 3190, 4378, 763, 4119, 3763, 4379, 7506,  # 4518
    7507, 1957, 1767, 2941, 3328, 3646, 1174, 452, 1477, 4380, 3329, 3130, 7508, 2825, 1253, 2382,  # 4534
    2186, 1091, 2285, 4120, 492, 7509, 638, 1169, 1824, 2135, 1752, 3911, 648, 926, 1021, 1324,  # 4550
    4381, 520, 4382, 997, 847, 1007, 892, 4383, 3764, 2262, 1871, 3647, 7510, 2400, 1784, 4384,  # 4566
    1952, 2942, 3080, 3191, 1728, 4121, 2043, 3648, 4385, 2007, 1701, 3131, 1551, 30, 2263, 4122,  # 4582
    7511, 2026, 4386, 3534, 7512, 501, 7513, 4123, 594, 3431, 2165, 1821, 3535, 3432, 3536, 3192,  # 4598
    829, 2826, 4124, 7514, 1680, 3132, 1225, 4125, 7515, 3262, 4387, 4126, 3133, 2336, 7516, 4388,  # 4614
    4127, 7517, 3912, 3913, 7518, 1847, 2383, 2596, 3330, 7519, 4389, 374, 3914, 652, 4128, 4129,  # 4630
    375, 1140, 798, 7520, 7521, 7522, 2361, 4390, 2264, 546, 1659, 138, 3031, 2445, 4391, 7523,  # 4646
    2250, 612, 1848, 910, 796, 3765, 1740, 1371, 825, 3766, 3767, 7524, 2906, 2554, 7525, 692,  # 4662
    444, 3032, 2624, 801, 4392, 4130, 7526, 1491, 244, 1053, 3033, 4131, 4132, 340, 7527, 3915,  # 4678
    1041, 2987, 293, 1168, 87, 1357, 7528, 1539, 959, 7529, 2236, 721, 694, 4133, 3768, 219,  # 4694
    1478, 644, 1417, 3331, 2656, 1413, 1401, 1335, 1389, 3916, 7530, 7531, 2988, 2362, 3134, 1825,  # 4710
    730, 1515, 184, 2827, 66, 4393, 7532, 1660, 2943, 246, 3332, 378, 1457, 226, 3433, 975,  # 4726
    3917, 2944, 1264, 3537, 674, 696, 7533, 163, 7534, 1141, 2417, 2166, 713, 3538, 3333, 4394,  # 4742
    3918, 7535, 7536, 1186, 15, 7537, 1079, 1070, 7538, 1522, 3193, 3539, 276, 1050, 2716, 758,  # 4758
    1126, 653, 2945, 3263, 7539, 2337, 889, 3540, 3919, 3081, 2989, 903, 1250, 4395, 3920, 3434,  # 4774
    3541, 1342, 1681, 1718, 766, 3264, 286, 89, 2946, 3649, 7540, 1713, 7541, 2597, 3334, 2990,  # 4790
    7542, 2947, 2215, 3194, 2866, 7543, 4396, 2498, 2526, 181, 387, 1075, 3921, 731, 2187, 3335,  # 4806
    7544, 3265, 310, 313, 3435, 2299, 770, 4134, 54, 3034, 189, 4397, 3082, 3769, 3922, 7545,  # 4822
    1230, 1617, 1849, 355, 3542, 4135, 4398, 3336, 111, 4136, 3650, 1350, 3135, 3436, 3035, 4137,  # 4838
    2149, 3266, 3543, 7546, 2784, 3923, 3924, 2991, 722, 2008, 7547, 1071, 247, 1207, 2338, 2471,  # 4854
    1378, 4399, 2009, 864, 1437, 1214, 4400, 373, 3770, 1142, 2216, 667, 4401, 442, 2753, 2555,  # 4870
    3771, 3925, 1968, 4138, 3267, 1839, 837, 170, 1107, 934, 1336, 1882, 7548, 7549, 2118, 4139,  # 4886
    2828, 743, 1569, 7550, 4402, 4140, 582, 2384, 1418, 3437, 7551, 1802, 7552, 357, 1395, 1729,  # 4902
    3651, 3268, 2418, 1564, 2237, 7553, 3083, 3772, 1633, 4403, 1114, 2085, 4141, 1532, 7554, 482,  # 4918
    2446, 4404, 7555, 7556, 1492, 833, 1466, 7557, 2717, 3544, 1641, 2829, 7558, 1526, 1272, 3652,  # 4934
    4142, 1686, 1794, 416, 2556, 1902, 1953, 1803, 7559, 3773, 2785, 3774, 1159, 2316, 7560, 2867,  # 4950
    4405, 1610, 1584, 3036, 2419, 2754, 443, 3269, 1163, 3136, 7561, 7562, 3926, 7563, 4143, 2499,  # 4966
    3037, 4406, 3927, 3137, 2103, 1647, 3545, 2010, 1872, 4144, 7564, 4145, 431, 3438, 7565, 250,  # 4982
    97, 81, 4146, 7566, 1648, 1850, 1558, 160, 848, 7567, 866, 740, 1694, 7568, 2201, 2830,  # 4998
    3195, 4147, 4407, 3653, 1687, 950, 2472, 426, 469, 3196, 3654, 3655, 3928, 7569, 7570, 1188,  # 5014
    424, 1995, 861, 3546, 4148, 3775, 2202, 2685, 168, 1235, 3547, 4149, 7571, 2086, 1674, 4408,  # 5030
    3337, 3270, 220, 2557, 1009, 7572, 3776, 670, 2992, 332, 1208, 717, 7573, 7574, 3548, 2447,  # 5046
    3929, 3338, 7575, 513, 7576, 1209, 2868, 3339, 3138, 4409, 1080, 7577, 7578, 7579, 7580, 2527,  # 5062
    3656, 3549, 815, 1587, 3930, 3931, 7581, 3550, 3439, 3777, 1254, 4410, 1328, 3038, 1390, 3932,  # 5078
    1741, 3933, 3778, 3934, 7582, 236, 3779, 2448, 3271, 7583, 7584, 3657, 3780, 1273, 3781, 4411,  # 5094
    7585, 308, 7586, 4412, 245, 4413, 1851, 2473, 1307, 2575, 430, 715, 2136, 2449, 7587, 270,  # 5110
    199, 2869, 3935, 7588, 3551, 2718, 1753, 761, 1754, 725, 1661, 1840, 4414, 3440, 3658, 7589,  # 5126
    7590, 587, 14, 3272, 227, 2598, 326, 480, 2265, 943, 2755, 3552, 291, 650, 1883, 7591,  # 5142
    1702, 1226, 102, 1547, 62, 3441, 904, 4415, 3442, 1164, 4150, 7592, 7593, 1224, 1548, 2756,  # 5158
    391, 498, 1493, 7594, 1386, 1419, 7595, 2055, 1177, 4416, 813, 880, 1081, 2363, 566, 1145,  # 5174
    4417, 2286, 1001, 1035, 2558, 2599, 2238, 394, 1286, 7596, 7597, 2068, 7598, 86, 1494, 1730,  # 5190
    3936, 491, 1588, 745, 897, 2948, 843, 3340, 3937, 2757, 2870, 3273, 1768, 998, 2217, 2069,  # 5206
    397, 1826, 1195, 1969, 3659, 2993, 3341, 284, 7599, 3782, 2500, 2137, 2119, 1903, 7600, 3938,  # 5222
    2150, 3939, 4151, 1036, 3443, 1904, 114, 2559, 4152, 209, 1527, 7601, 7602, 2949, 2831, 2625,  # 5238
    2385, 2719, 3139, 812, 2560, 7603, 3274, 7604, 1559, 737, 1884, 3660, 1210, 885, 28, 2686,  # 5254
    3553, 3783, 7605, 4153, 1004, 1779, 4418, 7606, 346, 1981, 2218, 2687, 4419, 3784, 1742, 797,  # 5270
    1642, 3940, 1933, 1072, 1384, 2151, 896, 3941, 3275, 3661, 3197, 2871, 3554, 7607, 2561, 1958,  # 5286
    4420, 2450, 1785, 7608, 7609, 7610, 3942, 4154, 1005, 1308, 3662, 4155, 2720, 4421, 4422, 1528,  # 5302
    2600, 161, 1178, 4156, 1982, 987, 4423, 1101, 4157, 631, 3943, 1157, 3198, 2420, 1343, 1241,  # 5318
    1016, 2239, 2562, 372, 877, 2339, 2501, 1160, 555, 1934, 911, 3944, 7611, 466, 1170, 169,  # 5334
    1051, 2907, 2688, 3663, 2474, 2994, 1182, 2011, 2563, 1251, 2626, 7612, 992, 2340, 3444, 1540,  # 5350
    2721, 1201, 2070, 2401, 1996, 2475, 7613, 4424, 528, 1922, 2188, 1503, 1873, 1570, 2364, 3342,  # 5366
    3276, 7614, 557, 1073, 7615, 1827, 3445, 2087, 2266, 3140, 3039, 3084, 767, 3085, 2786, 4425,  # 5382
    1006, 4158, 4426, 2341, 1267, 2176, 3664, 3199, 778, 3945, 3200, 2722, 1597, 2657, 7616, 4427,  # 5398
    7617, 3446, 7618, 7619, 7620, 3277, 2689, 1433, 3278, 131, 95, 1504, 3946, 723, 4159, 3141,  # 5414
    1841, 3555, 2758, 2189, 3947, 2027, 2104, 3665, 7621, 2995, 3948, 1218, 7622, 3343, 3201, 3949,  # 5430
    4160, 2576, 248, 1634, 3785, 912, 7623, 2832, 3666, 3040, 3786, 654, 53, 7624, 2996, 7625,  # 5446
    1688, 4428, 777, 3447, 1032, 3950, 1425, 7626, 191, 820, 2120, 2833, 971, 4429, 931, 3202,  # 5462
    135, 664, 783, 3787, 1997, 772, 2908, 1935, 3951, 3788, 4430, 2909, 3203, 282, 2723, 640,  # 5478
    1372, 3448, 1127, 922, 325, 3344, 7627, 7628, 711, 2044, 7629, 7630, 3952, 2219, 2787, 1936,  # 5494
    3953, 3345, 2220, 2251, 3789, 2300, 7631, 4431, 3790, 1258, 3279, 3954, 3204, 2138, 2950, 3955,  # 5510
    3956, 7632, 2221, 258, 3205, 4432, 101, 1227, 7633, 3280, 1755, 7634, 1391, 3281, 7635, 2910,  # 5526
    2056, 893, 7636, 7637, 7638, 1402, 4161, 2342, 7639, 7640, 3206, 3556, 7641, 7642, 878, 1325,  # 5542
    1780, 2788, 4433, 259, 1385, 2577, 744, 1183, 2267, 4434, 7643, 3957, 2502, 7644, 684, 1024,  # 5558
    4162, 7645, 472, 3557, 3449, 1165, 3282, 3958, 3959, 322, 2152, 881, 455, 1695, 1152, 1340,  # 5574
    660, 554, 2153, 4435, 1058, 4436, 4163, 830, 1065, 3346, 3960, 4437, 1923, 7646, 1703, 1918,  # 5590
    7647, 932, 2268, 122, 7648, 4438, 947, 677, 7649, 3791, 2627, 297, 1905, 1924, 2269, 4439,  # 5606
    2317, 3283, 7650, 7651, 4164, 7652, 4165, 84, 4166, 112, 989, 7653, 547, 1059, 3961, 701,  # 5622
    3558, 1019, 7654, 4167, 7655, 3450, 942, 639, 457, 2301, 2451, 993, 2951, 407, 851, 494,  # 5638
    4440, 3347, 927, 7656, 1237, 7657, 2421, 3348, 573, 4168, 680, 921, 2911, 1279, 1874, 285,  # 5654
    790, 1448, 1983, 719, 2167, 7658, 7659, 4441, 3962, 3963, 1649, 7660, 1541, 563, 7661, 1077,  # 5670
    7662, 3349, 3041, 3451, 511, 2997, 3964, 3965, 3667, 3966, 1268, 2564, 3350, 3207, 4442, 4443,  # 5686
    7663, 535, 1048, 1276, 1189, 2912, 2028, 3142, 1438, 1373, 2834, 2952, 1134, 2012, 7664, 4169,  # 5702
    1238, 2578, 3086, 1259, 7665, 700, 7666, 2953, 3143, 3668, 4170, 7667, 4171, 1146, 1875, 1906,  # 5718
    4444, 2601, 3967, 781, 2422, 132, 1589, 203, 147, 273, 2789, 2402, 898, 1786, 2154, 3968,  # 5734
    3969, 7668, 3792, 2790, 7669, 7670, 4445, 4446, 7671, 3208, 7672, 1635, 3793, 965, 7673, 1804,  # 5750
    2690, 1516, 3559, 1121, 1082, 1329, 3284, 3970, 1449, 3794, 65, 1128, 2835, 2913, 2759, 1590,  # 5766
    3795, 7674, 7675, 12, 2658, 45, 976, 2579, 3144, 4447, 517, 2528, 1013, 1037, 3209, 7676,  # 5782
    3796, 2836, 7677, 3797, 7678, 3452, 7679, 2602, 614, 1998, 2318, 3798, 3087, 2724, 2628, 7680,  # 5798
    2580, 4172, 599, 1269, 7681, 1810, 3669, 7682, 2691, 3088, 759, 1060, 489, 1805, 3351, 3285,  # 5814
    1358, 7683, 7684, 2386, 1387, 1215, 2629, 2252, 490, 7685, 7686, 4173, 1759, 2387, 2343, 7687,  # 5830
    4448, 3799, 1907, 3971, 2630, 1806, 3210, 4449, 3453, 3286, 2760, 2344, 874, 7688, 7689, 3454,  # 5846
    3670, 1858, 91, 2914, 3671, 3042, 3800, 4450, 7690, 3145, 3972, 2659, 7691, 3455, 1202, 1403,  # 5862
    3801, 2954, 2529, 1517, 2503, 4451, 3456, 2504, 7692, 4452, 7693, 2692, 1885, 1495, 1731, 3973,  # 5878
    2365, 4453, 7694, 2029, 7695, 7696, 3974, 2693, 1216, 237, 2581, 4174, 2319, 3975, 3802, 4454,  # 5894
    4455, 2694, 3560, 3457, 445, 4456, 7697, 7698, 7699, 7700, 2761, 61, 3976, 3672, 1822, 3977,  # 5910
    7701, 687, 2045, 935, 925, 405, 2660, 703, 1096, 1859, 2725, 4457, 3978, 1876, 1367, 2695,  # 5926
    3352, 918, 2105, 1781, 2476, 334, 3287, 1611, 1093, 4458, 564, 3146, 3458, 3673, 3353, 945,  # 5942
    2631, 2057, 4459, 7702, 1925, 872, 4175, 7703, 3459, 2696, 3089, 349, 4176, 3674, 3979, 4460,  # 5958
    3803, 4177, 3675, 2155, 3980, 4461, 4462, 4178, 4463, 2403, 2046, 782, 3981, 400, 251, 4179,  # 5974
    1624, 7704, 7705, 277, 3676, 299, 1265, 476, 1191, 3804, 2121, 4180, 4181, 1109, 205, 7706,  # 5990
    2582, 1000, 2156, 3561, 1860, 7707, 7708, 7709, 4464, 7710, 4465, 2565, 107, 2477, 2157, 3982,  # 6006
    3460, 3147, 7711, 1533, 541, 1301, 158, 753, 4182, 2872, 3562, 7712, 1696, 370, 1088, 4183,  # 6022
    4466, 3563, 579, 327, 440, 162, 2240, 269, 1937, 1374, 3461, 968, 3043, 56, 1396, 3090,  # 6038
    2106, 3288, 3354, 7713, 1926, 2158, 4467, 2998, 7714, 3564, 7715, 7716, 3677, 4468, 2478, 7717,  # 6054
    2791, 7718, 1650, 4469, 7719, 2603, 7720, 7721, 3983, 2661, 3355, 1149, 3356, 3984, 3805, 3985,  # 6070
    7722, 1076, 49, 7723, 951, 3211, 3289, 3290, 450, 2837, 920, 7724, 1811, 2792, 2366, 4184,  # 6086
    1908, 1138, 2367, 3806, 3462, 7725, 3212, 4470, 1909, 1147, 1518, 2423, 4471, 3807, 7726, 4472,  # 6102
    2388, 2604, 260, 1795, 3213, 7727, 7728, 3808, 3291, 708, 7729, 3565, 1704, 7730, 3566, 1351,  # 6118
    1618, 3357, 2999, 1886, 944, 4185, 3358, 4186, 3044, 3359, 4187, 7731, 3678, 422, 413, 1714,  # 6134
    3292, 500, 2058, 2345, 4188, 2479, 7732, 1344, 1910, 954, 7733, 1668, 7734, 7735, 3986, 2404,  # 6150
    4189, 3567, 3809, 4190, 7736, 2302, 1318, 2505, 3091, 133, 3092, 2873, 4473, 629, 31, 2838,  # 6166
    2697, 3810, 4474, 850, 949, 4475, 3987, 2955, 1732, 2088, 4191, 1496, 1852, 7737, 3988, 620,  # 6182
    3214, 981, 1242, 3679, 3360, 1619, 3680, 1643, 3293, 2139, 2452, 1970, 1719, 3463, 2168, 7738,  # 6198
    3215, 7739, 7740, 3361, 1828, 7741, 1277, 4476, 1565, 2047, 7742, 1636, 3568, 3093, 7743, 869,  # 6214
    2839, 655, 3811, 3812, 3094, 3989, 3000, 3813, 1310, 3569, 4477, 7744, 7745, 7746, 1733, 558,  # 6230
    4478, 3681, 335, 1549, 3045, 1756, 4192, 3682, 1945, 3464, 1829, 1291, 1192, 470, 2726, 2107,  # 6246
    2793, 913, 1054, 3990, 7747, 1027, 7748, 3046, 3991, 4479, 982, 2662, 3362, 3148, 3465, 3216,  # 6262
    3217, 1946, 2794, 7749, 571, 4480, 7750, 1830, 7751, 3570, 2583, 1523, 2424, 7752, 2089, 984,  # 6278
    4481, 3683, 1959, 7753, 3684, 852, 923, 2795, 3466, 3685, 969, 1519, 999, 2048, 2320, 1705,  # 6294
    7754, 3095, 615, 1662, 151, 597, 3992, 2405, 2321, 1049, 275, 4482, 3686, 4193, 568, 3687,  # 6310
    3571, 2480, 4194, 3688, 7755, 2425, 2270, 409, 3218, 7756, 1566, 2874, 3467, 1002, 769, 2840,  # 6326
    194, 2090, 3149, 3689, 2222, 3294, 4195, 628, 1505, 7757, 7758, 1763, 2177, 3001, 3993, 521,  # 6342
    1161, 2584, 1787, 2203, 2406, 4483, 3994, 1625, 4196, 4197, 412, 42, 3096, 464, 7759, 2632,  # 6358
    4484, 3363, 1760, 1571, 2875, 3468, 2530, 1219, 2204, 3814, 2633, 2140, 2368, 4485, 4486, 3295,  # 6374
    1651, 3364, 3572, 7760, 7761, 3573, 2481, 3469, 7762, 3690, 7763, 7764, 2271, 2091, 460, 7765,  # 6390
    4487, 7766, 3002, 962, 588, 3574, 289, 3219, 2634, 1116, 52, 7767, 3047, 1796, 7768, 7769,  # 6406
    7770, 1467, 7771, 1598, 1143, 3691, 4198, 1984, 1734, 1067, 4488, 1280, 3365, 465, 4489, 1572,  # 6422
    510, 7772, 1927, 2241, 1812, 1644, 3575, 7773, 4490, 3692, 7774, 7775, 2663, 1573, 1534, 7776,  # 6438
    7777, 4199, 536, 1807, 1761, 3470, 3815, 3150, 2635, 7778, 7779, 7780, 4491, 3471, 2915, 1911,  # 6454
    2796, 7781, 3296, 1122, 377, 3220, 7782, 360, 7783, 7784, 4200, 1529, 551, 7785, 2059, 3693,  # 6470
    1769, 2426, 7786, 2916, 4201, 3297, 3097, 2322, 2108, 2030, 4492, 1404, 136, 1468, 1479, 672,  # 6486
    1171, 3221, 2303, 271, 3151, 7787, 2762, 7788, 2049, 678, 2727, 865, 1947, 4493, 7789, 2013,  # 6502
    3995, 2956, 7790, 2728, 2223, 1397, 3048, 3694, 4494, 4495, 1735, 2917, 3366, 3576, 7791, 3816,  # 6518
    509, 2841, 2453, 2876, 3817, 7792, 7793, 3152, 3153, 4496, 4202, 2531, 4497, 2304, 1166, 1010,  # 6534
    552, 681, 1887, 7794, 7795, 2957, 2958, 3996, 1287, 1596, 1861, 3154, 358, 453, 736, 175,  # 6550
    478, 1117, 905, 1167, 1097, 7796, 1853, 1530, 7797, 1706, 7798, 2178, 3472, 2287, 3695, 3473,  # 6566
    3577, 4203, 2092, 4204, 7799, 3367, 1193, 2482, 4205, 1458, 2190, 2205, 1862, 1888, 1421, 3298,  # 6582
    2918, 3049, 2179, 3474, 595, 2122, 7800, 3997, 7801, 7802, 4206, 1707, 2636, 223, 3696, 1359,  # 6598
    751, 3098, 183, 3475, 7803, 2797, 3003, 419, 2369, 633, 704, 3818, 2389, 241, 7804, 7805,  # 6614
    7806, 838, 3004, 3697, 2272, 2763, 2454, 3819, 1938, 2050, 3998, 1309, 3099, 2242, 1181, 7807,  # 6630
    1136, 2206, 3820, 2370, 1446, 4207, 2305, 4498, 7808, 7809, 4208, 1055, 2605, 484, 3698, 7810,  # 6646
    3999, 625, 4209, 2273, 3368, 1499, 4210, 4000, 7811, 4001, 4211, 3222, 2274, 2275, 3476, 7812,  # 6662
    7813, 2764, 808, 2606, 3699, 3369, 4002, 4212, 3100, 2532, 526, 3370, 3821, 4213, 955, 7814,  # 6678
    1620, 4214, 2637, 2427, 7815, 1429, 3700, 1669, 1831, 994, 928, 7816, 3578, 1260, 7817, 7818,  # 6694
    7819, 1948, 2288, 741, 2919, 1626, 4215, 2729, 2455, 867, 1184, 362, 3371, 1392, 7820, 7821,  # 6710
    4003, 4216, 1770, 1736, 3223, 2920, 4499, 4500, 1928, 2698, 1459, 1158, 7822, 3050, 3372, 2877,  # 6726
    1292, 1929, 2506, 2842, 3701, 1985, 1187, 2071, 2014, 2607, 4217, 7823, 2566, 2507, 2169, 3702,  # 6742
    2483, 3299, 7824, 3703, 4501, 7825, 7826, 666, 1003, 3005, 1022, 3579, 4218, 7827, 4502, 1813,  # 6758
    2253, 574, 3822, 1603, 295, 1535, 705, 3823, 4219, 283, 858, 417, 7828, 7829, 3224, 4503,  # 6774
    4504, 3051, 1220, 1889, 1046, 2276, 2456, 4004, 1393, 1599, 689, 2567, 388, 4220, 7830, 2484,  # 6790
    802, 7831, 2798, 3824, 2060, 1405, 2254, 7832, 4505, 3825, 2109, 1052, 1345, 3225, 1585, 7833,  # 6806
    809, 7834, 7835, 7836, 575, 2730, 3477, 956, 1552, 1469, 1144, 2323, 7837, 2324, 1560, 2457,  # 6822
    3580, 3226, 4005, 616, 2207, 3155, 2180, 2289, 7838, 1832, 7839, 3478, 4506, 7840, 1319, 3704,  # 6838
    3705, 1211, 3581, 1023, 3227, 1293, 2799, 7841, 7842, 7843, 3826, 607, 2306, 3827, 762, 2878,  # 6854
    1439, 4221, 1360, 7844, 1485, 3052, 7845, 4507, 1038, 4222, 1450, 2061, 2638, 4223, 1379, 4508,  # 6870
    2585, 7846, 7847, 4224, 1352, 1414, 2325, 2921, 1172, 7848, 7849, 3828, 3829, 7850, 1797, 1451,  # 6886
    7851, 7852, 7853, 7854, 2922, 4006, 4007, 2485, 2346, 411, 4008, 4009, 3582, 3300, 3101, 4509,  # 6902
    1561, 2664, 1452, 4010, 1375, 7855, 7856, 47, 2959, 316, 7857, 1406, 1591, 2923, 3156, 7858,  # 6918
    1025, 2141, 3102, 3157, 354, 2731, 884, 2224, 4225, 2407, 508, 3706, 726, 3583, 996, 2428,  # 6934
    3584, 729, 7859, 392, 2191, 1453, 4011, 4510, 3707, 7860, 7861, 2458, 3585, 2608, 1675, 2800,  # 6950
    919, 2347, 2960, 2348, 1270, 4511, 4012, 73, 7862, 7863, 647, 7864, 3228, 2843, 2255, 1550,  # 6966
    1346, 3006, 7865, 1332, 883, 3479, 7866, 7867, 7868, 7869, 3301, 2765, 7870, 1212, 831, 1347,  # 6982
    4226, 4512, 2326, 3830, 1863, 3053, 720, 3831, 4513, 4514, 3832, 7871, 4227, 7872, 7873, 4515,  # 6998
    7874, 7875, 1798, 4516, 3708, 2609, 4517, 3586, 1645, 2371, 7876, 7877, 2924, 669, 2208, 2665,  # 7014
    2429, 7878, 2879, 7879, 7880, 1028, 3229, 7881, 4228, 2408, 7882, 2256, 1353, 7883, 7884, 4518,  # 7030
    3158, 518, 7885, 4013, 7886, 4229, 1960, 7887, 2142, 4230, 7888, 7889, 3007, 2349, 2350, 3833,  # 7046
    516, 1833, 1454, 4014, 2699, 4231, 4519, 2225, 2610, 1971, 1129, 3587, 7890, 2766, 7891, 2961,  # 7062
    1422, 577, 1470, 3008, 1524, 3373, 7892, 7893, 432, 4232, 3054, 3480, 7894, 2586, 1455, 2508,  # 7078
    2226, 1972, 1175, 7895, 1020, 2732, 4015, 3481, 4520, 7896, 2733, 7897, 1743, 1361, 3055, 3482,  # 7094
    2639, 4016, 4233, 4521, 2290, 895, 924, 4234, 2170, 331, 2243, 3056, 166, 1627, 3057, 1098,  # 7110
    7898, 1232, 2880, 2227, 3374, 4522, 657, 403, 1196, 2372, 542, 3709, 3375, 1600, 4235, 3483,  # 7126
    7899, 4523, 2767, 3230, 576, 530, 1362, 7900, 4524, 2533, 2666, 3710, 4017, 7901, 842, 3834,  # 7142
    7902, 2801, 2031, 1014, 4018, 213, 2700, 3376, 665, 621, 4236, 7903, 3711, 2925, 2430, 7904,  # 7158
    2431, 3302, 3588, 3377, 7905, 4237, 2534, 4238, 4525, 3589, 1682, 4239, 3484, 1380, 7906, 724,  # 7174
    2277, 600, 1670, 7907, 1337, 1233, 4526, 3103, 2244, 7908, 1621, 4527, 7909, 651, 4240, 7910,  # 7190
    1612, 4241, 2611, 7911, 2844, 7912, 2734, 2307, 3058, 7913, 716, 2459, 3059, 174, 1255, 2701,  # 7206
    4019, 3590, 548, 1320, 1398, 728, 4020, 1574, 7914, 1890, 1197, 3060, 4021, 7915, 3061, 3062,  # 7222
    3712, 3591, 3713, 747, 7916, 635, 4242, 4528, 7917, 7918, 7919, 4243, 7920, 7921, 4529, 7922,  # 7238
    3378, 4530, 2432, 451, 7923, 3714, 2535, 2072, 4244, 2735, 4245, 4022, 7924, 1764, 4531, 7925,  # 7254
    4246, 350, 7926, 2278, 2390, 2486, 7927, 4247, 4023, 2245, 1434, 4024, 488, 4532, 458, 4248,  # 7270
    4025, 3715, 771, 1330, 2391, 3835, 2568, 3159, 2159, 2409, 1553, 2667, 3160, 4249, 7928, 2487,  # 7286
    2881, 2612, 1720, 2702, 4250, 3379, 4533, 7929, 2536, 4251, 7930, 3231, 4252, 2768, 7931, 2015,  # 7302
    2736, 7932, 1155, 1017, 3716, 3836, 7933, 3303, 2308, 201, 1864, 4253, 1430, 7934, 4026, 7935,  # 7318
    7936, 7937, 7938, 7939, 4254, 1604, 7940, 414, 1865, 371, 2587, 4534, 4535, 3485, 2016, 3104,  # 7334
    4536, 1708, 960, 4255, 887, 389, 2171, 1536, 1663, 1721, 7941, 2228, 4027, 2351, 2926, 1580,  # 7350
    7942, 7943, 7944, 1744, 7945, 2537, 4537, 4538, 7946, 4539, 7947, 2073, 7948, 7949, 3592, 3380,  # 7366
    2882, 4256, 7950, 4257, 2640, 3381, 2802, 673, 2703, 2460, 709, 3486, 4028, 3593, 4258, 7951,  # 7382
    1148, 502, 634, 7952, 7953, 1204, 4540, 3594, 1575, 4541, 2613, 3717, 7954, 3718, 3105, 948,  # 7398
    3232, 121, 1745, 3837, 1110, 7955, 4259, 3063, 2509, 3009, 4029, 3719, 1151, 1771, 3838, 1488,  # 7414
    4030, 1986, 7956, 2433, 3487, 7957, 7958, 2093, 7959, 4260, 3839, 1213, 1407, 2803, 531, 2737,  # 7430
    2538, 3233, 1011, 1537, 7960, 2769, 4261, 3106, 1061, 7961, 3720, 3721, 1866, 2883, 7962, 2017,  # 7446
    120, 4262, 4263, 2062, 3595, 3234, 2309, 3840, 2668, 3382, 1954, 4542, 7963, 7964, 3488, 1047,  # 7462
    2704, 1266, 7965, 1368, 4543, 2845, 649, 3383, 3841, 2539, 2738, 1102, 2846, 2669, 7966, 7967,  # 7478
    1999, 7968, 1111, 3596, 2962, 7969, 2488, 3842, 3597, 2804, 1854, 3384, 3722, 7970, 7971, 3385,  # 7494
    2410, 2884, 3304, 3235, 3598, 7972, 2569, 7973, 3599, 2805, 4031, 1460, 856, 7974, 3600, 7975,  # 7510
    2885, 2963, 7976, 2886, 3843, 7977, 4264, 632, 2510, 875, 3844, 1697, 3845, 2291, 7978, 7979,  # 7526
    4544, 3010, 1239, 580, 4545, 4265, 7980, 914, 936, 2074, 1190, 4032, 1039, 2123, 7981, 7982,  # 7542
    7983, 3386, 1473, 7984, 1354, 4266, 3846, 7985, 2172, 3064, 4033, 915, 3305, 4267, 4268, 3306,  # 7558
    1605, 1834, 7986, 2739, 398, 3601, 4269, 3847, 4034, 328, 1912, 2847, 4035, 3848, 1331, 4270,  # 7574
    3011, 937, 4271, 7987, 3602, 4036, 4037, 3387, 2160, 4546, 3388, 524, 742, 538, 3065, 1012,  # 7590
    7988, 7989, 3849, 2461, 7990, 658, 1103, 225, 3850, 7991, 7992, 4547, 7993, 4548, 7994, 3236,  # 7606
    1243, 7995, 4038, 963, 2246, 4549, 7996, 2705, 3603, 3161, 7997, 7998, 2588, 2327, 7999, 4550,  # 7622
    8000, 8001, 8002, 3489, 3307, 957, 3389, 2540, 2032, 1930, 2927, 2462, 870, 2018, 3604, 1746,  # 7638
    2770, 2771, 2434, 2463, 8003, 3851, 8004, 3723, 3107, 3724, 3490, 3390, 3725, 8005, 1179, 3066,  # 7654
    8006, 3162, 2373, 4272, 3726, 2541, 3163, 3108, 2740, 4039, 8007, 3391, 1556, 2542, 2292, 977,  # 7670
    2887, 2033, 4040, 1205, 3392, 8008, 1765, 3393, 3164, 2124, 1271, 1689, 714, 4551, 3491, 8009,  # 7686
    2328, 3852, 533, 4273, 3605, 2181, 617, 8010, 2464, 3308, 3492, 2310, 8011, 8012, 3165, 8013,  # 7702
    8014, 3853, 1987, 618, 427, 2641, 3493, 3394, 8015, 8016, 1244, 1690, 8017, 2806, 4274, 4552,  # 7718
    8018, 3494, 8019, 8020, 2279, 1576, 473, 3606, 4275, 3395, 972, 8021, 3607, 8022, 3067, 8023,  # 7734
    8024, 4553, 4554, 8025, 3727, 4041, 4042, 8026, 153, 4555, 356, 8027, 1891, 2888, 4276, 2143,  # 7750
    408, 803, 2352, 8028, 3854, 8029, 4277, 1646, 2570, 2511, 4556, 4557, 3855, 8030, 3856, 4278,  # 7766
    8031, 2411, 3396, 752, 8032, 8033, 1961, 2964, 8034, 746, 3012, 2465, 8035, 4279, 3728, 698,  # 7782
    4558, 1892, 4280, 3608, 2543, 4559, 3609, 3857, 8036, 3166, 3397, 8037, 1823, 1302, 4043, 2706,  # 7798
    3858, 1973, 4281, 8038, 4282, 3167, 823, 1303, 1288, 1236, 2848, 3495, 4044, 3398, 774, 3859,  # 7814
    8039, 1581, 4560, 1304, 2849, 3860, 4561, 8040, 2435, 2161, 1083, 3237, 4283, 4045, 4284, 344,  # 7830
    1173, 288, 2311, 454, 1683, 8041, 8042, 1461, 4562, 4046, 2589, 8043, 8044, 4563, 985, 894,  # 7846
    8045, 3399, 3168, 8046, 1913, 2928, 3729, 1988, 8047, 2110, 1974, 8048, 4047, 8049, 2571, 1194,  # 7862
    425, 8050, 4564, 3169, 1245, 3730, 4285, 8051, 8052, 2850, 8053, 636, 4565, 1855, 3861, 760,  # 7878
    1799, 8054, 4286, 2209, 1508, 4566, 4048, 1893, 1684, 2293, 8055, 8056, 8057, 4287, 4288, 2210,  # 7894
    479, 8058, 8059, 832, 8060, 4049, 2489, 8061, 2965, 2490, 3731, 990, 3109, 627, 1814, 2642,  # 7910
    4289, 1582, 4290, 2125, 2111, 3496, 4567, 8062, 799, 4291, 3170, 8063, 4568, 2112, 1737, 3013,  # 7926
    1018, 543, 754, 4292, 3309, 1676, 4569, 4570, 4050, 8064, 1489, 8065, 3497, 8066, 2614, 2889,  # 7942
    4051, 8067, 8068, 2966, 8069, 8070, 8071, 8072, 3171, 4571, 4572, 2182, 1722, 8073, 3238, 3239,  # 7958
    1842, 3610, 1715, 481, 365, 1975, 1856, 8074, 8075, 1962, 2491, 4573, 8076, 2126, 3611, 3240,  # 7974
    433, 1894, 2063, 2075, 8077, 602, 2741, 8078, 8079, 8080, 8081, 8082, 3014, 1628, 3400, 8083,  # 7990
    3172, 4574, 4052, 2890, 4575, 2512, 8084, 2544, 2772, 8085, 8086, 8087, 3310, 4576, 2891, 8088,  # 8006
    4577, 8089, 2851, 4578, 4579, 1221, 2967, 4053, 2513, 8090, 8091, 8092, 1867, 1989, 8093, 8094,  # 8022
    8095, 1895, 8096, 8097, 4580, 1896, 4054, 318, 8098, 2094, 4055, 4293, 8099, 8100, 485, 8101,  # 8038
    938, 3862, 553, 2670, 116, 8102, 3863, 3612, 8103, 3498, 2671, 2773, 3401, 3311, 2807, 8104,  # 8054
    3613, 2929, 4056, 1747, 2930, 2968, 8105, 8106, 207, 8107, 8108, 2672, 4581, 2514, 8109, 3015,  # 8070
    890, 3614, 3864, 8110, 1877, 3732, 3402, 8111, 2183, 2353, 3403, 1652, 8112, 8113, 8114, 941,  # 8086
    2294, 208, 3499, 4057, 2019, 330, 4294, 3865, 2892, 2492, 3733, 4295, 8115, 8116, 8117, 8118,  # 8102
)
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/jpcntx.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import List, Tuple, Union

# This is hiragana 2-char sequence table, the number in each cell represents its frequency category
# fmt: off
jp2_char_context = (
    (0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1),
    (2, 4, 0, 4, 0, 3, 0, 4, 0, 3, 4, 4, 4, 2, 4, 3, 3, 4, 3, 2, 3, 3, 4, 2, 3, 3, 3, 2, 4, 1, 4, 3, 3, 1, 5, 4, 3, 4, 3, 4, 3, 5, 3, 0, 3, 5, 4, 2, 0, 3, 1, 0, 3, 3, 0, 3, 3, 0, 1, 1, 0, 4, 3, 0, 3, 3, 0, 4, 0, 2, 0, 3, 5, 5, 5, 5, 4, 0, 4, 1, 0, 3, 4),
    (0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2),
    (0, 4, 0, 5, 0, 5, 0, 4, 0, 4, 5, 4, 4, 3, 5, 3, 5, 1, 5, 3, 4, 3, 4, 4, 3, 4, 3, 3, 4, 3, 5, 4, 4, 3, 5, 5, 3, 5, 5, 5, 3, 5, 5, 3, 4, 5, 5, 3, 1, 3, 2, 0, 3, 4, 0, 4, 2, 0, 4, 2, 1, 5, 3, 2, 3, 5, 0, 4, 0, 2, 0, 5, 4, 4, 5, 4, 5, 0, 4, 0, 0, 4, 4),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    (0, 3, 0, 4, 0, 3, 0, 3, 0, 4, 5, 4, 3, 3, 3, 3, 4, 3, 5, 4, 4, 3, 5, 4, 4, 3, 4, 3, 4, 4, 4, 4, 5, 3, 4, 4, 3, 4, 5, 5, 4, 5, 5, 1, 4, 5, 4, 3, 0, 3, 3, 1, 3, 3, 0, 4, 4, 0, 3, 3, 1, 5, 3, 3, 3, 5, 0, 4, 0, 3, 0, 4, 4, 3, 4, 3, 3, 0, 4, 1, 1, 3, 4),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    (0, 4, 0, 3, 0, 3, 0, 4, 0, 3, 4, 4, 3, 2, 2, 1, 2, 1, 3, 1, 3, 3, 3, 3, 3, 4, 3, 1, 3, 3, 5, 3, 3, 0, 4, 3, 0, 5, 4, 3, 3, 5, 4, 4, 3, 4, 4, 5, 0, 1, 2, 0, 1, 2, 0, 2, 2, 0, 1, 0, 0, 5, 2, 2, 1, 4, 0, 3, 0, 1, 0, 4, 4, 3, 5, 4, 3, 0, 2, 1, 0, 4, 3),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    (0, 3, 0, 5, 0, 4, 0, 2, 1, 4, 4, 2, 4, 1, 4, 2, 4, 2, 4, 3, 3, 3, 4, 3, 3, 3, 3, 1, 4, 2, 3, 3, 3, 1, 4, 4, 1, 1, 1, 4, 3, 3, 2, 0, 2, 4, 3, 2, 0, 3, 3, 0, 3, 1, 1, 0, 0, 0, 3, 3, 0, 4, 2, 2, 3, 4, 0, 4, 0, 3, 0, 4, 4, 5, 3, 4, 4, 0, 3, 0, 0, 1, 4),
    (1, 4, 0, 4, 0, 4, 0, 4, 0, 3, 5, 4, 4, 3, 4, 3, 5, 4, 3, 3, 4, 3, 5, 4, 4, 4, 4, 3, 4, 2, 4, 3, 3, 1, 5, 4, 3, 2, 4, 5, 4, 5, 5, 4, 4, 5, 4, 4, 0, 3, 2, 2, 3, 3, 0, 4, 3, 1, 3, 2, 1, 4, 3, 3, 4, 5, 0, 3, 0, 2, 0, 4, 5, 5, 4, 5, 4, 0, 4, 0, 0, 5, 4),
    (0, 5, 0, 5, 0, 4, 0, 3, 0, 4, 4, 3, 4, 3, 3, 3, 4, 0, 4, 4, 4, 3, 4, 3, 4, 3, 3, 1, 4, 2, 4, 3, 4, 0, 5, 4, 1, 4, 5, 4, 4, 5, 3, 2, 4, 3, 4, 3, 2, 4, 1, 3, 3, 3, 2, 3, 2, 0, 4, 3, 3, 4, 3, 3, 3, 4, 0, 4, 0, 3, 0, 4, 5, 4, 4, 4, 3, 0, 4, 1, 0, 1, 3),
    (0, 3, 1, 4, 0, 3, 0, 2, 0, 3, 4, 4, 3, 1, 4, 2, 3, 3, 4, 3, 4, 3, 4, 3, 4, 4, 3, 2, 3, 1, 5, 4, 4, 1, 4, 4, 3, 5, 4, 4, 3, 5, 5, 4, 3, 4, 4, 3, 1, 2, 3, 1, 2, 2, 0, 3, 2, 0, 3, 1, 0, 5, 3, 3, 3, 4, 3, 3, 3, 3, 4, 4, 4, 4, 5, 4, 2, 0, 3, 3, 2, 4, 3),
    (0, 2, 0, 3, 0, 1, 0, 1, 0, 0, 3, 2, 0, 0, 2, 0, 1, 0, 2, 1, 3, 3, 3, 1, 2, 3, 1, 0, 1, 0, 4, 2, 1, 1, 3, 3, 0, 4, 3, 3, 1, 4, 3, 3, 0, 3, 3, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 4, 1, 0, 2, 3, 2, 2, 2, 1, 3, 3, 3, 4, 4, 3, 2, 0, 3, 1, 0, 3, 3),
    (0, 4, 0, 4, 0, 3, 0, 3, 0, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 2, 4, 3, 4, 3, 3, 2, 4, 3, 4, 5, 4, 1, 4, 5, 3, 5, 4, 5, 3, 5, 4, 0, 3, 5, 5, 3, 1, 3, 3, 2, 2, 3, 0, 3, 4, 1, 3, 3, 2, 4, 3, 3, 3, 4, 0, 4, 0, 3, 0, 4, 5, 4, 4, 5, 3, 0, 4, 1, 0, 3, 4),
    (0, 2, 0, 3, 0, 3, 0, 0, 0, 2, 2, 2, 1, 0, 1, 0, 0, 0, 3, 0, 3, 0, 3, 0, 1, 3, 1, 0, 3, 1, 3, 3, 3, 1, 3, 3, 3, 0, 1, 3, 1, 3, 4, 0, 0, 3, 1, 1, 0, 3, 2, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 3, 3, 2, 0, 3, 0, 0, 0, 0, 0, 3, 4, 3, 4, 3, 3, 0, 3, 0, 0, 2, 3),
    (2, 3, 0, 3, 0, 2, 0, 1, 0, 3, 3, 4, 3, 1, 3, 1, 1, 1, 3, 1, 4, 3, 4, 3, 3, 3, 0, 0, 3, 1, 5, 4, 3, 1, 4, 3, 2, 5, 5, 4, 4, 4, 4, 3, 3, 4, 4, 4, 0, 2, 1, 1, 3, 2, 0, 1, 2, 0, 0, 1, 0, 4, 1, 3, 3, 3, 0, 3, 0, 1, 0, 4, 4, 4, 5, 5, 3, 0, 2, 0, 0, 4, 4),
    (0, 2, 0, 1, 0, 3, 1, 3, 0, 2, 3, 3, 3, 0, 3, 1, 0, 0, 3, 0, 3, 2, 3, 1, 3, 2, 1, 1, 0, 0, 4, 2, 1, 0, 2, 3, 1, 4, 3, 2, 0, 4, 4, 3, 1, 3, 1, 3, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 4, 1, 1, 1, 2, 0, 3, 0, 0, 0, 3, 4, 2, 4, 3, 2, 0, 1, 0, 0, 3, 3),
    (0, 1, 0, 4, 0, 5, 0, 4, 0, 2, 4, 4, 2, 3, 3, 2, 3, 3, 5, 3, 3, 3, 4, 3, 4, 2, 3, 0, 4, 3, 3, 3, 4, 1, 4, 3, 2, 1, 5, 5, 3, 4, 5, 1, 3, 5, 4, 2, 0, 3, 3, 0, 1, 3, 0, 4, 2, 0, 1, 3, 1, 4, 3, 3, 3, 3, 0, 3, 0, 1, 0, 3, 4, 4, 4, 5, 5, 0, 3, 0, 1, 4, 5),
    (0, 2, 0, 3, 0, 3, 0, 0, 0, 2, 3, 1, 3, 0, 4, 0, 1, 1, 3, 0, 3, 4, 3, 2, 3, 1, 0, 3, 3, 2, 3, 1, 3, 0, 2, 3, 0, 2, 1, 4, 1, 2, 2, 0, 0, 3, 3, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 3, 2, 1, 3, 3, 0, 2, 0, 2, 0, 0, 3, 3, 1, 2, 4, 0, 3, 0, 2, 2, 3),
    (2, 4, 0, 5, 0, 4, 0, 4, 0, 2, 4, 4, 4, 3, 4, 3, 3, 3, 1, 2, 4, 3, 4, 3, 4, 4, 5, 0, 3, 3, 3, 3, 2, 0, 4, 3, 1, 4, 3, 4, 1, 4, 4, 3, 3, 4, 4, 3, 1, 2, 3, 0, 4, 2, 0, 4, 1, 0, 3, 3, 0, 4, 3, 3, 3, 4, 0, 4, 0, 2, 0, 3, 5, 3, 4, 5, 2, 0, 3, 0, 0, 4, 5),
    (0, 3, 0, 4, 0, 1, 0, 1, 0, 1, 3, 2, 2, 1, 3, 0, 3, 0, 2, 0, 2, 0, 3, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 0, 0, 4, 0, 3, 1, 0, 2, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 2, 2, 3, 1, 0, 3, 0, 0, 0, 1, 4, 4, 4, 3, 0, 0, 4, 0, 0, 1, 4),
    (1, 4, 1, 5, 0, 3, 0, 3, 0, 4, 5, 4, 4, 3, 5, 3, 3, 4, 4, 3, 4, 1, 3, 3, 3, 3, 2, 1, 4, 1, 5, 4, 3, 1, 4, 4, 3, 5, 4, 4, 3, 5, 4, 3, 3, 4, 4, 4, 0, 3, 3, 1, 2, 3, 0, 3, 1, 0, 3, 3, 0, 5, 4, 4, 4, 4, 4, 4, 3, 3, 5, 4, 4, 3, 3, 5, 4, 0, 3, 2, 0, 4, 4),
    (0, 2, 0, 3, 0, 1, 0, 0, 0, 1, 3, 3, 3, 2, 4, 1, 3, 0, 3, 1, 3, 0, 2, 2, 1, 1, 0, 0, 2, 0, 4, 3, 1, 0, 4, 3, 0, 4, 4, 4, 1, 4, 3, 1, 1, 3, 3, 1, 0, 2, 0, 0, 1, 3, 0, 0, 0, 0, 2, 0, 0, 4, 3, 2, 4, 3, 5, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3, 0, 2, 1, 0, 3, 3),
    (0, 2, 0, 4, 0, 3, 0, 2, 0, 2, 5, 5, 3, 4, 4, 4, 4, 1, 4, 3, 3, 0, 4, 3, 4, 3, 1, 3, 3, 2, 4, 3, 0, 3, 4, 3, 0, 3, 4, 4, 2, 4, 4, 0, 4, 5, 3, 3, 2, 2, 1, 1, 1, 2, 0, 1, 5, 0, 3, 3, 2, 4, 3, 3, 3, 4, 0, 3, 0, 2, 0, 4, 4, 3, 5, 5, 0, 0, 3, 0, 2, 3, 3),
    (0, 3, 0, 4, 0, 3, 0, 1, 0, 3, 4, 3, 3, 1, 3, 3, 3, 0, 3, 1, 3, 0, 4, 3, 3, 1, 1, 0, 3, 0, 3, 3, 0, 0, 4, 4, 0, 1, 5, 4, 3, 3, 5, 0, 3, 3, 4, 3, 0, 2, 0, 1, 1, 1, 0, 1, 3, 0, 1, 2, 1, 3, 3, 2, 3, 3, 0, 3, 0, 1, 0, 1, 3, 3, 4, 4, 1, 0, 1, 2, 2, 1, 3),
    (0, 1, 0, 4, 0, 4, 0, 3, 0, 1, 3, 3, 3, 2, 3, 1, 1, 0, 3, 0, 3, 3, 4, 3, 2, 4, 2, 0, 1, 0, 4, 3, 2, 0, 4, 3, 0, 5, 3, 3, 2, 4, 4, 4, 3, 3, 3, 4, 0, 1, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 4, 2, 3, 3, 3, 0, 3, 0, 0, 0, 4, 4, 4, 5, 3, 2, 0, 3, 3, 0, 3, 5),
    (0, 2, 0, 3, 0, 0, 0, 3, 0, 1, 3, 0, 2, 0, 0, 0, 1, 0, 3, 1, 1, 3, 3, 0, 0, 3, 0, 0, 3, 0, 2, 3, 1, 0, 3, 1, 0, 3, 3, 2, 0, 4, 2, 2, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 1, 0, 1, 0, 0, 0, 1, 3, 1, 2, 0, 0, 0, 1, 0, 0, 1, 4),
    (0, 3, 0, 3, 0, 5, 0, 1, 0, 2, 4, 3, 1, 3, 3, 2, 1, 1, 5, 2, 1, 0, 5, 1, 2, 0, 0, 0, 3, 3, 2, 2, 3, 2, 4, 3, 0, 0, 3, 3, 1, 3, 3, 0, 2, 5, 3, 4, 0, 3, 3, 0, 1, 2, 0, 2, 2, 0, 3, 2, 0, 2, 2, 3, 3, 3, 0, 2, 0, 1, 0, 3, 4, 4, 2, 5, 4, 0, 3, 0, 0, 3, 5),
    (0, 3, 0, 3, 0, 3, 0, 1, 0, 3, 3, 3, 3, 0, 3, 0, 2, 0, 2, 1, 1, 0, 2, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 0, 3, 2, 0, 0, 3, 3, 1, 2, 3, 1, 0, 3, 3, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 3, 1, 2, 3, 0, 3, 0, 1, 0, 3, 2, 1, 0, 4, 3, 0, 1, 1, 0, 3, 3),
    (0, 4, 0, 5, 0, 3, 0, 3, 0, 4, 5, 5, 4, 3, 5, 3, 4, 3, 5, 3, 3, 2, 5, 3, 4, 4, 4, 3, 4, 3, 4, 5, 5, 3, 4, 4, 3, 4, 4, 5, 4, 4, 4, 3, 4, 5, 5, 4, 2, 3, 4, 2, 3, 4, 0, 3, 3, 1, 4, 3, 2, 4, 3, 3, 5, 5, 0, 3, 0, 3, 0, 5, 5, 5, 5, 4, 4, 0, 4, 0, 1, 4, 4),
    (0, 4, 0, 4, 0, 3, 0, 3, 0, 3, 5, 4, 4, 2, 3, 2, 5, 1, 3, 2, 5, 1, 4, 2, 3, 2, 3, 3, 4, 3, 3, 3, 3, 2, 5, 4, 1, 3, 3, 5, 3, 4, 4, 0, 4, 4, 3, 1, 1, 3, 1, 0, 2, 3, 0, 2, 3, 0, 3, 0, 0, 4, 3, 1, 3, 4, 0, 3, 0, 2, 0, 4, 4, 4, 3, 4, 5, 0, 4, 0, 0, 3, 4),
    (0, 3, 0, 3, 0, 3, 1, 2, 0, 3, 4, 4, 3, 3, 3, 0, 2, 2, 4, 3, 3, 1, 3, 3, 3, 1, 1, 0, 3, 1, 4, 3, 2, 3, 4, 4, 2, 4, 4, 4, 3, 4, 4, 3, 2, 4, 4, 3, 1, 3, 3, 1, 3, 3, 0, 4, 1, 0, 2, 2, 1, 4, 3, 2, 3, 3, 5, 4, 3, 3, 5, 4, 4, 3, 3, 0, 4, 0, 3, 2, 2, 4, 4),
    (0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 3, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 1, 3, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 0, 3, 4, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1),
    (0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 4, 1, 4, 0, 3, 0, 4, 0, 3, 0, 4, 0, 3, 0, 3, 0, 4, 1, 5, 1, 4, 0, 0, 3, 0, 5, 0, 5, 2, 0, 1, 0, 0, 0, 2, 1, 4, 0, 1, 3, 0, 0, 3, 0, 0, 3, 1, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0),
    (1, 4, 0, 5, 0, 3, 0, 2, 0, 3, 5, 4, 4, 3, 4, 3, 5, 3, 4, 3, 3, 0, 4, 3, 3, 3, 3, 3, 3, 2, 4, 4, 3, 1, 3, 4, 4, 5, 4, 4, 3, 4, 4, 1, 3, 5, 4, 3, 3, 3, 1, 2, 2, 3, 3, 1, 3, 1, 3, 3, 3, 5, 3, 3, 4, 5, 0, 3, 0, 3, 0, 3, 4, 3, 4, 4, 3, 0, 3, 0, 2, 4, 3),
    (0, 1, 0, 4, 0, 0, 0, 0, 0, 1, 4, 0, 4, 1, 4, 2, 4, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 3, 1, 1, 1, 0, 3, 0, 0, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 3, 2, 0, 2, 2, 0, 1, 0, 0, 0, 2, 3, 2, 3, 3, 0, 0, 0, 0, 2, 1, 0),
    (0, 5, 1, 5, 0, 3, 0, 3, 0, 5, 4, 4, 5, 1, 5, 3, 3, 0, 4, 3, 4, 3, 5, 3, 4, 3, 3, 2, 4, 3, 4, 3, 3, 0, 3, 3, 1, 4, 4, 3, 4, 4, 4, 3, 4, 5, 5, 3, 2, 3, 1, 1, 3, 3, 1, 3, 1, 1, 3, 3, 2, 4, 5, 3, 3, 5, 0, 4, 0, 3, 0, 4, 4, 3, 5, 3, 3, 0, 3, 4, 0, 4, 3),
    (0, 5, 0, 5, 0, 3, 0, 2, 0, 4, 4, 3, 5, 2, 4, 3, 3, 3, 4, 4, 4, 3, 5, 3, 5, 3, 3, 1, 4, 0, 4, 3, 3, 0, 3, 3, 0, 4, 4, 4, 4, 5, 4, 3, 3, 5, 5, 3, 2, 3, 1, 2, 3, 2, 0, 1, 0, 0, 3, 2, 2, 4, 4, 3, 1, 5, 0, 4, 0, 3, 0, 4, 3, 1, 3, 2, 1, 0, 3, 3, 0, 3, 3),
    (0, 4, 0, 5, 0, 5, 0, 4, 0, 4, 5, 5, 5, 3, 4, 3, 3, 2, 5, 4, 4, 3, 5, 3, 5, 3, 4, 0, 4, 3, 4, 4, 3, 2, 4, 4, 3, 4, 5, 4, 4, 5, 5, 0, 3, 5, 5, 4, 1, 3, 3, 2, 3, 3, 1, 3, 1, 0, 4, 3, 1, 4, 4, 3, 4, 5, 0, 4, 0, 2, 0, 4, 3, 4, 4, 3, 3, 0, 4, 0, 0, 5, 5),
    (0, 4, 0, 4, 0, 5, 0, 1, 1, 3, 3, 4, 4, 3, 4, 1, 3, 0, 5, 1, 3, 0, 3, 1, 3, 1, 1, 0, 3, 0, 3, 3, 4, 0, 4, 3, 0, 4, 4, 4, 3, 4, 4, 0, 3, 5, 4, 1, 0, 3, 0, 0, 2, 3, 0, 3, 1, 0, 3, 1, 0, 3, 2, 1, 3, 5, 0, 3, 0, 1, 0, 3, 2, 3, 3, 4, 4, 0, 2, 2, 0, 4, 4),
    (2, 4, 0, 5, 0, 4, 0, 3, 0, 4, 5, 5, 4, 3, 5, 3, 5, 3, 5, 3, 5, 2, 5, 3, 4, 3, 3, 4, 3, 4, 5, 3, 2, 1, 5, 4, 3, 2, 3, 4, 5, 3, 4, 1, 2, 5, 4, 3, 0, 3, 3, 0, 3, 2, 0, 2, 3, 0, 4, 1, 0, 3, 4, 3, 3, 5, 0, 3, 0, 1, 0, 4, 5, 5, 5, 4, 3, 0, 4, 2, 0, 3, 5),
    (0, 5, 0, 4, 0, 4, 0, 2, 0, 5, 4, 3, 4, 3, 4, 3, 3, 3, 4, 3, 4, 2, 5, 3, 5, 3, 4, 1, 4, 3, 4, 4, 4, 0, 3, 5, 0, 4, 4, 4, 4, 5, 3, 1, 3, 4, 5, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 0, 3, 3, 2, 4, 3, 3, 3, 5, 3, 4, 1, 3, 3, 5, 3, 2, 0, 0, 0, 0, 4, 3, 1, 3, 3),
    (0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 3, 3, 3, 2, 3, 3, 3, 0, 3, 0, 0, 0, 3, 1, 3, 0, 0, 0, 2, 2, 2, 3, 0, 0, 3, 2, 0, 1, 2, 4, 1, 3, 3, 0, 0, 3, 3, 3, 0, 1, 0, 0, 2, 1, 0, 0, 3, 0, 3, 1, 0, 3, 0, 0, 1, 3, 0, 2, 0, 1, 0, 3, 3, 1, 3, 3, 0, 0, 1, 1, 0, 3, 3),
    (0, 2, 0, 3, 0, 2, 1, 4, 0, 2, 2, 3, 1, 1, 3, 1, 1, 0, 2, 0, 3, 1, 2, 3, 1, 3, 0, 0, 1, 0, 4, 3, 2, 3, 3, 3, 1, 4, 2, 3, 3, 3, 3, 1, 0, 3, 1, 4, 0, 1, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 0, 3, 1, 3, 2, 2, 0, 1, 0, 0, 0, 2, 3, 3, 3, 1, 0, 0, 0, 0, 0, 2, 3),
    (0, 5, 0, 4, 0, 5, 0, 2, 0, 4, 5, 5, 3, 3, 4, 3, 3, 1, 5, 4, 4, 2, 4, 4, 4, 3, 4, 2, 4, 3, 5, 5, 4, 3, 3, 4, 3, 3, 5, 5, 4, 5, 5, 1, 3, 4, 5, 3, 1, 4, 3, 1, 3, 3, 0, 3, 3, 1, 4, 3, 1, 4, 5, 3, 3, 5, 0, 4, 0, 3, 0, 5, 3, 3, 1, 4, 3, 0, 4, 0, 1, 5, 3),
    (0, 5, 0, 5, 0, 4, 0, 2, 0, 4, 4, 3, 4, 3, 3, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 3, 3, 5, 2, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 5, 5, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 1, 2, 2, 1, 4, 3, 3, 5, 4, 4, 3, 4, 0, 4, 0, 3, 0, 4, 4, 4, 4, 4, 1, 0, 4, 2, 0, 2, 4),
    (0, 4, 0, 4, 0, 3, 0, 1, 0, 3, 5, 2, 3, 0, 3, 0, 2, 1, 4, 2, 3, 3, 4, 1, 4, 3, 3, 2, 4, 1, 3, 3, 3, 0, 3, 3, 0, 0, 3, 3, 3, 5, 3, 3, 3, 3, 3, 2, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 3, 1, 2, 2, 3, 0, 3, 0, 2, 0, 4, 4, 3, 3, 4, 1, 0, 3, 0, 0, 2, 4),
    (0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 3, 1, 3, 0, 3, 2, 0, 0, 0, 1, 0, 3, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 2, 0, 0, 0, 0, 0, 0, 2),
    (0, 2, 1, 3, 0, 2, 0, 2, 0, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 4, 2, 2, 1, 2, 1, 4, 0, 4, 3, 1, 3, 3, 3, 2, 4, 3, 5, 4, 3, 3, 3, 3, 3, 3, 3, 0, 1, 3, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 4, 2, 0, 2, 3, 0, 3, 3, 0, 3, 3, 4, 2, 3, 1, 4, 0, 1, 2, 0, 2, 3),
    (0, 3, 0, 3, 0, 1, 0, 3, 0, 2, 3, 3, 3, 0, 3, 1, 2, 0, 3, 3, 2, 3, 3, 2, 3, 2, 3, 1, 3, 0, 4, 3, 2, 0, 3, 3, 1, 4, 3, 3, 2, 3, 4, 3, 1, 3, 3, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 4, 1, 1, 0, 3, 0, 3, 1, 0, 2, 3, 3, 3, 3, 3, 1, 0, 0, 2, 0, 3, 3),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3),
    (0, 2, 0, 3, 1, 3, 0, 3, 0, 2, 3, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3, 0, 2, 3, 1, 1, 4, 3, 3, 2, 3, 3, 1, 2, 2, 4, 1, 3, 3, 0, 1, 4, 2, 3, 0, 1, 3, 0, 3, 0, 0, 1, 3, 0, 2, 0, 0, 3, 3, 2, 1, 3, 0, 3, 0, 2, 0, 3, 4, 4, 4, 3, 1, 0, 3, 0, 0, 3, 3),
    (0, 2, 0, 1, 0, 2, 0, 0, 0, 1, 3, 2, 2, 1, 3, 0, 1, 1, 3, 0, 3, 2, 3, 1, 2, 0, 2, 0, 1, 1, 3, 3, 3, 0, 3, 3, 1, 1, 2, 3, 2, 3, 3, 1, 2, 3, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 2, 1, 2, 1, 3, 0, 3, 0, 0, 0, 3, 4, 4, 4, 3, 2, 0, 2, 0, 0, 2, 4),
    (0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 3),
    (0, 3, 0, 3, 0, 2, 0, 3, 0, 3, 3, 3, 2, 3, 2, 2, 2, 0, 3, 1, 3, 3, 3, 2, 3, 3, 0, 0, 3, 0, 3, 2, 2, 0, 2, 3, 1, 4, 3, 4, 3, 3, 2, 3, 1, 5, 4, 4, 0, 3, 1, 2, 1, 3, 0, 3, 1, 1, 2, 0, 2, 3, 1, 3, 1, 3, 0, 3, 0, 1, 0, 3, 3, 4, 4, 2, 1, 0, 2, 1, 0, 2, 4),
    (0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 4, 2, 5, 1, 4, 0, 2, 0, 2, 1, 3, 1, 4, 0, 2, 1, 0, 0, 2, 1, 4, 1, 1, 0, 3, 3, 0, 5, 1, 3, 2, 3, 3, 1, 0, 3, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 1, 0, 3, 0, 2, 0, 1, 0, 3, 3, 3, 4, 3, 3, 0, 0, 0, 0, 2, 3),
    (0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 3),
    (0, 1, 0, 3, 0, 4, 0, 3, 0, 2, 4, 3, 1, 0, 3, 2, 2, 1, 3, 1, 2, 2, 3, 1, 1, 1, 2, 1, 3, 0, 1, 2, 0, 1, 3, 2, 1, 3, 0, 5, 5, 1, 0, 0, 1, 3, 2, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 3, 4, 0, 1, 1, 1, 3, 2, 0, 2, 0, 1, 0, 2, 3, 3, 1, 2, 3, 0, 1, 0, 1, 0, 4),
    (0, 0, 0, 1, 0, 3, 0, 3, 0, 2, 2, 1, 0, 0, 4, 0, 3, 0, 3, 1, 3, 0, 3, 0, 3, 0, 1, 0, 3, 0, 3, 1, 3, 0, 3, 3, 0, 0, 1, 2, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 2, 3, 3, 3, 3, 0, 0, 0, 0, 1, 4),
    (0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 2, 0, 2, 3, 0, 0, 2, 2, 3, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 2, 3),
    (2, 4, 0, 5, 0, 5, 0, 4, 0, 3, 4, 3, 3, 3, 4, 3, 3, 3, 4, 3, 4, 4, 5, 4, 5, 5, 5, 2, 3, 0, 5, 5, 4, 1, 5, 4, 3, 1, 5, 4, 3, 4, 4, 3, 3, 4, 3, 3, 0, 3, 2, 0, 2, 3, 0, 3, 0, 0, 3, 3, 0, 5, 3, 2, 3, 3, 0, 3, 0, 3, 0, 3, 4, 5, 4, 5, 3, 0, 4, 3, 0, 3, 4),
    (0, 3, 0, 3, 0, 3, 0, 3, 0, 3, 3, 4, 3, 2, 3, 2, 3, 0, 4, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 2, 4, 3, 3, 1, 3, 4, 3, 4, 4, 4, 3, 4, 4, 3, 2, 4, 4, 1, 0, 2, 0, 0, 1, 1, 0, 2, 0, 0, 3, 1, 0, 5, 3, 2, 1, 3, 0, 3, 0, 1, 2, 4, 3, 2, 4, 3, 3, 0, 3, 2, 0, 4, 4),
    (0, 3, 0, 3, 0, 1, 0, 0, 0, 1, 4, 3, 3, 2, 3, 1, 3, 1, 4, 2, 3, 2, 4, 2, 3, 4, 3, 0, 2, 2, 3, 3, 3, 0, 3, 3, 3, 0, 3, 4, 1, 3, 3, 0, 3, 4, 3, 3, 0, 1, 1, 0, 1, 0, 0, 0, 4, 0, 3, 0, 0, 3, 1, 2, 1, 3, 0, 4, 0, 1, 0, 4, 3, 3, 4, 3, 3, 0, 2, 0, 0, 3, 3),
    (0, 3, 0, 4, 0, 1, 0, 3, 0, 3, 4, 3, 3, 0, 3, 3, 3, 1, 3, 1, 3, 3, 4, 3, 3, 3, 0, 0, 3, 1, 5, 3, 3, 1, 3, 3, 2, 5, 4, 3, 3, 4, 5, 3, 2, 5, 3, 4, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 4, 2, 2, 1, 3, 0, 3, 0, 2, 0, 4, 4, 3, 5, 3, 2, 0, 1, 1, 0, 3, 4),
    (0, 5, 0, 4, 0, 5, 0, 2, 0, 4, 4, 3, 3, 2, 3, 3, 3, 1, 4, 3, 4, 1, 5, 3, 4, 3, 4, 0, 4, 2, 4, 3, 4, 1, 5, 4, 0, 4, 4, 4, 4, 5, 4, 1, 3, 5, 4, 2, 1, 4, 1, 1, 3, 2, 0, 3, 1, 0, 3, 2, 1, 4, 3, 3, 3, 4, 0, 4, 0, 3, 0, 4, 4, 4, 3, 3, 3, 0, 4, 2, 0, 3, 4),
    (1, 4, 0, 4, 0, 3, 0, 1, 0, 3, 3, 3, 1, 1, 3, 3, 2, 2, 3, 3, 1, 0, 3, 2, 2, 1, 2, 0, 3, 1, 2, 1, 2, 0, 3, 2, 0, 2, 2, 3, 3, 4, 3, 0, 3, 3, 1, 2, 0, 1, 1, 3, 1, 2, 0, 0, 3, 0, 1, 1, 0, 3, 2, 2, 3, 3, 0, 3, 0, 0, 0, 2, 3, 3, 4, 3, 3, 0, 1, 0, 0, 1, 4),
    (0, 4, 0, 4, 0, 4, 0, 0, 0, 3, 4, 4, 3, 1, 4, 2, 3, 2, 3, 3, 3, 1, 4, 3, 4, 0, 3, 0, 4, 2, 3, 3, 2, 2, 5, 4, 2, 1, 3, 4, 3, 4, 3, 1, 3, 3, 4, 2, 0, 2, 1, 0, 3, 3, 0, 0, 2, 0, 3, 1, 0, 4, 4, 3, 4, 3, 0, 4, 0, 1, 0, 2, 4, 4, 4, 4, 4, 0, 3, 2, 0, 3, 3),
    (0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2),
    (0, 2, 0, 3, 0, 4, 0, 4, 0, 1, 3, 3, 3, 0, 4, 0, 2, 1, 2, 1, 1, 1, 2, 0, 3, 1, 1, 0, 1, 0, 3, 1, 0, 0, 3, 3, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 2, 0, 3, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 3, 0, 0, 0, 0, 1, 0, 0, 3, 3, 4, 3, 1, 0, 1, 0, 3, 0, 2),
    (0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 1, 0, 2, 0, 3, 1, 0, 1, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 4, 0, 0, 0, 2, 3, 0, 1, 4, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 3),
    (0, 2, 0, 5, 0, 5, 0, 1, 0, 2, 4, 3, 3, 2, 5, 1, 3, 2, 3, 3, 3, 0, 4, 1, 2, 0, 3, 0, 4, 0, 2, 2, 1, 1, 5, 3, 0, 0, 1, 4, 2, 3, 2, 0, 3, 3, 3, 2, 0, 2, 4, 1, 1, 2, 0, 1, 1, 0, 3, 1, 0, 1, 3, 1, 2, 3, 0, 2, 0, 0, 0, 1, 3, 5, 4, 4, 4, 0, 3, 0, 0, 1, 3),
    (0, 4, 0, 5, 0, 4, 0, 4, 0, 4, 5, 4, 3, 3, 4, 3, 3, 3, 4, 3, 4, 4, 5, 3, 4, 5, 4, 2, 4, 2, 3, 4, 3, 1, 4, 4, 1, 3, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 2, 3, 3, 1, 4, 3, 1, 3, 3, 0, 3, 3, 1, 4, 3, 4, 4, 4, 0, 3, 0, 4, 0, 3, 3, 4, 4, 5, 0, 0, 4, 3, 0, 4, 5),
    (0, 4, 0, 4, 0, 3, 0, 3, 0, 3, 4, 4, 4, 3, 3, 2, 4, 3, 4, 3, 4, 3, 5, 3, 4, 3, 2, 1, 4, 2, 4, 4, 3, 1, 3, 4, 2, 4, 5, 5, 3, 4, 5, 4, 1, 5, 4, 3, 0, 3, 2, 2, 3, 2, 1, 3, 1, 0, 3, 3, 3, 5, 3, 3, 3, 5, 4, 4, 2, 3, 3, 4, 3, 3, 3, 2, 1, 0, 3, 2, 1, 4, 3),
    (0, 4, 0, 5, 0, 4, 0, 3, 0, 3, 5, 5, 3, 2, 4, 3, 4, 0, 5, 4, 4, 1, 4, 4, 4, 3, 3, 3, 4, 3, 5, 5, 2, 3, 3, 4, 1, 2, 5, 5, 3, 5, 5, 2, 3, 5, 5, 4, 0, 3, 2, 0, 3, 3, 1, 1, 5, 1, 4, 1, 0, 4, 3, 2, 3, 5, 0, 4, 0, 3, 0, 5, 4, 3, 4, 3, 0, 0, 4, 1, 0, 4, 4),
    (1, 3, 0, 4, 0, 2, 0, 2, 0, 2, 5, 5, 3, 3, 3, 3, 3, 0, 4, 2, 3, 4, 4, 4, 3, 4, 0, 0, 3, 4, 5, 4, 3, 3, 3, 3, 2, 5, 5, 4, 5, 5, 5, 4, 3, 5, 5, 5, 1, 3, 1, 0, 1, 0, 0, 3, 2, 0, 4, 2, 0, 5, 2, 3, 2, 4, 1, 3, 0, 3, 0, 4, 5, 4, 5, 4, 3, 0, 4, 2, 0, 5, 4),
    (0, 3, 0, 4, 0, 5, 0, 3, 0, 3, 4, 4, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 4, 3, 3, 2, 2, 0, 3, 3, 3, 3, 3, 1, 3, 3, 3, 0, 4, 4, 3, 4, 4, 1, 1, 4, 4, 2, 0, 3, 1, 0, 1, 1, 0, 4, 1, 0, 2, 3, 1, 3, 3, 1, 3, 4, 0, 3, 0, 1, 0, 3, 1, 3, 0, 0, 1, 0, 2, 0, 0, 4, 4),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
    (0, 3, 0, 3, 0, 2, 0, 3, 0, 1, 5, 4, 3, 3, 3, 1, 4, 2, 1, 2, 3, 4, 4, 2, 4, 4, 5, 0, 3, 1, 4, 3, 4, 0, 4, 3, 3, 3, 2, 3, 2, 5, 3, 4, 3, 2, 2, 3, 0, 0, 3, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 2, 1, 1, 3, 1, 0, 2, 0, 4, 0, 3, 4, 4, 4, 5, 2, 0, 2, 0, 0, 1, 3),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 3, 2, 0, 0, 3, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 4, 0, 4, 2, 1, 0, 0, 0, 0, 0, 1),
    (0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 0, 2, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2),
    (0, 4, 0, 4, 0, 4, 0, 3, 0, 4, 4, 3, 4, 2, 4, 3, 2, 0, 4, 4, 4, 3, 5, 3, 5, 3, 3, 2, 4, 2, 4, 3, 4, 3, 1, 4, 0, 2, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 3, 4, 1, 3, 4, 3, 2, 1, 2, 1, 3, 3, 3, 4, 4, 3, 3, 5, 0, 4, 0, 3, 0, 4, 3, 3, 3, 2, 1, 0, 3, 0, 0, 3, 3),
    (0, 4, 0, 3, 0, 3, 0, 3, 0, 3, 5, 5, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 3, 5, 3, 3, 1, 3, 2, 4, 5, 5, 5, 5, 4, 3, 4, 5, 5, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 1, 2, 3, 2, 4, 3, 3, 3, 4, 0, 4, 0, 2, 0, 4, 3, 2, 2, 1, 2, 0, 3, 0, 0, 4, 1),
)
# fmt: on


class JapaneseContextAnalysis:
    NUM_OF_CATEGORY = 6
    DONT_KNOW = -1
    ENOUGH_REL_THRESHOLD = 100
    MAX_REL_THRESHOLD = 1000
    MINIMUM_DATA_THRESHOLD = 4

    def __init__(self) -> None:
        self._total_rel = 0
        self._rel_sample: List[int] = []
        self._need_to_skip_char_num = 0
        self._last_char_order = -1
        self._done = False
        self.reset()

    def reset(self) -> None:
        self._total_rel = 0  # total sequence received
        # category counters, each integer counts sequence in its category
        self._rel_sample = [0] * self.NUM_OF_CATEGORY
        # if last byte in current buffer is not the last byte of a character,
        # we need to know how many bytes to skip in next buffer
        self._need_to_skip_char_num = 0
        self._last_char_order = -1  # The order of previous char
        # If this flag is set to True, detection is done and conclusion has
        # been made
        self._done = False

    def feed(self, byte_str: Union[bytes, bytearray], num_bytes: int) -> None:
        if self._done:
            return

        # The buffer we got is byte oriented, and a character may span in more than one
        # buffers. In case the last one or two byte in last buffer is not
        # complete, we record how many byte needed to complete that character
        # and skip these bytes here.  We can choose to record those bytes as
        # well and analyse the character once it is complete, but since a
        # character will not make much difference, by simply skipping
        # this character will simply our logic and improve performance.
        i = self._need_to_skip_char_num
        while i < num_bytes:
            order, char_len = self.get_order(byte_str[i : i + 2])
            i += char_len
            if i > num_bytes:
                self._need_to_skip_char_num = i - num_bytes
                self._last_char_order = -1
            else:
                if (order != -1) and (self._last_char_order != -1):
                    self._total_rel += 1
                    if self._total_rel > self.MAX_REL_THRESHOLD:
                        self._done = True
                        break
                    self._rel_sample[
                        jp2_char_context[self._last_char_order][order]
                    ] += 1
                self._last_char_order = order

    def got_enough_data(self) -> bool:
        return self._total_rel > self.ENOUGH_REL_THRESHOLD

    def get_confidence(self) -> float:
        # This is just one way to calculate confidence. It works well for me.
        if self._total_rel > self.MINIMUM_DATA_THRESHOLD:
            return (self._total_rel - self._rel_sample[0]) / self._total_rel
        return self.DONT_KNOW

    def get_order(self, _: Union[bytes, bytearray]) -> Tuple[int, int]:
        return -1, 1


class SJISContextAnalysis(JapaneseContextAnalysis):
    def __init__(self) -> None:
        super().__init__()
        self._charset_name = "SHIFT_JIS"

    @property
    def charset_name(self) -> str:
        return self._charset_name

    def get_order(self, byte_str: Union[bytes, bytearray]) -> Tuple[int, int]:
        if not byte_str:
            return -1, 1
        # find out current char's byte length
        first_char = byte_str[0]
        if (0x81 <= first_char <= 0x9F) or (0xE0 <= first_char <= 0xFC):
            char_len = 2
            if (first_char == 0x87) or (0xFA <= first_char <= 0xFC):
                self._charset_name = "CP932"
        else:
            char_len = 1

        # return its order if it is hiragana
        if len(byte_str) > 1:
            second_char = byte_str[1]
            if (first_char == 202) and (0x9F <= second_char <= 0xF1):
                return second_char - 0x9F, char_len

        return -1, char_len


class EUCJPContextAnalysis(JapaneseContextAnalysis):
    def get_order(self, byte_str: Union[bytes, bytearray]) -> Tuple[int, int]:
        if not byte_str:
            return -1, 1
        # find out current char's byte length
        first_char = byte_str[0]
        if (first_char == 0x8E) or (0xA1 <= first_char <= 0xFE):
            char_len = 2
        elif first_char == 0x8F:
            char_len = 3
        else:
            char_len = 1

        # return its order if it is hiragana
        if len(byte_str) > 1:
            second_char = byte_str[1]
            if (first_char == 0xA4) and (0xA1 <= second_char <= 0xF3):
                return second_char - 0xA1, char_len

        return -1, char_len


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langhungarianmodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

HUNGARIAN_LANG_MODEL = {
    28: {  # 'A'
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 2,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 2,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 2,  # 'K'
        41: 2,  # 'L'
        34: 1,  # 'M'
        35: 2,  # 'N'
        47: 1,  # 'O'
        46: 2,  # 'P'
        43: 2,  # 'R'
        33: 2,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 2,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 1,  # 'j'
        7: 2,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 2,  # 'n'
        8: 0,  # 'o'
        23: 2,  # 'p'
        10: 2,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 1,  # 'u'
        19: 1,  # 'v'
        62: 1,  # 'x'
        16: 0,  # 'y'
        11: 3,  # 'z'
        51: 1,  # ''
        44: 0,  # ''
        61: 1,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    40: {  # 'B'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 0,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 3,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 2,  # 'i'
        22: 1,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 3,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    54: {  # 'C'
        28: 1,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 0,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 2,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 0,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 1,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 3,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 1,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    45: {  # 'D'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 0,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 0,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 3,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 1,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 1,  # 'o'
        23: 0,  # 'p'
        10: 2,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 2,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 1,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 0,  # ''
    },
    32: {  # 'E'
        28: 1,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 2,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 2,  # 'K'
        41: 2,  # 'L'
        34: 2,  # 'M'
        35: 2,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 2,  # 'R'
        33: 2,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 1,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 3,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 2,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 2,  # 's'
        3: 1,  # 't'
        21: 2,  # 'u'
        19: 1,  # 'v'
        62: 1,  # 'x'
        16: 0,  # 'y'
        11: 3,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 0,  # ''
        63: 1,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 1,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    50: {  # 'F'
        28: 1,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 0,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 0,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 0,  # 'V'
        55: 1,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 1,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 2,  # 'i'
        22: 1,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 2,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 0,  # ''
        63: 1,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 2,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    49: {  # 'G'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 2,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 1,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 2,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 2,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 0,  # ''
    },
    38: {  # 'H'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 0,  # 'D'
        32: 1,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 1,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 1,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 1,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 0,  # 'V'
        55: 1,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 2,  # 'i'
        22: 1,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 0,  # 'n'
        8: 3,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 2,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 2,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 1,  # ''
        30: 2,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    39: {  # 'I'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 2,  # 'K'
        41: 2,  # 'L'
        34: 1,  # 'M'
        35: 2,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 2,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 2,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 0,  # 'e'
        27: 1,  # 'f'
        12: 2,  # 'g'
        20: 1,  # 'h'
        9: 0,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 1,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 2,  # 's'
        3: 2,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    53: {  # 'J'
        28: 2,  # 'A'
        40: 0,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 1,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 1,  # 'o'
        23: 0,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 2,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 2,  # ''
        24: 2,  # ''
        31: 1,  # ''
        29: 0,  # ''
        42: 1,  # ''
        56: 0,  # ''
    },
    36: {  # 'K'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 0,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 1,  # 'f'
        12: 0,  # 'g'
        20: 1,  # 'h'
        9: 3,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 2,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 2,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 2,  # ''
        31: 1,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 0,  # ''
    },
    41: {  # 'L'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 2,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 3,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 2,  # 'i'
        22: 1,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 2,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 0,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    34: {  # 'M'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 0,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 3,  # 'a'
        18: 0,  # 'b'
        26: 1,  # 'c'
        17: 0,  # 'd'
        1: 3,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 3,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 3,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 2,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 1,  # ''
    },
    35: {  # 'N'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 2,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 2,  # 'Y'
        52: 1,  # 'Z'
        2: 3,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 3,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 2,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 2,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 1,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 1,  # ''
        56: 0,  # ''
    },
    47: {  # 'O'
        28: 1,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 2,  # 'K'
        41: 2,  # 'L'
        34: 2,  # 'M'
        35: 2,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 2,  # 'R'
        33: 2,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 1,  # 'j'
        7: 2,  # 'k'
        6: 2,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 1,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 1,  # 's'
        3: 2,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 1,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 1,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    46: {  # 'P'
        28: 1,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 0,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 2,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 1,  # 'f'
        12: 0,  # 'g'
        20: 1,  # 'h'
        9: 2,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 2,  # 'r'
        5: 1,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 0,  # ''
        63: 1,  # ''
        14: 3,  # ''
        15: 2,  # ''
        30: 0,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 0,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 0,  # ''
    },
    43: {  # 'R'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 2,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 1,  # 'h'
        9: 2,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 2,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 2,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    33: {  # 'S'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 2,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 3,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 1,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 1,  # 'h'
        9: 2,  # 'i'
        22: 0,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 1,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 1,  # 't'
        21: 1,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 3,  # 'z'
        51: 2,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    37: {  # 'T'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 1,  # 'P'
        43: 2,  # 'R'
        33: 1,  # 'S'
        37: 2,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 2,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 1,  # 'h'
        9: 2,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 0,  # 't'
        21: 2,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 1,  # 'z'
        51: 2,  # ''
        44: 2,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 2,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    57: {  # 'U'
        28: 1,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 2,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 1,  # 'e'
        27: 0,  # 'f'
        12: 2,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 1,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    48: {  # 'V'
        28: 2,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 0,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 2,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 2,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 2,  # 'o'
        23: 0,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 2,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 0,  # ''
        63: 1,  # ''
        14: 2,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 0,  # ''
        24: 1,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    55: {  # 'Y'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 1,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 2,  # 'Z'
        2: 1,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 1,  # 'd'
        1: 1,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        8: 1,  # 'o'
        23: 1,  # 'p'
        10: 0,  # 'r'
        5: 0,  # 's'
        3: 0,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 1,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    52: {  # 'Z'
        28: 2,  # 'A'
        40: 1,  # 'B'
        54: 0,  # 'C'
        45: 1,  # 'D'
        32: 2,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 2,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 2,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 2,  # 'S'
        37: 1,  # 'T'
        57: 1,  # 'U'
        48: 1,  # 'V'
        55: 1,  # 'Y'
        52: 1,  # 'Z'
        2: 1,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 1,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 1,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        8: 1,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 2,  # 's'
        3: 0,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 2,  # ''
        44: 1,  # ''
        61: 1,  # ''
        58: 1,  # ''
        59: 1,  # ''
        60: 1,  # ''
        63: 1,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    2: {  # 'a'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 2,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 2,  # 'o'
        23: 3,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 1,  # 'x'
        16: 2,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    18: {  # 'b'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 2,  # 'k'
        6: 2,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 2,  # 's'
        3: 1,  # 't'
        21: 3,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 3,  # ''
        24: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        42: 2,  # ''
        56: 1,  # ''
    },
    26: {  # 'c'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 1,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 1,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 1,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 1,  # 'j'
        7: 2,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 3,  # 's'
        3: 2,  # 't'
        21: 2,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 2,  # ''
        15: 2,  # ''
        30: 2,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    17: {  # 'd'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 2,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 2,  # 'k'
        6: 1,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 2,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 3,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 2,  # ''
        29: 2,  # ''
        42: 2,  # ''
        56: 1,  # ''
    },
    1: {  # 'e'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 2,  # 'e'
        27: 3,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 2,  # 'o'
        23: 3,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 2,  # 'u'
        19: 3,  # 'v'
        62: 2,  # 'x'
        16: 2,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    27: {  # 'f'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 3,  # 'o'
        23: 0,  # 'p'
        10: 3,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 2,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 0,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 3,  # ''
        31: 1,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    12: {  # 'g'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 2,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 2,  # 'k'
        6: 3,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 3,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 3,  # ''
        24: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        42: 2,  # ''
        56: 1,  # ''
    },
    20: {  # 'h'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 0,  # 'd'
        1: 3,  # 'e'
        27: 0,  # 'f'
        12: 1,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 3,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 2,  # 's'
        3: 1,  # 't'
        21: 3,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 2,  # 'y'
        11: 0,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 3,  # ''
        25: 2,  # ''
        24: 2,  # ''
        31: 2,  # ''
        29: 1,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    9: {  # 'i'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 3,  # 'e'
        27: 3,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 2,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 2,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 1,  # 'x'
        16: 1,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 3,  # ''
        24: 1,  # ''
        31: 2,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 1,  # ''
    },
    22: {  # 'j'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 2,  # 'b'
        26: 1,  # 'c'
        17: 3,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 2,  # 'h'
        9: 1,  # 'i'
        22: 2,  # 'j'
        7: 2,  # 'k'
        6: 2,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 2,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 1,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 3,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    7: {  # 'k'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 1,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 2,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 3,  # ''
        25: 2,  # ''
        24: 3,  # ''
        31: 1,  # ''
        29: 3,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    6: {  # 'l'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 1,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 1,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 2,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 3,  # 'e'
        27: 3,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 2,  # 'p'
        10: 2,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 3,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 3,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 2,  # ''
        29: 2,  # ''
        42: 3,  # ''
        56: 1,  # ''
    },
    13: {  # 'm'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 1,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        8: 3,  # 'o'
        23: 3,  # 'p'
        10: 2,  # 'r'
        5: 2,  # 's'
        3: 2,  # 't'
        21: 3,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 2,  # ''
        24: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 2,  # ''
    },
    4: {  # 'n'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 2,  # 'p'
        10: 2,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 2,  # 'v'
        62: 1,  # 'x'
        16: 3,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 2,  # ''
        24: 3,  # ''
        31: 2,  # ''
        29: 3,  # ''
        42: 2,  # ''
        56: 1,  # ''
    },
    8: {  # 'o'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 1,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 2,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 2,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 1,  # 'o'
        23: 3,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 2,  # 'u'
        19: 3,  # 'v'
        62: 1,  # 'x'
        16: 1,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    23: {  # 'p'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 1,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 2,  # 'k'
        6: 3,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        8: 3,  # 'o'
        23: 3,  # 'p'
        10: 3,  # 'r'
        5: 2,  # 's'
        3: 2,  # 't'
        21: 3,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 2,  # ''
        24: 2,  # ''
        31: 1,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    10: {  # 'r'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 1,  # 'x'
        16: 2,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 3,  # ''
        29: 3,  # ''
        42: 2,  # ''
        56: 2,  # ''
    },
    5: {  # 's'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 2,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 2,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 1,  # 'j'
        7: 3,  # 'k'
        6: 2,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 3,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 3,  # ''
        29: 3,  # ''
        42: 2,  # ''
        56: 1,  # ''
    },
    3: {  # 't'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 1,  # 'g'
        20: 3,  # 'h'
        9: 3,  # 'i'
        22: 3,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 3,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 3,  # ''
        29: 3,  # ''
        42: 3,  # ''
        56: 2,  # ''
    },
    21: {  # 'u'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 2,  # 'b'
        26: 2,  # 'c'
        17: 3,  # 'd'
        1: 2,  # 'e'
        27: 1,  # 'f'
        12: 3,  # 'g'
        20: 2,  # 'h'
        9: 2,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 1,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 1,  # 'u'
        19: 3,  # 'v'
        62: 1,  # 'x'
        16: 1,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 2,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 0,  # ''
        31: 1,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    19: {  # 'v'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 2,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 3,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 1,  # 'r'
        5: 2,  # 's'
        3: 2,  # 't'
        21: 2,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 2,  # ''
        24: 2,  # ''
        31: 1,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 1,  # ''
    },
    62: {  # 'x'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 0,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 1,  # 'i'
        22: 0,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 1,  # 'o'
        23: 1,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 1,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 1,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    16: {  # 'y'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 2,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 3,  # 'e'
        27: 2,  # 'f'
        12: 2,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 2,  # 'j'
        7: 2,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 2,  # 'p'
        10: 2,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 2,  # ''
        25: 2,  # ''
        24: 3,  # ''
        31: 2,  # ''
        29: 2,  # ''
        42: 1,  # ''
        56: 2,  # ''
    },
    11: {  # 'z'
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 3,  # 'a'
        18: 2,  # 'b'
        26: 1,  # 'c'
        17: 3,  # 'd'
        1: 3,  # 'e'
        27: 1,  # 'f'
        12: 2,  # 'g'
        20: 2,  # 'h'
        9: 3,  # 'i'
        22: 1,  # 'j'
        7: 3,  # 'k'
        6: 2,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 3,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 3,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 3,  # ''
        15: 3,  # ''
        30: 3,  # ''
        25: 3,  # ''
        24: 3,  # ''
        31: 2,  # ''
        29: 3,  # ''
        42: 2,  # ''
        56: 1,  # ''
    },
    51: {  # ''
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 0,  # 'E'
        50: 1,  # 'F'
        49: 2,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 2,  # 'L'
        34: 1,  # 'M'
        35: 2,  # 'N'
        47: 0,  # 'O'
        46: 1,  # 'P'
        43: 2,  # 'R'
        33: 2,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 0,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 1,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    44: {  # ''
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 1,  # 'E'
        50: 0,  # 'F'
        49: 2,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 2,  # 'L'
        34: 1,  # 'M'
        35: 2,  # 'N'
        47: 0,  # 'O'
        46: 1,  # 'P'
        43: 2,  # 'R'
        33: 2,  # 'S'
        37: 2,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 0,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 2,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 3,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 0,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    61: {  # ''
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 0,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 1,  # 'J'
        36: 0,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 0,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 2,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 1,  # 'm'
        4: 0,  # 'n'
        8: 0,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 0,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    58: {  # ''
        28: 1,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 0,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 1,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 2,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 0,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 2,  # 'h'
        9: 0,  # 'i'
        22: 0,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 0,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 1,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    59: {  # ''
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 0,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 0,  # 'O'
        46: 1,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 0,  # 'b'
        26: 1,  # 'c'
        17: 1,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 0,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 0,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        8: 0,  # 'o'
        23: 0,  # 'p'
        10: 2,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    60: {  # ''
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 1,  # 'C'
        45: 1,  # 'D'
        32: 0,  # 'E'
        50: 1,  # 'F'
        49: 1,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 0,  # 'b'
        26: 0,  # 'c'
        17: 0,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 2,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 2,  # 'j'
        7: 0,  # 'k'
        6: 0,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        8: 0,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 0,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 0,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    63: {  # ''
        28: 0,  # 'A'
        40: 1,  # 'B'
        54: 0,  # 'C'
        45: 1,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 1,  # 'G'
        38: 1,  # 'H'
        39: 0,  # 'I'
        53: 1,  # 'J'
        36: 1,  # 'K'
        41: 1,  # 'L'
        34: 1,  # 'M'
        35: 1,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 1,  # 'R'
        33: 1,  # 'S'
        37: 1,  # 'T'
        57: 0,  # 'U'
        48: 1,  # 'V'
        55: 0,  # 'Y'
        52: 1,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 0,  # 'c'
        17: 1,  # 'd'
        1: 0,  # 'e'
        27: 0,  # 'f'
        12: 1,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 0,  # 'j'
        7: 0,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        8: 0,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 1,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    14: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 3,  # 'b'
        26: 3,  # 'c'
        17: 3,  # 'd'
        1: 1,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 2,  # 'h'
        9: 2,  # 'i'
        22: 3,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 1,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 2,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 1,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 2,  # ''
        30: 1,  # ''
        25: 0,  # ''
        24: 1,  # ''
        31: 0,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    15: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 3,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 3,  # 'g'
        20: 3,  # 'h'
        9: 2,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 1,  # 'o'
        23: 3,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 0,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    30: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 0,  # 'a'
        18: 1,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 0,  # 'e'
        27: 1,  # 'f'
        12: 3,  # 'g'
        20: 0,  # 'h'
        9: 0,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 2,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 2,  # 's'
        3: 3,  # 't'
        21: 0,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    25: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 2,  # 'a'
        18: 3,  # 'b'
        26: 2,  # 'c'
        17: 3,  # 'd'
        1: 1,  # 'e'
        27: 2,  # 'f'
        12: 2,  # 'g'
        20: 2,  # 'h'
        9: 2,  # 'i'
        22: 2,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        8: 1,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 1,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 0,  # ''
        24: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    24: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 0,  # 'a'
        18: 3,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 0,  # 'e'
        27: 1,  # 'f'
        12: 2,  # 'g'
        20: 1,  # 'h'
        9: 0,  # 'i'
        22: 1,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        8: 0,  # 'o'
        23: 2,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 3,  # 't'
        21: 0,  # 'u'
        19: 3,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 3,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    31: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 1,  # 'b'
        26: 2,  # 'c'
        17: 1,  # 'd'
        1: 1,  # 'e'
        27: 2,  # 'f'
        12: 3,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 3,  # 'j'
        7: 1,  # 'k'
        6: 3,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 3,  # 'r'
        5: 3,  # 's'
        3: 2,  # 't'
        21: 1,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 1,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    29: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 1,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 3,  # 'g'
        20: 2,  # 'h'
        9: 1,  # 'i'
        22: 1,  # 'j'
        7: 3,  # 'k'
        6: 3,  # 'l'
        13: 1,  # 'm'
        4: 3,  # 'n'
        8: 0,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 2,  # 's'
        3: 2,  # 't'
        21: 0,  # 'u'
        19: 2,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 1,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    42: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 2,  # 'b'
        26: 1,  # 'c'
        17: 2,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 1,  # 'j'
        7: 2,  # 'k'
        6: 3,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        8: 1,  # 'o'
        23: 1,  # 'p'
        10: 2,  # 'r'
        5: 2,  # 's'
        3: 2,  # 't'
        21: 1,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 1,  # ''
        30: 1,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 1,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
    56: {  # ''
        28: 0,  # 'A'
        40: 0,  # 'B'
        54: 0,  # 'C'
        45: 0,  # 'D'
        32: 0,  # 'E'
        50: 0,  # 'F'
        49: 0,  # 'G'
        38: 0,  # 'H'
        39: 0,  # 'I'
        53: 0,  # 'J'
        36: 0,  # 'K'
        41: 0,  # 'L'
        34: 0,  # 'M'
        35: 0,  # 'N'
        47: 0,  # 'O'
        46: 0,  # 'P'
        43: 0,  # 'R'
        33: 0,  # 'S'
        37: 0,  # 'T'
        57: 0,  # 'U'
        48: 0,  # 'V'
        55: 0,  # 'Y'
        52: 0,  # 'Z'
        2: 1,  # 'a'
        18: 1,  # 'b'
        26: 0,  # 'c'
        17: 1,  # 'd'
        1: 1,  # 'e'
        27: 1,  # 'f'
        12: 1,  # 'g'
        20: 1,  # 'h'
        9: 1,  # 'i'
        22: 1,  # 'j'
        7: 1,  # 'k'
        6: 1,  # 'l'
        13: 0,  # 'm'
        4: 2,  # 'n'
        8: 0,  # 'o'
        23: 0,  # 'p'
        10: 1,  # 'r'
        5: 1,  # 's'
        3: 1,  # 't'
        21: 0,  # 'u'
        19: 1,  # 'v'
        62: 0,  # 'x'
        16: 0,  # 'y'
        11: 2,  # 'z'
        51: 0,  # ''
        44: 0,  # ''
        61: 0,  # ''
        58: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
        63: 0,  # ''
        14: 0,  # ''
        15: 0,  # ''
        30: 0,  # ''
        25: 0,  # ''
        24: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        42: 0,  # ''
        56: 0,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
WINDOWS_1250_HUNGARIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 28,  # 'A'
    66: 40,  # 'B'
    67: 54,  # 'C'
    68: 45,  # 'D'
    69: 32,  # 'E'
    70: 50,  # 'F'
    71: 49,  # 'G'
    72: 38,  # 'H'
    73: 39,  # 'I'
    74: 53,  # 'J'
    75: 36,  # 'K'
    76: 41,  # 'L'
    77: 34,  # 'M'
    78: 35,  # 'N'
    79: 47,  # 'O'
    80: 46,  # 'P'
    81: 72,  # 'Q'
    82: 43,  # 'R'
    83: 33,  # 'S'
    84: 37,  # 'T'
    85: 57,  # 'U'
    86: 48,  # 'V'
    87: 64,  # 'W'
    88: 68,  # 'X'
    89: 55,  # 'Y'
    90: 52,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 2,  # 'a'
    98: 18,  # 'b'
    99: 26,  # 'c'
    100: 17,  # 'd'
    101: 1,  # 'e'
    102: 27,  # 'f'
    103: 12,  # 'g'
    104: 20,  # 'h'
    105: 9,  # 'i'
    106: 22,  # 'j'
    107: 7,  # 'k'
    108: 6,  # 'l'
    109: 13,  # 'm'
    110: 4,  # 'n'
    111: 8,  # 'o'
    112: 23,  # 'p'
    113: 67,  # 'q'
    114: 10,  # 'r'
    115: 5,  # 's'
    116: 3,  # 't'
    117: 21,  # 'u'
    118: 19,  # 'v'
    119: 65,  # 'w'
    120: 62,  # 'x'
    121: 16,  # 'y'
    122: 11,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 161,  # ''
    129: 162,  # None
    130: 163,  # ''
    131: 164,  # None
    132: 165,  # ''
    133: 166,  # ''
    134: 167,  # ''
    135: 168,  # ''
    136: 169,  # None
    137: 170,  # ''
    138: 171,  # ''
    139: 172,  # ''
    140: 173,  # ''
    141: 174,  # ''
    142: 175,  # ''
    143: 176,  # ''
    144: 177,  # None
    145: 178,  # ''
    146: 179,  # ''
    147: 180,  # ''
    148: 78,  # ''
    149: 181,  # ''
    150: 69,  # ''
    151: 182,  # ''
    152: 183,  # None
    153: 184,  # ''
    154: 185,  # ''
    155: 186,  # ''
    156: 187,  # ''
    157: 188,  # ''
    158: 189,  # ''
    159: 190,  # ''
    160: 191,  # '\xa0'
    161: 192,  # ''
    162: 193,  # ''
    163: 194,  # ''
    164: 195,  # ''
    165: 196,  # ''
    166: 197,  # ''
    167: 76,  # ''
    168: 198,  # ''
    169: 199,  # ''
    170: 200,  # ''
    171: 201,  # ''
    172: 202,  # ''
    173: 203,  # '\xad'
    174: 204,  # ''
    175: 205,  # ''
    176: 81,  # ''
    177: 206,  # ''
    178: 207,  # ''
    179: 208,  # ''
    180: 209,  # ''
    181: 210,  # ''
    182: 211,  # ''
    183: 212,  # ''
    184: 213,  # ''
    185: 214,  # ''
    186: 215,  # ''
    187: 216,  # ''
    188: 217,  # ''
    189: 218,  # ''
    190: 219,  # ''
    191: 220,  # ''
    192: 221,  # ''
    193: 51,  # ''
    194: 83,  # ''
    195: 222,  # ''
    196: 80,  # ''
    197: 223,  # ''
    198: 224,  # ''
    199: 225,  # ''
    200: 226,  # ''
    201: 44,  # ''
    202: 227,  # ''
    203: 228,  # ''
    204: 229,  # ''
    205: 61,  # ''
    206: 230,  # ''
    207: 231,  # ''
    208: 232,  # ''
    209: 233,  # ''
    210: 234,  # ''
    211: 58,  # ''
    212: 235,  # ''
    213: 66,  # ''
    214: 59,  # ''
    215: 236,  # ''
    216: 237,  # ''
    217: 238,  # ''
    218: 60,  # ''
    219: 70,  # ''
    220: 63,  # ''
    221: 239,  # ''
    222: 240,  # ''
    223: 241,  # ''
    224: 84,  # ''
    225: 14,  # ''
    226: 75,  # ''
    227: 242,  # ''
    228: 71,  # ''
    229: 82,  # ''
    230: 243,  # ''
    231: 73,  # ''
    232: 244,  # ''
    233: 15,  # ''
    234: 85,  # ''
    235: 79,  # ''
    236: 86,  # ''
    237: 30,  # ''
    238: 77,  # ''
    239: 87,  # ''
    240: 245,  # ''
    241: 246,  # ''
    242: 247,  # ''
    243: 25,  # ''
    244: 74,  # ''
    245: 42,  # ''
    246: 24,  # ''
    247: 248,  # ''
    248: 249,  # ''
    249: 250,  # ''
    250: 31,  # ''
    251: 56,  # ''
    252: 29,  # ''
    253: 251,  # ''
    254: 252,  # ''
    255: 253,  # ''
}

WINDOWS_1250_HUNGARIAN_MODEL = SingleByteCharSetModel(
    charset_name="windows-1250",
    language="Hungarian",
    char_to_order_map=WINDOWS_1250_HUNGARIAN_CHAR_TO_ORDER,
    language_model=HUNGARIAN_LANG_MODEL,
    typical_positive_ratio=0.947368,
    keep_ascii_letters=True,
    alphabet="ABCDEFGHIJKLMNOPRSTUVZabcdefghijklmnoprstuvz",
)

ISO_8859_2_HUNGARIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 28,  # 'A'
    66: 40,  # 'B'
    67: 54,  # 'C'
    68: 45,  # 'D'
    69: 32,  # 'E'
    70: 50,  # 'F'
    71: 49,  # 'G'
    72: 38,  # 'H'
    73: 39,  # 'I'
    74: 53,  # 'J'
    75: 36,  # 'K'
    76: 41,  # 'L'
    77: 34,  # 'M'
    78: 35,  # 'N'
    79: 47,  # 'O'
    80: 46,  # 'P'
    81: 71,  # 'Q'
    82: 43,  # 'R'
    83: 33,  # 'S'
    84: 37,  # 'T'
    85: 57,  # 'U'
    86: 48,  # 'V'
    87: 64,  # 'W'
    88: 68,  # 'X'
    89: 55,  # 'Y'
    90: 52,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 2,  # 'a'
    98: 18,  # 'b'
    99: 26,  # 'c'
    100: 17,  # 'd'
    101: 1,  # 'e'
    102: 27,  # 'f'
    103: 12,  # 'g'
    104: 20,  # 'h'
    105: 9,  # 'i'
    106: 22,  # 'j'
    107: 7,  # 'k'
    108: 6,  # 'l'
    109: 13,  # 'm'
    110: 4,  # 'n'
    111: 8,  # 'o'
    112: 23,  # 'p'
    113: 67,  # 'q'
    114: 10,  # 'r'
    115: 5,  # 's'
    116: 3,  # 't'
    117: 21,  # 'u'
    118: 19,  # 'v'
    119: 65,  # 'w'
    120: 62,  # 'x'
    121: 16,  # 'y'
    122: 11,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 159,  # '\x80'
    129: 160,  # '\x81'
    130: 161,  # '\x82'
    131: 162,  # '\x83'
    132: 163,  # '\x84'
    133: 164,  # '\x85'
    134: 165,  # '\x86'
    135: 166,  # '\x87'
    136: 167,  # '\x88'
    137: 168,  # '\x89'
    138: 169,  # '\x8a'
    139: 170,  # '\x8b'
    140: 171,  # '\x8c'
    141: 172,  # '\x8d'
    142: 173,  # '\x8e'
    143: 174,  # '\x8f'
    144: 175,  # '\x90'
    145: 176,  # '\x91'
    146: 177,  # '\x92'
    147: 178,  # '\x93'
    148: 179,  # '\x94'
    149: 180,  # '\x95'
    150: 181,  # '\x96'
    151: 182,  # '\x97'
    152: 183,  # '\x98'
    153: 184,  # '\x99'
    154: 185,  # '\x9a'
    155: 186,  # '\x9b'
    156: 187,  # '\x9c'
    157: 188,  # '\x9d'
    158: 189,  # '\x9e'
    159: 190,  # '\x9f'
    160: 191,  # '\xa0'
    161: 192,  # ''
    162: 193,  # ''
    163: 194,  # ''
    164: 195,  # ''
    165: 196,  # ''
    166: 197,  # ''
    167: 75,  # ''
    168: 198,  # ''
    169: 199,  # ''
    170: 200,  # ''
    171: 201,  # ''
    172: 202,  # ''
    173: 203,  # '\xad'
    174: 204,  # ''
    175: 205,  # ''
    176: 79,  # ''
    177: 206,  # ''
    178: 207,  # ''
    179: 208,  # ''
    180: 209,  # ''
    181: 210,  # ''
    182: 211,  # ''
    183: 212,  # ''
    184: 213,  # ''
    185: 214,  # ''
    186: 215,  # ''
    187: 216,  # ''
    188: 217,  # ''
    189: 218,  # ''
    190: 219,  # ''
    191: 220,  # ''
    192: 221,  # ''
    193: 51,  # ''
    194: 81,  # ''
    195: 222,  # ''
    196: 78,  # ''
    197: 223,  # ''
    198: 224,  # ''
    199: 225,  # ''
    200: 226,  # ''
    201: 44,  # ''
    202: 227,  # ''
    203: 228,  # ''
    204: 229,  # ''
    205: 61,  # ''
    206: 230,  # ''
    207: 231,  # ''
    208: 232,  # ''
    209: 233,  # ''
    210: 234,  # ''
    211: 58,  # ''
    212: 235,  # ''
    213: 66,  # ''
    214: 59,  # ''
    215: 236,  # ''
    216: 237,  # ''
    217: 238,  # ''
    218: 60,  # ''
    219: 69,  # ''
    220: 63,  # ''
    221: 239,  # ''
    222: 240,  # ''
    223: 241,  # ''
    224: 82,  # ''
    225: 14,  # ''
    226: 74,  # ''
    227: 242,  # ''
    228: 70,  # ''
    229: 80,  # ''
    230: 243,  # ''
    231: 72,  # ''
    232: 244,  # ''
    233: 15,  # ''
    234: 83,  # ''
    235: 77,  # ''
    236: 84,  # ''
    237: 30,  # ''
    238: 76,  # ''
    239: 85,  # ''
    240: 245,  # ''
    241: 246,  # ''
    242: 247,  # ''
    243: 25,  # ''
    244: 73,  # ''
    245: 42,  # ''
    246: 24,  # ''
    247: 248,  # ''
    248: 249,  # ''
    249: 250,  # ''
    250: 31,  # ''
    251: 56,  # ''
    252: 29,  # ''
    253: 251,  # ''
    254: 252,  # ''
    255: 253,  # ''
}

ISO_8859_2_HUNGARIAN_MODEL = SingleByteCharSetModel(
    charset_name="ISO-8859-2",
    language="Hungarian",
    char_to_order_map=ISO_8859_2_HUNGARIAN_CHAR_TO_ORDER,
    language_model=HUNGARIAN_LANG_MODEL,
    typical_positive_ratio=0.947368,
    keep_ascii_letters=True,
    alphabet="ABCDEFGHIJKLMNOPRSTUVZabcdefghijklmnoprstuvz",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/version.py
# ========================================================
"""
This module exists only to simplify retrieving the version number of chardet
from within setuptools and from chardet subpackages.

:author: Dan Blanchard (dan.blanchard@gmail.com)
"""

__version__ = "5.1.0"
VERSION = __version__.split(".")


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/euctwprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCTWDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCTW_SM_MODEL


class EUCTWProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)
        self.distribution_analyzer = EUCTWDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return "EUC-TW"

    @property
    def language(self) -> str:
        return "Taiwan"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/__init__.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import List, Union

from .charsetgroupprober import CharSetGroupProber
from .charsetprober import CharSetProber
from .enums import InputState
from .resultdict import ResultDict
from .universaldetector import UniversalDetector
from .version import VERSION, __version__

__all__ = ["UniversalDetector", "detect", "detect_all", "__version__", "VERSION"]


def detect(
    byte_str: Union[bytes, bytearray], should_rename_legacy: bool = False
) -> ResultDict:
    """
    Detect the encoding of the given byte string.

    :param byte_str:     The byte sequence to examine.
    :type byte_str:      ``bytes`` or ``bytearray``
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    :type should_rename_legacy:   ``bool``
    """
    if not isinstance(byte_str, bytearray):
        if not isinstance(byte_str, bytes):
            raise TypeError(
                f"Expected object of type bytes or bytearray, got: {type(byte_str)}"
            )
        byte_str = bytearray(byte_str)
    detector = UniversalDetector(should_rename_legacy=should_rename_legacy)
    detector.feed(byte_str)
    return detector.close()


def detect_all(
    byte_str: Union[bytes, bytearray],
    ignore_threshold: bool = False,
    should_rename_legacy: bool = False,
) -> List[ResultDict]:
    """
    Detect all the possible encodings of the given byte string.

    :param byte_str:          The byte sequence to examine.
    :type byte_str:           ``bytes`` or ``bytearray``
    :param ignore_threshold:  Include encodings that are below
                              ``UniversalDetector.MINIMUM_THRESHOLD``
                              in results.
    :type ignore_threshold:   ``bool``
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    :type should_rename_legacy:   ``bool``
    """
    if not isinstance(byte_str, bytearray):
        if not isinstance(byte_str, bytes):
            raise TypeError(
                f"Expected object of type bytes or bytearray, got: {type(byte_str)}"
            )
        byte_str = bytearray(byte_str)

    detector = UniversalDetector(should_rename_legacy=should_rename_legacy)
    detector.feed(byte_str)
    detector.close()

    if detector.input_state == InputState.HIGH_BYTE:
        results: List[ResultDict] = []
        probers: List[CharSetProber] = []
        for prober in detector.charset_probers:
            if isinstance(prober, CharSetGroupProber):
                probers.extend(p for p in prober.probers)
            else:
                probers.append(prober)
        for prober in probers:
            if ignore_threshold or prober.get_confidence() > detector.MINIMUM_THRESHOLD:
                charset_name = prober.charset_name or ""
                lower_charset_name = charset_name.lower()
                # Use Windows encoding name instead of ISO-8859 if we saw any
                # extra Windows-specific bytes
                if lower_charset_name.startswith("iso-8859") and detector.has_win_bytes:
                    charset_name = detector.ISO_WIN_MAP.get(
                        lower_charset_name, charset_name
                    )
                # Rename legacy encodings with superset encodings if asked
                if should_rename_legacy:
                    charset_name = detector.LEGACY_MAP.get(
                        charset_name.lower(), charset_name
                    )
                results.append(
                    {
                        "encoding": charset_name,
                        "confidence": prober.get_confidence(),
                        "language": prober.language,
                    }
                )
        if len(results) > 0:
            return sorted(results, key=lambda result: -result["confidence"])

    return [detector.result]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/utf1632prober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
#
# Contributor(s):
#   Jason Zavaglia
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################
from typing import List, Union

from .charsetprober import CharSetProber
from .enums import ProbingState


class UTF1632Prober(CharSetProber):
    """
    This class simply looks for occurrences of zero bytes, and infers
    whether the file is UTF16 or UTF32 (low-endian or big-endian)
    For instance, files looking like ( \0 \0 \0 [nonzero] )+
    have a good probability to be UTF32BE.  Files looking like ( \0 [nonzero] )+
    may be guessed to be UTF16BE, and inversely for little-endian varieties.
    """

    # how many logical characters to scan before feeling confident of prediction
    MIN_CHARS_FOR_DETECTION = 20
    # a fixed constant ratio of expected zeros or non-zeros in modulo-position.
    EXPECTED_RATIO = 0.94

    def __init__(self) -> None:
        super().__init__()
        self.position = 0
        self.zeros_at_mod = [0] * 4
        self.nonzeros_at_mod = [0] * 4
        self._state = ProbingState.DETECTING
        self.quad = [0, 0, 0, 0]
        self.invalid_utf16be = False
        self.invalid_utf16le = False
        self.invalid_utf32be = False
        self.invalid_utf32le = False
        self.first_half_surrogate_pair_detected_16be = False
        self.first_half_surrogate_pair_detected_16le = False
        self.reset()

    def reset(self) -> None:
        super().reset()
        self.position = 0
        self.zeros_at_mod = [0] * 4
        self.nonzeros_at_mod = [0] * 4
        self._state = ProbingState.DETECTING
        self.invalid_utf16be = False
        self.invalid_utf16le = False
        self.invalid_utf32be = False
        self.invalid_utf32le = False
        self.first_half_surrogate_pair_detected_16be = False
        self.first_half_surrogate_pair_detected_16le = False
        self.quad = [0, 0, 0, 0]

    @property
    def charset_name(self) -> str:
        if self.is_likely_utf32be():
            return "utf-32be"
        if self.is_likely_utf32le():
            return "utf-32le"
        if self.is_likely_utf16be():
            return "utf-16be"
        if self.is_likely_utf16le():
            return "utf-16le"
        # default to something valid
        return "utf-16"

    @property
    def language(self) -> str:
        return ""

    def approx_32bit_chars(self) -> float:
        return max(1.0, self.position / 4.0)

    def approx_16bit_chars(self) -> float:
        return max(1.0, self.position / 2.0)

    def is_likely_utf32be(self) -> bool:
        approx_chars = self.approx_32bit_chars()
        return approx_chars >= self.MIN_CHARS_FOR_DETECTION and (
            self.zeros_at_mod[0] / approx_chars > self.EXPECTED_RATIO
            and self.zeros_at_mod[1] / approx_chars > self.EXPECTED_RATIO
            and self.zeros_at_mod[2] / approx_chars > self.EXPECTED_RATIO
            and self.nonzeros_at_mod[3] / approx_chars > self.EXPECTED_RATIO
            and not self.invalid_utf32be
        )

    def is_likely_utf32le(self) -> bool:
        approx_chars = self.approx_32bit_chars()
        return approx_chars >= self.MIN_CHARS_FOR_DETECTION and (
            self.nonzeros_at_mod[0] / approx_chars > self.EXPECTED_RATIO
            and self.zeros_at_mod[1] / approx_chars > self.EXPECTED_RATIO
            and self.zeros_at_mod[2] / approx_chars > self.EXPECTED_RATIO
            and self.zeros_at_mod[3] / approx_chars > self.EXPECTED_RATIO
            and not self.invalid_utf32le
        )

    def is_likely_utf16be(self) -> bool:
        approx_chars = self.approx_16bit_chars()
        return approx_chars >= self.MIN_CHARS_FOR_DETECTION and (
            (self.nonzeros_at_mod[1] + self.nonzeros_at_mod[3]) / approx_chars
            > self.EXPECTED_RATIO
            and (self.zeros_at_mod[0] + self.zeros_at_mod[2]) / approx_chars
            > self.EXPECTED_RATIO
            and not self.invalid_utf16be
        )

    def is_likely_utf16le(self) -> bool:
        approx_chars = self.approx_16bit_chars()
        return approx_chars >= self.MIN_CHARS_FOR_DETECTION and (
            (self.nonzeros_at_mod[0] + self.nonzeros_at_mod[2]) / approx_chars
            > self.EXPECTED_RATIO
            and (self.zeros_at_mod[1] + self.zeros_at_mod[3]) / approx_chars
            > self.EXPECTED_RATIO
            and not self.invalid_utf16le
        )

    def validate_utf32_characters(self, quad: List[int]) -> None:
        """
        Validate if the quad of bytes is valid UTF-32.

        UTF-32 is valid in the range 0x00000000 - 0x0010FFFF
        excluding 0x0000D800 - 0x0000DFFF

        https://en.wikipedia.org/wiki/UTF-32
        """
        if (
            quad[0] != 0
            or quad[1] > 0x10
            or (quad[0] == 0 and quad[1] == 0 and 0xD8 <= quad[2] <= 0xDF)
        ):
            self.invalid_utf32be = True
        if (
            quad[3] != 0
            or quad[2] > 0x10
            or (quad[3] == 0 and quad[2] == 0 and 0xD8 <= quad[1] <= 0xDF)
        ):
            self.invalid_utf32le = True

    def validate_utf16_characters(self, pair: List[int]) -> None:
        """
        Validate if the pair of bytes is  valid UTF-16.

        UTF-16 is valid in the range 0x0000 - 0xFFFF excluding 0xD800 - 0xFFFF
        with an exception for surrogate pairs, which must be in the range
        0xD800-0xDBFF followed by 0xDC00-0xDFFF

        https://en.wikipedia.org/wiki/UTF-16
        """
        if not self.first_half_surrogate_pair_detected_16be:
            if 0xD8 <= pair[0] <= 0xDB:
                self.first_half_surrogate_pair_detected_16be = True
            elif 0xDC <= pair[0] <= 0xDF:
                self.invalid_utf16be = True
        else:
            if 0xDC <= pair[0] <= 0xDF:
                self.first_half_surrogate_pair_detected_16be = False
            else:
                self.invalid_utf16be = True

        if not self.first_half_surrogate_pair_detected_16le:
            if 0xD8 <= pair[1] <= 0xDB:
                self.first_half_surrogate_pair_detected_16le = True
            elif 0xDC <= pair[1] <= 0xDF:
                self.invalid_utf16le = True
        else:
            if 0xDC <= pair[1] <= 0xDF:
                self.first_half_surrogate_pair_detected_16le = False
            else:
                self.invalid_utf16le = True

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        for c in byte_str:
            mod4 = self.position % 4
            self.quad[mod4] = c
            if mod4 == 3:
                self.validate_utf32_characters(self.quad)
                self.validate_utf16_characters(self.quad[0:2])
                self.validate_utf16_characters(self.quad[2:4])
            if c == 0:
                self.zeros_at_mod[mod4] += 1
            else:
                self.nonzeros_at_mod[mod4] += 1
            self.position += 1
        return self.state

    @property
    def state(self) -> ProbingState:
        if self._state in {ProbingState.NOT_ME, ProbingState.FOUND_IT}:
            # terminal, decided states
            return self._state
        if self.get_confidence() > 0.80:
            self._state = ProbingState.FOUND_IT
        elif self.position > 4 * 1024:
            # if we get to 4kb into the file, and we can't conclude it's UTF,
            # let's give up
            self._state = ProbingState.NOT_ME
        return self._state

    def get_confidence(self) -> float:
        return (
            0.85
            if (
                self.is_likely_utf16le()
                or self.is_likely_utf16be()
                or self.is_likely_utf32le()
                or self.is_likely_utf32be()
            )
            else 0.00
        )


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/mbcssm.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .codingstatemachinedict import CodingStateMachineDict
from .enums import MachineState

# BIG5

# fmt: off
BIG5_CLS = (
    1, 1, 1, 1, 1, 1, 1, 1,  # 00 - 07    #allow 0x00 as legal value
    1, 1, 1, 1, 1, 1, 0, 0,  # 08 - 0f
    1, 1, 1, 1, 1, 1, 1, 1,  # 10 - 17
    1, 1, 1, 0, 1, 1, 1, 1,  # 18 - 1f
    1, 1, 1, 1, 1, 1, 1, 1,  # 20 - 27
    1, 1, 1, 1, 1, 1, 1, 1,  # 28 - 2f
    1, 1, 1, 1, 1, 1, 1, 1,  # 30 - 37
    1, 1, 1, 1, 1, 1, 1, 1,  # 38 - 3f
    2, 2, 2, 2, 2, 2, 2, 2,  # 40 - 47
    2, 2, 2, 2, 2, 2, 2, 2,  # 48 - 4f
    2, 2, 2, 2, 2, 2, 2, 2,  # 50 - 57
    2, 2, 2, 2, 2, 2, 2, 2,  # 58 - 5f
    2, 2, 2, 2, 2, 2, 2, 2,  # 60 - 67
    2, 2, 2, 2, 2, 2, 2, 2,  # 68 - 6f
    2, 2, 2, 2, 2, 2, 2, 2,  # 70 - 77
    2, 2, 2, 2, 2, 2, 2, 1,  # 78 - 7f
    4, 4, 4, 4, 4, 4, 4, 4,  # 80 - 87
    4, 4, 4, 4, 4, 4, 4, 4,  # 88 - 8f
    4, 4, 4, 4, 4, 4, 4, 4,  # 90 - 97
    4, 4, 4, 4, 4, 4, 4, 4,  # 98 - 9f
    4, 3, 3, 3, 3, 3, 3, 3,  # a0 - a7
    3, 3, 3, 3, 3, 3, 3, 3,  # a8 - af
    3, 3, 3, 3, 3, 3, 3, 3,  # b0 - b7
    3, 3, 3, 3, 3, 3, 3, 3,  # b8 - bf
    3, 3, 3, 3, 3, 3, 3, 3,  # c0 - c7
    3, 3, 3, 3, 3, 3, 3, 3,  # c8 - cf
    3, 3, 3, 3, 3, 3, 3, 3,  # d0 - d7
    3, 3, 3, 3, 3, 3, 3, 3,  # d8 - df
    3, 3, 3, 3, 3, 3, 3, 3,  # e0 - e7
    3, 3, 3, 3, 3, 3, 3, 3,  # e8 - ef
    3, 3, 3, 3, 3, 3, 3, 3,  # f0 - f7
    3, 3, 3, 3, 3, 3, 3, 0  # f8 - ff
)

BIG5_ST = (
    MachineState.ERROR,MachineState.START,MachineState.START,     3,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#00-07
    MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,#08-0f
    MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START#10-17
)
# fmt: on

BIG5_CHAR_LEN_TABLE = (0, 1, 1, 2, 0)

BIG5_SM_MODEL: CodingStateMachineDict = {
    "class_table": BIG5_CLS,
    "class_factor": 5,
    "state_table": BIG5_ST,
    "char_len_table": BIG5_CHAR_LEN_TABLE,
    "name": "Big5",
}

# CP949
# fmt: off
CP949_CLS  = (
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,  # 00 - 0f
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,  # 10 - 1f
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # 20 - 2f
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  # 30 - 3f
    1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,  # 40 - 4f
    4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1,  # 50 - 5f
    1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,  # 60 - 6f
    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 1,  # 70 - 7f
    0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,  # 80 - 8f
    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,  # 90 - 9f
    6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8,  # a0 - af
    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,  # b0 - bf
    7, 7, 7, 7, 7, 7, 9, 2, 2, 3, 2, 2, 2, 2, 2, 2,  # c0 - cf
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # d0 - df
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # e0 - ef
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,  # f0 - ff
)

CP949_ST = (
#cls=    0      1      2      3      4      5      6      7      8      9  # previous state =
    MachineState.ERROR,MachineState.START,     3,MachineState.ERROR,MachineState.START,MachineState.START,     4,     5,MachineState.ERROR,     6, # MachineState.START
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR, # MachineState.ERROR
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME, # MachineState.ITS_ME
    MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START, # 3
    MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START, # 4
    MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START, # 5
    MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START, # 6
)
# fmt: on

CP949_CHAR_LEN_TABLE = (0, 1, 2, 0, 1, 1, 2, 2, 0, 2)

CP949_SM_MODEL: CodingStateMachineDict = {
    "class_table": CP949_CLS,
    "class_factor": 10,
    "state_table": CP949_ST,
    "char_len_table": CP949_CHAR_LEN_TABLE,
    "name": "CP949",
}

# EUC-JP
# fmt: off
EUCJP_CLS = (
    4, 4, 4, 4, 4, 4, 4, 4,  # 00 - 07
    4, 4, 4, 4, 4, 4, 5, 5,  # 08 - 0f
    4, 4, 4, 4, 4, 4, 4, 4,  # 10 - 17
    4, 4, 4, 5, 4, 4, 4, 4,  # 18 - 1f
    4, 4, 4, 4, 4, 4, 4, 4,  # 20 - 27
    4, 4, 4, 4, 4, 4, 4, 4,  # 28 - 2f
    4, 4, 4, 4, 4, 4, 4, 4,  # 30 - 37
    4, 4, 4, 4, 4, 4, 4, 4,  # 38 - 3f
    4, 4, 4, 4, 4, 4, 4, 4,  # 40 - 47
    4, 4, 4, 4, 4, 4, 4, 4,  # 48 - 4f
    4, 4, 4, 4, 4, 4, 4, 4,  # 50 - 57
    4, 4, 4, 4, 4, 4, 4, 4,  # 58 - 5f
    4, 4, 4, 4, 4, 4, 4, 4,  # 60 - 67
    4, 4, 4, 4, 4, 4, 4, 4,  # 68 - 6f
    4, 4, 4, 4, 4, 4, 4, 4,  # 70 - 77
    4, 4, 4, 4, 4, 4, 4, 4,  # 78 - 7f
    5, 5, 5, 5, 5, 5, 5, 5,  # 80 - 87
    5, 5, 5, 5, 5, 5, 1, 3,  # 88 - 8f
    5, 5, 5, 5, 5, 5, 5, 5,  # 90 - 97
    5, 5, 5, 5, 5, 5, 5, 5,  # 98 - 9f
    5, 2, 2, 2, 2, 2, 2, 2,  # a0 - a7
    2, 2, 2, 2, 2, 2, 2, 2,  # a8 - af
    2, 2, 2, 2, 2, 2, 2, 2,  # b0 - b7
    2, 2, 2, 2, 2, 2, 2, 2,  # b8 - bf
    2, 2, 2, 2, 2, 2, 2, 2,  # c0 - c7
    2, 2, 2, 2, 2, 2, 2, 2,  # c8 - cf
    2, 2, 2, 2, 2, 2, 2, 2,  # d0 - d7
    2, 2, 2, 2, 2, 2, 2, 2,  # d8 - df
    0, 0, 0, 0, 0, 0, 0, 0,  # e0 - e7
    0, 0, 0, 0, 0, 0, 0, 0,  # e8 - ef
    0, 0, 0, 0, 0, 0, 0, 0,  # f0 - f7
    0, 0, 0, 0, 0, 0, 0, 5  # f8 - ff
)

EUCJP_ST = (
          3,     4,     3,     5,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#00-07
     MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,#08-0f
     MachineState.ITS_ME,MachineState.ITS_ME,MachineState.START,MachineState.ERROR,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#10-17
     MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,     3,MachineState.ERROR,#18-1f
          3,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START#20-27
)
# fmt: on

EUCJP_CHAR_LEN_TABLE = (2, 2, 2, 3, 1, 0)

EUCJP_SM_MODEL: CodingStateMachineDict = {
    "class_table": EUCJP_CLS,
    "class_factor": 6,
    "state_table": EUCJP_ST,
    "char_len_table": EUCJP_CHAR_LEN_TABLE,
    "name": "EUC-JP",
}

# EUC-KR
# fmt: off
EUCKR_CLS  = (
    1, 1, 1, 1, 1, 1, 1, 1,  # 00 - 07
    1, 1, 1, 1, 1, 1, 0, 0,  # 08 - 0f
    1, 1, 1, 1, 1, 1, 1, 1,  # 10 - 17
    1, 1, 1, 0, 1, 1, 1, 1,  # 18 - 1f
    1, 1, 1, 1, 1, 1, 1, 1,  # 20 - 27
    1, 1, 1, 1, 1, 1, 1, 1,  # 28 - 2f
    1, 1, 1, 1, 1, 1, 1, 1,  # 30 - 37
    1, 1, 1, 1, 1, 1, 1, 1,  # 38 - 3f
    1, 1, 1, 1, 1, 1, 1, 1,  # 40 - 47
    1, 1, 1, 1, 1, 1, 1, 1,  # 48 - 4f
    1, 1, 1, 1, 1, 1, 1, 1,  # 50 - 57
    1, 1, 1, 1, 1, 1, 1, 1,  # 58 - 5f
    1, 1, 1, 1, 1, 1, 1, 1,  # 60 - 67
    1, 1, 1, 1, 1, 1, 1, 1,  # 68 - 6f
    1, 1, 1, 1, 1, 1, 1, 1,  # 70 - 77
    1, 1, 1, 1, 1, 1, 1, 1,  # 78 - 7f
    0, 0, 0, 0, 0, 0, 0, 0,  # 80 - 87
    0, 0, 0, 0, 0, 0, 0, 0,  # 88 - 8f
    0, 0, 0, 0, 0, 0, 0, 0,  # 90 - 97
    0, 0, 0, 0, 0, 0, 0, 0,  # 98 - 9f
    0, 2, 2, 2, 2, 2, 2, 2,  # a0 - a7
    2, 2, 2, 2, 2, 3, 3, 3,  # a8 - af
    2, 2, 2, 2, 2, 2, 2, 2,  # b0 - b7
    2, 2, 2, 2, 2, 2, 2, 2,  # b8 - bf
    2, 2, 2, 2, 2, 2, 2, 2,  # c0 - c7
    2, 3, 2, 2, 2, 2, 2, 2,  # c8 - cf
    2, 2, 2, 2, 2, 2, 2, 2,  # d0 - d7
    2, 2, 2, 2, 2, 2, 2, 2,  # d8 - df
    2, 2, 2, 2, 2, 2, 2, 2,  # e0 - e7
    2, 2, 2, 2, 2, 2, 2, 2,  # e8 - ef
    2, 2, 2, 2, 2, 2, 2, 2,  # f0 - f7
    2, 2, 2, 2, 2, 2, 2, 0   # f8 - ff
)

EUCKR_ST = (
    MachineState.ERROR,MachineState.START,     3,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#00-07
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START #08-0f
)
# fmt: on

EUCKR_CHAR_LEN_TABLE = (0, 1, 2, 0)

EUCKR_SM_MODEL: CodingStateMachineDict = {
    "class_table": EUCKR_CLS,
    "class_factor": 4,
    "state_table": EUCKR_ST,
    "char_len_table": EUCKR_CHAR_LEN_TABLE,
    "name": "EUC-KR",
}

# JOHAB
# fmt: off
JOHAB_CLS = (
    4,4,4,4,4,4,4,4,  # 00 - 07
    4,4,4,4,4,4,0,0,  # 08 - 0f
    4,4,4,4,4,4,4,4,  # 10 - 17
    4,4,4,0,4,4,4,4,  # 18 - 1f
    4,4,4,4,4,4,4,4,  # 20 - 27
    4,4,4,4,4,4,4,4,  # 28 - 2f
    4,3,3,3,3,3,3,3,  # 30 - 37
    3,3,3,3,3,3,3,3,  # 38 - 3f
    3,1,1,1,1,1,1,1,  # 40 - 47
    1,1,1,1,1,1,1,1,  # 48 - 4f
    1,1,1,1,1,1,1,1,  # 50 - 57
    1,1,1,1,1,1,1,1,  # 58 - 5f
    1,1,1,1,1,1,1,1,  # 60 - 67
    1,1,1,1,1,1,1,1,  # 68 - 6f
    1,1,1,1,1,1,1,1,  # 70 - 77
    1,1,1,1,1,1,1,2,  # 78 - 7f
    6,6,6,6,8,8,8,8,  # 80 - 87
    8,8,8,8,8,8,8,8,  # 88 - 8f
    8,7,7,7,7,7,7,7,  # 90 - 97
    7,7,7,7,7,7,7,7,  # 98 - 9f
    7,7,7,7,7,7,7,7,  # a0 - a7
    7,7,7,7,7,7,7,7,  # a8 - af
    7,7,7,7,7,7,7,7,  # b0 - b7
    7,7,7,7,7,7,7,7,  # b8 - bf
    7,7,7,7,7,7,7,7,  # c0 - c7
    7,7,7,7,7,7,7,7,  # c8 - cf
    7,7,7,7,5,5,5,5,  # d0 - d7
    5,9,9,9,9,9,9,5,  # d8 - df
    9,9,9,9,9,9,9,9,  # e0 - e7
    9,9,9,9,9,9,9,9,  # e8 - ef
    9,9,9,9,9,9,9,9,  # f0 - f7
    9,9,5,5,5,5,5,0   # f8 - ff
)

JOHAB_ST = (
# cls = 0                   1                   2                   3                   4                   5                   6                   7                   8                   9
    MachineState.ERROR ,MachineState.START ,MachineState.START ,MachineState.START ,MachineState.START ,MachineState.ERROR ,MachineState.ERROR ,3                  ,3                  ,4                  ,  # MachineState.START
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,  # MachineState.ITS_ME
    MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,MachineState.ERROR ,  # MachineState.ERROR
    MachineState.ERROR ,MachineState.START ,MachineState.START ,MachineState.ERROR ,MachineState.ERROR ,MachineState.START ,MachineState.START ,MachineState.START ,MachineState.START ,MachineState.START ,  # 3
    MachineState.ERROR ,MachineState.START ,MachineState.ERROR ,MachineState.START ,MachineState.ERROR ,MachineState.START ,MachineState.ERROR ,MachineState.START ,MachineState.ERROR ,MachineState.START ,  # 4
)
# fmt: on

JOHAB_CHAR_LEN_TABLE = (0, 1, 1, 1, 1, 0, 0, 2, 2, 2)

JOHAB_SM_MODEL: CodingStateMachineDict = {
    "class_table": JOHAB_CLS,
    "class_factor": 10,
    "state_table": JOHAB_ST,
    "char_len_table": JOHAB_CHAR_LEN_TABLE,
    "name": "Johab",
}

# EUC-TW
# fmt: off
EUCTW_CLS = (
    2, 2, 2, 2, 2, 2, 2, 2,  # 00 - 07
    2, 2, 2, 2, 2, 2, 0, 0,  # 08 - 0f
    2, 2, 2, 2, 2, 2, 2, 2,  # 10 - 17
    2, 2, 2, 0, 2, 2, 2, 2,  # 18 - 1f
    2, 2, 2, 2, 2, 2, 2, 2,  # 20 - 27
    2, 2, 2, 2, 2, 2, 2, 2,  # 28 - 2f
    2, 2, 2, 2, 2, 2, 2, 2,  # 30 - 37
    2, 2, 2, 2, 2, 2, 2, 2,  # 38 - 3f
    2, 2, 2, 2, 2, 2, 2, 2,  # 40 - 47
    2, 2, 2, 2, 2, 2, 2, 2,  # 48 - 4f
    2, 2, 2, 2, 2, 2, 2, 2,  # 50 - 57
    2, 2, 2, 2, 2, 2, 2, 2,  # 58 - 5f
    2, 2, 2, 2, 2, 2, 2, 2,  # 60 - 67
    2, 2, 2, 2, 2, 2, 2, 2,  # 68 - 6f
    2, 2, 2, 2, 2, 2, 2, 2,  # 70 - 77
    2, 2, 2, 2, 2, 2, 2, 2,  # 78 - 7f
    0, 0, 0, 0, 0, 0, 0, 0,  # 80 - 87
    0, 0, 0, 0, 0, 0, 6, 0,  # 88 - 8f
    0, 0, 0, 0, 0, 0, 0, 0,  # 90 - 97
    0, 0, 0, 0, 0, 0, 0, 0,  # 98 - 9f
    0, 3, 4, 4, 4, 4, 4, 4,  # a0 - a7
    5, 5, 1, 1, 1, 1, 1, 1,  # a8 - af
    1, 1, 1, 1, 1, 1, 1, 1,  # b0 - b7
    1, 1, 1, 1, 1, 1, 1, 1,  # b8 - bf
    1, 1, 3, 1, 3, 3, 3, 3,  # c0 - c7
    3, 3, 3, 3, 3, 3, 3, 3,  # c8 - cf
    3, 3, 3, 3, 3, 3, 3, 3,  # d0 - d7
    3, 3, 3, 3, 3, 3, 3, 3,  # d8 - df
    3, 3, 3, 3, 3, 3, 3, 3,  # e0 - e7
    3, 3, 3, 3, 3, 3, 3, 3,  # e8 - ef
    3, 3, 3, 3, 3, 3, 3, 3,  # f0 - f7
    3, 3, 3, 3, 3, 3, 3, 0   # f8 - ff
)

EUCTW_ST = (
    MachineState.ERROR,MachineState.ERROR,MachineState.START,     3,     3,     3,     4,MachineState.ERROR,#00-07
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,#08-0f
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.START,MachineState.ERROR,#10-17
    MachineState.START,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#18-1f
         5,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.ERROR,MachineState.START,MachineState.START,#20-27
    MachineState.START,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START #28-2f
)
# fmt: on

EUCTW_CHAR_LEN_TABLE = (0, 0, 1, 2, 2, 2, 3)

EUCTW_SM_MODEL: CodingStateMachineDict = {
    "class_table": EUCTW_CLS,
    "class_factor": 7,
    "state_table": EUCTW_ST,
    "char_len_table": EUCTW_CHAR_LEN_TABLE,
    "name": "x-euc-tw",
}

# GB2312
# fmt: off
GB2312_CLS = (
    1, 1, 1, 1, 1, 1, 1, 1,  # 00 - 07
    1, 1, 1, 1, 1, 1, 0, 0,  # 08 - 0f
    1, 1, 1, 1, 1, 1, 1, 1,  # 10 - 17
    1, 1, 1, 0, 1, 1, 1, 1,  # 18 - 1f
    1, 1, 1, 1, 1, 1, 1, 1,  # 20 - 27
    1, 1, 1, 1, 1, 1, 1, 1,  # 28 - 2f
    3, 3, 3, 3, 3, 3, 3, 3,  # 30 - 37
    3, 3, 1, 1, 1, 1, 1, 1,  # 38 - 3f
    2, 2, 2, 2, 2, 2, 2, 2,  # 40 - 47
    2, 2, 2, 2, 2, 2, 2, 2,  # 48 - 4f
    2, 2, 2, 2, 2, 2, 2, 2,  # 50 - 57
    2, 2, 2, 2, 2, 2, 2, 2,  # 58 - 5f
    2, 2, 2, 2, 2, 2, 2, 2,  # 60 - 67
    2, 2, 2, 2, 2, 2, 2, 2,  # 68 - 6f
    2, 2, 2, 2, 2, 2, 2, 2,  # 70 - 77
    2, 2, 2, 2, 2, 2, 2, 4,  # 78 - 7f
    5, 6, 6, 6, 6, 6, 6, 6,  # 80 - 87
    6, 6, 6, 6, 6, 6, 6, 6,  # 88 - 8f
    6, 6, 6, 6, 6, 6, 6, 6,  # 90 - 97
    6, 6, 6, 6, 6, 6, 6, 6,  # 98 - 9f
    6, 6, 6, 6, 6, 6, 6, 6,  # a0 - a7
    6, 6, 6, 6, 6, 6, 6, 6,  # a8 - af
    6, 6, 6, 6, 6, 6, 6, 6,  # b0 - b7
    6, 6, 6, 6, 6, 6, 6, 6,  # b8 - bf
    6, 6, 6, 6, 6, 6, 6, 6,  # c0 - c7
    6, 6, 6, 6, 6, 6, 6, 6,  # c8 - cf
    6, 6, 6, 6, 6, 6, 6, 6,  # d0 - d7
    6, 6, 6, 6, 6, 6, 6, 6,  # d8 - df
    6, 6, 6, 6, 6, 6, 6, 6,  # e0 - e7
    6, 6, 6, 6, 6, 6, 6, 6,  # e8 - ef
    6, 6, 6, 6, 6, 6, 6, 6,  # f0 - f7
    6, 6, 6, 6, 6, 6, 6, 0   # f8 - ff
)

GB2312_ST = (
    MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,     3,MachineState.ERROR,#00-07
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,#08-0f
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.START,#10-17
         4,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#18-1f
    MachineState.ERROR,MachineState.ERROR,     5,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,#20-27
    MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.START #28-2f
)
# fmt: on

# To be accurate, the length of class 6 can be either 2 or 4.
# But it is not necessary to discriminate between the two since
# it is used for frequency analysis only, and we are validating
# each code range there as well. So it is safe to set it to be
# 2 here.
GB2312_CHAR_LEN_TABLE = (0, 1, 1, 1, 1, 1, 2)

GB2312_SM_MODEL: CodingStateMachineDict = {
    "class_table": GB2312_CLS,
    "class_factor": 7,
    "state_table": GB2312_ST,
    "char_len_table": GB2312_CHAR_LEN_TABLE,
    "name": "GB2312",
}

# Shift_JIS
# fmt: off
SJIS_CLS = (
    1, 1, 1, 1, 1, 1, 1, 1,  # 00 - 07
    1, 1, 1, 1, 1, 1, 0, 0,  # 08 - 0f
    1, 1, 1, 1, 1, 1, 1, 1,  # 10 - 17
    1, 1, 1, 0, 1, 1, 1, 1,  # 18 - 1f
    1, 1, 1, 1, 1, 1, 1, 1,  # 20 - 27
    1, 1, 1, 1, 1, 1, 1, 1,  # 28 - 2f
    1, 1, 1, 1, 1, 1, 1, 1,  # 30 - 37
    1, 1, 1, 1, 1, 1, 1, 1,  # 38 - 3f
    2, 2, 2, 2, 2, 2, 2, 2,  # 40 - 47
    2, 2, 2, 2, 2, 2, 2, 2,  # 48 - 4f
    2, 2, 2, 2, 2, 2, 2, 2,  # 50 - 57
    2, 2, 2, 2, 2, 2, 2, 2,  # 58 - 5f
    2, 2, 2, 2, 2, 2, 2, 2,  # 60 - 67
    2, 2, 2, 2, 2, 2, 2, 2,  # 68 - 6f
    2, 2, 2, 2, 2, 2, 2, 2,  # 70 - 77
    2, 2, 2, 2, 2, 2, 2, 1,  # 78 - 7f
    3, 3, 3, 3, 3, 2, 2, 3,  # 80 - 87
    3, 3, 3, 3, 3, 3, 3, 3,  # 88 - 8f
    3, 3, 3, 3, 3, 3, 3, 3,  # 90 - 97
    3, 3, 3, 3, 3, 3, 3, 3,  # 98 - 9f
    #0xa0 is illegal in sjis encoding, but some pages does
    #contain such byte. We need to be more error forgiven.
    2, 2, 2, 2, 2, 2, 2, 2,  # a0 - a7
    2, 2, 2, 2, 2, 2, 2, 2,  # a8 - af
    2, 2, 2, 2, 2, 2, 2, 2,  # b0 - b7
    2, 2, 2, 2, 2, 2, 2, 2,  # b8 - bf
    2, 2, 2, 2, 2, 2, 2, 2,  # c0 - c7
    2, 2, 2, 2, 2, 2, 2, 2,  # c8 - cf
    2, 2, 2, 2, 2, 2, 2, 2,  # d0 - d7
    2, 2, 2, 2, 2, 2, 2, 2,  # d8 - df
    3, 3, 3, 3, 3, 3, 3, 3,  # e0 - e7
    3, 3, 3, 3, 3, 4, 4, 4,  # e8 - ef
    3, 3, 3, 3, 3, 3, 3, 3,  # f0 - f7
    3, 3, 3, 3, 3, 0, 0, 0,  # f8 - ff
)

SJIS_ST = (
    MachineState.ERROR,MachineState.START,MachineState.START,     3,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#00-07
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,#08-0f
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START #10-17
)
# fmt: on

SJIS_CHAR_LEN_TABLE = (0, 1, 1, 2, 0, 0)

SJIS_SM_MODEL: CodingStateMachineDict = {
    "class_table": SJIS_CLS,
    "class_factor": 6,
    "state_table": SJIS_ST,
    "char_len_table": SJIS_CHAR_LEN_TABLE,
    "name": "Shift_JIS",
}

# UCS2-BE
# fmt: off
UCS2BE_CLS = (
    0, 0, 0, 0, 0, 0, 0, 0,  # 00 - 07
    0, 0, 1, 0, 0, 2, 0, 0,  # 08 - 0f
    0, 0, 0, 0, 0, 0, 0, 0,  # 10 - 17
    0, 0, 0, 3, 0, 0, 0, 0,  # 18 - 1f
    0, 0, 0, 0, 0, 0, 0, 0,  # 20 - 27
    0, 3, 3, 3, 3, 3, 0, 0,  # 28 - 2f
    0, 0, 0, 0, 0, 0, 0, 0,  # 30 - 37
    0, 0, 0, 0, 0, 0, 0, 0,  # 38 - 3f
    0, 0, 0, 0, 0, 0, 0, 0,  # 40 - 47
    0, 0, 0, 0, 0, 0, 0, 0,  # 48 - 4f
    0, 0, 0, 0, 0, 0, 0, 0,  # 50 - 57
    0, 0, 0, 0, 0, 0, 0, 0,  # 58 - 5f
    0, 0, 0, 0, 0, 0, 0, 0,  # 60 - 67
    0, 0, 0, 0, 0, 0, 0, 0,  # 68 - 6f
    0, 0, 0, 0, 0, 0, 0, 0,  # 70 - 77
    0, 0, 0, 0, 0, 0, 0, 0,  # 78 - 7f
    0, 0, 0, 0, 0, 0, 0, 0,  # 80 - 87
    0, 0, 0, 0, 0, 0, 0, 0,  # 88 - 8f
    0, 0, 0, 0, 0, 0, 0, 0,  # 90 - 97
    0, 0, 0, 0, 0, 0, 0, 0,  # 98 - 9f
    0, 0, 0, 0, 0, 0, 0, 0,  # a0 - a7
    0, 0, 0, 0, 0, 0, 0, 0,  # a8 - af
    0, 0, 0, 0, 0, 0, 0, 0,  # b0 - b7
    0, 0, 0, 0, 0, 0, 0, 0,  # b8 - bf
    0, 0, 0, 0, 0, 0, 0, 0,  # c0 - c7
    0, 0, 0, 0, 0, 0, 0, 0,  # c8 - cf
    0, 0, 0, 0, 0, 0, 0, 0,  # d0 - d7
    0, 0, 0, 0, 0, 0, 0, 0,  # d8 - df
    0, 0, 0, 0, 0, 0, 0, 0,  # e0 - e7
    0, 0, 0, 0, 0, 0, 0, 0,  # e8 - ef
    0, 0, 0, 0, 0, 0, 0, 0,  # f0 - f7
    0, 0, 0, 0, 0, 0, 4, 5   # f8 - ff
)

UCS2BE_ST  = (
          5,     7,     7,MachineState.ERROR,     4,     3,MachineState.ERROR,MachineState.ERROR,#00-07
     MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,#08-0f
     MachineState.ITS_ME,MachineState.ITS_ME,     6,     6,     6,     6,MachineState.ERROR,MachineState.ERROR,#10-17
          6,     6,     6,     6,     6,MachineState.ITS_ME,     6,     6,#18-1f
          6,     6,     6,     6,     5,     7,     7,MachineState.ERROR,#20-27
          5,     8,     6,     6,MachineState.ERROR,     6,     6,     6,#28-2f
          6,     6,     6,     6,MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START #30-37
)
# fmt: on

UCS2BE_CHAR_LEN_TABLE = (2, 2, 2, 0, 2, 2)

UCS2BE_SM_MODEL: CodingStateMachineDict = {
    "class_table": UCS2BE_CLS,
    "class_factor": 6,
    "state_table": UCS2BE_ST,
    "char_len_table": UCS2BE_CHAR_LEN_TABLE,
    "name": "UTF-16BE",
}

# UCS2-LE
# fmt: off
UCS2LE_CLS = (
    0, 0, 0, 0, 0, 0, 0, 0,  # 00 - 07
    0, 0, 1, 0, 0, 2, 0, 0,  # 08 - 0f
    0, 0, 0, 0, 0, 0, 0, 0,  # 10 - 17
    0, 0, 0, 3, 0, 0, 0, 0,  # 18 - 1f
    0, 0, 0, 0, 0, 0, 0, 0,  # 20 - 27
    0, 3, 3, 3, 3, 3, 0, 0,  # 28 - 2f
    0, 0, 0, 0, 0, 0, 0, 0,  # 30 - 37
    0, 0, 0, 0, 0, 0, 0, 0,  # 38 - 3f
    0, 0, 0, 0, 0, 0, 0, 0,  # 40 - 47
    0, 0, 0, 0, 0, 0, 0, 0,  # 48 - 4f
    0, 0, 0, 0, 0, 0, 0, 0,  # 50 - 57
    0, 0, 0, 0, 0, 0, 0, 0,  # 58 - 5f
    0, 0, 0, 0, 0, 0, 0, 0,  # 60 - 67
    0, 0, 0, 0, 0, 0, 0, 0,  # 68 - 6f
    0, 0, 0, 0, 0, 0, 0, 0,  # 70 - 77
    0, 0, 0, 0, 0, 0, 0, 0,  # 78 - 7f
    0, 0, 0, 0, 0, 0, 0, 0,  # 80 - 87
    0, 0, 0, 0, 0, 0, 0, 0,  # 88 - 8f
    0, 0, 0, 0, 0, 0, 0, 0,  # 90 - 97
    0, 0, 0, 0, 0, 0, 0, 0,  # 98 - 9f
    0, 0, 0, 0, 0, 0, 0, 0,  # a0 - a7
    0, 0, 0, 0, 0, 0, 0, 0,  # a8 - af
    0, 0, 0, 0, 0, 0, 0, 0,  # b0 - b7
    0, 0, 0, 0, 0, 0, 0, 0,  # b8 - bf
    0, 0, 0, 0, 0, 0, 0, 0,  # c0 - c7
    0, 0, 0, 0, 0, 0, 0, 0,  # c8 - cf
    0, 0, 0, 0, 0, 0, 0, 0,  # d0 - d7
    0, 0, 0, 0, 0, 0, 0, 0,  # d8 - df
    0, 0, 0, 0, 0, 0, 0, 0,  # e0 - e7
    0, 0, 0, 0, 0, 0, 0, 0,  # e8 - ef
    0, 0, 0, 0, 0, 0, 0, 0,  # f0 - f7
    0, 0, 0, 0, 0, 0, 4, 5   # f8 - ff
)

UCS2LE_ST = (
          6,     6,     7,     6,     4,     3,MachineState.ERROR,MachineState.ERROR,#00-07
     MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,#08-0f
     MachineState.ITS_ME,MachineState.ITS_ME,     5,     5,     5,MachineState.ERROR,MachineState.ITS_ME,MachineState.ERROR,#10-17
          5,     5,     5,MachineState.ERROR,     5,MachineState.ERROR,     6,     6,#18-1f
          7,     6,     8,     8,     5,     5,     5,MachineState.ERROR,#20-27
          5,     5,     5,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,     5,     5,#28-2f
          5,     5,     5,MachineState.ERROR,     5,MachineState.ERROR,MachineState.START,MachineState.START #30-37
)
# fmt: on

UCS2LE_CHAR_LEN_TABLE = (2, 2, 2, 2, 2, 2)

UCS2LE_SM_MODEL: CodingStateMachineDict = {
    "class_table": UCS2LE_CLS,
    "class_factor": 6,
    "state_table": UCS2LE_ST,
    "char_len_table": UCS2LE_CHAR_LEN_TABLE,
    "name": "UTF-16LE",
}

# UTF-8
# fmt: off
UTF8_CLS = (
    1, 1, 1, 1, 1, 1, 1, 1,  # 00 - 07  #allow 0x00 as a legal value
    1, 1, 1, 1, 1, 1, 0, 0,  # 08 - 0f
    1, 1, 1, 1, 1, 1, 1, 1,  # 10 - 17
    1, 1, 1, 0, 1, 1, 1, 1,  # 18 - 1f
    1, 1, 1, 1, 1, 1, 1, 1,  # 20 - 27
    1, 1, 1, 1, 1, 1, 1, 1,  # 28 - 2f
    1, 1, 1, 1, 1, 1, 1, 1,  # 30 - 37
    1, 1, 1, 1, 1, 1, 1, 1,  # 38 - 3f
    1, 1, 1, 1, 1, 1, 1, 1,  # 40 - 47
    1, 1, 1, 1, 1, 1, 1, 1,  # 48 - 4f
    1, 1, 1, 1, 1, 1, 1, 1,  # 50 - 57
    1, 1, 1, 1, 1, 1, 1, 1,  # 58 - 5f
    1, 1, 1, 1, 1, 1, 1, 1,  # 60 - 67
    1, 1, 1, 1, 1, 1, 1, 1,  # 68 - 6f
    1, 1, 1, 1, 1, 1, 1, 1,  # 70 - 77
    1, 1, 1, 1, 1, 1, 1, 1,  # 78 - 7f
    2, 2, 2, 2, 3, 3, 3, 3,  # 80 - 87
    4, 4, 4, 4, 4, 4, 4, 4,  # 88 - 8f
    4, 4, 4, 4, 4, 4, 4, 4,  # 90 - 97
    4, 4, 4, 4, 4, 4, 4, 4,  # 98 - 9f
    5, 5, 5, 5, 5, 5, 5, 5,  # a0 - a7
    5, 5, 5, 5, 5, 5, 5, 5,  # a8 - af
    5, 5, 5, 5, 5, 5, 5, 5,  # b0 - b7
    5, 5, 5, 5, 5, 5, 5, 5,  # b8 - bf
    0, 0, 6, 6, 6, 6, 6, 6,  # c0 - c7
    6, 6, 6, 6, 6, 6, 6, 6,  # c8 - cf
    6, 6, 6, 6, 6, 6, 6, 6,  # d0 - d7
    6, 6, 6, 6, 6, 6, 6, 6,  # d8 - df
    7, 8, 8, 8, 8, 8, 8, 8,  # e0 - e7
    8, 8, 8, 8, 8, 9, 8, 8,  # e8 - ef
    10, 11, 11, 11, 11, 11, 11, 11,  # f0 - f7
    12, 13, 13, 13, 14, 15, 0, 0    # f8 - ff
)

UTF8_ST = (
    MachineState.ERROR,MachineState.START,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,     12,   10,#00-07
         9,     11,     8,     7,     6,     5,     4,    3,#08-0f
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#10-17
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#18-1f
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,#20-27
    MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,MachineState.ITS_ME,#28-2f
    MachineState.ERROR,MachineState.ERROR,     5,     5,     5,     5,MachineState.ERROR,MachineState.ERROR,#30-37
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#38-3f
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,     5,     5,     5,MachineState.ERROR,MachineState.ERROR,#40-47
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#48-4f
    MachineState.ERROR,MachineState.ERROR,     7,     7,     7,     7,MachineState.ERROR,MachineState.ERROR,#50-57
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#58-5f
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,     7,     7,MachineState.ERROR,MachineState.ERROR,#60-67
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#68-6f
    MachineState.ERROR,MachineState.ERROR,     9,     9,     9,     9,MachineState.ERROR,MachineState.ERROR,#70-77
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#78-7f
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,     9,MachineState.ERROR,MachineState.ERROR,#80-87
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#88-8f
    MachineState.ERROR,MachineState.ERROR,    12,    12,    12,    12,MachineState.ERROR,MachineState.ERROR,#90-97
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#98-9f
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,    12,MachineState.ERROR,MachineState.ERROR,#a0-a7
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#a8-af
    MachineState.ERROR,MachineState.ERROR,    12,    12,    12,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#b0-b7
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,#b8-bf
    MachineState.ERROR,MachineState.ERROR,MachineState.START,MachineState.START,MachineState.START,MachineState.START,MachineState.ERROR,MachineState.ERROR,#c0-c7
    MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR,MachineState.ERROR #c8-cf
)
# fmt: on

UTF8_CHAR_LEN_TABLE = (0, 1, 0, 0, 0, 0, 2, 3, 3, 3, 4, 4, 5, 5, 6, 6)

UTF8_SM_MODEL: CodingStateMachineDict = {
    "class_table": UTF8_CLS,
    "class_factor": 16,
    "state_table": UTF8_ST,
    "char_len_table": UTF8_CHAR_LEN_TABLE,
    "name": "UTF-8",
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/metadata/__init__.py
# ========================================================


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/metadata/languages.py
# ========================================================
"""
Metadata about languages used by our model training code for our
SingleByteCharSetProbers.  Could be used for other things in the future.

This code is based on the language metadata from the uchardet project.
"""

from string import ascii_letters
from typing import List, Optional

# TODO: Add Ukrainian (KOI8-U)


class Language:
    """Metadata about a language useful for training models

    :ivar name: The human name for the language, in English.
    :type name: str
    :ivar iso_code: 2-letter ISO 639-1 if possible, 3-letter ISO code otherwise,
                    or use another catalog as a last resort.
    :type iso_code: str
    :ivar use_ascii: Whether or not ASCII letters should be included in trained
                     models.
    :type use_ascii: bool
    :ivar charsets: The charsets we want to support and create data for.
    :type charsets: list of str
    :ivar alphabet: The characters in the language's alphabet. If `use_ascii` is
                    `True`, you only need to add those not in the ASCII set.
    :type alphabet: str
    :ivar wiki_start_pages: The Wikipedia pages to start from if we're crawling
                            Wikipedia for training data.
    :type wiki_start_pages: list of str
    """

    def __init__(
        self,
        name: Optional[str] = None,
        iso_code: Optional[str] = None,
        use_ascii: bool = True,
        charsets: Optional[List[str]] = None,
        alphabet: Optional[str] = None,
        wiki_start_pages: Optional[List[str]] = None,
    ) -> None:
        super().__init__()
        self.name = name
        self.iso_code = iso_code
        self.use_ascii = use_ascii
        self.charsets = charsets
        if self.use_ascii:
            if alphabet:
                alphabet += ascii_letters
            else:
                alphabet = ascii_letters
        elif not alphabet:
            raise ValueError("Must supply alphabet if use_ascii is False")
        self.alphabet = "".join(sorted(set(alphabet))) if alphabet else None
        self.wiki_start_pages = wiki_start_pages

    def __repr__(self) -> str:
        param_str = ", ".join(
            f"{k}={v!r}" for k, v in self.__dict__.items() if not k.startswith("_")
        )
        return f"{self.__class__.__name__}({param_str})"


LANGUAGES = {
    "Arabic": Language(
        name="Arabic",
        iso_code="ar",
        use_ascii=False,
        # We only support encodings that use isolated
        # forms, because the current recommendation is
        # that the rendering system handles presentation
        # forms. This means we purposefully skip IBM864.
        charsets=["ISO-8859-6", "WINDOWS-1256", "CP720", "CP864"],
        alphabet="",
        wiki_start_pages=["_"],
    ),
    "Belarusian": Language(
        name="Belarusian",
        iso_code="be",
        use_ascii=False,
        charsets=["ISO-8859-5", "WINDOWS-1251", "IBM866", "MacCyrillic"],
        alphabet="",
        wiki_start_pages=["_"],
    ),
    "Bulgarian": Language(
        name="Bulgarian",
        iso_code="bg",
        use_ascii=False,
        charsets=["ISO-8859-5", "WINDOWS-1251", "IBM855"],
        alphabet="",
        wiki_start_pages=["_"],
    ),
    "Czech": Language(
        name="Czech",
        iso_code="cz",
        use_ascii=True,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="",
        wiki_start_pages=["Hlavn_strana"],
    ),
    "Danish": Language(
        name="Danish",
        iso_code="da",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Forside"],
    ),
    "German": Language(
        name="German",
        iso_code="de",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Wikipedia:Hauptseite"],
    ),
    "Greek": Language(
        name="Greek",
        iso_code="el",
        use_ascii=False,
        charsets=["ISO-8859-7", "WINDOWS-1253"],
        alphabet="",
        wiki_start_pages=[":"],
    ),
    "English": Language(
        name="English",
        iso_code="en",
        use_ascii=True,
        charsets=["ISO-8859-1", "WINDOWS-1252", "MacRoman"],
        wiki_start_pages=["Main_Page"],
    ),
    "Esperanto": Language(
        name="Esperanto",
        iso_code="eo",
        # Q, W, X, and Y not used at all
        use_ascii=False,
        charsets=["ISO-8859-3"],
        alphabet="abcdefghijklmnoprstuvzABCDEFGHIJKLMNOPRSTUVZ",
        wiki_start_pages=["Vikipedio:efpao"],
    ),
    "Spanish": Language(
        name="Spanish",
        iso_code="es",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Wikipedia:Portada"],
    ),
    "Estonian": Language(
        name="Estonian",
        iso_code="et",
        use_ascii=False,
        charsets=["ISO-8859-4", "ISO-8859-13", "WINDOWS-1257"],
        # C, F, , Q, W, X, Y, Z,  are only for
        # loanwords
        alphabet="ABDEGHIJKLMNOPRSTUVabdeghijklmnoprstuv",
        wiki_start_pages=["Esileht"],
    ),
    "Finnish": Language(
        name="Finnish",
        iso_code="fi",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Wikipedia:Etusivu"],
    ),
    "French": Language(
        name="French",
        iso_code="fr",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Wikipdia:Accueil_principal", "Buf (animal)"],
    ),
    "Hebrew": Language(
        name="Hebrew",
        iso_code="he",
        use_ascii=False,
        charsets=["ISO-8859-8", "WINDOWS-1255"],
        alphabet="",
        wiki_start_pages=["_"],
    ),
    "Croatian": Language(
        name="Croatian",
        iso_code="hr",
        # Q, W, X, Y are only used for foreign words.
        use_ascii=False,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="abcdefghijklmnoprstuvzABCDEFGHIJKLMNOPRSTUVZ",
        wiki_start_pages=["Glavna_stranica"],
    ),
    "Hungarian": Language(
        name="Hungarian",
        iso_code="hu",
        # Q, W, X, Y are only used for foreign words.
        use_ascii=False,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="abcdefghijklmnoprstuvzABCDEFGHIJKLMNOPRSTUVZ",
        wiki_start_pages=["Kezdlap"],
    ),
    "Italian": Language(
        name="Italian",
        iso_code="it",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Pagina_principale"],
    ),
    "Lithuanian": Language(
        name="Lithuanian",
        iso_code="lt",
        use_ascii=False,
        charsets=["ISO-8859-13", "WINDOWS-1257", "ISO-8859-4"],
        # Q, W, and X not used at all
        alphabet="ABCDEFGHIYJKLMNOPRSTUVZabcdefghiyjklmnoprstuvz",
        wiki_start_pages=["Pagrindinis_puslapis"],
    ),
    "Latvian": Language(
        name="Latvian",
        iso_code="lv",
        use_ascii=False,
        charsets=["ISO-8859-13", "WINDOWS-1257", "ISO-8859-4"],
        # Q, W, X, Y are only for loanwords
        alphabet="ABCDEFGHIJKLMNOPRSTUVZabcdefghijklmnoprstuvz",
        wiki_start_pages=["Skumlapa"],
    ),
    "Macedonian": Language(
        name="Macedonian",
        iso_code="mk",
        use_ascii=False,
        charsets=["ISO-8859-5", "WINDOWS-1251", "MacCyrillic", "IBM855"],
        alphabet="",
        wiki_start_pages=["_"],
    ),
    "Dutch": Language(
        name="Dutch",
        iso_code="nl",
        use_ascii=True,
        charsets=["ISO-8859-1", "WINDOWS-1252", "MacRoman"],
        wiki_start_pages=["Hoofdpagina"],
    ),
    "Polish": Language(
        name="Polish",
        iso_code="pl",
        # Q and X are only used for foreign words.
        use_ascii=False,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="ABCDEFGHIJKLMNOPRSTUWYZabcdefghijklmnoprstuwyz",
        wiki_start_pages=["Wikipedia:Strona_gwna"],
    ),
    "Portuguese": Language(
        name="Portuguese",
        iso_code="pt",
        use_ascii=True,
        charsets=["ISO-8859-1", "ISO-8859-15", "WINDOWS-1252", "MacRoman"],
        alphabet="",
        wiki_start_pages=["Wikipdia:Pgina_principal"],
    ),
    "Romanian": Language(
        name="Romanian",
        iso_code="ro",
        use_ascii=True,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="",
        wiki_start_pages=["Pagina_principal"],
    ),
    "Russian": Language(
        name="Russian",
        iso_code="ru",
        use_ascii=False,
        charsets=[
            "ISO-8859-5",
            "WINDOWS-1251",
            "KOI8-R",
            "MacCyrillic",
            "IBM866",
            "IBM855",
        ],
        alphabet="",
        wiki_start_pages=["_"],
    ),
    "Slovak": Language(
        name="Slovak",
        iso_code="sk",
        use_ascii=True,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="",
        wiki_start_pages=["Hlavn_strnka"],
    ),
    "Slovene": Language(
        name="Slovene",
        iso_code="sl",
        # Q, W, X, Y are only used for foreign words.
        use_ascii=False,
        charsets=["ISO-8859-2", "WINDOWS-1250"],
        alphabet="abcdefghijklmnoprstuvzABCDEFGHIJKLMNOPRSTUVZ",
        wiki_start_pages=["Glavna_stran"],
    ),
    # Serbian can be written in both Latin and Cyrillic, but there's no
    # simple way to get the Latin alphabet pages from Wikipedia through
    # the API, so for now we just support Cyrillic.
    "Serbian": Language(
        name="Serbian",
        iso_code="sr",
        alphabet="",
        charsets=["ISO-8859-5", "WINDOWS-1251", "MacCyrillic", "IBM855"],
        wiki_start_pages=["_"],
    ),
    "Thai": Language(
        name="Thai",
        iso_code="th",
        use_ascii=False,
        charsets=["ISO-8859-11", "TIS-620", "CP874"],
        alphabet="",
        wiki_start_pages=[""],
    ),
    "Turkish": Language(
        name="Turkish",
        iso_code="tr",
        # Q, W, and X are not used by Turkish
        use_ascii=False,
        charsets=["ISO-8859-3", "ISO-8859-9", "WINDOWS-1254"],
        alphabet="abcdefghijklmnoprstuvyzABCDEFGHIJKLMNOPRSTUVYZ",
        wiki_start_pages=["Ana_Sayfa"],
    ),
    "Vietnamese": Language(
        name="Vietnamese",
        iso_code="vi",
        use_ascii=False,
        # Windows-1258 is the only common 8-bit
        # Vietnamese encoding supported by Python.
        # From Wikipedia:
        # For systems that lack support for Unicode,
        # dozens of 8-bit Vietnamese code pages are
        # available.[1] The most common are VISCII
        # (TCVN 5712:1993), VPS, and Windows-1258.[3]
        # Where ASCII is required, such as when
        # ensuring readability in plain text e-mail,
        # Vietnamese letters are often encoded
        # according to Vietnamese Quoted-Readable
        # (VIQR) or VSCII Mnemonic (VSCII-MNEM),[4]
        # though usage of either variable-width
        # scheme has declined dramatically following
        # the adoption of Unicode on the World Wide
        # Web.
        charsets=["WINDOWS-1258"],
        alphabet="abcdeghiklmnopqrstuvxyABCDEGHIKLMNOPQRSTUVXY",
        wiki_start_pages=["Ch_Quc_ng"],
    ),
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langbulgarianmodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

BULGARIAN_LANG_MODEL = {
    63: {  # 'e'
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 1,  # ''
        9: 1,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 1,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 0,  # ''
        26: 1,  # ''
        12: 1,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 1,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 1,  # ''
        5: 1,  # ''
        19: 0,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    45: {  # '\xad'
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        38: 1,  # ''
        36: 0,  # ''
        41: 1,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 1,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 1,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 0,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 0,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 0,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    31: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 1,  # ''
        32: 1,  # ''
        35: 2,  # ''
        43: 1,  # ''
        37: 2,  # ''
        44: 2,  # ''
        55: 1,  # ''
        47: 2,  # ''
        40: 1,  # ''
        59: 1,  # ''
        33: 1,  # ''
        46: 2,  # ''
        38: 1,  # ''
        36: 2,  # ''
        41: 1,  # ''
        30: 2,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 1,  # ''
        48: 2,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 2,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 1,  # ''
        18: 2,  # ''
        9: 2,  # ''
        20: 2,  # ''
        11: 2,  # ''
        3: 1,  # ''
        23: 1,  # ''
        15: 2,  # ''
        2: 0,  # ''
        26: 2,  # ''
        12: 2,  # ''
        10: 3,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 0,  # ''
        13: 2,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 1,  # ''
        29: 2,  # ''
        25: 1,  # ''
        22: 1,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    32: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 2,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 2,  # ''
        44: 1,  # ''
        55: 1,  # ''
        47: 2,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 2,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 1,  # ''
        48: 2,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 0,  # ''
        57: 1,  # ''
        61: 2,  # ''
        60: 1,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 2,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 1,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    35: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 2,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 1,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 2,  # ''
        49: 0,  # ''
        53: 1,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 2,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 2,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 2,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 2,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    43: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 2,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 0,  # ''
        39: 1,  # ''
        28: 1,  # ''
        34: 0,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 1,  # ''
        61: 1,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 1,  # ''
        9: 1,  # ''
        20: 0,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 2,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 1,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    37: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 2,  # ''
        43: 1,  # ''
        37: 2,  # ''
        44: 2,  # ''
        55: 2,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 2,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 2,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 2,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    44: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 1,  # ''
        35: 2,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 1,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 1,  # ''
        33: 2,  # ''
        46: 2,  # ''
        38: 1,  # ''
        36: 2,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 1,  # ''
        48: 2,  # ''
        49: 1,  # ''
        53: 2,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 1,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 0,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 2,  # ''
        3: 0,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 0,  # ''
        26: 1,  # ''
        12: 2,  # ''
        10: 2,  # ''
        14: 2,  # ''
        6: 2,  # ''
        4: 0,  # ''
        13: 1,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 1,  # ''
        19: 1,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    55: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 1,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    47: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 2,  # ''
        41: 1,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 1,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 2,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 1,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 1,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    40: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 1,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 2,  # ''
        55: 1,  # ''
        47: 2,  # ''
        40: 1,  # ''
        59: 1,  # ''
        33: 2,  # ''
        46: 2,  # ''
        38: 2,  # ''
        36: 2,  # ''
        41: 1,  # ''
        30: 1,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 0,  # ''
        48: 1,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 1,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 2,  # ''
        1: 1,  # ''
        18: 1,  # ''
        9: 3,  # ''
        20: 2,  # ''
        11: 1,  # ''
        3: 1,  # ''
        23: 0,  # ''
        15: 3,  # ''
        2: 0,  # ''
        26: 1,  # ''
        12: 1,  # ''
        10: 2,  # ''
        14: 2,  # ''
        6: 2,  # ''
        4: 0,  # ''
        13: 1,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 0,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 1,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    59: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 1,  # ''
        34: 1,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 1,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 0,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 1,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 0,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 2,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    33: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 0,  # ''
        36: 2,  # ''
        41: 2,  # ''
        30: 2,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 1,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 1,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 2,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 3,  # ''
        8: 1,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    46: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 2,  # ''
        37: 1,  # ''
        44: 2,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 0,  # ''
        28: 1,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 1,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 1,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 2,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    38: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 2,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 1,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 2,  # ''
        14: 0,  # ''
        6: 2,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    36: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 2,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 2,  # ''
        44: 2,  # ''
        55: 1,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 1,  # ''
        33: 2,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 1,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 1,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 2,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    41: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 1,  # ''
        35: 2,  # ''
        43: 1,  # ''
        37: 2,  # ''
        44: 1,  # ''
        55: 1,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 1,  # ''
        33: 2,  # ''
        46: 2,  # ''
        38: 2,  # ''
        36: 2,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 1,  # ''
        53: 0,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 1,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 1,  # ''
        18: 2,  # ''
        9: 2,  # ''
        20: 2,  # ''
        11: 1,  # ''
        3: 1,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 0,  # ''
        26: 1,  # ''
        12: 2,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 0,  # ''
        13: 2,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 3,  # ''
        19: 1,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 1,  # ''
        21: 2,  # ''
        27: 0,  # ''
        24: 2,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    30: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 2,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 2,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 3,  # ''
        14: 0,  # ''
        6: 1,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 3,  # ''
        8: 1,  # ''
        5: 1,  # ''
        19: 2,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    39: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 2,  # ''
        37: 2,  # ''
        44: 2,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 2,  # ''
        39: 1,  # ''
        28: 1,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 1,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 1,  # ''
        5: 0,  # ''
        19: 3,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    28: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 3,  # ''
        32: 2,  # ''
        35: 2,  # ''
        43: 1,  # ''
        37: 2,  # ''
        44: 2,  # ''
        55: 1,  # ''
        47: 1,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 2,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 2,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 2,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 1,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 2,  # ''
        10: 3,  # ''
        14: 2,  # ''
        6: 1,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 2,  # ''
        8: 0,  # ''
        5: 3,  # ''
        19: 2,  # ''
        29: 2,  # ''
        25: 1,  # ''
        22: 1,  # ''
        21: 1,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    34: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 2,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 2,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 2,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 2,  # ''
        30: 1,  # ''
        39: 2,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 1,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 1,  # ''
        60: 0,  # ''
        56: 1,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 1,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 1,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 3,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    51: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 1,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 2,  # ''
        55: 1,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 0,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 1,  # ''
        34: 2,  # ''
        51: 0,  # ''
        48: 1,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 1,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 2,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 2,  # ''
        26: 1,  # ''
        12: 2,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 2,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 2,  # ''
        5: 1,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 2,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    48: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 2,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 1,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 2,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 2,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    49: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 1,  # ''
        39: 1,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 1,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 1,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 0,  # ''
        4: 2,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    53: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 2,  # ''
        59: 0,  # ''
        33: 2,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 1,  # ''
        28: 2,  # ''
        34: 0,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 2,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 1,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    50: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 2,  # ''
        32: 1,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 1,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 2,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 1,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    54: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 1,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 1,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 1,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 2,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 2,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 1,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    57: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 1,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 1,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 1,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 1,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    61: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 1,  # ''
        44: 0,  # ''
        55: 1,  # ''
        47: 1,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 2,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 0,  # ''
        30: 1,  # ''
        39: 2,  # ''
        28: 1,  # ''
        34: 1,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 1,  # ''
        53: 1,  # ''
        50: 1,  # ''
        54: 1,  # ''
        57: 1,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 0,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 0,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 1,  # ''
        14: 0,  # ''
        6: 1,  # ''
        4: 0,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    60: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 1,  # ''
        35: 0,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 0,  # ''
        55: 1,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 0,  # ''
        36: 1,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 1,  # ''
        28: 1,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 1,  # ''
        9: 1,  # ''
        20: 2,  # ''
        11: 1,  # ''
        3: 0,  # ''
        23: 2,  # ''
        15: 1,  # ''
        2: 1,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 0,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 1,  # ''
        5: 1,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    56: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 1,  # ''
        35: 1,  # ''
        43: 1,  # ''
        37: 1,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        38: 1,  # ''
        36: 1,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 1,  # ''
        34: 2,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 1,  # ''
        9: 1,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 0,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 1,  # ''
        26: 1,  # ''
        12: 1,  # ''
        10: 1,  # ''
        14: 2,  # ''
        6: 2,  # ''
        4: 0,  # ''
        13: 2,  # ''
        7: 1,  # ''
        8: 1,  # ''
        5: 1,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    1: {  # ''
        63: 1,  # 'e'
        45: 1,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 1,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 1,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 3,  # ''
        2: 3,  # ''
        26: 3,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 2,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 3,  # ''
        25: 3,  # ''
        22: 3,  # ''
        21: 3,  # ''
        27: 3,  # ''
        24: 3,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    18: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 3,  # ''
        20: 1,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 3,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 1,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 0,  # ''
        19: 3,  # ''
        29: 0,  # ''
        25: 2,  # ''
        22: 1,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 3,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    9: {  # ''
        63: 1,  # 'e'
        45: 1,  # '\xad'
        31: 0,  # ''
        32: 1,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 0,  # ''
        20: 2,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 3,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 2,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 3,  # ''
        27: 2,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    20: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 3,  # ''
        14: 1,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 1,  # ''
        7: 3,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 3,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    11: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 2,  # ''
        9: 3,  # ''
        20: 2,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 2,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 1,  # ''
        19: 3,  # ''
        29: 1,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    3: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 2,  # ''
        23: 3,  # ''
        15: 3,  # ''
        2: 2,  # ''
        26: 3,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 2,  # ''
        29: 3,  # ''
        25: 3,  # ''
        22: 3,  # ''
        21: 3,  # ''
        27: 3,  # ''
        24: 3,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    23: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 2,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 3,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 1,  # ''
        5: 1,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 1,  # ''
        21: 1,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    15: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 1,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 2,  # ''
        24: 1,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    2: {  # ''
        63: 1,  # 'e'
        45: 1,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 1,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 1,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 1,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 3,  # ''
        2: 3,  # ''
        26: 3,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 2,  # ''
        29: 3,  # ''
        25: 3,  # ''
        22: 3,  # ''
        21: 3,  # ''
        27: 3,  # ''
        24: 3,  # ''
        17: 2,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    26: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 1,  # ''
        18: 2,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 2,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 2,  # ''
        2: 1,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 2,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 2,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 1,  # ''
        29: 2,  # ''
        25: 1,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    12: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 1,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 1,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 3,  # ''
        20: 2,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 2,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 3,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 1,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 3,  # ''
        21: 2,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    10: {  # ''
        63: 1,  # 'e'
        45: 1,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 1,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 2,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 1,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 2,  # ''
        7: 2,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 2,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 2,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 2,  # ''
        42: 3,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    14: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 1,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 2,  # ''
        10: 3,  # ''
        14: 1,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 1,  # ''
        19: 3,  # ''
        29: 2,  # ''
        25: 1,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 2,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    6: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 1,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 2,  # ''
        9: 2,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 2,  # ''
        15: 2,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 1,  # ''
        7: 2,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 3,  # ''
        25: 2,  # ''
        22: 3,  # ''
        21: 3,  # ''
        27: 2,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 2,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    4: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 2,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 3,  # ''
        2: 3,  # ''
        26: 3,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 2,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 2,  # ''
        29: 3,  # ''
        25: 3,  # ''
        22: 3,  # ''
        21: 3,  # ''
        27: 3,  # ''
        24: 3,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    13: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 1,  # ''
        12: 2,  # ''
        10: 3,  # ''
        14: 1,  # ''
        6: 2,  # ''
        4: 3,  # ''
        13: 1,  # ''
        7: 3,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 3,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    7: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 3,  # ''
        23: 3,  # ''
        15: 2,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 2,  # ''
        7: 1,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 2,  # ''
        25: 3,  # ''
        22: 3,  # ''
        21: 2,  # ''
        27: 3,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 1,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    8: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 2,  # ''
        9: 3,  # ''
        20: 2,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 1,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 2,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 2,  # ''
        24: 0,  # ''
        17: 3,  # ''
        52: 2,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    5: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 2,  # ''
        11: 2,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 2,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 3,  # ''
        29: 1,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 2,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 3,  # ''
        52: 2,  # ''
        42: 2,  # ''
        16: 3,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    19: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 2,  # ''
        23: 3,  # ''
        15: 3,  # ''
        2: 2,  # ''
        26: 2,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 2,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 1,  # ''
        29: 2,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 3,  # ''
        27: 3,  # ''
        24: 2,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    29: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 1,  # ''
        20: 1,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 2,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 2,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 2,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    25: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 3,  # ''
        20: 0,  # ''
        11: 1,  # ''
        3: 2,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 2,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 1,  # ''
        7: 3,  # ''
        8: 1,  # ''
        5: 2,  # ''
        19: 3,  # ''
        29: 0,  # ''
        25: 1,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    22: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 2,  # ''
        20: 1,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 1,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 2,  # ''
        10: 1,  # ''
        14: 1,  # ''
        6: 1,  # ''
        4: 2,  # ''
        13: 1,  # ''
        7: 1,  # ''
        8: 1,  # ''
        5: 1,  # ''
        19: 2,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 1,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 0,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    21: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 1,  # ''
        9: 3,  # ''
        20: 1,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 1,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 2,  # ''
        14: 2,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 2,  # ''
        8: 0,  # ''
        5: 2,  # ''
        19: 3,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 1,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    27: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 2,  # ''
        20: 0,  # ''
        11: 1,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 3,  # ''
        10: 2,  # ''
        14: 1,  # ''
        6: 3,  # ''
        4: 2,  # ''
        13: 2,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 1,  # ''
        19: 2,  # ''
        29: 1,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 1,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 2,  # ''
        52: 1,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    24: {  # ''
        63: 1,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 3,  # ''
        18: 0,  # ''
        9: 1,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 3,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 3,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 2,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 1,  # ''
        8: 0,  # ''
        5: 2,  # ''
        19: 3,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 1,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 2,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    17: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 1,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 3,  # ''
        11: 3,  # ''
        3: 2,  # ''
        23: 3,  # ''
        15: 3,  # ''
        2: 1,  # ''
        26: 2,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 3,  # ''
        13: 3,  # ''
        7: 3,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 1,  # ''
        29: 1,  # ''
        25: 2,  # ''
        22: 2,  # ''
        21: 3,  # ''
        27: 2,  # ''
        24: 3,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 2,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    52: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 1,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 0,  # ''
        26: 0,  # ''
        12: 1,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 1,  # ''
        4: 3,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 1,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 1,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 1,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    42: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 1,  # ''
        18: 2,  # ''
        9: 1,  # ''
        20: 2,  # ''
        11: 2,  # ''
        3: 1,  # ''
        23: 2,  # ''
        15: 2,  # ''
        2: 1,  # ''
        26: 1,  # ''
        12: 2,  # ''
        10: 2,  # ''
        14: 2,  # ''
        6: 2,  # ''
        4: 1,  # ''
        13: 1,  # ''
        7: 2,  # ''
        8: 2,  # ''
        5: 2,  # ''
        19: 1,  # ''
        29: 1,  # ''
        25: 1,  # ''
        22: 2,  # ''
        21: 3,  # ''
        27: 1,  # ''
        24: 1,  # ''
        17: 1,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    16: {  # ''
        63: 0,  # 'e'
        45: 1,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 3,  # ''
        9: 3,  # ''
        20: 2,  # ''
        11: 3,  # ''
        3: 2,  # ''
        23: 1,  # ''
        15: 2,  # ''
        2: 1,  # ''
        26: 2,  # ''
        12: 3,  # ''
        10: 3,  # ''
        14: 3,  # ''
        6: 3,  # ''
        4: 1,  # ''
        13: 2,  # ''
        7: 2,  # ''
        8: 3,  # ''
        5: 3,  # ''
        19: 1,  # ''
        29: 1,  # ''
        25: 3,  # ''
        22: 2,  # ''
        21: 1,  # ''
        27: 1,  # ''
        24: 2,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 1,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    58: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 0,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 0,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 0,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
    62: {  # ''
        63: 0,  # 'e'
        45: 0,  # '\xad'
        31: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        43: 0,  # ''
        37: 0,  # ''
        44: 0,  # ''
        55: 0,  # ''
        47: 0,  # ''
        40: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        38: 0,  # ''
        36: 0,  # ''
        41: 0,  # ''
        30: 0,  # ''
        39: 0,  # ''
        28: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        48: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        50: 0,  # ''
        54: 0,  # ''
        57: 0,  # ''
        61: 0,  # ''
        60: 0,  # ''
        56: 0,  # ''
        1: 0,  # ''
        18: 0,  # ''
        9: 0,  # ''
        20: 0,  # ''
        11: 0,  # ''
        3: 0,  # ''
        23: 0,  # ''
        15: 0,  # ''
        2: 0,  # ''
        26: 0,  # ''
        12: 0,  # ''
        10: 0,  # ''
        14: 0,  # ''
        6: 0,  # ''
        4: 0,  # ''
        13: 0,  # ''
        7: 0,  # ''
        8: 0,  # ''
        5: 0,  # ''
        19: 0,  # ''
        29: 0,  # ''
        25: 0,  # ''
        22: 0,  # ''
        21: 0,  # ''
        27: 0,  # ''
        24: 0,  # ''
        17: 0,  # ''
        52: 0,  # ''
        42: 0,  # ''
        16: 0,  # ''
        58: 0,  # ''
        62: 0,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
ISO_8859_5_BULGARIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 77,  # 'A'
    66: 90,  # 'B'
    67: 99,  # 'C'
    68: 100,  # 'D'
    69: 72,  # 'E'
    70: 109,  # 'F'
    71: 107,  # 'G'
    72: 101,  # 'H'
    73: 79,  # 'I'
    74: 185,  # 'J'
    75: 81,  # 'K'
    76: 102,  # 'L'
    77: 76,  # 'M'
    78: 94,  # 'N'
    79: 82,  # 'O'
    80: 110,  # 'P'
    81: 186,  # 'Q'
    82: 108,  # 'R'
    83: 91,  # 'S'
    84: 74,  # 'T'
    85: 119,  # 'U'
    86: 84,  # 'V'
    87: 96,  # 'W'
    88: 111,  # 'X'
    89: 187,  # 'Y'
    90: 115,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 65,  # 'a'
    98: 69,  # 'b'
    99: 70,  # 'c'
    100: 66,  # 'd'
    101: 63,  # 'e'
    102: 68,  # 'f'
    103: 112,  # 'g'
    104: 103,  # 'h'
    105: 92,  # 'i'
    106: 194,  # 'j'
    107: 104,  # 'k'
    108: 95,  # 'l'
    109: 86,  # 'm'
    110: 87,  # 'n'
    111: 71,  # 'o'
    112: 116,  # 'p'
    113: 195,  # 'q'
    114: 85,  # 'r'
    115: 93,  # 's'
    116: 97,  # 't'
    117: 113,  # 'u'
    118: 196,  # 'v'
    119: 197,  # 'w'
    120: 198,  # 'x'
    121: 199,  # 'y'
    122: 200,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 194,  # '\x80'
    129: 195,  # '\x81'
    130: 196,  # '\x82'
    131: 197,  # '\x83'
    132: 198,  # '\x84'
    133: 199,  # '\x85'
    134: 200,  # '\x86'
    135: 201,  # '\x87'
    136: 202,  # '\x88'
    137: 203,  # '\x89'
    138: 204,  # '\x8a'
    139: 205,  # '\x8b'
    140: 206,  # '\x8c'
    141: 207,  # '\x8d'
    142: 208,  # '\x8e'
    143: 209,  # '\x8f'
    144: 210,  # '\x90'
    145: 211,  # '\x91'
    146: 212,  # '\x92'
    147: 213,  # '\x93'
    148: 214,  # '\x94'
    149: 215,  # '\x95'
    150: 216,  # '\x96'
    151: 217,  # '\x97'
    152: 218,  # '\x98'
    153: 219,  # '\x99'
    154: 220,  # '\x9a'
    155: 221,  # '\x9b'
    156: 222,  # '\x9c'
    157: 223,  # '\x9d'
    158: 224,  # '\x9e'
    159: 225,  # '\x9f'
    160: 81,  # '\xa0'
    161: 226,  # ''
    162: 227,  # ''
    163: 228,  # ''
    164: 229,  # ''
    165: 230,  # ''
    166: 105,  # ''
    167: 231,  # ''
    168: 232,  # ''
    169: 233,  # ''
    170: 234,  # ''
    171: 235,  # ''
    172: 236,  # ''
    173: 45,  # '\xad'
    174: 237,  # ''
    175: 238,  # ''
    176: 31,  # ''
    177: 32,  # ''
    178: 35,  # ''
    179: 43,  # ''
    180: 37,  # ''
    181: 44,  # ''
    182: 55,  # ''
    183: 47,  # ''
    184: 40,  # ''
    185: 59,  # ''
    186: 33,  # ''
    187: 46,  # ''
    188: 38,  # ''
    189: 36,  # ''
    190: 41,  # ''
    191: 30,  # ''
    192: 39,  # ''
    193: 28,  # ''
    194: 34,  # ''
    195: 51,  # ''
    196: 48,  # ''
    197: 49,  # ''
    198: 53,  # ''
    199: 50,  # ''
    200: 54,  # ''
    201: 57,  # ''
    202: 61,  # ''
    203: 239,  # ''
    204: 67,  # ''
    205: 240,  # ''
    206: 60,  # ''
    207: 56,  # ''
    208: 1,  # ''
    209: 18,  # ''
    210: 9,  # ''
    211: 20,  # ''
    212: 11,  # ''
    213: 3,  # ''
    214: 23,  # ''
    215: 15,  # ''
    216: 2,  # ''
    217: 26,  # ''
    218: 12,  # ''
    219: 10,  # ''
    220: 14,  # ''
    221: 6,  # ''
    222: 4,  # ''
    223: 13,  # ''
    224: 7,  # ''
    225: 8,  # ''
    226: 5,  # ''
    227: 19,  # ''
    228: 29,  # ''
    229: 25,  # ''
    230: 22,  # ''
    231: 21,  # ''
    232: 27,  # ''
    233: 24,  # ''
    234: 17,  # ''
    235: 75,  # ''
    236: 52,  # ''
    237: 241,  # ''
    238: 42,  # ''
    239: 16,  # ''
    240: 62,  # ''
    241: 242,  # ''
    242: 243,  # ''
    243: 244,  # ''
    244: 58,  # ''
    245: 245,  # ''
    246: 98,  # ''
    247: 246,  # ''
    248: 247,  # ''
    249: 248,  # ''
    250: 249,  # ''
    251: 250,  # ''
    252: 251,  # ''
    253: 91,  # ''
    254: 252,  # ''
    255: 253,  # ''
}

ISO_8859_5_BULGARIAN_MODEL = SingleByteCharSetModel(
    charset_name="ISO-8859-5",
    language="Bulgarian",
    char_to_order_map=ISO_8859_5_BULGARIAN_CHAR_TO_ORDER,
    language_model=BULGARIAN_LANG_MODEL,
    typical_positive_ratio=0.969392,
    keep_ascii_letters=False,
    alphabet="",
)

WINDOWS_1251_BULGARIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 77,  # 'A'
    66: 90,  # 'B'
    67: 99,  # 'C'
    68: 100,  # 'D'
    69: 72,  # 'E'
    70: 109,  # 'F'
    71: 107,  # 'G'
    72: 101,  # 'H'
    73: 79,  # 'I'
    74: 185,  # 'J'
    75: 81,  # 'K'
    76: 102,  # 'L'
    77: 76,  # 'M'
    78: 94,  # 'N'
    79: 82,  # 'O'
    80: 110,  # 'P'
    81: 186,  # 'Q'
    82: 108,  # 'R'
    83: 91,  # 'S'
    84: 74,  # 'T'
    85: 119,  # 'U'
    86: 84,  # 'V'
    87: 96,  # 'W'
    88: 111,  # 'X'
    89: 187,  # 'Y'
    90: 115,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 65,  # 'a'
    98: 69,  # 'b'
    99: 70,  # 'c'
    100: 66,  # 'd'
    101: 63,  # 'e'
    102: 68,  # 'f'
    103: 112,  # 'g'
    104: 103,  # 'h'
    105: 92,  # 'i'
    106: 194,  # 'j'
    107: 104,  # 'k'
    108: 95,  # 'l'
    109: 86,  # 'm'
    110: 87,  # 'n'
    111: 71,  # 'o'
    112: 116,  # 'p'
    113: 195,  # 'q'
    114: 85,  # 'r'
    115: 93,  # 's'
    116: 97,  # 't'
    117: 113,  # 'u'
    118: 196,  # 'v'
    119: 197,  # 'w'
    120: 198,  # 'x'
    121: 199,  # 'y'
    122: 200,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 206,  # ''
    129: 207,  # ''
    130: 208,  # ''
    131: 209,  # ''
    132: 210,  # ''
    133: 211,  # ''
    134: 212,  # ''
    135: 213,  # ''
    136: 120,  # ''
    137: 214,  # ''
    138: 215,  # ''
    139: 216,  # ''
    140: 217,  # ''
    141: 218,  # ''
    142: 219,  # ''
    143: 220,  # ''
    144: 221,  # ''
    145: 78,  # ''
    146: 64,  # ''
    147: 83,  # ''
    148: 121,  # ''
    149: 98,  # ''
    150: 117,  # ''
    151: 105,  # ''
    152: 222,  # None
    153: 223,  # ''
    154: 224,  # ''
    155: 225,  # ''
    156: 226,  # ''
    157: 227,  # ''
    158: 228,  # ''
    159: 229,  # ''
    160: 88,  # '\xa0'
    161: 230,  # ''
    162: 231,  # ''
    163: 232,  # ''
    164: 233,  # ''
    165: 122,  # ''
    166: 89,  # ''
    167: 106,  # ''
    168: 234,  # ''
    169: 235,  # ''
    170: 236,  # ''
    171: 237,  # ''
    172: 238,  # ''
    173: 45,  # '\xad'
    174: 239,  # ''
    175: 240,  # ''
    176: 73,  # ''
    177: 80,  # ''
    178: 118,  # ''
    179: 114,  # ''
    180: 241,  # ''
    181: 242,  # ''
    182: 243,  # ''
    183: 244,  # ''
    184: 245,  # ''
    185: 62,  # ''
    186: 58,  # ''
    187: 246,  # ''
    188: 247,  # ''
    189: 248,  # ''
    190: 249,  # ''
    191: 250,  # ''
    192: 31,  # ''
    193: 32,  # ''
    194: 35,  # ''
    195: 43,  # ''
    196: 37,  # ''
    197: 44,  # ''
    198: 55,  # ''
    199: 47,  # ''
    200: 40,  # ''
    201: 59,  # ''
    202: 33,  # ''
    203: 46,  # ''
    204: 38,  # ''
    205: 36,  # ''
    206: 41,  # ''
    207: 30,  # ''
    208: 39,  # ''
    209: 28,  # ''
    210: 34,  # ''
    211: 51,  # ''
    212: 48,  # ''
    213: 49,  # ''
    214: 53,  # ''
    215: 50,  # ''
    216: 54,  # ''
    217: 57,  # ''
    218: 61,  # ''
    219: 251,  # ''
    220: 67,  # ''
    221: 252,  # ''
    222: 60,  # ''
    223: 56,  # ''
    224: 1,  # ''
    225: 18,  # ''
    226: 9,  # ''
    227: 20,  # ''
    228: 11,  # ''
    229: 3,  # ''
    230: 23,  # ''
    231: 15,  # ''
    232: 2,  # ''
    233: 26,  # ''
    234: 12,  # ''
    235: 10,  # ''
    236: 14,  # ''
    237: 6,  # ''
    238: 4,  # ''
    239: 13,  # ''
    240: 7,  # ''
    241: 8,  # ''
    242: 5,  # ''
    243: 19,  # ''
    244: 29,  # ''
    245: 25,  # ''
    246: 22,  # ''
    247: 21,  # ''
    248: 27,  # ''
    249: 24,  # ''
    250: 17,  # ''
    251: 75,  # ''
    252: 52,  # ''
    253: 253,  # ''
    254: 42,  # ''
    255: 16,  # ''
}

WINDOWS_1251_BULGARIAN_MODEL = SingleByteCharSetModel(
    charset_name="windows-1251",
    language="Bulgarian",
    char_to_order_map=WINDOWS_1251_BULGARIAN_CHAR_TO_ORDER,
    language_model=BULGARIAN_LANG_MODEL,
    typical_positive_ratio=0.969392,
    keep_ascii_letters=False,
    alphabet="",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/charsetgroupprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import List, Optional, Union

from .charsetprober import CharSetProber
from .enums import LanguageFilter, ProbingState


class CharSetGroupProber(CharSetProber):
    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self._active_num = 0
        self.probers: List[CharSetProber] = []
        self._best_guess_prober: Optional[CharSetProber] = None

    def reset(self) -> None:
        super().reset()
        self._active_num = 0
        for prober in self.probers:
            prober.reset()
            prober.active = True
            self._active_num += 1
        self._best_guess_prober = None

    @property
    def charset_name(self) -> Optional[str]:
        if not self._best_guess_prober:
            self.get_confidence()
            if not self._best_guess_prober:
                return None
        return self._best_guess_prober.charset_name

    @property
    def language(self) -> Optional[str]:
        if not self._best_guess_prober:
            self.get_confidence()
            if not self._best_guess_prober:
                return None
        return self._best_guess_prober.language

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        for prober in self.probers:
            if not prober.active:
                continue
            state = prober.feed(byte_str)
            if not state:
                continue
            if state == ProbingState.FOUND_IT:
                self._best_guess_prober = prober
                self._state = ProbingState.FOUND_IT
                return self.state
            if state == ProbingState.NOT_ME:
                prober.active = False
                self._active_num -= 1
                if self._active_num <= 0:
                    self._state = ProbingState.NOT_ME
                    return self.state
        return self.state

    def get_confidence(self) -> float:
        state = self.state
        if state == ProbingState.FOUND_IT:
            return 0.99
        if state == ProbingState.NOT_ME:
            return 0.01
        best_conf = 0.0
        self._best_guess_prober = None
        for prober in self.probers:
            if not prober.active:
                self.logger.debug("%s not active", prober.charset_name)
                continue
            conf = prober.get_confidence()
            self.logger.debug(
                "%s %s confidence = %s", prober.charset_name, prober.language, conf
            )
            if best_conf < conf:
                best_conf = conf
                self._best_guess_prober = prober
        if not self._best_guess_prober:
            return 0.0
        return best_conf


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/macromanprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# This code was modified from latin1prober.py by Rob Speer <rob@lumino.so>.
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Rob Speer - adapt to MacRoman encoding
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import List, Union

from .charsetprober import CharSetProber
from .enums import ProbingState

FREQ_CAT_NUM = 4

UDF = 0  # undefined
OTH = 1  # other
ASC = 2  # ascii capital letter
ASS = 3  # ascii small letter
ACV = 4  # accent capital vowel
ACO = 5  # accent capital other
ASV = 6  # accent small vowel
ASO = 7  # accent small other
ODD = 8  # character that is unlikely to appear
CLASS_NUM = 9  # total classes

# The change from Latin1 is that we explicitly look for extended characters
# that are infrequently-occurring symbols, and consider them to always be
# improbable. This should let MacRoman get out of the way of more likely
# encodings in most situations.

# fmt: off
MacRoman_CharToClass = (
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 00 - 07
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 08 - 0F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 10 - 17
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 18 - 1F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 20 - 27
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 28 - 2F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 30 - 37
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # 38 - 3F
    OTH, ASC, ASC, ASC, ASC, ASC, ASC, ASC,  # 40 - 47
    ASC, ASC, ASC, ASC, ASC, ASC, ASC, ASC,  # 48 - 4F
    ASC, ASC, ASC, ASC, ASC, ASC, ASC, ASC,  # 50 - 57
    ASC, ASC, ASC, OTH, OTH, OTH, OTH, OTH,  # 58 - 5F
    OTH, ASS, ASS, ASS, ASS, ASS, ASS, ASS,  # 60 - 67
    ASS, ASS, ASS, ASS, ASS, ASS, ASS, ASS,  # 68 - 6F
    ASS, ASS, ASS, ASS, ASS, ASS, ASS, ASS,  # 70 - 77
    ASS, ASS, ASS, OTH, OTH, OTH, OTH, OTH,  # 78 - 7F
    ACV, ACV, ACO, ACV, ACO, ACV, ACV, ASV,  # 80 - 87
    ASV, ASV, ASV, ASV, ASV, ASO, ASV, ASV,  # 88 - 8F
    ASV, ASV, ASV, ASV, ASV, ASV, ASO, ASV,  # 90 - 97
    ASV, ASV, ASV, ASV, ASV, ASV, ASV, ASV,  # 98 - 9F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, ASO,  # A0 - A7
    OTH, OTH, ODD, ODD, OTH, OTH, ACV, ACV,  # A8 - AF
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,  # B0 - B7
    OTH, OTH, OTH, OTH, OTH, OTH, ASV, ASV,  # B8 - BF
    OTH, OTH, ODD, OTH, ODD, OTH, OTH, OTH,  # C0 - C7
    OTH, OTH, OTH, ACV, ACV, ACV, ACV, ASV,  # C8 - CF
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, ODD,  # D0 - D7
    ASV, ACV, ODD, OTH, OTH, OTH, OTH, OTH,  # D8 - DF
    OTH, OTH, OTH, OTH, OTH, ACV, ACV, ACV,  # E0 - E7
    ACV, ACV, ACV, ACV, ACV, ACV, ACV, ACV,  # E8 - EF
    ODD, ACV, ACV, ACV, ACV, ASV, ODD, ODD,  # F0 - F7
    ODD, ODD, ODD, ODD, ODD, ODD, ODD, ODD,  # F8 - FF
)

# 0 : illegal
# 1 : very unlikely
# 2 : normal
# 3 : very likely
MacRomanClassModel = (
# UDF OTH ASC ASS ACV ACO ASV ASO ODD
    0,  0,  0,  0,  0,  0,  0,  0,  0,  # UDF
    0,  3,  3,  3,  3,  3,  3,  3,  1,  # OTH
    0,  3,  3,  3,  3,  3,  3,  3,  1,  # ASC
    0,  3,  3,  3,  1,  1,  3,  3,  1,  # ASS
    0,  3,  3,  3,  1,  2,  1,  2,  1,  # ACV
    0,  3,  3,  3,  3,  3,  3,  3,  1,  # ACO
    0,  3,  1,  3,  1,  1,  1,  3,  1,  # ASV
    0,  3,  1,  3,  1,  1,  3,  3,  1,  # ASO
    0,  1,  1,  1,  1,  1,  1,  1,  1,  # ODD
)
# fmt: on


class MacRomanProber(CharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self._last_char_class = OTH
        self._freq_counter: List[int] = []
        self.reset()

    def reset(self) -> None:
        self._last_char_class = OTH
        self._freq_counter = [0] * FREQ_CAT_NUM

        # express the prior that MacRoman is a somewhat rare encoding;
        # this can be done by starting out in a slightly improbable state
        # that must be overcome
        self._freq_counter[2] = 10

        super().reset()

    @property
    def charset_name(self) -> str:
        return "MacRoman"

    @property
    def language(self) -> str:
        return ""

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        byte_str = self.remove_xml_tags(byte_str)
        for c in byte_str:
            char_class = MacRoman_CharToClass[c]
            freq = MacRomanClassModel[(self._last_char_class * CLASS_NUM) + char_class]
            if freq == 0:
                self._state = ProbingState.NOT_ME
                break
            self._freq_counter[freq] += 1
            self._last_char_class = char_class

        return self.state

    def get_confidence(self) -> float:
        if self.state == ProbingState.NOT_ME:
            return 0.01

        total = sum(self._freq_counter)
        confidence = (
            0.0
            if total < 0.01
            else (self._freq_counter[3] - self._freq_counter[1] * 20.0) / total
        )
        confidence = max(confidence, 0.0)
        # lower the confidence of MacRoman so that other more accurate
        # detector can take priority.
        confidence *= 0.73
        return confidence


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/sjisprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Union

from .chardistribution import SJISDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .enums import MachineState, ProbingState
from .jpcntx import SJISContextAnalysis
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import SJIS_SM_MODEL


class SJISProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(SJIS_SM_MODEL)
        self.distribution_analyzer = SJISDistributionAnalysis()
        self.context_analyzer = SJISContextAnalysis()
        self.reset()

    def reset(self) -> None:
        super().reset()
        self.context_analyzer.reset()

    @property
    def charset_name(self) -> str:
        return self.context_analyzer.charset_name

    @property
    def language(self) -> str:
        return "Japanese"

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        assert self.coding_sm is not None
        assert self.distribution_analyzer is not None

        for i, byte in enumerate(byte_str):
            coding_state = self.coding_sm.next_state(byte)
            if coding_state == MachineState.ERROR:
                self.logger.debug(
                    "%s %s prober hit error at byte %s",
                    self.charset_name,
                    self.language,
                    i,
                )
                self._state = ProbingState.NOT_ME
                break
            if coding_state == MachineState.ITS_ME:
                self._state = ProbingState.FOUND_IT
                break
            if coding_state == MachineState.START:
                char_len = self.coding_sm.get_current_charlen()
                if i == 0:
                    self._last_char[1] = byte
                    self.context_analyzer.feed(
                        self._last_char[2 - char_len :], char_len
                    )
                    self.distribution_analyzer.feed(self._last_char, char_len)
                else:
                    self.context_analyzer.feed(
                        byte_str[i + 1 - char_len : i + 3 - char_len], char_len
                    )
                    self.distribution_analyzer.feed(byte_str[i - 1 : i + 1], char_len)

        self._last_char[0] = byte_str[-1]

        if self.state == ProbingState.DETECTING:
            if self.context_analyzer.got_enough_data() and (
                self.get_confidence() > self.SHORTCUT_THRESHOLD
            ):
                self._state = ProbingState.FOUND_IT

        return self.state

    def get_confidence(self) -> float:
        assert self.distribution_analyzer is not None

        context_conf = self.context_analyzer.get_confidence()
        distrib_conf = self.distribution_analyzer.get_confidence()
        return max(context_conf, distrib_conf)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/gb2312prober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import GB2312DistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import GB2312_SM_MODEL


class GB2312Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(GB2312_SM_MODEL)
        self.distribution_analyzer = GB2312DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return "GB2312"

    @property
    def language(self) -> str:
        return "Chinese"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/escsm.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License,  or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not,  write to the Free Software
# Foundation,  Inc.,  51 Franklin St,  Fifth Floor,  Boston,  MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .codingstatemachinedict import CodingStateMachineDict
from .enums import MachineState

# fmt: off
HZ_CLS = (
    1, 0, 0, 0, 0, 0, 0, 0,  # 00 - 07
    0, 0, 0, 0, 0, 0, 0, 0,  # 08 - 0f
    0, 0, 0, 0, 0, 0, 0, 0,  # 10 - 17
    0, 0, 0, 1, 0, 0, 0, 0,  # 18 - 1f
    0, 0, 0, 0, 0, 0, 0, 0,  # 20 - 27
    0, 0, 0, 0, 0, 0, 0, 0,  # 28 - 2f
    0, 0, 0, 0, 0, 0, 0, 0,  # 30 - 37
    0, 0, 0, 0, 0, 0, 0, 0,  # 38 - 3f
    0, 0, 0, 0, 0, 0, 0, 0,  # 40 - 47
    0, 0, 0, 0, 0, 0, 0, 0,  # 48 - 4f
    0, 0, 0, 0, 0, 0, 0, 0,  # 50 - 57
    0, 0, 0, 0, 0, 0, 0, 0,  # 58 - 5f
    0, 0, 0, 0, 0, 0, 0, 0,  # 60 - 67
    0, 0, 0, 0, 0, 0, 0, 0,  # 68 - 6f
    0, 0, 0, 0, 0, 0, 0, 0,  # 70 - 77
    0, 0, 0, 4, 0, 5, 2, 0,  # 78 - 7f
    1, 1, 1, 1, 1, 1, 1, 1,  # 80 - 87
    1, 1, 1, 1, 1, 1, 1, 1,  # 88 - 8f
    1, 1, 1, 1, 1, 1, 1, 1,  # 90 - 97
    1, 1, 1, 1, 1, 1, 1, 1,  # 98 - 9f
    1, 1, 1, 1, 1, 1, 1, 1,  # a0 - a7
    1, 1, 1, 1, 1, 1, 1, 1,  # a8 - af
    1, 1, 1, 1, 1, 1, 1, 1,  # b0 - b7
    1, 1, 1, 1, 1, 1, 1, 1,  # b8 - bf
    1, 1, 1, 1, 1, 1, 1, 1,  # c0 - c7
    1, 1, 1, 1, 1, 1, 1, 1,  # c8 - cf
    1, 1, 1, 1, 1, 1, 1, 1,  # d0 - d7
    1, 1, 1, 1, 1, 1, 1, 1,  # d8 - df
    1, 1, 1, 1, 1, 1, 1, 1,  # e0 - e7
    1, 1, 1, 1, 1, 1, 1, 1,  # e8 - ef
    1, 1, 1, 1, 1, 1, 1, 1,  # f0 - f7
    1, 1, 1, 1, 1, 1, 1, 1,  # f8 - ff
)

HZ_ST = (
MachineState.START, MachineState.ERROR,      3, MachineState.START, MachineState.START, MachineState.START, MachineState.ERROR, MachineState.ERROR, # 00-07
MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, # 08-0f
MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, MachineState.START, MachineState.START,      4, MachineState.ERROR, # 10-17
     5, MachineState.ERROR,      6, MachineState.ERROR,      5,      5,      4, MachineState.ERROR, # 18-1f
     4, MachineState.ERROR,      4,      4,      4, MachineState.ERROR,      4, MachineState.ERROR, # 20-27
     4, MachineState.ITS_ME, MachineState.START, MachineState.START, MachineState.START, MachineState.START, MachineState.START, MachineState.START, # 28-2f
)
# fmt: on

HZ_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0)

HZ_SM_MODEL: CodingStateMachineDict = {
    "class_table": HZ_CLS,
    "class_factor": 6,
    "state_table": HZ_ST,
    "char_len_table": HZ_CHAR_LEN_TABLE,
    "name": "HZ-GB-2312",
    "language": "Chinese",
}

# fmt: off
ISO2022CN_CLS = (
    2, 0, 0, 0, 0, 0, 0, 0,  # 00 - 07
    0, 0, 0, 0, 0, 0, 0, 0,  # 08 - 0f
    0, 0, 0, 0, 0, 0, 0, 0,  # 10 - 17
    0, 0, 0, 1, 0, 0, 0, 0,  # 18 - 1f
    0, 0, 0, 0, 0, 0, 0, 0,  # 20 - 27
    0, 3, 0, 0, 0, 0, 0, 0,  # 28 - 2f
    0, 0, 0, 0, 0, 0, 0, 0,  # 30 - 37
    0, 0, 0, 0, 0, 0, 0, 0,  # 38 - 3f
    0, 0, 0, 4, 0, 0, 0, 0,  # 40 - 47
    0, 0, 0, 0, 0, 0, 0, 0,  # 48 - 4f
    0, 0, 0, 0, 0, 0, 0, 0,  # 50 - 57
    0, 0, 0, 0, 0, 0, 0, 0,  # 58 - 5f
    0, 0, 0, 0, 0, 0, 0, 0,  # 60 - 67
    0, 0, 0, 0, 0, 0, 0, 0,  # 68 - 6f
    0, 0, 0, 0, 0, 0, 0, 0,  # 70 - 77
    0, 0, 0, 0, 0, 0, 0, 0,  # 78 - 7f
    2, 2, 2, 2, 2, 2, 2, 2,  # 80 - 87
    2, 2, 2, 2, 2, 2, 2, 2,  # 88 - 8f
    2, 2, 2, 2, 2, 2, 2, 2,  # 90 - 97
    2, 2, 2, 2, 2, 2, 2, 2,  # 98 - 9f
    2, 2, 2, 2, 2, 2, 2, 2,  # a0 - a7
    2, 2, 2, 2, 2, 2, 2, 2,  # a8 - af
    2, 2, 2, 2, 2, 2, 2, 2,  # b0 - b7
    2, 2, 2, 2, 2, 2, 2, 2,  # b8 - bf
    2, 2, 2, 2, 2, 2, 2, 2,  # c0 - c7
    2, 2, 2, 2, 2, 2, 2, 2,  # c8 - cf
    2, 2, 2, 2, 2, 2, 2, 2,  # d0 - d7
    2, 2, 2, 2, 2, 2, 2, 2,  # d8 - df
    2, 2, 2, 2, 2, 2, 2, 2,  # e0 - e7
    2, 2, 2, 2, 2, 2, 2, 2,  # e8 - ef
    2, 2, 2, 2, 2, 2, 2, 2,  # f0 - f7
    2, 2, 2, 2, 2, 2, 2, 2,  # f8 - ff
)

ISO2022CN_ST = (
    MachineState.START,      3, MachineState.ERROR, MachineState.START, MachineState.START, MachineState.START, MachineState.START, MachineState.START, # 00-07
    MachineState.START, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 08-0f
    MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, # 10-17
    MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR,      4, MachineState.ERROR, # 18-1f
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 20-27
        5,      6, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 28-2f
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 30-37
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ERROR, MachineState.START, # 38-3f
)
# fmt: on

ISO2022CN_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0, 0, 0, 0)

ISO2022CN_SM_MODEL: CodingStateMachineDict = {
    "class_table": ISO2022CN_CLS,
    "class_factor": 9,
    "state_table": ISO2022CN_ST,
    "char_len_table": ISO2022CN_CHAR_LEN_TABLE,
    "name": "ISO-2022-CN",
    "language": "Chinese",
}

# fmt: off
ISO2022JP_CLS = (
    2, 0, 0, 0, 0, 0, 0, 0,  # 00 - 07
    0, 0, 0, 0, 0, 0, 2, 2,  # 08 - 0f
    0, 0, 0, 0, 0, 0, 0, 0,  # 10 - 17
    0, 0, 0, 1, 0, 0, 0, 0,  # 18 - 1f
    0, 0, 0, 0, 7, 0, 0, 0,  # 20 - 27
    3, 0, 0, 0, 0, 0, 0, 0,  # 28 - 2f
    0, 0, 0, 0, 0, 0, 0, 0,  # 30 - 37
    0, 0, 0, 0, 0, 0, 0, 0,  # 38 - 3f
    6, 0, 4, 0, 8, 0, 0, 0,  # 40 - 47
    0, 9, 5, 0, 0, 0, 0, 0,  # 48 - 4f
    0, 0, 0, 0, 0, 0, 0, 0,  # 50 - 57
    0, 0, 0, 0, 0, 0, 0, 0,  # 58 - 5f
    0, 0, 0, 0, 0, 0, 0, 0,  # 60 - 67
    0, 0, 0, 0, 0, 0, 0, 0,  # 68 - 6f
    0, 0, 0, 0, 0, 0, 0, 0,  # 70 - 77
    0, 0, 0, 0, 0, 0, 0, 0,  # 78 - 7f
    2, 2, 2, 2, 2, 2, 2, 2,  # 80 - 87
    2, 2, 2, 2, 2, 2, 2, 2,  # 88 - 8f
    2, 2, 2, 2, 2, 2, 2, 2,  # 90 - 97
    2, 2, 2, 2, 2, 2, 2, 2,  # 98 - 9f
    2, 2, 2, 2, 2, 2, 2, 2,  # a0 - a7
    2, 2, 2, 2, 2, 2, 2, 2,  # a8 - af
    2, 2, 2, 2, 2, 2, 2, 2,  # b0 - b7
    2, 2, 2, 2, 2, 2, 2, 2,  # b8 - bf
    2, 2, 2, 2, 2, 2, 2, 2,  # c0 - c7
    2, 2, 2, 2, 2, 2, 2, 2,  # c8 - cf
    2, 2, 2, 2, 2, 2, 2, 2,  # d0 - d7
    2, 2, 2, 2, 2, 2, 2, 2,  # d8 - df
    2, 2, 2, 2, 2, 2, 2, 2,  # e0 - e7
    2, 2, 2, 2, 2, 2, 2, 2,  # e8 - ef
    2, 2, 2, 2, 2, 2, 2, 2,  # f0 - f7
    2, 2, 2, 2, 2, 2, 2, 2,  # f8 - ff
)

ISO2022JP_ST = (
    MachineState.START,      3, MachineState.ERROR, MachineState.START, MachineState.START, MachineState.START, MachineState.START, MachineState.START, # 00-07
    MachineState.START, MachineState.START, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 08-0f
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, # 10-17
    MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, # 18-1f
    MachineState.ERROR,      5, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR,      4, MachineState.ERROR, MachineState.ERROR, # 20-27
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR,      6, MachineState.ITS_ME, MachineState.ERROR, MachineState.ITS_ME, MachineState.ERROR, # 28-2f
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ITS_ME, # 30-37
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 38-3f
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ERROR, MachineState.START, MachineState.START, # 40-47
)
# fmt: on

ISO2022JP_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

ISO2022JP_SM_MODEL: CodingStateMachineDict = {
    "class_table": ISO2022JP_CLS,
    "class_factor": 10,
    "state_table": ISO2022JP_ST,
    "char_len_table": ISO2022JP_CHAR_LEN_TABLE,
    "name": "ISO-2022-JP",
    "language": "Japanese",
}

# fmt: off
ISO2022KR_CLS = (
    2, 0, 0, 0, 0, 0, 0, 0,  # 00 - 07
    0, 0, 0, 0, 0, 0, 0, 0,  # 08 - 0f
    0, 0, 0, 0, 0, 0, 0, 0,  # 10 - 17
    0, 0, 0, 1, 0, 0, 0, 0,  # 18 - 1f
    0, 0, 0, 0, 3, 0, 0, 0,  # 20 - 27
    0, 4, 0, 0, 0, 0, 0, 0,  # 28 - 2f
    0, 0, 0, 0, 0, 0, 0, 0,  # 30 - 37
    0, 0, 0, 0, 0, 0, 0, 0,  # 38 - 3f
    0, 0, 0, 5, 0, 0, 0, 0,  # 40 - 47
    0, 0, 0, 0, 0, 0, 0, 0,  # 48 - 4f
    0, 0, 0, 0, 0, 0, 0, 0,  # 50 - 57
    0, 0, 0, 0, 0, 0, 0, 0,  # 58 - 5f
    0, 0, 0, 0, 0, 0, 0, 0,  # 60 - 67
    0, 0, 0, 0, 0, 0, 0, 0,  # 68 - 6f
    0, 0, 0, 0, 0, 0, 0, 0,  # 70 - 77
    0, 0, 0, 0, 0, 0, 0, 0,  # 78 - 7f
    2, 2, 2, 2, 2, 2, 2, 2,  # 80 - 87
    2, 2, 2, 2, 2, 2, 2, 2,  # 88 - 8f
    2, 2, 2, 2, 2, 2, 2, 2,  # 90 - 97
    2, 2, 2, 2, 2, 2, 2, 2,  # 98 - 9f
    2, 2, 2, 2, 2, 2, 2, 2,  # a0 - a7
    2, 2, 2, 2, 2, 2, 2, 2,  # a8 - af
    2, 2, 2, 2, 2, 2, 2, 2,  # b0 - b7
    2, 2, 2, 2, 2, 2, 2, 2,  # b8 - bf
    2, 2, 2, 2, 2, 2, 2, 2,  # c0 - c7
    2, 2, 2, 2, 2, 2, 2, 2,  # c8 - cf
    2, 2, 2, 2, 2, 2, 2, 2,  # d0 - d7
    2, 2, 2, 2, 2, 2, 2, 2,  # d8 - df
    2, 2, 2, 2, 2, 2, 2, 2,  # e0 - e7
    2, 2, 2, 2, 2, 2, 2, 2,  # e8 - ef
    2, 2, 2, 2, 2, 2, 2, 2,  # f0 - f7
    2, 2, 2, 2, 2, 2, 2, 2,  # f8 - ff
)

ISO2022KR_ST = (
    MachineState.START,      3, MachineState.ERROR, MachineState.START, MachineState.START, MachineState.START, MachineState.ERROR, MachineState.ERROR, # 00-07
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ITS_ME, # 08-0f
    MachineState.ITS_ME, MachineState.ITS_ME, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR,      4, MachineState.ERROR, MachineState.ERROR, # 10-17
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR,      5, MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, # 18-1f
    MachineState.ERROR, MachineState.ERROR, MachineState.ERROR, MachineState.ITS_ME, MachineState.START, MachineState.START, MachineState.START, MachineState.START, # 20-27
)
# fmt: on

ISO2022KR_CHAR_LEN_TABLE = (0, 0, 0, 0, 0, 0)

ISO2022KR_SM_MODEL: CodingStateMachineDict = {
    "class_table": ISO2022KR_CLS,
    "class_factor": 6,
    "state_table": ISO2022KR_ST,
    "char_len_table": ISO2022KR_CHAR_LEN_TABLE,
    "name": "ISO-2022-KR",
    "language": "Korean",
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langrussianmodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

RUSSIAN_LANG_MODEL = {
    37: {  # ''
        37: 0,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 1,  # ''
        51: 1,  # ''
        42: 1,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 2,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 1,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 1,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 1,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 0,  # ''
        24: 1,  # ''
        20: 1,  # ''
        4: 0,  # ''
        23: 1,  # ''
        11: 2,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 0,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 2,  # ''
        39: 2,  # ''
        26: 2,  # ''
        28: 0,  # ''
        22: 1,  # ''
        25: 2,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    44: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 1,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 2,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    33: {  # ''
        37: 2,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 1,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 2,  # ''
        21: 1,  # ''
        10: 1,  # ''
        19: 1,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 2,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 2,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 1,  # ''
        22: 2,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 1,  # ''
        18: 3,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 0,  # ''
        16: 1,  # ''
    },
    46: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 2,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 1,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 1,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    41: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 2,  # ''
        56: 1,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 2,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 3,  # ''
        20: 1,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 1,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    48: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 1,  # ''
        51: 1,  # ''
        42: 1,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 2,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 2,  # ''
        32: 2,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 1,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 0,  # ''
        21: 0,  # ''
        10: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 2,  # ''
        24: 1,  # ''
        20: 1,  # ''
        4: 0,  # ''
        23: 2,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 1,  # ''
        1: 0,  # ''
        15: 1,  # ''
        9: 1,  # ''
        7: 3,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 2,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    56: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 1,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 1,  # ''
        10: 0,  # ''
        19: 1,  # ''
        13: 1,  # ''
        2: 2,  # ''
        24: 1,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 1,  # ''
        5: 0,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 2,  # ''
        16: 0,  # ''
    },
    51: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 0,  # ''
        13: 2,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 1,  # ''
        12: 1,  # ''
        5: 2,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 1,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 1,  # ''
    },
    42: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 2,  # ''
        56: 1,  # ''
        51: 1,  # ''
        42: 1,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 2,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 1,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 1,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 2,  # ''
        4: 1,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 1,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 1,  # ''
        39: 1,  # ''
        26: 2,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    60: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 1,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 0,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    36: {  # ''
        37: 2,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 1,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 2,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 1,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 0,  # ''
        5: 1,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    49: {  # ''
        37: 2,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 1,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 1,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 0,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 0,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 1,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 1,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 1,  # ''
        12: 0,  # ''
        5: 1,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 2,  # ''
        16: 1,  # ''
    },
    38: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 1,  # ''
        55: 1,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 0,  # ''
        47: 1,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 1,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 1,  # ''
        12: 1,  # ''
        5: 2,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 1,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    31: {  # ''
        37: 2,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 1,  # ''
        42: 2,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 1,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 1,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 3,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 2,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    34: {  # ''
        37: 0,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 2,  # ''
        48: 1,  # ''
        56: 1,  # ''
        51: 1,  # ''
        42: 1,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 2,  # ''
        38: 1,  # ''
        31: 2,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 2,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 1,  # ''
        55: 1,  # ''
        58: 0,  # ''
        50: 1,  # ''
        57: 1,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 1,  # ''
        21: 2,  # ''
        10: 1,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 0,  # ''
        24: 1,  # ''
        20: 1,  # ''
        4: 0,  # ''
        23: 1,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 3,  # ''
        1: 0,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 1,  # ''
        39: 1,  # ''
        26: 2,  # ''
        28: 1,  # ''
        22: 2,  # ''
        25: 2,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    35: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 2,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 1,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 0,  # ''
        5: 1,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 3,  # ''
        7: 1,  # ''
        6: 1,  # ''
        14: 2,  # ''
        39: 1,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 2,  # ''
        30: 1,  # ''
        27: 0,  # ''
        16: 2,  # ''
    },
    45: {  # ''
        37: 2,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 2,  # ''
        56: 1,  # ''
        51: 0,  # ''
        42: 2,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 2,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 1,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 1,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 1,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 2,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 2,  # ''
    },
    32: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 2,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 1,  # ''
        57: 1,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 1,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 2,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 1,  # ''
        20: 1,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 2,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 1,  # ''
        6: 3,  # ''
        14: 2,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 1,  # ''
        22: 1,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 1,  # ''
        18: 1,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    40: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 2,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 1,  # ''
        47: 1,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 1,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 1,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    52: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 1,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 0,  # ''
        50: 1,  # ''
        57: 1,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 1,  # ''
        43: 0,  # ''
        3: 1,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        2: 1,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 2,  # ''
        23: 1,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 1,  # ''
        1: 2,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 0,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 1,  # ''
        22: 2,  # ''
        25: 1,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    53: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 0,  # ''
        6: 1,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    55: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 2,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 0,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 1,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 1,  # ''
        30: 1,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    58: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 1,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 1,  # ''
        21: 0,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 0,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 1,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    50: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 1,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 1,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 1,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 1,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 3,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    57: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 0,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 1,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 1,  # ''
        1: 2,  # ''
        15: 2,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 2,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    63: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 1,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 1,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 1,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 1,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 1,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 1,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    62: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 1,  # ''
        50: 0,  # ''
        57: 1,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 0,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 0,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    61: {  # ''
        37: 0,  # ''
        44: 1,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 0,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 1,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 1,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 1,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 1,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 0,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 0,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 0,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    47: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 1,  # ''
        36: 1,  # ''
        49: 1,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 1,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 1,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        2: 0,  # ''
        24: 1,  # ''
        20: 0,  # ''
        4: 0,  # ''
        23: 2,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 0,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 1,  # ''
        6: 3,  # ''
        14: 1,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    59: {  # ''
        37: 1,  # ''
        44: 1,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 1,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 0,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 1,  # ''
        10: 0,  # ''
        19: 1,  # ''
        13: 1,  # ''
        2: 0,  # ''
        24: 1,  # ''
        20: 0,  # ''
        4: 0,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 2,  # ''
        1: 0,  # ''
        15: 1,  # ''
        9: 1,  # ''
        7: 1,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    43: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 1,  # ''
        46: 1,  # ''
        41: 0,  # ''
        48: 1,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 1,  # ''
        58: 0,  # ''
        50: 1,  # ''
        57: 0,  # ''
        63: 1,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 1,  # ''
        43: 1,  # ''
        3: 0,  # ''
        21: 1,  # ''
        10: 1,  # ''
        19: 1,  # ''
        13: 1,  # ''
        2: 0,  # ''
        24: 0,  # ''
        20: 1,  # ''
        4: 0,  # ''
        23: 1,  # ''
        11: 1,  # ''
        8: 1,  # ''
        12: 1,  # ''
        5: 2,  # ''
        1: 0,  # ''
        15: 1,  # ''
        9: 1,  # ''
        7: 1,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 0,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    3: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 1,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 3,  # ''
        4: 3,  # ''
        23: 3,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 2,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 3,  # ''
        22: 3,  # ''
        25: 3,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 2,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    21: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 1,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 1,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 0,  # ''
        26: 2,  # ''
        28: 1,  # ''
        22: 1,  # ''
        25: 2,  # ''
        29: 3,  # ''
        54: 2,  # ''
        18: 3,  # ''
        17: 2,  # ''
        30: 1,  # ''
        27: 2,  # ''
        16: 3,  # ''
    },
    10: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 2,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 1,  # ''
        20: 3,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 2,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 3,  # ''
        29: 2,  # ''
        54: 2,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 3,  # ''
    },
    19: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 3,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 1,  # ''
        22: 2,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 1,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    13: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 3,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 2,  # ''
        28: 3,  # ''
        22: 2,  # ''
        25: 2,  # ''
        29: 1,  # ''
        54: 2,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 1,  # ''
        27: 2,  # ''
        16: 3,  # ''
    },
    2: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 3,  # ''
        4: 2,  # ''
        23: 3,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 2,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 3,  # ''
        22: 3,  # ''
        25: 3,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 2,  # ''
        16: 3,  # ''
    },
    24: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 1,  # ''
        19: 2,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 3,  # ''
        1: 2,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 1,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 0,  # ''
        28: 1,  # ''
        22: 2,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 2,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    20: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 3,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 1,  # ''
        22: 2,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 2,  # ''
        18: 3,  # ''
        17: 2,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 3,  # ''
    },
    4: {  # ''
        37: 1,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 3,  # ''
        4: 3,  # ''
        23: 3,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 2,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 3,  # ''
        22: 3,  # ''
        25: 3,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 2,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    23: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 1,  # ''
        21: 1,  # ''
        10: 1,  # ''
        19: 2,  # ''
        13: 3,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 2,  # ''
        4: 1,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 2,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 1,  # ''
        39: 2,  # ''
        26: 1,  # ''
        28: 2,  # ''
        22: 3,  # ''
        25: 2,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 2,  # ''
    },
    11: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 3,  # ''
        19: 1,  # ''
        13: 1,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 3,  # ''
        12: 1,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 2,  # ''
        28: 2,  # ''
        22: 1,  # ''
        25: 2,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 1,  # ''
        17: 1,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    8: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 3,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 1,  # ''
        7: 3,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 2,  # ''
        28: 1,  # ''
        22: 3,  # ''
        25: 2,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 1,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    12: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 2,  # ''
        13: 1,  # ''
        2: 3,  # ''
        24: 1,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 2,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 1,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 2,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 3,  # ''
    },
    5: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 2,  # ''
        28: 3,  # ''
        22: 3,  # ''
        25: 2,  # ''
        29: 2,  # ''
        54: 1,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 1,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    1: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 3,  # ''
        4: 3,  # ''
        23: 3,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 2,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 2,  # ''
        22: 3,  # ''
        25: 3,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 2,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    15: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 3,  # ''
        12: 1,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 3,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 0,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 1,  # ''
        29: 1,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 2,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 3,  # ''
    },
    9: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 2,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 3,  # ''
        29: 2,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 2,  # ''
        27: 2,  # ''
        16: 3,  # ''
    },
    7: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 1,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 2,  # ''
        22: 3,  # ''
        25: 2,  # ''
        29: 1,  # ''
        54: 2,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 2,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    6: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 2,  # ''
        10: 3,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 1,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 2,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 2,  # ''
        29: 2,  # ''
        54: 2,  # ''
        18: 3,  # ''
        17: 3,  # ''
        30: 2,  # ''
        27: 2,  # ''
        16: 3,  # ''
    },
    14: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 3,  # ''
        4: 2,  # ''
        23: 2,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 2,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 1,  # ''
        39: 2,  # ''
        26: 3,  # ''
        28: 2,  # ''
        22: 3,  # ''
        25: 3,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 2,  # ''
        27: 3,  # ''
        16: 2,  # ''
    },
    39: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 0,  # ''
        19: 1,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 1,  # ''
        1: 3,  # ''
        15: 1,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 2,  # ''
        39: 2,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 1,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 2,  # ''
        17: 1,  # ''
        30: 2,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    26: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 3,  # ''
        19: 1,  # ''
        13: 1,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 1,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 1,  # ''
        9: 3,  # ''
        7: 2,  # ''
        6: 2,  # ''
        14: 2,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 1,  # ''
        22: 1,  # ''
        25: 2,  # ''
        29: 0,  # ''
        54: 1,  # ''
        18: 0,  # ''
        17: 1,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    28: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 1,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 1,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 2,  # ''
        8: 1,  # ''
        12: 1,  # ''
        5: 1,  # ''
        1: 3,  # ''
        15: 0,  # ''
        9: 1,  # ''
        7: 0,  # ''
        6: 1,  # ''
        14: 3,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 1,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 3,  # ''
        17: 1,  # ''
        30: 0,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    22: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 1,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 2,  # ''
        12: 1,  # ''
        5: 3,  # ''
        1: 2,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 1,  # ''
        6: 3,  # ''
        14: 3,  # ''
        39: 1,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 1,  # ''
        25: 2,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 3,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    25: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 1,  # ''
        10: 2,  # ''
        19: 1,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 2,  # ''
        5: 3,  # ''
        1: 3,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 1,  # ''
        6: 2,  # ''
        14: 3,  # ''
        39: 2,  # ''
        26: 1,  # ''
        28: 1,  # ''
        22: 1,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 3,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 0,  # ''
    },
    29: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 3,  # ''
        21: 0,  # ''
        10: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 3,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 3,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 1,  # ''
        5: 2,  # ''
        1: 1,  # ''
        15: 0,  # ''
        9: 2,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 2,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 0,  # ''
    },
    54: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 0,  # ''
        10: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        20: 0,  # ''
        4: 0,  # ''
        23: 0,  # ''
        11: 0,  # ''
        8: 0,  # ''
        12: 0,  # ''
        5: 0,  # ''
        1: 0,  # ''
        15: 0,  # ''
        9: 0,  # ''
        7: 0,  # ''
        6: 0,  # ''
        14: 0,  # ''
        39: 0,  # ''
        26: 0,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 0,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 1,  # ''
        16: 2,  # ''
    },
    18: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 3,  # ''
        10: 3,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 2,  # ''
        23: 3,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 1,  # ''
        15: 3,  # ''
        9: 3,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 1,  # ''
        39: 0,  # ''
        26: 3,  # ''
        28: 2,  # ''
        22: 3,  # ''
        25: 3,  # ''
        29: 2,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 0,  # ''
        16: 2,  # ''
    },
    17: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 2,  # ''
        10: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        2: 3,  # ''
        24: 1,  # ''
        20: 3,  # ''
        4: 2,  # ''
        23: 0,  # ''
        11: 3,  # ''
        8: 0,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 2,  # ''
        15: 2,  # ''
        9: 1,  # ''
        7: 3,  # ''
        6: 2,  # ''
        14: 0,  # ''
        39: 2,  # ''
        26: 1,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 3,  # ''
        29: 2,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 3,  # ''
        16: 3,  # ''
    },
    30: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 1,  # ''
        31: 1,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 1,  # ''
        32: 1,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 1,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 1,  # ''
        10: 1,  # ''
        19: 1,  # ''
        13: 2,  # ''
        2: 1,  # ''
        24: 0,  # ''
        20: 1,  # ''
        4: 0,  # ''
        23: 2,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 0,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 2,  # ''
        6: 3,  # ''
        14: 1,  # ''
        39: 2,  # ''
        26: 1,  # ''
        28: 0,  # ''
        22: 0,  # ''
        25: 1,  # ''
        29: 0,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 1,  # ''
        16: 1,  # ''
    },
    27: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 2,  # ''
        21: 3,  # ''
        10: 1,  # ''
        19: 2,  # ''
        13: 3,  # ''
        2: 1,  # ''
        24: 2,  # ''
        20: 2,  # ''
        4: 1,  # ''
        23: 1,  # ''
        11: 2,  # ''
        8: 2,  # ''
        12: 2,  # ''
        5: 2,  # ''
        1: 1,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 0,  # ''
        39: 1,  # ''
        26: 2,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 2,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        27: 2,  # ''
        16: 1,  # ''
    },
    16: {  # ''
        37: 0,  # ''
        44: 0,  # ''
        33: 0,  # ''
        46: 0,  # ''
        41: 0,  # ''
        48: 0,  # ''
        56: 0,  # ''
        51: 0,  # ''
        42: 0,  # ''
        60: 0,  # ''
        36: 0,  # ''
        49: 0,  # ''
        38: 0,  # ''
        31: 0,  # ''
        34: 0,  # ''
        35: 0,  # ''
        45: 0,  # ''
        32: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        58: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        63: 0,  # ''
        62: 0,  # ''
        61: 0,  # ''
        47: 0,  # ''
        59: 0,  # ''
        43: 0,  # ''
        3: 0,  # ''
        21: 2,  # ''
        10: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        20: 3,  # ''
        4: 2,  # ''
        23: 2,  # ''
        11: 3,  # ''
        8: 3,  # ''
        12: 3,  # ''
        5: 3,  # ''
        1: 0,  # ''
        15: 2,  # ''
        9: 2,  # ''
        7: 3,  # ''
        6: 3,  # ''
        14: 1,  # ''
        39: 1,  # ''
        26: 3,  # ''
        28: 2,  # ''
        22: 2,  # ''
        25: 2,  # ''
        29: 3,  # ''
        54: 0,  # ''
        18: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        27: 2,  # ''
        16: 2,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
IBM866_RUSSIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 142,  # 'A'
    66: 143,  # 'B'
    67: 144,  # 'C'
    68: 145,  # 'D'
    69: 146,  # 'E'
    70: 147,  # 'F'
    71: 148,  # 'G'
    72: 149,  # 'H'
    73: 150,  # 'I'
    74: 151,  # 'J'
    75: 152,  # 'K'
    76: 74,  # 'L'
    77: 153,  # 'M'
    78: 75,  # 'N'
    79: 154,  # 'O'
    80: 155,  # 'P'
    81: 156,  # 'Q'
    82: 157,  # 'R'
    83: 158,  # 'S'
    84: 159,  # 'T'
    85: 160,  # 'U'
    86: 161,  # 'V'
    87: 162,  # 'W'
    88: 163,  # 'X'
    89: 164,  # 'Y'
    90: 165,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 71,  # 'a'
    98: 172,  # 'b'
    99: 66,  # 'c'
    100: 173,  # 'd'
    101: 65,  # 'e'
    102: 174,  # 'f'
    103: 76,  # 'g'
    104: 175,  # 'h'
    105: 64,  # 'i'
    106: 176,  # 'j'
    107: 177,  # 'k'
    108: 77,  # 'l'
    109: 72,  # 'm'
    110: 178,  # 'n'
    111: 69,  # 'o'
    112: 67,  # 'p'
    113: 179,  # 'q'
    114: 78,  # 'r'
    115: 73,  # 's'
    116: 180,  # 't'
    117: 181,  # 'u'
    118: 79,  # 'v'
    119: 182,  # 'w'
    120: 183,  # 'x'
    121: 184,  # 'y'
    122: 185,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 37,  # ''
    129: 44,  # ''
    130: 33,  # ''
    131: 46,  # ''
    132: 41,  # ''
    133: 48,  # ''
    134: 56,  # ''
    135: 51,  # ''
    136: 42,  # ''
    137: 60,  # ''
    138: 36,  # ''
    139: 49,  # ''
    140: 38,  # ''
    141: 31,  # ''
    142: 34,  # ''
    143: 35,  # ''
    144: 45,  # ''
    145: 32,  # ''
    146: 40,  # ''
    147: 52,  # ''
    148: 53,  # ''
    149: 55,  # ''
    150: 58,  # ''
    151: 50,  # ''
    152: 57,  # ''
    153: 63,  # ''
    154: 70,  # ''
    155: 62,  # ''
    156: 61,  # ''
    157: 47,  # ''
    158: 59,  # ''
    159: 43,  # ''
    160: 3,  # ''
    161: 21,  # ''
    162: 10,  # ''
    163: 19,  # ''
    164: 13,  # ''
    165: 2,  # ''
    166: 24,  # ''
    167: 20,  # ''
    168: 4,  # ''
    169: 23,  # ''
    170: 11,  # ''
    171: 8,  # ''
    172: 12,  # ''
    173: 5,  # ''
    174: 1,  # ''
    175: 15,  # ''
    176: 191,  # ''
    177: 192,  # ''
    178: 193,  # ''
    179: 194,  # ''
    180: 195,  # ''
    181: 196,  # ''
    182: 197,  # ''
    183: 198,  # ''
    184: 199,  # ''
    185: 200,  # ''
    186: 201,  # ''
    187: 202,  # ''
    188: 203,  # ''
    189: 204,  # ''
    190: 205,  # ''
    191: 206,  # ''
    192: 207,  # ''
    193: 208,  # ''
    194: 209,  # ''
    195: 210,  # ''
    196: 211,  # ''
    197: 212,  # ''
    198: 213,  # ''
    199: 214,  # ''
    200: 215,  # ''
    201: 216,  # ''
    202: 217,  # ''
    203: 218,  # ''
    204: 219,  # ''
    205: 220,  # ''
    206: 221,  # ''
    207: 222,  # ''
    208: 223,  # ''
    209: 224,  # ''
    210: 225,  # ''
    211: 226,  # ''
    212: 227,  # ''
    213: 228,  # ''
    214: 229,  # ''
    215: 230,  # ''
    216: 231,  # ''
    217: 232,  # ''
    218: 233,  # ''
    219: 234,  # ''
    220: 235,  # ''
    221: 236,  # ''
    222: 237,  # ''
    223: 238,  # ''
    224: 9,  # ''
    225: 7,  # ''
    226: 6,  # ''
    227: 14,  # ''
    228: 39,  # ''
    229: 26,  # ''
    230: 28,  # ''
    231: 22,  # ''
    232: 25,  # ''
    233: 29,  # ''
    234: 54,  # ''
    235: 18,  # ''
    236: 17,  # ''
    237: 30,  # ''
    238: 27,  # ''
    239: 16,  # ''
    240: 239,  # ''
    241: 68,  # ''
    242: 240,  # ''
    243: 241,  # ''
    244: 242,  # ''
    245: 243,  # ''
    246: 244,  # ''
    247: 245,  # ''
    248: 246,  # ''
    249: 247,  # ''
    250: 248,  # ''
    251: 249,  # ''
    252: 250,  # ''
    253: 251,  # ''
    254: 252,  # ''
    255: 255,  # '\xa0'
}

IBM866_RUSSIAN_MODEL = SingleByteCharSetModel(
    charset_name="IBM866",
    language="Russian",
    char_to_order_map=IBM866_RUSSIAN_CHAR_TO_ORDER,
    language_model=RUSSIAN_LANG_MODEL,
    typical_positive_ratio=0.976601,
    keep_ascii_letters=False,
    alphabet="",
)

WINDOWS_1251_RUSSIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 142,  # 'A'
    66: 143,  # 'B'
    67: 144,  # 'C'
    68: 145,  # 'D'
    69: 146,  # 'E'
    70: 147,  # 'F'
    71: 148,  # 'G'
    72: 149,  # 'H'
    73: 150,  # 'I'
    74: 151,  # 'J'
    75: 152,  # 'K'
    76: 74,  # 'L'
    77: 153,  # 'M'
    78: 75,  # 'N'
    79: 154,  # 'O'
    80: 155,  # 'P'
    81: 156,  # 'Q'
    82: 157,  # 'R'
    83: 158,  # 'S'
    84: 159,  # 'T'
    85: 160,  # 'U'
    86: 161,  # 'V'
    87: 162,  # 'W'
    88: 163,  # 'X'
    89: 164,  # 'Y'
    90: 165,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 71,  # 'a'
    98: 172,  # 'b'
    99: 66,  # 'c'
    100: 173,  # 'd'
    101: 65,  # 'e'
    102: 174,  # 'f'
    103: 76,  # 'g'
    104: 175,  # 'h'
    105: 64,  # 'i'
    106: 176,  # 'j'
    107: 177,  # 'k'
    108: 77,  # 'l'
    109: 72,  # 'm'
    110: 178,  # 'n'
    111: 69,  # 'o'
    112: 67,  # 'p'
    113: 179,  # 'q'
    114: 78,  # 'r'
    115: 73,  # 's'
    116: 180,  # 't'
    117: 181,  # 'u'
    118: 79,  # 'v'
    119: 182,  # 'w'
    120: 183,  # 'x'
    121: 184,  # 'y'
    122: 185,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 191,  # ''
    129: 192,  # ''
    130: 193,  # ''
    131: 194,  # ''
    132: 195,  # ''
    133: 196,  # ''
    134: 197,  # ''
    135: 198,  # ''
    136: 199,  # ''
    137: 200,  # ''
    138: 201,  # ''
    139: 202,  # ''
    140: 203,  # ''
    141: 204,  # ''
    142: 205,  # ''
    143: 206,  # ''
    144: 207,  # ''
    145: 208,  # ''
    146: 209,  # ''
    147: 210,  # ''
    148: 211,  # ''
    149: 212,  # ''
    150: 213,  # ''
    151: 214,  # ''
    152: 215,  # None
    153: 216,  # ''
    154: 217,  # ''
    155: 218,  # ''
    156: 219,  # ''
    157: 220,  # ''
    158: 221,  # ''
    159: 222,  # ''
    160: 223,  # '\xa0'
    161: 224,  # ''
    162: 225,  # ''
    163: 226,  # ''
    164: 227,  # ''
    165: 228,  # ''
    166: 229,  # ''
    167: 230,  # ''
    168: 231,  # ''
    169: 232,  # ''
    170: 233,  # ''
    171: 234,  # ''
    172: 235,  # ''
    173: 236,  # '\xad'
    174: 237,  # ''
    175: 238,  # ''
    176: 239,  # ''
    177: 240,  # ''
    178: 241,  # ''
    179: 242,  # ''
    180: 243,  # ''
    181: 244,  # ''
    182: 245,  # ''
    183: 246,  # ''
    184: 68,  # ''
    185: 247,  # ''
    186: 248,  # ''
    187: 249,  # ''
    188: 250,  # ''
    189: 251,  # ''
    190: 252,  # ''
    191: 253,  # ''
    192: 37,  # ''
    193: 44,  # ''
    194: 33,  # ''
    195: 46,  # ''
    196: 41,  # ''
    197: 48,  # ''
    198: 56,  # ''
    199: 51,  # ''
    200: 42,  # ''
    201: 60,  # ''
    202: 36,  # ''
    203: 49,  # ''
    204: 38,  # ''
    205: 31,  # ''
    206: 34,  # ''
    207: 35,  # ''
    208: 45,  # ''
    209: 32,  # ''
    210: 40,  # ''
    211: 52,  # ''
    212: 53,  # ''
    213: 55,  # ''
    214: 58,  # ''
    215: 50,  # ''
    216: 57,  # ''
    217: 63,  # ''
    218: 70,  # ''
    219: 62,  # ''
    220: 61,  # ''
    221: 47,  # ''
    222: 59,  # ''
    223: 43,  # ''
    224: 3,  # ''
    225: 21,  # ''
    226: 10,  # ''
    227: 19,  # ''
    228: 13,  # ''
    229: 2,  # ''
    230: 24,  # ''
    231: 20,  # ''
    232: 4,  # ''
    233: 23,  # ''
    234: 11,  # ''
    235: 8,  # ''
    236: 12,  # ''
    237: 5,  # ''
    238: 1,  # ''
    239: 15,  # ''
    240: 9,  # ''
    241: 7,  # ''
    242: 6,  # ''
    243: 14,  # ''
    244: 39,  # ''
    245: 26,  # ''
    246: 28,  # ''
    247: 22,  # ''
    248: 25,  # ''
    249: 29,  # ''
    250: 54,  # ''
    251: 18,  # ''
    252: 17,  # ''
    253: 30,  # ''
    254: 27,  # ''
    255: 16,  # ''
}

WINDOWS_1251_RUSSIAN_MODEL = SingleByteCharSetModel(
    charset_name="windows-1251",
    language="Russian",
    char_to_order_map=WINDOWS_1251_RUSSIAN_CHAR_TO_ORDER,
    language_model=RUSSIAN_LANG_MODEL,
    typical_positive_ratio=0.976601,
    keep_ascii_letters=False,
    alphabet="",
)

IBM855_RUSSIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 142,  # 'A'
    66: 143,  # 'B'
    67: 144,  # 'C'
    68: 145,  # 'D'
    69: 146,  # 'E'
    70: 147,  # 'F'
    71: 148,  # 'G'
    72: 149,  # 'H'
    73: 150,  # 'I'
    74: 151,  # 'J'
    75: 152,  # 'K'
    76: 74,  # 'L'
    77: 153,  # 'M'
    78: 75,  # 'N'
    79: 154,  # 'O'
    80: 155,  # 'P'
    81: 156,  # 'Q'
    82: 157,  # 'R'
    83: 158,  # 'S'
    84: 159,  # 'T'
    85: 160,  # 'U'
    86: 161,  # 'V'
    87: 162,  # 'W'
    88: 163,  # 'X'
    89: 164,  # 'Y'
    90: 165,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 71,  # 'a'
    98: 172,  # 'b'
    99: 66,  # 'c'
    100: 173,  # 'd'
    101: 65,  # 'e'
    102: 174,  # 'f'
    103: 76,  # 'g'
    104: 175,  # 'h'
    105: 64,  # 'i'
    106: 176,  # 'j'
    107: 177,  # 'k'
    108: 77,  # 'l'
    109: 72,  # 'm'
    110: 178,  # 'n'
    111: 69,  # 'o'
    112: 67,  # 'p'
    113: 179,  # 'q'
    114: 78,  # 'r'
    115: 73,  # 's'
    116: 180,  # 't'
    117: 181,  # 'u'
    118: 79,  # 'v'
    119: 182,  # 'w'
    120: 183,  # 'x'
    121: 184,  # 'y'
    122: 185,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 191,  # ''
    129: 192,  # ''
    130: 193,  # ''
    131: 194,  # ''
    132: 68,  # ''
    133: 195,  # ''
    134: 196,  # ''
    135: 197,  # ''
    136: 198,  # ''
    137: 199,  # ''
    138: 200,  # ''
    139: 201,  # ''
    140: 202,  # ''
    141: 203,  # ''
    142: 204,  # ''
    143: 205,  # ''
    144: 206,  # ''
    145: 207,  # ''
    146: 208,  # ''
    147: 209,  # ''
    148: 210,  # ''
    149: 211,  # ''
    150: 212,  # ''
    151: 213,  # ''
    152: 214,  # ''
    153: 215,  # ''
    154: 216,  # ''
    155: 217,  # ''
    156: 27,  # ''
    157: 59,  # ''
    158: 54,  # ''
    159: 70,  # ''
    160: 3,  # ''
    161: 37,  # ''
    162: 21,  # ''
    163: 44,  # ''
    164: 28,  # ''
    165: 58,  # ''
    166: 13,  # ''
    167: 41,  # ''
    168: 2,  # ''
    169: 48,  # ''
    170: 39,  # ''
    171: 53,  # ''
    172: 19,  # ''
    173: 46,  # ''
    174: 218,  # ''
    175: 219,  # ''
    176: 220,  # ''
    177: 221,  # ''
    178: 222,  # ''
    179: 223,  # ''
    180: 224,  # ''
    181: 26,  # ''
    182: 55,  # ''
    183: 4,  # ''
    184: 42,  # ''
    185: 225,  # ''
    186: 226,  # ''
    187: 227,  # ''
    188: 228,  # ''
    189: 23,  # ''
    190: 60,  # ''
    191: 229,  # ''
    192: 230,  # ''
    193: 231,  # ''
    194: 232,  # ''
    195: 233,  # ''
    196: 234,  # ''
    197: 235,  # ''
    198: 11,  # ''
    199: 36,  # ''
    200: 236,  # ''
    201: 237,  # ''
    202: 238,  # ''
    203: 239,  # ''
    204: 240,  # ''
    205: 241,  # ''
    206: 242,  # ''
    207: 243,  # ''
    208: 8,  # ''
    209: 49,  # ''
    210: 12,  # ''
    211: 38,  # ''
    212: 5,  # ''
    213: 31,  # ''
    214: 1,  # ''
    215: 34,  # ''
    216: 15,  # ''
    217: 244,  # ''
    218: 245,  # ''
    219: 246,  # ''
    220: 247,  # ''
    221: 35,  # ''
    222: 16,  # ''
    223: 248,  # ''
    224: 43,  # ''
    225: 9,  # ''
    226: 45,  # ''
    227: 7,  # ''
    228: 32,  # ''
    229: 6,  # ''
    230: 40,  # ''
    231: 14,  # ''
    232: 52,  # ''
    233: 24,  # ''
    234: 56,  # ''
    235: 10,  # ''
    236: 33,  # ''
    237: 17,  # ''
    238: 61,  # ''
    239: 249,  # ''
    240: 250,  # '\xad'
    241: 18,  # ''
    242: 62,  # ''
    243: 20,  # ''
    244: 51,  # ''
    245: 25,  # ''
    246: 57,  # ''
    247: 30,  # ''
    248: 47,  # ''
    249: 29,  # ''
    250: 63,  # ''
    251: 22,  # ''
    252: 50,  # ''
    253: 251,  # ''
    254: 252,  # ''
    255: 255,  # '\xa0'
}

IBM855_RUSSIAN_MODEL = SingleByteCharSetModel(
    charset_name="IBM855",
    language="Russian",
    char_to_order_map=IBM855_RUSSIAN_CHAR_TO_ORDER,
    language_model=RUSSIAN_LANG_MODEL,
    typical_positive_ratio=0.976601,
    keep_ascii_letters=False,
    alphabet="",
)

KOI8_R_RUSSIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 142,  # 'A'
    66: 143,  # 'B'
    67: 144,  # 'C'
    68: 145,  # 'D'
    69: 146,  # 'E'
    70: 147,  # 'F'
    71: 148,  # 'G'
    72: 149,  # 'H'
    73: 150,  # 'I'
    74: 151,  # 'J'
    75: 152,  # 'K'
    76: 74,  # 'L'
    77: 153,  # 'M'
    78: 75,  # 'N'
    79: 154,  # 'O'
    80: 155,  # 'P'
    81: 156,  # 'Q'
    82: 157,  # 'R'
    83: 158,  # 'S'
    84: 159,  # 'T'
    85: 160,  # 'U'
    86: 161,  # 'V'
    87: 162,  # 'W'
    88: 163,  # 'X'
    89: 164,  # 'Y'
    90: 165,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 71,  # 'a'
    98: 172,  # 'b'
    99: 66,  # 'c'
    100: 173,  # 'd'
    101: 65,  # 'e'
    102: 174,  # 'f'
    103: 76,  # 'g'
    104: 175,  # 'h'
    105: 64,  # 'i'
    106: 176,  # 'j'
    107: 177,  # 'k'
    108: 77,  # 'l'
    109: 72,  # 'm'
    110: 178,  # 'n'
    111: 69,  # 'o'
    112: 67,  # 'p'
    113: 179,  # 'q'
    114: 78,  # 'r'
    115: 73,  # 's'
    116: 180,  # 't'
    117: 181,  # 'u'
    118: 79,  # 'v'
    119: 182,  # 'w'
    120: 183,  # 'x'
    121: 184,  # 'y'
    122: 185,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 191,  # ''
    129: 192,  # ''
    130: 193,  # ''
    131: 194,  # ''
    132: 195,  # ''
    133: 196,  # ''
    134: 197,  # ''
    135: 198,  # ''
    136: 199,  # ''
    137: 200,  # ''
    138: 201,  # ''
    139: 202,  # ''
    140: 203,  # ''
    141: 204,  # ''
    142: 205,  # ''
    143: 206,  # ''
    144: 207,  # ''
    145: 208,  # ''
    146: 209,  # ''
    147: 210,  # ''
    148: 211,  # ''
    149: 212,  # ''
    150: 213,  # ''
    151: 214,  # ''
    152: 215,  # ''
    153: 216,  # ''
    154: 217,  # '\xa0'
    155: 218,  # ''
    156: 219,  # ''
    157: 220,  # ''
    158: 221,  # ''
    159: 222,  # ''
    160: 223,  # ''
    161: 224,  # ''
    162: 225,  # ''
    163: 68,  # ''
    164: 226,  # ''
    165: 227,  # ''
    166: 228,  # ''
    167: 229,  # ''
    168: 230,  # ''
    169: 231,  # ''
    170: 232,  # ''
    171: 233,  # ''
    172: 234,  # ''
    173: 235,  # ''
    174: 236,  # ''
    175: 237,  # ''
    176: 238,  # ''
    177: 239,  # ''
    178: 240,  # ''
    179: 241,  # ''
    180: 242,  # ''
    181: 243,  # ''
    182: 244,  # ''
    183: 245,  # ''
    184: 246,  # ''
    185: 247,  # ''
    186: 248,  # ''
    187: 249,  # ''
    188: 250,  # ''
    189: 251,  # ''
    190: 252,  # ''
    191: 253,  # ''
    192: 27,  # ''
    193: 3,  # ''
    194: 21,  # ''
    195: 28,  # ''
    196: 13,  # ''
    197: 2,  # ''
    198: 39,  # ''
    199: 19,  # ''
    200: 26,  # ''
    201: 4,  # ''
    202: 23,  # ''
    203: 11,  # ''
    204: 8,  # ''
    205: 12,  # ''
    206: 5,  # ''
    207: 1,  # ''
    208: 15,  # ''
    209: 16,  # ''
    210: 9,  # ''
    211: 7,  # ''
    212: 6,  # ''
    213: 14,  # ''
    214: 24,  # ''
    215: 10,  # ''
    216: 17,  # ''
    217: 18,  # ''
    218: 20,  # ''
    219: 25,  # ''
    220: 30,  # ''
    221: 29,  # ''
    222: 22,  # ''
    223: 54,  # ''
    224: 59,  # ''
    225: 37,  # ''
    226: 44,  # ''
    227: 58,  # ''
    228: 41,  # ''
    229: 48,  # ''
    230: 53,  # ''
    231: 46,  # ''
    232: 55,  # ''
    233: 42,  # ''
    234: 60,  # ''
    235: 36,  # ''
    236: 49,  # ''
    237: 38,  # ''
    238: 31,  # ''
    239: 34,  # ''
    240: 35,  # ''
    241: 43,  # ''
    242: 45,  # ''
    243: 32,  # ''
    244: 40,  # ''
    245: 52,  # ''
    246: 56,  # ''
    247: 33,  # ''
    248: 61,  # ''
    249: 62,  # ''
    250: 51,  # ''
    251: 57,  # ''
    252: 47,  # ''
    253: 63,  # ''
    254: 50,  # ''
    255: 70,  # ''
}

KOI8_R_RUSSIAN_MODEL = SingleByteCharSetModel(
    charset_name="KOI8-R",
    language="Russian",
    char_to_order_map=KOI8_R_RUSSIAN_CHAR_TO_ORDER,
    language_model=RUSSIAN_LANG_MODEL,
    typical_positive_ratio=0.976601,
    keep_ascii_letters=False,
    alphabet="",
)

MACCYRILLIC_RUSSIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 142,  # 'A'
    66: 143,  # 'B'
    67: 144,  # 'C'
    68: 145,  # 'D'
    69: 146,  # 'E'
    70: 147,  # 'F'
    71: 148,  # 'G'
    72: 149,  # 'H'
    73: 150,  # 'I'
    74: 151,  # 'J'
    75: 152,  # 'K'
    76: 74,  # 'L'
    77: 153,  # 'M'
    78: 75,  # 'N'
    79: 154,  # 'O'
    80: 155,  # 'P'
    81: 156,  # 'Q'
    82: 157,  # 'R'
    83: 158,  # 'S'
    84: 159,  # 'T'
    85: 160,  # 'U'
    86: 161,  # 'V'
    87: 162,  # 'W'
    88: 163,  # 'X'
    89: 164,  # 'Y'
    90: 165,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 71,  # 'a'
    98: 172,  # 'b'
    99: 66,  # 'c'
    100: 173,  # 'd'
    101: 65,  # 'e'
    102: 174,  # 'f'
    103: 76,  # 'g'
    104: 175,  # 'h'
    105: 64,  # 'i'
    106: 176,  # 'j'
    107: 177,  # 'k'
    108: 77,  # 'l'
    109: 72,  # 'm'
    110: 178,  # 'n'
    111: 69,  # 'o'
    112: 67,  # 'p'
    113: 179,  # 'q'
    114: 78,  # 'r'
    115: 73,  # 's'
    116: 180,  # 't'
    117: 181,  # 'u'
    118: 79,  # 'v'
    119: 182,  # 'w'
    120: 183,  # 'x'
    121: 184,  # 'y'
    122: 185,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 37,  # ''
    129: 44,  # ''
    130: 33,  # ''
    131: 46,  # ''
    132: 41,  # ''
    133: 48,  # ''
    134: 56,  # ''
    135: 51,  # ''
    136: 42,  # ''
    137: 60,  # ''
    138: 36,  # ''
    139: 49,  # ''
    140: 38,  # ''
    141: 31,  # ''
    142: 34,  # ''
    143: 35,  # ''
    144: 45,  # ''
    145: 32,  # ''
    146: 40,  # ''
    147: 52,  # ''
    148: 53,  # ''
    149: 55,  # ''
    150: 58,  # ''
    151: 50,  # ''
    152: 57,  # ''
    153: 63,  # ''
    154: 70,  # ''
    155: 62,  # ''
    156: 61,  # ''
    157: 47,  # ''
    158: 59,  # ''
    159: 43,  # ''
    160: 191,  # ''
    161: 192,  # ''
    162: 193,  # ''
    163: 194,  # ''
    164: 195,  # ''
    165: 196,  # ''
    166: 197,  # ''
    167: 198,  # ''
    168: 199,  # ''
    169: 200,  # ''
    170: 201,  # ''
    171: 202,  # ''
    172: 203,  # ''
    173: 204,  # ''
    174: 205,  # ''
    175: 206,  # ''
    176: 207,  # ''
    177: 208,  # ''
    178: 209,  # ''
    179: 210,  # ''
    180: 211,  # ''
    181: 212,  # ''
    182: 213,  # ''
    183: 214,  # ''
    184: 215,  # ''
    185: 216,  # ''
    186: 217,  # ''
    187: 218,  # ''
    188: 219,  # ''
    189: 220,  # ''
    190: 221,  # ''
    191: 222,  # ''
    192: 223,  # ''
    193: 224,  # ''
    194: 225,  # ''
    195: 226,  # ''
    196: 227,  # ''
    197: 228,  # ''
    198: 229,  # ''
    199: 230,  # ''
    200: 231,  # ''
    201: 232,  # ''
    202: 233,  # '\xa0'
    203: 234,  # ''
    204: 235,  # ''
    205: 236,  # ''
    206: 237,  # ''
    207: 238,  # ''
    208: 239,  # ''
    209: 240,  # ''
    210: 241,  # ''
    211: 242,  # ''
    212: 243,  # ''
    213: 244,  # ''
    214: 245,  # ''
    215: 246,  # ''
    216: 247,  # ''
    217: 248,  # ''
    218: 249,  # ''
    219: 250,  # ''
    220: 251,  # ''
    221: 252,  # ''
    222: 68,  # ''
    223: 16,  # ''
    224: 3,  # ''
    225: 21,  # ''
    226: 10,  # ''
    227: 19,  # ''
    228: 13,  # ''
    229: 2,  # ''
    230: 24,  # ''
    231: 20,  # ''
    232: 4,  # ''
    233: 23,  # ''
    234: 11,  # ''
    235: 8,  # ''
    236: 12,  # ''
    237: 5,  # ''
    238: 1,  # ''
    239: 15,  # ''
    240: 9,  # ''
    241: 7,  # ''
    242: 6,  # ''
    243: 14,  # ''
    244: 39,  # ''
    245: 26,  # ''
    246: 28,  # ''
    247: 22,  # ''
    248: 25,  # ''
    249: 29,  # ''
    250: 54,  # ''
    251: 18,  # ''
    252: 17,  # ''
    253: 30,  # ''
    254: 27,  # ''
    255: 255,  # ''
}

MACCYRILLIC_RUSSIAN_MODEL = SingleByteCharSetModel(
    charset_name="MacCyrillic",
    language="Russian",
    char_to_order_map=MACCYRILLIC_RUSSIAN_CHAR_TO_ORDER,
    language_model=RUSSIAN_LANG_MODEL,
    typical_positive_ratio=0.976601,
    keep_ascii_letters=False,
    alphabet="",
)

ISO_8859_5_RUSSIAN_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 142,  # 'A'
    66: 143,  # 'B'
    67: 144,  # 'C'
    68: 145,  # 'D'
    69: 146,  # 'E'
    70: 147,  # 'F'
    71: 148,  # 'G'
    72: 149,  # 'H'
    73: 150,  # 'I'
    74: 151,  # 'J'
    75: 152,  # 'K'
    76: 74,  # 'L'
    77: 153,  # 'M'
    78: 75,  # 'N'
    79: 154,  # 'O'
    80: 155,  # 'P'
    81: 156,  # 'Q'
    82: 157,  # 'R'
    83: 158,  # 'S'
    84: 159,  # 'T'
    85: 160,  # 'U'
    86: 161,  # 'V'
    87: 162,  # 'W'
    88: 163,  # 'X'
    89: 164,  # 'Y'
    90: 165,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 71,  # 'a'
    98: 172,  # 'b'
    99: 66,  # 'c'
    100: 173,  # 'd'
    101: 65,  # 'e'
    102: 174,  # 'f'
    103: 76,  # 'g'
    104: 175,  # 'h'
    105: 64,  # 'i'
    106: 176,  # 'j'
    107: 177,  # 'k'
    108: 77,  # 'l'
    109: 72,  # 'm'
    110: 178,  # 'n'
    111: 69,  # 'o'
    112: 67,  # 'p'
    113: 179,  # 'q'
    114: 78,  # 'r'
    115: 73,  # 's'
    116: 180,  # 't'
    117: 181,  # 'u'
    118: 79,  # 'v'
    119: 182,  # 'w'
    120: 183,  # 'x'
    121: 184,  # 'y'
    122: 185,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 191,  # '\x80'
    129: 192,  # '\x81'
    130: 193,  # '\x82'
    131: 194,  # '\x83'
    132: 195,  # '\x84'
    133: 196,  # '\x85'
    134: 197,  # '\x86'
    135: 198,  # '\x87'
    136: 199,  # '\x88'
    137: 200,  # '\x89'
    138: 201,  # '\x8a'
    139: 202,  # '\x8b'
    140: 203,  # '\x8c'
    141: 204,  # '\x8d'
    142: 205,  # '\x8e'
    143: 206,  # '\x8f'
    144: 207,  # '\x90'
    145: 208,  # '\x91'
    146: 209,  # '\x92'
    147: 210,  # '\x93'
    148: 211,  # '\x94'
    149: 212,  # '\x95'
    150: 213,  # '\x96'
    151: 214,  # '\x97'
    152: 215,  # '\x98'
    153: 216,  # '\x99'
    154: 217,  # '\x9a'
    155: 218,  # '\x9b'
    156: 219,  # '\x9c'
    157: 220,  # '\x9d'
    158: 221,  # '\x9e'
    159: 222,  # '\x9f'
    160: 223,  # '\xa0'
    161: 224,  # ''
    162: 225,  # ''
    163: 226,  # ''
    164: 227,  # ''
    165: 228,  # ''
    166: 229,  # ''
    167: 230,  # ''
    168: 231,  # ''
    169: 232,  # ''
    170: 233,  # ''
    171: 234,  # ''
    172: 235,  # ''
    173: 236,  # '\xad'
    174: 237,  # ''
    175: 238,  # ''
    176: 37,  # ''
    177: 44,  # ''
    178: 33,  # ''
    179: 46,  # ''
    180: 41,  # ''
    181: 48,  # ''
    182: 56,  # ''
    183: 51,  # ''
    184: 42,  # ''
    185: 60,  # ''
    186: 36,  # ''
    187: 49,  # ''
    188: 38,  # ''
    189: 31,  # ''
    190: 34,  # ''
    191: 35,  # ''
    192: 45,  # ''
    193: 32,  # ''
    194: 40,  # ''
    195: 52,  # ''
    196: 53,  # ''
    197: 55,  # ''
    198: 58,  # ''
    199: 50,  # ''
    200: 57,  # ''
    201: 63,  # ''
    202: 70,  # ''
    203: 62,  # ''
    204: 61,  # ''
    205: 47,  # ''
    206: 59,  # ''
    207: 43,  # ''
    208: 3,  # ''
    209: 21,  # ''
    210: 10,  # ''
    211: 19,  # ''
    212: 13,  # ''
    213: 2,  # ''
    214: 24,  # ''
    215: 20,  # ''
    216: 4,  # ''
    217: 23,  # ''
    218: 11,  # ''
    219: 8,  # ''
    220: 12,  # ''
    221: 5,  # ''
    222: 1,  # ''
    223: 15,  # ''
    224: 9,  # ''
    225: 7,  # ''
    226: 6,  # ''
    227: 14,  # ''
    228: 39,  # ''
    229: 26,  # ''
    230: 28,  # ''
    231: 22,  # ''
    232: 25,  # ''
    233: 29,  # ''
    234: 54,  # ''
    235: 18,  # ''
    236: 17,  # ''
    237: 30,  # ''
    238: 27,  # ''
    239: 16,  # ''
    240: 239,  # ''
    241: 68,  # ''
    242: 240,  # ''
    243: 241,  # ''
    244: 242,  # ''
    245: 243,  # ''
    246: 244,  # ''
    247: 245,  # ''
    248: 246,  # ''
    249: 247,  # ''
    250: 248,  # ''
    251: 249,  # ''
    252: 250,  # ''
    253: 251,  # ''
    254: 252,  # ''
    255: 255,  # ''
}

ISO_8859_5_RUSSIAN_MODEL = SingleByteCharSetModel(
    charset_name="ISO-8859-5",
    language="Russian",
    char_to_order_map=ISO_8859_5_RUSSIAN_CHAR_TO_ORDER,
    language_model=RUSSIAN_LANG_MODEL,
    typical_positive_ratio=0.976601,
    keep_ascii_letters=False,
    alphabet="",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langhebrewmodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

HEBREW_LANG_MODEL = {
    50: {  # 'a'
        50: 0,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 2,  # 'l'
        54: 2,  # 'n'
        49: 0,  # 'o'
        51: 2,  # 'r'
        43: 1,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 1,  # ''
        7: 0,  # ''
        10: 1,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    60: {  # 'c'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 0,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 0,  # 'n'
        49: 1,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    61: {  # 'd'
        50: 1,  # 'a'
        60: 0,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 1,  # 'n'
        49: 2,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 0,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 1,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    42: {  # 'e'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 2,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 2,  # 'l'
        54: 2,  # 'n'
        49: 1,  # 'o'
        51: 2,  # 'r'
        43: 2,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 1,  # ''
        52: 2,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    53: {  # 'i'
        50: 1,  # 'a'
        60: 2,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 0,  # 'i'
        56: 1,  # 'l'
        54: 2,  # 'n'
        49: 2,  # 'o'
        51: 1,  # 'r'
        43: 2,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    56: {  # 'l'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 2,  # 'e'
        53: 2,  # 'i'
        56: 2,  # 'l'
        54: 1,  # 'n'
        49: 1,  # 'o'
        51: 0,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    54: {  # 'n'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 1,  # 'n'
        49: 1,  # 'o'
        51: 0,  # 'r'
        43: 1,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 2,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    49: {  # 'o'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 2,  # 'n'
        49: 1,  # 'o'
        51: 2,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    51: {  # 'r'
        50: 2,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 2,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 1,  # 'n'
        49: 2,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 2,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    43: {  # 's'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 0,  # 'd'
        42: 2,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 1,  # 'n'
        49: 1,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 2,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    44: {  # 't'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 0,  # 'd'
        42: 2,  # 'e'
        53: 2,  # 'i'
        56: 1,  # 'l'
        54: 0,  # 'n'
        49: 1,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 1,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 2,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    63: {  # 'u'
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 1,  # 'n'
        49: 0,  # 'o'
        51: 1,  # 'r'
        43: 2,  # 's'
        44: 1,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    34: {  # '\xa0'
        50: 1,  # 'a'
        60: 0,  # 'c'
        61: 1,  # 'd'
        42: 0,  # 'e'
        53: 1,  # 'i'
        56: 0,  # 'l'
        54: 1,  # 'n'
        49: 1,  # 'o'
        51: 0,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 0,  # 'u'
        34: 2,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 1,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 2,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 2,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 1,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    55: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 1,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 2,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 1,  # ''
        12: 1,  # ''
        19: 1,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    48: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 1,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    39: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    57: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    30: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 1,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 1,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 2,  # ''
        16: 2,  # ''
        3: 2,  # ''
        2: 2,  # ''
        24: 2,  # ''
        14: 2,  # ''
        22: 2,  # ''
        1: 2,  # ''
        25: 2,  # ''
        15: 2,  # ''
        4: 2,  # ''
        11: 1,  # ''
        6: 2,  # ''
        23: 0,  # ''
        12: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        26: 0,  # ''
        18: 2,  # ''
        27: 0,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    59: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 1,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 1,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 2,  # ''
        11: 0,  # ''
        6: 2,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    41: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 2,  # ''
        20: 1,  # ''
        16: 2,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 1,  # ''
        25: 1,  # ''
        15: 1,  # ''
        4: 2,  # ''
        11: 0,  # ''
        6: 2,  # ''
        23: 0,  # ''
        12: 2,  # ''
        19: 1,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 2,  # ''
        17: 1,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    33: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 1,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 1,  # ''
        37: 0,  # ''
        36: 1,  # ''
        31: 0,  # ''
        29: 1,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 1,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 2,  # ''
        20: 2,  # ''
        16: 2,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 2,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 2,  # ''
        4: 2,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 2,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 2,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    37: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 1,  # ''
        16: 2,  # ''
        3: 2,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 2,  # ''
        22: 1,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 1,  # ''
        4: 2,  # ''
        11: 2,  # ''
        6: 1,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        26: 1,  # ''
        18: 1,  # ''
        27: 1,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    36: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 1,  # ''
        16: 2,  # ''
        3: 2,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 2,  # ''
        22: 1,  # ''
        1: 2,  # ''
        25: 2,  # ''
        15: 1,  # ''
        4: 2,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 2,  # ''
        13: 1,  # ''
        26: 1,  # ''
        18: 1,  # ''
        27: 2,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    31: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 1,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 1,  # ''
        31: 0,  # ''
        29: 2,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 2,  # ''
        16: 2,  # ''
        3: 2,  # ''
        2: 1,  # ''
        24: 2,  # ''
        14: 2,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 2,  # ''
        4: 2,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        26: 2,  # ''
        18: 2,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    29: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 1,  # ''
        29: 2,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 1,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 2,  # ''
        16: 2,  # ''
        3: 3,  # ''
        2: 2,  # ''
        24: 2,  # ''
        14: 2,  # ''
        22: 1,  # ''
        1: 2,  # ''
        25: 2,  # ''
        15: 2,  # ''
        4: 2,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        26: 1,  # ''
        18: 2,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    35: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 1,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 1,  # ''
        16: 2,  # ''
        3: 2,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 1,  # ''
        25: 1,  # ''
        15: 2,  # ''
        4: 2,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 2,  # ''
        13: 2,  # ''
        26: 1,  # ''
        18: 2,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    62: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 1,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 2,  # ''
        11: 1,  # ''
        6: 1,  # ''
        23: 1,  # ''
        12: 1,  # ''
        19: 1,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    28: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 3,  # ''
        59: 0,  # ''
        41: 1,  # ''
        33: 3,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 3,  # ''
        29: 3,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 0,  # ''
        38: 2,  # ''
        45: 1,  # ''
        9: 2,  # ''
        8: 2,  # ''
        20: 1,  # ''
        16: 2,  # ''
        3: 1,  # ''
        2: 2,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 2,  # ''
        25: 2,  # ''
        15: 2,  # ''
        4: 2,  # ''
        11: 1,  # ''
        6: 2,  # ''
        23: 1,  # ''
        12: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        26: 1,  # ''
        18: 1,  # ''
        27: 1,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 2,  # ''
        10: 2,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    38: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 2,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    45: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 1,  # ''
        36: 2,  # ''
        31: 1,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 1,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 2,  # ''
        24: 0,  # ''
        14: 1,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 1,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 0,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 0,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    9: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 2,  # ''
        41: 2,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 2,  # ''
        26: 3,  # ''
        18: 3,  # ''
        27: 1,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    8: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 1,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 3,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 1,  # ''
        18: 3,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 1,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    20: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 2,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 1,  # ''
        37: 1,  # ''
        36: 1,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 0,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        20: 2,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 2,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 1,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 2,  # ''
        27: 1,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    16: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 1,  # ''
        14: 2,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 2,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 0,  # ''
        21: 2,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    3: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 1,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 1,  # ''
        59: 1,  # ''
        41: 2,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 3,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 0,  # ''
        18: 3,  # ''
        27: 1,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 1,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    2: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 1,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 1,  # ''
        36: 1,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 3,  # ''
        62: 0,  # ''
        28: 3,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 3,  # ''
        18: 3,  # ''
        27: 3,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 1,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    24: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 1,  # ''
        33: 1,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 2,  # ''
        20: 2,  # ''
        16: 2,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        14: 2,  # ''
        22: 1,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 2,  # ''
        19: 1,  # ''
        13: 2,  # ''
        26: 1,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 2,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 1,  # ''
        5: 2,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    14: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 1,  # ''
        41: 2,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        20: 2,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 2,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 2,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 1,  # ''
        26: 2,  # ''
        18: 2,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    22: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 1,  # ''
        36: 1,  # ''
        31: 2,  # ''
        29: 1,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 1,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 1,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        14: 3,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 2,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 3,  # ''
        10: 2,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    1: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 1,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 3,  # ''
        18: 3,  # ''
        27: 3,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 1,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    25: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 2,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 1,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 1,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 1,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    15: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 3,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 2,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 2,  # ''
        26: 3,  # ''
        18: 3,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 2,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    4: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 3,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 1,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    11: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 1,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 0,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 1,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    6: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 0,  # ''
        18: 3,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    23: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 1,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 0,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 1,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 1,  # ''
        13: 1,  # ''
        26: 1,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 1,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 1,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    12: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    19: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 1,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 1,  # ''
        35: 1,  # ''
        62: 2,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 1,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        26: 3,  # ''
        18: 3,  # ''
        27: 0,  # ''
        21: 2,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 1,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    13: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 1,  # ''
        59: 1,  # ''
        41: 2,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 1,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 2,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 2,  # ''
        26: 1,  # ''
        18: 2,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    26: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 1,  # ''
        24: 0,  # ''
        14: 1,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 1,  # ''
        13: 0,  # ''
        26: 1,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 1,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    18: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 1,  # ''
        36: 2,  # ''
        31: 1,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 2,  # ''
        20: 3,  # ''
        16: 2,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 2,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 2,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    27: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 1,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 0,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    21: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 1,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 2,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 1,  # ''
        14: 3,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 1,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 1,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 2,  # ''
        21: 2,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 0,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    17: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 1,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 1,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 2,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 1,  # ''
        15: 1,  # ''
        4: 3,  # ''
        11: 2,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 2,  # ''
        21: 3,  # ''
        17: 2,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    7: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 2,  # ''
        48: 1,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 1,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 2,  # ''
        62: 1,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 3,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 3,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 3,  # ''
        21: 3,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    10: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 1,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 1,  # ''
        37: 1,  # ''
        36: 1,  # ''
        31: 1,  # ''
        29: 1,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 3,  # ''
        45: 2,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 3,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        14: 3,  # ''
        22: 3,  # ''
        1: 3,  # ''
        25: 3,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 2,  # ''
        12: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 1,  # ''
    },
    5: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 1,  # '\xa0'
        55: 0,  # ''
        48: 1,  # ''
        39: 1,  # ''
        57: 0,  # ''
        30: 2,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 2,  # ''
        37: 2,  # ''
        36: 2,  # ''
        31: 2,  # ''
        29: 2,  # ''
        35: 1,  # ''
        62: 1,  # ''
        28: 2,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        20: 3,  # ''
        16: 2,  # ''
        3: 3,  # ''
        2: 3,  # ''
        24: 2,  # ''
        14: 3,  # ''
        22: 2,  # ''
        1: 3,  # ''
        25: 2,  # ''
        15: 3,  # ''
        4: 3,  # ''
        11: 3,  # ''
        6: 3,  # ''
        23: 3,  # ''
        12: 3,  # ''
        19: 2,  # ''
        13: 3,  # ''
        26: 2,  # ''
        18: 3,  # ''
        27: 1,  # ''
        21: 2,  # ''
        17: 3,  # ''
        7: 3,  # ''
        10: 3,  # ''
        5: 3,  # ''
        32: 1,  # ''
        52: 1,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
    32: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 1,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 1,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 0,  # ''
        14: 1,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 1,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 1,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    52: {  # ''
        50: 1,  # 'a'
        60: 0,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 1,  # 'r'
        43: 2,  # 's'
        44: 2,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 1,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    47: {  # ''
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 1,  # 'l'
        54: 1,  # 'n'
        49: 1,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 1,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 1,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 1,  # ''
        22: 1,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 1,  # ''
        13: 1,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 1,  # ''
        17: 1,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    46: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 1,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 1,  # ''
        20: 1,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 1,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 0,  # ''
        40: 0,  # ''
    },
    58: {  # ''
        50: 0,  # 'a'
        60: 0,  # 'c'
        61: 0,  # 'd'
        42: 0,  # 'e'
        53: 0,  # 'i'
        56: 0,  # 'l'
        54: 0,  # 'n'
        49: 0,  # 'o'
        51: 0,  # 'r'
        43: 0,  # 's'
        44: 0,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 0,  # ''
        2: 0,  # ''
        24: 0,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 0,  # ''
        25: 0,  # ''
        15: 0,  # ''
        4: 0,  # ''
        11: 0,  # ''
        6: 0,  # ''
        23: 0,  # ''
        12: 0,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 0,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 0,  # ''
        10: 0,  # ''
        5: 0,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 0,  # ''
        58: 2,  # ''
        40: 0,  # ''
    },
    40: {  # ''
        50: 1,  # 'a'
        60: 1,  # 'c'
        61: 1,  # 'd'
        42: 1,  # 'e'
        53: 1,  # 'i'
        56: 0,  # 'l'
        54: 1,  # 'n'
        49: 0,  # 'o'
        51: 1,  # 'r'
        43: 1,  # 's'
        44: 1,  # 't'
        63: 0,  # 'u'
        34: 0,  # '\xa0'
        55: 0,  # ''
        48: 0,  # ''
        39: 0,  # ''
        57: 0,  # ''
        30: 0,  # ''
        59: 0,  # ''
        41: 0,  # ''
        33: 0,  # ''
        37: 0,  # ''
        36: 0,  # ''
        31: 0,  # ''
        29: 0,  # ''
        35: 0,  # ''
        62: 0,  # ''
        28: 0,  # ''
        38: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        8: 0,  # ''
        20: 0,  # ''
        16: 0,  # ''
        3: 1,  # ''
        2: 1,  # ''
        24: 1,  # ''
        14: 0,  # ''
        22: 0,  # ''
        1: 1,  # ''
        25: 0,  # ''
        15: 1,  # ''
        4: 1,  # ''
        11: 0,  # ''
        6: 1,  # ''
        23: 0,  # ''
        12: 1,  # ''
        19: 0,  # ''
        13: 0,  # ''
        26: 0,  # ''
        18: 1,  # ''
        27: 0,  # ''
        21: 0,  # ''
        17: 0,  # ''
        7: 1,  # ''
        10: 1,  # ''
        5: 1,  # ''
        32: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        46: 1,  # ''
        58: 0,  # ''
        40: 2,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
WINDOWS_1255_HEBREW_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 69,  # 'A'
    66: 91,  # 'B'
    67: 79,  # 'C'
    68: 80,  # 'D'
    69: 92,  # 'E'
    70: 89,  # 'F'
    71: 97,  # 'G'
    72: 90,  # 'H'
    73: 68,  # 'I'
    74: 111,  # 'J'
    75: 112,  # 'K'
    76: 82,  # 'L'
    77: 73,  # 'M'
    78: 95,  # 'N'
    79: 85,  # 'O'
    80: 78,  # 'P'
    81: 121,  # 'Q'
    82: 86,  # 'R'
    83: 71,  # 'S'
    84: 67,  # 'T'
    85: 102,  # 'U'
    86: 107,  # 'V'
    87: 84,  # 'W'
    88: 114,  # 'X'
    89: 103,  # 'Y'
    90: 115,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 50,  # 'a'
    98: 74,  # 'b'
    99: 60,  # 'c'
    100: 61,  # 'd'
    101: 42,  # 'e'
    102: 76,  # 'f'
    103: 70,  # 'g'
    104: 64,  # 'h'
    105: 53,  # 'i'
    106: 105,  # 'j'
    107: 93,  # 'k'
    108: 56,  # 'l'
    109: 65,  # 'm'
    110: 54,  # 'n'
    111: 49,  # 'o'
    112: 66,  # 'p'
    113: 110,  # 'q'
    114: 51,  # 'r'
    115: 43,  # 's'
    116: 44,  # 't'
    117: 63,  # 'u'
    118: 81,  # 'v'
    119: 77,  # 'w'
    120: 98,  # 'x'
    121: 75,  # 'y'
    122: 108,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 124,  # ''
    129: 202,  # None
    130: 203,  # ''
    131: 204,  # ''
    132: 205,  # ''
    133: 40,  # ''
    134: 58,  # ''
    135: 206,  # ''
    136: 207,  # ''
    137: 208,  # ''
    138: 209,  # None
    139: 210,  # ''
    140: 211,  # None
    141: 212,  # None
    142: 213,  # None
    143: 214,  # None
    144: 215,  # None
    145: 83,  # ''
    146: 52,  # ''
    147: 47,  # ''
    148: 46,  # ''
    149: 72,  # ''
    150: 32,  # ''
    151: 94,  # ''
    152: 216,  # ''
    153: 113,  # ''
    154: 217,  # None
    155: 109,  # ''
    156: 218,  # None
    157: 219,  # None
    158: 220,  # None
    159: 221,  # None
    160: 34,  # '\xa0'
    161: 116,  # ''
    162: 222,  # ''
    163: 118,  # ''
    164: 100,  # ''
    165: 223,  # ''
    166: 224,  # ''
    167: 117,  # ''
    168: 119,  # ''
    169: 104,  # ''
    170: 125,  # ''
    171: 225,  # ''
    172: 226,  # ''
    173: 87,  # '\xad'
    174: 99,  # ''
    175: 227,  # ''
    176: 106,  # ''
    177: 122,  # ''
    178: 123,  # ''
    179: 228,  # ''
    180: 55,  # ''
    181: 229,  # ''
    182: 230,  # ''
    183: 101,  # ''
    184: 231,  # ''
    185: 232,  # ''
    186: 120,  # ''
    187: 233,  # ''
    188: 48,  # ''
    189: 39,  # ''
    190: 57,  # ''
    191: 234,  # ''
    192: 30,  # ''
    193: 59,  # ''
    194: 41,  # ''
    195: 88,  # ''
    196: 33,  # ''
    197: 37,  # ''
    198: 36,  # ''
    199: 31,  # ''
    200: 29,  # ''
    201: 35,  # ''
    202: 235,  # None
    203: 62,  # ''
    204: 28,  # ''
    205: 236,  # ''
    206: 126,  # ''
    207: 237,  # ''
    208: 238,  # ''
    209: 38,  # ''
    210: 45,  # ''
    211: 239,  # ''
    212: 240,  # ''
    213: 241,  # ''
    214: 242,  # ''
    215: 243,  # ''
    216: 127,  # ''
    217: 244,  # None
    218: 245,  # None
    219: 246,  # None
    220: 247,  # None
    221: 248,  # None
    222: 249,  # None
    223: 250,  # None
    224: 9,  # ''
    225: 8,  # ''
    226: 20,  # ''
    227: 16,  # ''
    228: 3,  # ''
    229: 2,  # ''
    230: 24,  # ''
    231: 14,  # ''
    232: 22,  # ''
    233: 1,  # ''
    234: 25,  # ''
    235: 15,  # ''
    236: 4,  # ''
    237: 11,  # ''
    238: 6,  # ''
    239: 23,  # ''
    240: 12,  # ''
    241: 19,  # ''
    242: 13,  # ''
    243: 26,  # ''
    244: 18,  # ''
    245: 27,  # ''
    246: 21,  # ''
    247: 17,  # ''
    248: 7,  # ''
    249: 10,  # ''
    250: 5,  # ''
    251: 251,  # None
    252: 252,  # None
    253: 128,  # '\u200e'
    254: 96,  # '\u200f'
    255: 253,  # None
}

WINDOWS_1255_HEBREW_MODEL = SingleByteCharSetModel(
    charset_name="windows-1255",
    language="Hebrew",
    char_to_order_map=WINDOWS_1255_HEBREW_CHAR_TO_ORDER,
    language_model=HEBREW_LANG_MODEL,
    typical_positive_ratio=0.984004,
    keep_ascii_letters=False,
    alphabet="",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/johabfreq.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# The frequency data itself is the same as euc-kr.
# This is just a mapping table to euc-kr.

JOHAB_TO_EUCKR_ORDER_TABLE = {
    0x8861: 0,
    0x8862: 1,
    0x8865: 2,
    0x8868: 3,
    0x8869: 4,
    0x886A: 5,
    0x886B: 6,
    0x8871: 7,
    0x8873: 8,
    0x8874: 9,
    0x8875: 10,
    0x8876: 11,
    0x8877: 12,
    0x8878: 13,
    0x8879: 14,
    0x887B: 15,
    0x887C: 16,
    0x887D: 17,
    0x8881: 18,
    0x8882: 19,
    0x8885: 20,
    0x8889: 21,
    0x8891: 22,
    0x8893: 23,
    0x8895: 24,
    0x8896: 25,
    0x8897: 26,
    0x88A1: 27,
    0x88A2: 28,
    0x88A5: 29,
    0x88A9: 30,
    0x88B5: 31,
    0x88B7: 32,
    0x88C1: 33,
    0x88C5: 34,
    0x88C9: 35,
    0x88E1: 36,
    0x88E2: 37,
    0x88E5: 38,
    0x88E8: 39,
    0x88E9: 40,
    0x88EB: 41,
    0x88F1: 42,
    0x88F3: 43,
    0x88F5: 44,
    0x88F6: 45,
    0x88F7: 46,
    0x88F8: 47,
    0x88FB: 48,
    0x88FC: 49,
    0x88FD: 50,
    0x8941: 51,
    0x8945: 52,
    0x8949: 53,
    0x8951: 54,
    0x8953: 55,
    0x8955: 56,
    0x8956: 57,
    0x8957: 58,
    0x8961: 59,
    0x8962: 60,
    0x8963: 61,
    0x8965: 62,
    0x8968: 63,
    0x8969: 64,
    0x8971: 65,
    0x8973: 66,
    0x8975: 67,
    0x8976: 68,
    0x8977: 69,
    0x897B: 70,
    0x8981: 71,
    0x8985: 72,
    0x8989: 73,
    0x8993: 74,
    0x8995: 75,
    0x89A1: 76,
    0x89A2: 77,
    0x89A5: 78,
    0x89A8: 79,
    0x89A9: 80,
    0x89AB: 81,
    0x89AD: 82,
    0x89B0: 83,
    0x89B1: 84,
    0x89B3: 85,
    0x89B5: 86,
    0x89B7: 87,
    0x89B8: 88,
    0x89C1: 89,
    0x89C2: 90,
    0x89C5: 91,
    0x89C9: 92,
    0x89CB: 93,
    0x89D1: 94,
    0x89D3: 95,
    0x89D5: 96,
    0x89D7: 97,
    0x89E1: 98,
    0x89E5: 99,
    0x89E9: 100,
    0x89F3: 101,
    0x89F6: 102,
    0x89F7: 103,
    0x8A41: 104,
    0x8A42: 105,
    0x8A45: 106,
    0x8A49: 107,
    0x8A51: 108,
    0x8A53: 109,
    0x8A55: 110,
    0x8A57: 111,
    0x8A61: 112,
    0x8A65: 113,
    0x8A69: 114,
    0x8A73: 115,
    0x8A75: 116,
    0x8A81: 117,
    0x8A82: 118,
    0x8A85: 119,
    0x8A88: 120,
    0x8A89: 121,
    0x8A8A: 122,
    0x8A8B: 123,
    0x8A90: 124,
    0x8A91: 125,
    0x8A93: 126,
    0x8A95: 127,
    0x8A97: 128,
    0x8A98: 129,
    0x8AA1: 130,
    0x8AA2: 131,
    0x8AA5: 132,
    0x8AA9: 133,
    0x8AB6: 134,
    0x8AB7: 135,
    0x8AC1: 136,
    0x8AD5: 137,
    0x8AE1: 138,
    0x8AE2: 139,
    0x8AE5: 140,
    0x8AE9: 141,
    0x8AF1: 142,
    0x8AF3: 143,
    0x8AF5: 144,
    0x8B41: 145,
    0x8B45: 146,
    0x8B49: 147,
    0x8B61: 148,
    0x8B62: 149,
    0x8B65: 150,
    0x8B68: 151,
    0x8B69: 152,
    0x8B6A: 153,
    0x8B71: 154,
    0x8B73: 155,
    0x8B75: 156,
    0x8B77: 157,
    0x8B81: 158,
    0x8BA1: 159,
    0x8BA2: 160,
    0x8BA5: 161,
    0x8BA8: 162,
    0x8BA9: 163,
    0x8BAB: 164,
    0x8BB1: 165,
    0x8BB3: 166,
    0x8BB5: 167,
    0x8BB7: 168,
    0x8BB8: 169,
    0x8BBC: 170,
    0x8C61: 171,
    0x8C62: 172,
    0x8C63: 173,
    0x8C65: 174,
    0x8C69: 175,
    0x8C6B: 176,
    0x8C71: 177,
    0x8C73: 178,
    0x8C75: 179,
    0x8C76: 180,
    0x8C77: 181,
    0x8C7B: 182,
    0x8C81: 183,
    0x8C82: 184,
    0x8C85: 185,
    0x8C89: 186,
    0x8C91: 187,
    0x8C93: 188,
    0x8C95: 189,
    0x8C96: 190,
    0x8C97: 191,
    0x8CA1: 192,
    0x8CA2: 193,
    0x8CA9: 194,
    0x8CE1: 195,
    0x8CE2: 196,
    0x8CE3: 197,
    0x8CE5: 198,
    0x8CE9: 199,
    0x8CF1: 200,
    0x8CF3: 201,
    0x8CF5: 202,
    0x8CF6: 203,
    0x8CF7: 204,
    0x8D41: 205,
    0x8D42: 206,
    0x8D45: 207,
    0x8D51: 208,
    0x8D55: 209,
    0x8D57: 210,
    0x8D61: 211,
    0x8D65: 212,
    0x8D69: 213,
    0x8D75: 214,
    0x8D76: 215,
    0x8D7B: 216,
    0x8D81: 217,
    0x8DA1: 218,
    0x8DA2: 219,
    0x8DA5: 220,
    0x8DA7: 221,
    0x8DA9: 222,
    0x8DB1: 223,
    0x8DB3: 224,
    0x8DB5: 225,
    0x8DB7: 226,
    0x8DB8: 227,
    0x8DB9: 228,
    0x8DC1: 229,
    0x8DC2: 230,
    0x8DC9: 231,
    0x8DD6: 232,
    0x8DD7: 233,
    0x8DE1: 234,
    0x8DE2: 235,
    0x8DF7: 236,
    0x8E41: 237,
    0x8E45: 238,
    0x8E49: 239,
    0x8E51: 240,
    0x8E53: 241,
    0x8E57: 242,
    0x8E61: 243,
    0x8E81: 244,
    0x8E82: 245,
    0x8E85: 246,
    0x8E89: 247,
    0x8E90: 248,
    0x8E91: 249,
    0x8E93: 250,
    0x8E95: 251,
    0x8E97: 252,
    0x8E98: 253,
    0x8EA1: 254,
    0x8EA9: 255,
    0x8EB6: 256,
    0x8EB7: 257,
    0x8EC1: 258,
    0x8EC2: 259,
    0x8EC5: 260,
    0x8EC9: 261,
    0x8ED1: 262,
    0x8ED3: 263,
    0x8ED6: 264,
    0x8EE1: 265,
    0x8EE5: 266,
    0x8EE9: 267,
    0x8EF1: 268,
    0x8EF3: 269,
    0x8F41: 270,
    0x8F61: 271,
    0x8F62: 272,
    0x8F65: 273,
    0x8F67: 274,
    0x8F69: 275,
    0x8F6B: 276,
    0x8F70: 277,
    0x8F71: 278,
    0x8F73: 279,
    0x8F75: 280,
    0x8F77: 281,
    0x8F7B: 282,
    0x8FA1: 283,
    0x8FA2: 284,
    0x8FA5: 285,
    0x8FA9: 286,
    0x8FB1: 287,
    0x8FB3: 288,
    0x8FB5: 289,
    0x8FB7: 290,
    0x9061: 291,
    0x9062: 292,
    0x9063: 293,
    0x9065: 294,
    0x9068: 295,
    0x9069: 296,
    0x906A: 297,
    0x906B: 298,
    0x9071: 299,
    0x9073: 300,
    0x9075: 301,
    0x9076: 302,
    0x9077: 303,
    0x9078: 304,
    0x9079: 305,
    0x907B: 306,
    0x907D: 307,
    0x9081: 308,
    0x9082: 309,
    0x9085: 310,
    0x9089: 311,
    0x9091: 312,
    0x9093: 313,
    0x9095: 314,
    0x9096: 315,
    0x9097: 316,
    0x90A1: 317,
    0x90A2: 318,
    0x90A5: 319,
    0x90A9: 320,
    0x90B1: 321,
    0x90B7: 322,
    0x90E1: 323,
    0x90E2: 324,
    0x90E4: 325,
    0x90E5: 326,
    0x90E9: 327,
    0x90EB: 328,
    0x90EC: 329,
    0x90F1: 330,
    0x90F3: 331,
    0x90F5: 332,
    0x90F6: 333,
    0x90F7: 334,
    0x90FD: 335,
    0x9141: 336,
    0x9142: 337,
    0x9145: 338,
    0x9149: 339,
    0x9151: 340,
    0x9153: 341,
    0x9155: 342,
    0x9156: 343,
    0x9157: 344,
    0x9161: 345,
    0x9162: 346,
    0x9165: 347,
    0x9169: 348,
    0x9171: 349,
    0x9173: 350,
    0x9176: 351,
    0x9177: 352,
    0x917A: 353,
    0x9181: 354,
    0x9185: 355,
    0x91A1: 356,
    0x91A2: 357,
    0x91A5: 358,
    0x91A9: 359,
    0x91AB: 360,
    0x91B1: 361,
    0x91B3: 362,
    0x91B5: 363,
    0x91B7: 364,
    0x91BC: 365,
    0x91BD: 366,
    0x91C1: 367,
    0x91C5: 368,
    0x91C9: 369,
    0x91D6: 370,
    0x9241: 371,
    0x9245: 372,
    0x9249: 373,
    0x9251: 374,
    0x9253: 375,
    0x9255: 376,
    0x9261: 377,
    0x9262: 378,
    0x9265: 379,
    0x9269: 380,
    0x9273: 381,
    0x9275: 382,
    0x9277: 383,
    0x9281: 384,
    0x9282: 385,
    0x9285: 386,
    0x9288: 387,
    0x9289: 388,
    0x9291: 389,
    0x9293: 390,
    0x9295: 391,
    0x9297: 392,
    0x92A1: 393,
    0x92B6: 394,
    0x92C1: 395,
    0x92E1: 396,
    0x92E5: 397,
    0x92E9: 398,
    0x92F1: 399,
    0x92F3: 400,
    0x9341: 401,
    0x9342: 402,
    0x9349: 403,
    0x9351: 404,
    0x9353: 405,
    0x9357: 406,
    0x9361: 407,
    0x9362: 408,
    0x9365: 409,
    0x9369: 410,
    0x936A: 411,
    0x936B: 412,
    0x9371: 413,
    0x9373: 414,
    0x9375: 415,
    0x9377: 416,
    0x9378: 417,
    0x937C: 418,
    0x9381: 419,
    0x9385: 420,
    0x9389: 421,
    0x93A1: 422,
    0x93A2: 423,
    0x93A5: 424,
    0x93A9: 425,
    0x93AB: 426,
    0x93B1: 427,
    0x93B3: 428,
    0x93B5: 429,
    0x93B7: 430,
    0x93BC: 431,
    0x9461: 432,
    0x9462: 433,
    0x9463: 434,
    0x9465: 435,
    0x9468: 436,
    0x9469: 437,
    0x946A: 438,
    0x946B: 439,
    0x946C: 440,
    0x9470: 441,
    0x9471: 442,
    0x9473: 443,
    0x9475: 444,
    0x9476: 445,
    0x9477: 446,
    0x9478: 447,
    0x9479: 448,
    0x947D: 449,
    0x9481: 450,
    0x9482: 451,
    0x9485: 452,
    0x9489: 453,
    0x9491: 454,
    0x9493: 455,
    0x9495: 456,
    0x9496: 457,
    0x9497: 458,
    0x94A1: 459,
    0x94E1: 460,
    0x94E2: 461,
    0x94E3: 462,
    0x94E5: 463,
    0x94E8: 464,
    0x94E9: 465,
    0x94EB: 466,
    0x94EC: 467,
    0x94F1: 468,
    0x94F3: 469,
    0x94F5: 470,
    0x94F7: 471,
    0x94F9: 472,
    0x94FC: 473,
    0x9541: 474,
    0x9542: 475,
    0x9545: 476,
    0x9549: 477,
    0x9551: 478,
    0x9553: 479,
    0x9555: 480,
    0x9556: 481,
    0x9557: 482,
    0x9561: 483,
    0x9565: 484,
    0x9569: 485,
    0x9576: 486,
    0x9577: 487,
    0x9581: 488,
    0x9585: 489,
    0x95A1: 490,
    0x95A2: 491,
    0x95A5: 492,
    0x95A8: 493,
    0x95A9: 494,
    0x95AB: 495,
    0x95AD: 496,
    0x95B1: 497,
    0x95B3: 498,
    0x95B5: 499,
    0x95B7: 500,
    0x95B9: 501,
    0x95BB: 502,
    0x95C1: 503,
    0x95C5: 504,
    0x95C9: 505,
    0x95E1: 506,
    0x95F6: 507,
    0x9641: 508,
    0x9645: 509,
    0x9649: 510,
    0x9651: 511,
    0x9653: 512,
    0x9655: 513,
    0x9661: 514,
    0x9681: 515,
    0x9682: 516,
    0x9685: 517,
    0x9689: 518,
    0x9691: 519,
    0x9693: 520,
    0x9695: 521,
    0x9697: 522,
    0x96A1: 523,
    0x96B6: 524,
    0x96C1: 525,
    0x96D7: 526,
    0x96E1: 527,
    0x96E5: 528,
    0x96E9: 529,
    0x96F3: 530,
    0x96F5: 531,
    0x96F7: 532,
    0x9741: 533,
    0x9745: 534,
    0x9749: 535,
    0x9751: 536,
    0x9757: 537,
    0x9761: 538,
    0x9762: 539,
    0x9765: 540,
    0x9768: 541,
    0x9769: 542,
    0x976B: 543,
    0x9771: 544,
    0x9773: 545,
    0x9775: 546,
    0x9777: 547,
    0x9781: 548,
    0x97A1: 549,
    0x97A2: 550,
    0x97A5: 551,
    0x97A8: 552,
    0x97A9: 553,
    0x97B1: 554,
    0x97B3: 555,
    0x97B5: 556,
    0x97B6: 557,
    0x97B7: 558,
    0x97B8: 559,
    0x9861: 560,
    0x9862: 561,
    0x9865: 562,
    0x9869: 563,
    0x9871: 564,
    0x9873: 565,
    0x9875: 566,
    0x9876: 567,
    0x9877: 568,
    0x987D: 569,
    0x9881: 570,
    0x9882: 571,
    0x9885: 572,
    0x9889: 573,
    0x9891: 574,
    0x9893: 575,
    0x9895: 576,
    0x9896: 577,
    0x9897: 578,
    0x98E1: 579,
    0x98E2: 580,
    0x98E5: 581,
    0x98E9: 582,
    0x98EB: 583,
    0x98EC: 584,
    0x98F1: 585,
    0x98F3: 586,
    0x98F5: 587,
    0x98F6: 588,
    0x98F7: 589,
    0x98FD: 590,
    0x9941: 591,
    0x9942: 592,
    0x9945: 593,
    0x9949: 594,
    0x9951: 595,
    0x9953: 596,
    0x9955: 597,
    0x9956: 598,
    0x9957: 599,
    0x9961: 600,
    0x9976: 601,
    0x99A1: 602,
    0x99A2: 603,
    0x99A5: 604,
    0x99A9: 605,
    0x99B7: 606,
    0x99C1: 607,
    0x99C9: 608,
    0x99E1: 609,
    0x9A41: 610,
    0x9A45: 611,
    0x9A81: 612,
    0x9A82: 613,
    0x9A85: 614,
    0x9A89: 615,
    0x9A90: 616,
    0x9A91: 617,
    0x9A97: 618,
    0x9AC1: 619,
    0x9AE1: 620,
    0x9AE5: 621,
    0x9AE9: 622,
    0x9AF1: 623,
    0x9AF3: 624,
    0x9AF7: 625,
    0x9B61: 626,
    0x9B62: 627,
    0x9B65: 628,
    0x9B68: 629,
    0x9B69: 630,
    0x9B71: 631,
    0x9B73: 632,
    0x9B75: 633,
    0x9B81: 634,
    0x9B85: 635,
    0x9B89: 636,
    0x9B91: 637,
    0x9B93: 638,
    0x9BA1: 639,
    0x9BA5: 640,
    0x9BA9: 641,
    0x9BB1: 642,
    0x9BB3: 643,
    0x9BB5: 644,
    0x9BB7: 645,
    0x9C61: 646,
    0x9C62: 647,
    0x9C65: 648,
    0x9C69: 649,
    0x9C71: 650,
    0x9C73: 651,
    0x9C75: 652,
    0x9C76: 653,
    0x9C77: 654,
    0x9C78: 655,
    0x9C7C: 656,
    0x9C7D: 657,
    0x9C81: 658,
    0x9C82: 659,
    0x9C85: 660,
    0x9C89: 661,
    0x9C91: 662,
    0x9C93: 663,
    0x9C95: 664,
    0x9C96: 665,
    0x9C97: 666,
    0x9CA1: 667,
    0x9CA2: 668,
    0x9CA5: 669,
    0x9CB5: 670,
    0x9CB7: 671,
    0x9CE1: 672,
    0x9CE2: 673,
    0x9CE5: 674,
    0x9CE9: 675,
    0x9CF1: 676,
    0x9CF3: 677,
    0x9CF5: 678,
    0x9CF6: 679,
    0x9CF7: 680,
    0x9CFD: 681,
    0x9D41: 682,
    0x9D42: 683,
    0x9D45: 684,
    0x9D49: 685,
    0x9D51: 686,
    0x9D53: 687,
    0x9D55: 688,
    0x9D57: 689,
    0x9D61: 690,
    0x9D62: 691,
    0x9D65: 692,
    0x9D69: 693,
    0x9D71: 694,
    0x9D73: 695,
    0x9D75: 696,
    0x9D76: 697,
    0x9D77: 698,
    0x9D81: 699,
    0x9D85: 700,
    0x9D93: 701,
    0x9D95: 702,
    0x9DA1: 703,
    0x9DA2: 704,
    0x9DA5: 705,
    0x9DA9: 706,
    0x9DB1: 707,
    0x9DB3: 708,
    0x9DB5: 709,
    0x9DB7: 710,
    0x9DC1: 711,
    0x9DC5: 712,
    0x9DD7: 713,
    0x9DF6: 714,
    0x9E41: 715,
    0x9E45: 716,
    0x9E49: 717,
    0x9E51: 718,
    0x9E53: 719,
    0x9E55: 720,
    0x9E57: 721,
    0x9E61: 722,
    0x9E65: 723,
    0x9E69: 724,
    0x9E73: 725,
    0x9E75: 726,
    0x9E77: 727,
    0x9E81: 728,
    0x9E82: 729,
    0x9E85: 730,
    0x9E89: 731,
    0x9E91: 732,
    0x9E93: 733,
    0x9E95: 734,
    0x9E97: 735,
    0x9EA1: 736,
    0x9EB6: 737,
    0x9EC1: 738,
    0x9EE1: 739,
    0x9EE2: 740,
    0x9EE5: 741,
    0x9EE9: 742,
    0x9EF1: 743,
    0x9EF5: 744,
    0x9EF7: 745,
    0x9F41: 746,
    0x9F42: 747,
    0x9F45: 748,
    0x9F49: 749,
    0x9F51: 750,
    0x9F53: 751,
    0x9F55: 752,
    0x9F57: 753,
    0x9F61: 754,
    0x9F62: 755,
    0x9F65: 756,
    0x9F69: 757,
    0x9F71: 758,
    0x9F73: 759,
    0x9F75: 760,
    0x9F77: 761,
    0x9F78: 762,
    0x9F7B: 763,
    0x9F7C: 764,
    0x9FA1: 765,
    0x9FA2: 766,
    0x9FA5: 767,
    0x9FA9: 768,
    0x9FB1: 769,
    0x9FB3: 770,
    0x9FB5: 771,
    0x9FB7: 772,
    0xA061: 773,
    0xA062: 774,
    0xA065: 775,
    0xA067: 776,
    0xA068: 777,
    0xA069: 778,
    0xA06A: 779,
    0xA06B: 780,
    0xA071: 781,
    0xA073: 782,
    0xA075: 783,
    0xA077: 784,
    0xA078: 785,
    0xA07B: 786,
    0xA07D: 787,
    0xA081: 788,
    0xA082: 789,
    0xA085: 790,
    0xA089: 791,
    0xA091: 792,
    0xA093: 793,
    0xA095: 794,
    0xA096: 795,
    0xA097: 796,
    0xA098: 797,
    0xA0A1: 798,
    0xA0A2: 799,
    0xA0A9: 800,
    0xA0B7: 801,
    0xA0E1: 802,
    0xA0E2: 803,
    0xA0E5: 804,
    0xA0E9: 805,
    0xA0EB: 806,
    0xA0F1: 807,
    0xA0F3: 808,
    0xA0F5: 809,
    0xA0F7: 810,
    0xA0F8: 811,
    0xA0FD: 812,
    0xA141: 813,
    0xA142: 814,
    0xA145: 815,
    0xA149: 816,
    0xA151: 817,
    0xA153: 818,
    0xA155: 819,
    0xA156: 820,
    0xA157: 821,
    0xA161: 822,
    0xA162: 823,
    0xA165: 824,
    0xA169: 825,
    0xA175: 826,
    0xA176: 827,
    0xA177: 828,
    0xA179: 829,
    0xA181: 830,
    0xA1A1: 831,
    0xA1A2: 832,
    0xA1A4: 833,
    0xA1A5: 834,
    0xA1A9: 835,
    0xA1AB: 836,
    0xA1B1: 837,
    0xA1B3: 838,
    0xA1B5: 839,
    0xA1B7: 840,
    0xA1C1: 841,
    0xA1C5: 842,
    0xA1D6: 843,
    0xA1D7: 844,
    0xA241: 845,
    0xA245: 846,
    0xA249: 847,
    0xA253: 848,
    0xA255: 849,
    0xA257: 850,
    0xA261: 851,
    0xA265: 852,
    0xA269: 853,
    0xA273: 854,
    0xA275: 855,
    0xA281: 856,
    0xA282: 857,
    0xA283: 858,
    0xA285: 859,
    0xA288: 860,
    0xA289: 861,
    0xA28A: 862,
    0xA28B: 863,
    0xA291: 864,
    0xA293: 865,
    0xA295: 866,
    0xA297: 867,
    0xA29B: 868,
    0xA29D: 869,
    0xA2A1: 870,
    0xA2A5: 871,
    0xA2A9: 872,
    0xA2B3: 873,
    0xA2B5: 874,
    0xA2C1: 875,
    0xA2E1: 876,
    0xA2E5: 877,
    0xA2E9: 878,
    0xA341: 879,
    0xA345: 880,
    0xA349: 881,
    0xA351: 882,
    0xA355: 883,
    0xA361: 884,
    0xA365: 885,
    0xA369: 886,
    0xA371: 887,
    0xA375: 888,
    0xA3A1: 889,
    0xA3A2: 890,
    0xA3A5: 891,
    0xA3A8: 892,
    0xA3A9: 893,
    0xA3AB: 894,
    0xA3B1: 895,
    0xA3B3: 896,
    0xA3B5: 897,
    0xA3B6: 898,
    0xA3B7: 899,
    0xA3B9: 900,
    0xA3BB: 901,
    0xA461: 902,
    0xA462: 903,
    0xA463: 904,
    0xA464: 905,
    0xA465: 906,
    0xA468: 907,
    0xA469: 908,
    0xA46A: 909,
    0xA46B: 910,
    0xA46C: 911,
    0xA471: 912,
    0xA473: 913,
    0xA475: 914,
    0xA477: 915,
    0xA47B: 916,
    0xA481: 917,
    0xA482: 918,
    0xA485: 919,
    0xA489: 920,
    0xA491: 921,
    0xA493: 922,
    0xA495: 923,
    0xA496: 924,
    0xA497: 925,
    0xA49B: 926,
    0xA4A1: 927,
    0xA4A2: 928,
    0xA4A5: 929,
    0xA4B3: 930,
    0xA4E1: 931,
    0xA4E2: 932,
    0xA4E5: 933,
    0xA4E8: 934,
    0xA4E9: 935,
    0xA4EB: 936,
    0xA4F1: 937,
    0xA4F3: 938,
    0xA4F5: 939,
    0xA4F7: 940,
    0xA4F8: 941,
    0xA541: 942,
    0xA542: 943,
    0xA545: 944,
    0xA548: 945,
    0xA549: 946,
    0xA551: 947,
    0xA553: 948,
    0xA555: 949,
    0xA556: 950,
    0xA557: 951,
    0xA561: 952,
    0xA562: 953,
    0xA565: 954,
    0xA569: 955,
    0xA573: 956,
    0xA575: 957,
    0xA576: 958,
    0xA577: 959,
    0xA57B: 960,
    0xA581: 961,
    0xA585: 962,
    0xA5A1: 963,
    0xA5A2: 964,
    0xA5A3: 965,
    0xA5A5: 966,
    0xA5A9: 967,
    0xA5B1: 968,
    0xA5B3: 969,
    0xA5B5: 970,
    0xA5B7: 971,
    0xA5C1: 972,
    0xA5C5: 973,
    0xA5D6: 974,
    0xA5E1: 975,
    0xA5F6: 976,
    0xA641: 977,
    0xA642: 978,
    0xA645: 979,
    0xA649: 980,
    0xA651: 981,
    0xA653: 982,
    0xA661: 983,
    0xA665: 984,
    0xA681: 985,
    0xA682: 986,
    0xA685: 987,
    0xA688: 988,
    0xA689: 989,
    0xA68A: 990,
    0xA68B: 991,
    0xA691: 992,
    0xA693: 993,
    0xA695: 994,
    0xA697: 995,
    0xA69B: 996,
    0xA69C: 997,
    0xA6A1: 998,
    0xA6A9: 999,
    0xA6B6: 1000,
    0xA6C1: 1001,
    0xA6E1: 1002,
    0xA6E2: 1003,
    0xA6E5: 1004,
    0xA6E9: 1005,
    0xA6F7: 1006,
    0xA741: 1007,
    0xA745: 1008,
    0xA749: 1009,
    0xA751: 1010,
    0xA755: 1011,
    0xA757: 1012,
    0xA761: 1013,
    0xA762: 1014,
    0xA765: 1015,
    0xA769: 1016,
    0xA771: 1017,
    0xA773: 1018,
    0xA775: 1019,
    0xA7A1: 1020,
    0xA7A2: 1021,
    0xA7A5: 1022,
    0xA7A9: 1023,
    0xA7AB: 1024,
    0xA7B1: 1025,
    0xA7B3: 1026,
    0xA7B5: 1027,
    0xA7B7: 1028,
    0xA7B8: 1029,
    0xA7B9: 1030,
    0xA861: 1031,
    0xA862: 1032,
    0xA865: 1033,
    0xA869: 1034,
    0xA86B: 1035,
    0xA871: 1036,
    0xA873: 1037,
    0xA875: 1038,
    0xA876: 1039,
    0xA877: 1040,
    0xA87D: 1041,
    0xA881: 1042,
    0xA882: 1043,
    0xA885: 1044,
    0xA889: 1045,
    0xA891: 1046,
    0xA893: 1047,
    0xA895: 1048,
    0xA896: 1049,
    0xA897: 1050,
    0xA8A1: 1051,
    0xA8A2: 1052,
    0xA8B1: 1053,
    0xA8E1: 1054,
    0xA8E2: 1055,
    0xA8E5: 1056,
    0xA8E8: 1057,
    0xA8E9: 1058,
    0xA8F1: 1059,
    0xA8F5: 1060,
    0xA8F6: 1061,
    0xA8F7: 1062,
    0xA941: 1063,
    0xA957: 1064,
    0xA961: 1065,
    0xA962: 1066,
    0xA971: 1067,
    0xA973: 1068,
    0xA975: 1069,
    0xA976: 1070,
    0xA977: 1071,
    0xA9A1: 1072,
    0xA9A2: 1073,
    0xA9A5: 1074,
    0xA9A9: 1075,
    0xA9B1: 1076,
    0xA9B3: 1077,
    0xA9B7: 1078,
    0xAA41: 1079,
    0xAA61: 1080,
    0xAA77: 1081,
    0xAA81: 1082,
    0xAA82: 1083,
    0xAA85: 1084,
    0xAA89: 1085,
    0xAA91: 1086,
    0xAA95: 1087,
    0xAA97: 1088,
    0xAB41: 1089,
    0xAB57: 1090,
    0xAB61: 1091,
    0xAB65: 1092,
    0xAB69: 1093,
    0xAB71: 1094,
    0xAB73: 1095,
    0xABA1: 1096,
    0xABA2: 1097,
    0xABA5: 1098,
    0xABA9: 1099,
    0xABB1: 1100,
    0xABB3: 1101,
    0xABB5: 1102,
    0xABB7: 1103,
    0xAC61: 1104,
    0xAC62: 1105,
    0xAC64: 1106,
    0xAC65: 1107,
    0xAC68: 1108,
    0xAC69: 1109,
    0xAC6A: 1110,
    0xAC6B: 1111,
    0xAC71: 1112,
    0xAC73: 1113,
    0xAC75: 1114,
    0xAC76: 1115,
    0xAC77: 1116,
    0xAC7B: 1117,
    0xAC81: 1118,
    0xAC82: 1119,
    0xAC85: 1120,
    0xAC89: 1121,
    0xAC91: 1122,
    0xAC93: 1123,
    0xAC95: 1124,
    0xAC96: 1125,
    0xAC97: 1126,
    0xACA1: 1127,
    0xACA2: 1128,
    0xACA5: 1129,
    0xACA9: 1130,
    0xACB1: 1131,
    0xACB3: 1132,
    0xACB5: 1133,
    0xACB7: 1134,
    0xACC1: 1135,
    0xACC5: 1136,
    0xACC9: 1137,
    0xACD1: 1138,
    0xACD7: 1139,
    0xACE1: 1140,
    0xACE2: 1141,
    0xACE3: 1142,
    0xACE4: 1143,
    0xACE5: 1144,
    0xACE8: 1145,
    0xACE9: 1146,
    0xACEB: 1147,
    0xACEC: 1148,
    0xACF1: 1149,
    0xACF3: 1150,
    0xACF5: 1151,
    0xACF6: 1152,
    0xACF7: 1153,
    0xACFC: 1154,
    0xAD41: 1155,
    0xAD42: 1156,
    0xAD45: 1157,
    0xAD49: 1158,
    0xAD51: 1159,
    0xAD53: 1160,
    0xAD55: 1161,
    0xAD56: 1162,
    0xAD57: 1163,
    0xAD61: 1164,
    0xAD62: 1165,
    0xAD65: 1166,
    0xAD69: 1167,
    0xAD71: 1168,
    0xAD73: 1169,
    0xAD75: 1170,
    0xAD76: 1171,
    0xAD77: 1172,
    0xAD81: 1173,
    0xAD85: 1174,
    0xAD89: 1175,
    0xAD97: 1176,
    0xADA1: 1177,
    0xADA2: 1178,
    0xADA3: 1179,
    0xADA5: 1180,
    0xADA9: 1181,
    0xADAB: 1182,
    0xADB1: 1183,
    0xADB3: 1184,
    0xADB5: 1185,
    0xADB7: 1186,
    0xADBB: 1187,
    0xADC1: 1188,
    0xADC2: 1189,
    0xADC5: 1190,
    0xADC9: 1191,
    0xADD7: 1192,
    0xADE1: 1193,
    0xADE5: 1194,
    0xADE9: 1195,
    0xADF1: 1196,
    0xADF5: 1197,
    0xADF6: 1198,
    0xAE41: 1199,
    0xAE45: 1200,
    0xAE49: 1201,
    0xAE51: 1202,
    0xAE53: 1203,
    0xAE55: 1204,
    0xAE61: 1205,
    0xAE62: 1206,
    0xAE65: 1207,
    0xAE69: 1208,
    0xAE71: 1209,
    0xAE73: 1210,
    0xAE75: 1211,
    0xAE77: 1212,
    0xAE81: 1213,
    0xAE82: 1214,
    0xAE85: 1215,
    0xAE88: 1216,
    0xAE89: 1217,
    0xAE91: 1218,
    0xAE93: 1219,
    0xAE95: 1220,
    0xAE97: 1221,
    0xAE99: 1222,
    0xAE9B: 1223,
    0xAE9C: 1224,
    0xAEA1: 1225,
    0xAEB6: 1226,
    0xAEC1: 1227,
    0xAEC2: 1228,
    0xAEC5: 1229,
    0xAEC9: 1230,
    0xAED1: 1231,
    0xAED7: 1232,
    0xAEE1: 1233,
    0xAEE2: 1234,
    0xAEE5: 1235,
    0xAEE9: 1236,
    0xAEF1: 1237,
    0xAEF3: 1238,
    0xAEF5: 1239,
    0xAEF7: 1240,
    0xAF41: 1241,
    0xAF42: 1242,
    0xAF49: 1243,
    0xAF51: 1244,
    0xAF55: 1245,
    0xAF57: 1246,
    0xAF61: 1247,
    0xAF62: 1248,
    0xAF65: 1249,
    0xAF69: 1250,
    0xAF6A: 1251,
    0xAF71: 1252,
    0xAF73: 1253,
    0xAF75: 1254,
    0xAF77: 1255,
    0xAFA1: 1256,
    0xAFA2: 1257,
    0xAFA5: 1258,
    0xAFA8: 1259,
    0xAFA9: 1260,
    0xAFB0: 1261,
    0xAFB1: 1262,
    0xAFB3: 1263,
    0xAFB5: 1264,
    0xAFB7: 1265,
    0xAFBC: 1266,
    0xB061: 1267,
    0xB062: 1268,
    0xB064: 1269,
    0xB065: 1270,
    0xB069: 1271,
    0xB071: 1272,
    0xB073: 1273,
    0xB076: 1274,
    0xB077: 1275,
    0xB07D: 1276,
    0xB081: 1277,
    0xB082: 1278,
    0xB085: 1279,
    0xB089: 1280,
    0xB091: 1281,
    0xB093: 1282,
    0xB096: 1283,
    0xB097: 1284,
    0xB0B7: 1285,
    0xB0E1: 1286,
    0xB0E2: 1287,
    0xB0E5: 1288,
    0xB0E9: 1289,
    0xB0EB: 1290,
    0xB0F1: 1291,
    0xB0F3: 1292,
    0xB0F6: 1293,
    0xB0F7: 1294,
    0xB141: 1295,
    0xB145: 1296,
    0xB149: 1297,
    0xB185: 1298,
    0xB1A1: 1299,
    0xB1A2: 1300,
    0xB1A5: 1301,
    0xB1A8: 1302,
    0xB1A9: 1303,
    0xB1AB: 1304,
    0xB1B1: 1305,
    0xB1B3: 1306,
    0xB1B7: 1307,
    0xB1C1: 1308,
    0xB1C2: 1309,
    0xB1C5: 1310,
    0xB1D6: 1311,
    0xB1E1: 1312,
    0xB1F6: 1313,
    0xB241: 1314,
    0xB245: 1315,
    0xB249: 1316,
    0xB251: 1317,
    0xB253: 1318,
    0xB261: 1319,
    0xB281: 1320,
    0xB282: 1321,
    0xB285: 1322,
    0xB289: 1323,
    0xB291: 1324,
    0xB293: 1325,
    0xB297: 1326,
    0xB2A1: 1327,
    0xB2B6: 1328,
    0xB2C1: 1329,
    0xB2E1: 1330,
    0xB2E5: 1331,
    0xB357: 1332,
    0xB361: 1333,
    0xB362: 1334,
    0xB365: 1335,
    0xB369: 1336,
    0xB36B: 1337,
    0xB370: 1338,
    0xB371: 1339,
    0xB373: 1340,
    0xB381: 1341,
    0xB385: 1342,
    0xB389: 1343,
    0xB391: 1344,
    0xB3A1: 1345,
    0xB3A2: 1346,
    0xB3A5: 1347,
    0xB3A9: 1348,
    0xB3B1: 1349,
    0xB3B3: 1350,
    0xB3B5: 1351,
    0xB3B7: 1352,
    0xB461: 1353,
    0xB462: 1354,
    0xB465: 1355,
    0xB466: 1356,
    0xB467: 1357,
    0xB469: 1358,
    0xB46A: 1359,
    0xB46B: 1360,
    0xB470: 1361,
    0xB471: 1362,
    0xB473: 1363,
    0xB475: 1364,
    0xB476: 1365,
    0xB477: 1366,
    0xB47B: 1367,
    0xB47C: 1368,
    0xB481: 1369,
    0xB482: 1370,
    0xB485: 1371,
    0xB489: 1372,
    0xB491: 1373,
    0xB493: 1374,
    0xB495: 1375,
    0xB496: 1376,
    0xB497: 1377,
    0xB4A1: 1378,
    0xB4A2: 1379,
    0xB4A5: 1380,
    0xB4A9: 1381,
    0xB4AC: 1382,
    0xB4B1: 1383,
    0xB4B3: 1384,
    0xB4B5: 1385,
    0xB4B7: 1386,
    0xB4BB: 1387,
    0xB4BD: 1388,
    0xB4C1: 1389,
    0xB4C5: 1390,
    0xB4C9: 1391,
    0xB4D3: 1392,
    0xB4E1: 1393,
    0xB4E2: 1394,
    0xB4E5: 1395,
    0xB4E6: 1396,
    0xB4E8: 1397,
    0xB4E9: 1398,
    0xB4EA: 1399,
    0xB4EB: 1400,
    0xB4F1: 1401,
    0xB4F3: 1402,
    0xB4F4: 1403,
    0xB4F5: 1404,
    0xB4F6: 1405,
    0xB4F7: 1406,
    0xB4F8: 1407,
    0xB4FA: 1408,
    0xB4FC: 1409,
    0xB541: 1410,
    0xB542: 1411,
    0xB545: 1412,
    0xB549: 1413,
    0xB551: 1414,
    0xB553: 1415,
    0xB555: 1416,
    0xB557: 1417,
    0xB561: 1418,
    0xB562: 1419,
    0xB563: 1420,
    0xB565: 1421,
    0xB569: 1422,
    0xB56B: 1423,
    0xB56C: 1424,
    0xB571: 1425,
    0xB573: 1426,
    0xB574: 1427,
    0xB575: 1428,
    0xB576: 1429,
    0xB577: 1430,
    0xB57B: 1431,
    0xB57C: 1432,
    0xB57D: 1433,
    0xB581: 1434,
    0xB585: 1435,
    0xB589: 1436,
    0xB591: 1437,
    0xB593: 1438,
    0xB595: 1439,
    0xB596: 1440,
    0xB5A1: 1441,
    0xB5A2: 1442,
    0xB5A5: 1443,
    0xB5A9: 1444,
    0xB5AA: 1445,
    0xB5AB: 1446,
    0xB5AD: 1447,
    0xB5B0: 1448,
    0xB5B1: 1449,
    0xB5B3: 1450,
    0xB5B5: 1451,
    0xB5B7: 1452,
    0xB5B9: 1453,
    0xB5C1: 1454,
    0xB5C2: 1455,
    0xB5C5: 1456,
    0xB5C9: 1457,
    0xB5D1: 1458,
    0xB5D3: 1459,
    0xB5D5: 1460,
    0xB5D6: 1461,
    0xB5D7: 1462,
    0xB5E1: 1463,
    0xB5E2: 1464,
    0xB5E5: 1465,
    0xB5F1: 1466,
    0xB5F5: 1467,
    0xB5F7: 1468,
    0xB641: 1469,
    0xB642: 1470,
    0xB645: 1471,
    0xB649: 1472,
    0xB651: 1473,
    0xB653: 1474,
    0xB655: 1475,
    0xB657: 1476,
    0xB661: 1477,
    0xB662: 1478,
    0xB665: 1479,
    0xB669: 1480,
    0xB671: 1481,
    0xB673: 1482,
    0xB675: 1483,
    0xB677: 1484,
    0xB681: 1485,
    0xB682: 1486,
    0xB685: 1487,
    0xB689: 1488,
    0xB68A: 1489,
    0xB68B: 1490,
    0xB691: 1491,
    0xB693: 1492,
    0xB695: 1493,
    0xB697: 1494,
    0xB6A1: 1495,
    0xB6A2: 1496,
    0xB6A5: 1497,
    0xB6A9: 1498,
    0xB6B1: 1499,
    0xB6B3: 1500,
    0xB6B6: 1501,
    0xB6B7: 1502,
    0xB6C1: 1503,
    0xB6C2: 1504,
    0xB6C5: 1505,
    0xB6C9: 1506,
    0xB6D1: 1507,
    0xB6D3: 1508,
    0xB6D7: 1509,
    0xB6E1: 1510,
    0xB6E2: 1511,
    0xB6E5: 1512,
    0xB6E9: 1513,
    0xB6F1: 1514,
    0xB6F3: 1515,
    0xB6F5: 1516,
    0xB6F7: 1517,
    0xB741: 1518,
    0xB742: 1519,
    0xB745: 1520,
    0xB749: 1521,
    0xB751: 1522,
    0xB753: 1523,
    0xB755: 1524,
    0xB757: 1525,
    0xB759: 1526,
    0xB761: 1527,
    0xB762: 1528,
    0xB765: 1529,
    0xB769: 1530,
    0xB76F: 1531,
    0xB771: 1532,
    0xB773: 1533,
    0xB775: 1534,
    0xB777: 1535,
    0xB778: 1536,
    0xB779: 1537,
    0xB77A: 1538,
    0xB77B: 1539,
    0xB77C: 1540,
    0xB77D: 1541,
    0xB781: 1542,
    0xB785: 1543,
    0xB789: 1544,
    0xB791: 1545,
    0xB795: 1546,
    0xB7A1: 1547,
    0xB7A2: 1548,
    0xB7A5: 1549,
    0xB7A9: 1550,
    0xB7AA: 1551,
    0xB7AB: 1552,
    0xB7B0: 1553,
    0xB7B1: 1554,
    0xB7B3: 1555,
    0xB7B5: 1556,
    0xB7B6: 1557,
    0xB7B7: 1558,
    0xB7B8: 1559,
    0xB7BC: 1560,
    0xB861: 1561,
    0xB862: 1562,
    0xB865: 1563,
    0xB867: 1564,
    0xB868: 1565,
    0xB869: 1566,
    0xB86B: 1567,
    0xB871: 1568,
    0xB873: 1569,
    0xB875: 1570,
    0xB876: 1571,
    0xB877: 1572,
    0xB878: 1573,
    0xB881: 1574,
    0xB882: 1575,
    0xB885: 1576,
    0xB889: 1577,
    0xB891: 1578,
    0xB893: 1579,
    0xB895: 1580,
    0xB896: 1581,
    0xB897: 1582,
    0xB8A1: 1583,
    0xB8A2: 1584,
    0xB8A5: 1585,
    0xB8A7: 1586,
    0xB8A9: 1587,
    0xB8B1: 1588,
    0xB8B7: 1589,
    0xB8C1: 1590,
    0xB8C5: 1591,
    0xB8C9: 1592,
    0xB8E1: 1593,
    0xB8E2: 1594,
    0xB8E5: 1595,
    0xB8E9: 1596,
    0xB8EB: 1597,
    0xB8F1: 1598,
    0xB8F3: 1599,
    0xB8F5: 1600,
    0xB8F7: 1601,
    0xB8F8: 1602,
    0xB941: 1603,
    0xB942: 1604,
    0xB945: 1605,
    0xB949: 1606,
    0xB951: 1607,
    0xB953: 1608,
    0xB955: 1609,
    0xB957: 1610,
    0xB961: 1611,
    0xB965: 1612,
    0xB969: 1613,
    0xB971: 1614,
    0xB973: 1615,
    0xB976: 1616,
    0xB977: 1617,
    0xB981: 1618,
    0xB9A1: 1619,
    0xB9A2: 1620,
    0xB9A5: 1621,
    0xB9A9: 1622,
    0xB9AB: 1623,
    0xB9B1: 1624,
    0xB9B3: 1625,
    0xB9B5: 1626,
    0xB9B7: 1627,
    0xB9B8: 1628,
    0xB9B9: 1629,
    0xB9BD: 1630,
    0xB9C1: 1631,
    0xB9C2: 1632,
    0xB9C9: 1633,
    0xB9D3: 1634,
    0xB9D5: 1635,
    0xB9D7: 1636,
    0xB9E1: 1637,
    0xB9F6: 1638,
    0xB9F7: 1639,
    0xBA41: 1640,
    0xBA45: 1641,
    0xBA49: 1642,
    0xBA51: 1643,
    0xBA53: 1644,
    0xBA55: 1645,
    0xBA57: 1646,
    0xBA61: 1647,
    0xBA62: 1648,
    0xBA65: 1649,
    0xBA77: 1650,
    0xBA81: 1651,
    0xBA82: 1652,
    0xBA85: 1653,
    0xBA89: 1654,
    0xBA8A: 1655,
    0xBA8B: 1656,
    0xBA91: 1657,
    0xBA93: 1658,
    0xBA95: 1659,
    0xBA97: 1660,
    0xBAA1: 1661,
    0xBAB6: 1662,
    0xBAC1: 1663,
    0xBAE1: 1664,
    0xBAE2: 1665,
    0xBAE5: 1666,
    0xBAE9: 1667,
    0xBAF1: 1668,
    0xBAF3: 1669,
    0xBAF5: 1670,
    0xBB41: 1671,
    0xBB45: 1672,
    0xBB49: 1673,
    0xBB51: 1674,
    0xBB61: 1675,
    0xBB62: 1676,
    0xBB65: 1677,
    0xBB69: 1678,
    0xBB71: 1679,
    0xBB73: 1680,
    0xBB75: 1681,
    0xBB77: 1682,
    0xBBA1: 1683,
    0xBBA2: 1684,
    0xBBA5: 1685,
    0xBBA8: 1686,
    0xBBA9: 1687,
    0xBBAB: 1688,
    0xBBB1: 1689,
    0xBBB3: 1690,
    0xBBB5: 1691,
    0xBBB7: 1692,
    0xBBB8: 1693,
    0xBBBB: 1694,
    0xBBBC: 1695,
    0xBC61: 1696,
    0xBC62: 1697,
    0xBC65: 1698,
    0xBC67: 1699,
    0xBC69: 1700,
    0xBC6C: 1701,
    0xBC71: 1702,
    0xBC73: 1703,
    0xBC75: 1704,
    0xBC76: 1705,
    0xBC77: 1706,
    0xBC81: 1707,
    0xBC82: 1708,
    0xBC85: 1709,
    0xBC89: 1710,
    0xBC91: 1711,
    0xBC93: 1712,
    0xBC95: 1713,
    0xBC96: 1714,
    0xBC97: 1715,
    0xBCA1: 1716,
    0xBCA5: 1717,
    0xBCB7: 1718,
    0xBCE1: 1719,
    0xBCE2: 1720,
    0xBCE5: 1721,
    0xBCE9: 1722,
    0xBCF1: 1723,
    0xBCF3: 1724,
    0xBCF5: 1725,
    0xBCF6: 1726,
    0xBCF7: 1727,
    0xBD41: 1728,
    0xBD57: 1729,
    0xBD61: 1730,
    0xBD76: 1731,
    0xBDA1: 1732,
    0xBDA2: 1733,
    0xBDA5: 1734,
    0xBDA9: 1735,
    0xBDB1: 1736,
    0xBDB3: 1737,
    0xBDB5: 1738,
    0xBDB7: 1739,
    0xBDB9: 1740,
    0xBDC1: 1741,
    0xBDC2: 1742,
    0xBDC9: 1743,
    0xBDD6: 1744,
    0xBDE1: 1745,
    0xBDF6: 1746,
    0xBE41: 1747,
    0xBE45: 1748,
    0xBE49: 1749,
    0xBE51: 1750,
    0xBE53: 1751,
    0xBE77: 1752,
    0xBE81: 1753,
    0xBE82: 1754,
    0xBE85: 1755,
    0xBE89: 1756,
    0xBE91: 1757,
    0xBE93: 1758,
    0xBE97: 1759,
    0xBEA1: 1760,
    0xBEB6: 1761,
    0xBEB7: 1762,
    0xBEE1: 1763,
    0xBF41: 1764,
    0xBF61: 1765,
    0xBF71: 1766,
    0xBF75: 1767,
    0xBF77: 1768,
    0xBFA1: 1769,
    0xBFA2: 1770,
    0xBFA5: 1771,
    0xBFA9: 1772,
    0xBFB1: 1773,
    0xBFB3: 1774,
    0xBFB7: 1775,
    0xBFB8: 1776,
    0xBFBD: 1777,
    0xC061: 1778,
    0xC062: 1779,
    0xC065: 1780,
    0xC067: 1781,
    0xC069: 1782,
    0xC071: 1783,
    0xC073: 1784,
    0xC075: 1785,
    0xC076: 1786,
    0xC077: 1787,
    0xC078: 1788,
    0xC081: 1789,
    0xC082: 1790,
    0xC085: 1791,
    0xC089: 1792,
    0xC091: 1793,
    0xC093: 1794,
    0xC095: 1795,
    0xC096: 1796,
    0xC097: 1797,
    0xC0A1: 1798,
    0xC0A5: 1799,
    0xC0A7: 1800,
    0xC0A9: 1801,
    0xC0B1: 1802,
    0xC0B7: 1803,
    0xC0E1: 1804,
    0xC0E2: 1805,
    0xC0E5: 1806,
    0xC0E9: 1807,
    0xC0F1: 1808,
    0xC0F3: 1809,
    0xC0F5: 1810,
    0xC0F6: 1811,
    0xC0F7: 1812,
    0xC141: 1813,
    0xC142: 1814,
    0xC145: 1815,
    0xC149: 1816,
    0xC151: 1817,
    0xC153: 1818,
    0xC155: 1819,
    0xC157: 1820,
    0xC161: 1821,
    0xC165: 1822,
    0xC176: 1823,
    0xC181: 1824,
    0xC185: 1825,
    0xC197: 1826,
    0xC1A1: 1827,
    0xC1A2: 1828,
    0xC1A5: 1829,
    0xC1A9: 1830,
    0xC1B1: 1831,
    0xC1B3: 1832,
    0xC1B5: 1833,
    0xC1B7: 1834,
    0xC1C1: 1835,
    0xC1C5: 1836,
    0xC1C9: 1837,
    0xC1D7: 1838,
    0xC241: 1839,
    0xC245: 1840,
    0xC249: 1841,
    0xC251: 1842,
    0xC253: 1843,
    0xC255: 1844,
    0xC257: 1845,
    0xC261: 1846,
    0xC271: 1847,
    0xC281: 1848,
    0xC282: 1849,
    0xC285: 1850,
    0xC289: 1851,
    0xC291: 1852,
    0xC293: 1853,
    0xC295: 1854,
    0xC297: 1855,
    0xC2A1: 1856,
    0xC2B6: 1857,
    0xC2C1: 1858,
    0xC2C5: 1859,
    0xC2E1: 1860,
    0xC2E5: 1861,
    0xC2E9: 1862,
    0xC2F1: 1863,
    0xC2F3: 1864,
    0xC2F5: 1865,
    0xC2F7: 1866,
    0xC341: 1867,
    0xC345: 1868,
    0xC349: 1869,
    0xC351: 1870,
    0xC357: 1871,
    0xC361: 1872,
    0xC362: 1873,
    0xC365: 1874,
    0xC369: 1875,
    0xC371: 1876,
    0xC373: 1877,
    0xC375: 1878,
    0xC377: 1879,
    0xC3A1: 1880,
    0xC3A2: 1881,
    0xC3A5: 1882,
    0xC3A8: 1883,
    0xC3A9: 1884,
    0xC3AA: 1885,
    0xC3B1: 1886,
    0xC3B3: 1887,
    0xC3B5: 1888,
    0xC3B7: 1889,
    0xC461: 1890,
    0xC462: 1891,
    0xC465: 1892,
    0xC469: 1893,
    0xC471: 1894,
    0xC473: 1895,
    0xC475: 1896,
    0xC477: 1897,
    0xC481: 1898,
    0xC482: 1899,
    0xC485: 1900,
    0xC489: 1901,
    0xC491: 1902,
    0xC493: 1903,
    0xC495: 1904,
    0xC496: 1905,
    0xC497: 1906,
    0xC4A1: 1907,
    0xC4A2: 1908,
    0xC4B7: 1909,
    0xC4E1: 1910,
    0xC4E2: 1911,
    0xC4E5: 1912,
    0xC4E8: 1913,
    0xC4E9: 1914,
    0xC4F1: 1915,
    0xC4F3: 1916,
    0xC4F5: 1917,
    0xC4F6: 1918,
    0xC4F7: 1919,
    0xC541: 1920,
    0xC542: 1921,
    0xC545: 1922,
    0xC549: 1923,
    0xC551: 1924,
    0xC553: 1925,
    0xC555: 1926,
    0xC557: 1927,
    0xC561: 1928,
    0xC565: 1929,
    0xC569: 1930,
    0xC571: 1931,
    0xC573: 1932,
    0xC575: 1933,
    0xC576: 1934,
    0xC577: 1935,
    0xC581: 1936,
    0xC5A1: 1937,
    0xC5A2: 1938,
    0xC5A5: 1939,
    0xC5A9: 1940,
    0xC5B1: 1941,
    0xC5B3: 1942,
    0xC5B5: 1943,
    0xC5B7: 1944,
    0xC5C1: 1945,
    0xC5C2: 1946,
    0xC5C5: 1947,
    0xC5C9: 1948,
    0xC5D1: 1949,
    0xC5D7: 1950,
    0xC5E1: 1951,
    0xC5F7: 1952,
    0xC641: 1953,
    0xC649: 1954,
    0xC661: 1955,
    0xC681: 1956,
    0xC682: 1957,
    0xC685: 1958,
    0xC689: 1959,
    0xC691: 1960,
    0xC693: 1961,
    0xC695: 1962,
    0xC697: 1963,
    0xC6A1: 1964,
    0xC6A5: 1965,
    0xC6A9: 1966,
    0xC6B7: 1967,
    0xC6C1: 1968,
    0xC6D7: 1969,
    0xC6E1: 1970,
    0xC6E2: 1971,
    0xC6E5: 1972,
    0xC6E9: 1973,
    0xC6F1: 1974,
    0xC6F3: 1975,
    0xC6F5: 1976,
    0xC6F7: 1977,
    0xC741: 1978,
    0xC745: 1979,
    0xC749: 1980,
    0xC751: 1981,
    0xC761: 1982,
    0xC762: 1983,
    0xC765: 1984,
    0xC769: 1985,
    0xC771: 1986,
    0xC773: 1987,
    0xC777: 1988,
    0xC7A1: 1989,
    0xC7A2: 1990,
    0xC7A5: 1991,
    0xC7A9: 1992,
    0xC7B1: 1993,
    0xC7B3: 1994,
    0xC7B5: 1995,
    0xC7B7: 1996,
    0xC861: 1997,
    0xC862: 1998,
    0xC865: 1999,
    0xC869: 2000,
    0xC86A: 2001,
    0xC871: 2002,
    0xC873: 2003,
    0xC875: 2004,
    0xC876: 2005,
    0xC877: 2006,
    0xC881: 2007,
    0xC882: 2008,
    0xC885: 2009,
    0xC889: 2010,
    0xC891: 2011,
    0xC893: 2012,
    0xC895: 2013,
    0xC896: 2014,
    0xC897: 2015,
    0xC8A1: 2016,
    0xC8B7: 2017,
    0xC8E1: 2018,
    0xC8E2: 2019,
    0xC8E5: 2020,
    0xC8E9: 2021,
    0xC8EB: 2022,
    0xC8F1: 2023,
    0xC8F3: 2024,
    0xC8F5: 2025,
    0xC8F6: 2026,
    0xC8F7: 2027,
    0xC941: 2028,
    0xC942: 2029,
    0xC945: 2030,
    0xC949: 2031,
    0xC951: 2032,
    0xC953: 2033,
    0xC955: 2034,
    0xC957: 2035,
    0xC961: 2036,
    0xC965: 2037,
    0xC976: 2038,
    0xC981: 2039,
    0xC985: 2040,
    0xC9A1: 2041,
    0xC9A2: 2042,
    0xC9A5: 2043,
    0xC9A9: 2044,
    0xC9B1: 2045,
    0xC9B3: 2046,
    0xC9B5: 2047,
    0xC9B7: 2048,
    0xC9BC: 2049,
    0xC9C1: 2050,
    0xC9C5: 2051,
    0xC9E1: 2052,
    0xCA41: 2053,
    0xCA45: 2054,
    0xCA55: 2055,
    0xCA57: 2056,
    0xCA61: 2057,
    0xCA81: 2058,
    0xCA82: 2059,
    0xCA85: 2060,
    0xCA89: 2061,
    0xCA91: 2062,
    0xCA93: 2063,
    0xCA95: 2064,
    0xCA97: 2065,
    0xCAA1: 2066,
    0xCAB6: 2067,
    0xCAC1: 2068,
    0xCAE1: 2069,
    0xCAE2: 2070,
    0xCAE5: 2071,
    0xCAE9: 2072,
    0xCAF1: 2073,
    0xCAF3: 2074,
    0xCAF7: 2075,
    0xCB41: 2076,
    0xCB45: 2077,
    0xCB49: 2078,
    0xCB51: 2079,
    0xCB57: 2080,
    0xCB61: 2081,
    0xCB62: 2082,
    0xCB65: 2083,
    0xCB68: 2084,
    0xCB69: 2085,
    0xCB6B: 2086,
    0xCB71: 2087,
    0xCB73: 2088,
    0xCB75: 2089,
    0xCB81: 2090,
    0xCB85: 2091,
    0xCB89: 2092,
    0xCB91: 2093,
    0xCB93: 2094,
    0xCBA1: 2095,
    0xCBA2: 2096,
    0xCBA5: 2097,
    0xCBA9: 2098,
    0xCBB1: 2099,
    0xCBB3: 2100,
    0xCBB5: 2101,
    0xCBB7: 2102,
    0xCC61: 2103,
    0xCC62: 2104,
    0xCC63: 2105,
    0xCC65: 2106,
    0xCC69: 2107,
    0xCC6B: 2108,
    0xCC71: 2109,
    0xCC73: 2110,
    0xCC75: 2111,
    0xCC76: 2112,
    0xCC77: 2113,
    0xCC7B: 2114,
    0xCC81: 2115,
    0xCC82: 2116,
    0xCC85: 2117,
    0xCC89: 2118,
    0xCC91: 2119,
    0xCC93: 2120,
    0xCC95: 2121,
    0xCC96: 2122,
    0xCC97: 2123,
    0xCCA1: 2124,
    0xCCA2: 2125,
    0xCCE1: 2126,
    0xCCE2: 2127,
    0xCCE5: 2128,
    0xCCE9: 2129,
    0xCCF1: 2130,
    0xCCF3: 2131,
    0xCCF5: 2132,
    0xCCF6: 2133,
    0xCCF7: 2134,
    0xCD41: 2135,
    0xCD42: 2136,
    0xCD45: 2137,
    0xCD49: 2138,
    0xCD51: 2139,
    0xCD53: 2140,
    0xCD55: 2141,
    0xCD57: 2142,
    0xCD61: 2143,
    0xCD65: 2144,
    0xCD69: 2145,
    0xCD71: 2146,
    0xCD73: 2147,
    0xCD76: 2148,
    0xCD77: 2149,
    0xCD81: 2150,
    0xCD89: 2151,
    0xCD93: 2152,
    0xCD95: 2153,
    0xCDA1: 2154,
    0xCDA2: 2155,
    0xCDA5: 2156,
    0xCDA9: 2157,
    0xCDB1: 2158,
    0xCDB3: 2159,
    0xCDB5: 2160,
    0xCDB7: 2161,
    0xCDC1: 2162,
    0xCDD7: 2163,
    0xCE41: 2164,
    0xCE45: 2165,
    0xCE61: 2166,
    0xCE65: 2167,
    0xCE69: 2168,
    0xCE73: 2169,
    0xCE75: 2170,
    0xCE81: 2171,
    0xCE82: 2172,
    0xCE85: 2173,
    0xCE88: 2174,
    0xCE89: 2175,
    0xCE8B: 2176,
    0xCE91: 2177,
    0xCE93: 2178,
    0xCE95: 2179,
    0xCE97: 2180,
    0xCEA1: 2181,
    0xCEB7: 2182,
    0xCEE1: 2183,
    0xCEE5: 2184,
    0xCEE9: 2185,
    0xCEF1: 2186,
    0xCEF5: 2187,
    0xCF41: 2188,
    0xCF45: 2189,
    0xCF49: 2190,
    0xCF51: 2191,
    0xCF55: 2192,
    0xCF57: 2193,
    0xCF61: 2194,
    0xCF65: 2195,
    0xCF69: 2196,
    0xCF71: 2197,
    0xCF73: 2198,
    0xCF75: 2199,
    0xCFA1: 2200,
    0xCFA2: 2201,
    0xCFA5: 2202,
    0xCFA9: 2203,
    0xCFB1: 2204,
    0xCFB3: 2205,
    0xCFB5: 2206,
    0xCFB7: 2207,
    0xD061: 2208,
    0xD062: 2209,
    0xD065: 2210,
    0xD069: 2211,
    0xD06E: 2212,
    0xD071: 2213,
    0xD073: 2214,
    0xD075: 2215,
    0xD077: 2216,
    0xD081: 2217,
    0xD082: 2218,
    0xD085: 2219,
    0xD089: 2220,
    0xD091: 2221,
    0xD093: 2222,
    0xD095: 2223,
    0xD096: 2224,
    0xD097: 2225,
    0xD0A1: 2226,
    0xD0B7: 2227,
    0xD0E1: 2228,
    0xD0E2: 2229,
    0xD0E5: 2230,
    0xD0E9: 2231,
    0xD0EB: 2232,
    0xD0F1: 2233,
    0xD0F3: 2234,
    0xD0F5: 2235,
    0xD0F7: 2236,
    0xD141: 2237,
    0xD142: 2238,
    0xD145: 2239,
    0xD149: 2240,
    0xD151: 2241,
    0xD153: 2242,
    0xD155: 2243,
    0xD157: 2244,
    0xD161: 2245,
    0xD162: 2246,
    0xD165: 2247,
    0xD169: 2248,
    0xD171: 2249,
    0xD173: 2250,
    0xD175: 2251,
    0xD176: 2252,
    0xD177: 2253,
    0xD181: 2254,
    0xD185: 2255,
    0xD189: 2256,
    0xD193: 2257,
    0xD1A1: 2258,
    0xD1A2: 2259,
    0xD1A5: 2260,
    0xD1A9: 2261,
    0xD1AE: 2262,
    0xD1B1: 2263,
    0xD1B3: 2264,
    0xD1B5: 2265,
    0xD1B7: 2266,
    0xD1BB: 2267,
    0xD1C1: 2268,
    0xD1C2: 2269,
    0xD1C5: 2270,
    0xD1C9: 2271,
    0xD1D5: 2272,
    0xD1D7: 2273,
    0xD1E1: 2274,
    0xD1E2: 2275,
    0xD1E5: 2276,
    0xD1F5: 2277,
    0xD1F7: 2278,
    0xD241: 2279,
    0xD242: 2280,
    0xD245: 2281,
    0xD249: 2282,
    0xD253: 2283,
    0xD255: 2284,
    0xD257: 2285,
    0xD261: 2286,
    0xD265: 2287,
    0xD269: 2288,
    0xD273: 2289,
    0xD275: 2290,
    0xD281: 2291,
    0xD282: 2292,
    0xD285: 2293,
    0xD289: 2294,
    0xD28E: 2295,
    0xD291: 2296,
    0xD295: 2297,
    0xD297: 2298,
    0xD2A1: 2299,
    0xD2A5: 2300,
    0xD2A9: 2301,
    0xD2B1: 2302,
    0xD2B7: 2303,
    0xD2C1: 2304,
    0xD2C2: 2305,
    0xD2C5: 2306,
    0xD2C9: 2307,
    0xD2D7: 2308,
    0xD2E1: 2309,
    0xD2E2: 2310,
    0xD2E5: 2311,
    0xD2E9: 2312,
    0xD2F1: 2313,
    0xD2F3: 2314,
    0xD2F5: 2315,
    0xD2F7: 2316,
    0xD341: 2317,
    0xD342: 2318,
    0xD345: 2319,
    0xD349: 2320,
    0xD351: 2321,
    0xD355: 2322,
    0xD357: 2323,
    0xD361: 2324,
    0xD362: 2325,
    0xD365: 2326,
    0xD367: 2327,
    0xD368: 2328,
    0xD369: 2329,
    0xD36A: 2330,
    0xD371: 2331,
    0xD373: 2332,
    0xD375: 2333,
    0xD377: 2334,
    0xD37B: 2335,
    0xD381: 2336,
    0xD385: 2337,
    0xD389: 2338,
    0xD391: 2339,
    0xD393: 2340,
    0xD397: 2341,
    0xD3A1: 2342,
    0xD3A2: 2343,
    0xD3A5: 2344,
    0xD3A9: 2345,
    0xD3B1: 2346,
    0xD3B3: 2347,
    0xD3B5: 2348,
    0xD3B7: 2349,
}


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/resultdict.py
# ========================================================
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    # TypedDict was introduced in Python 3.8.
    #
    # TODO: Remove the else block and TYPE_CHECKING check when dropping support
    # for Python 3.7.
    from typing import TypedDict

    class ResultDict(TypedDict):
        encoding: Optional[str]
        confidence: float
        language: Optional[str]

else:
    ResultDict = dict


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/sbcharsetprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Dict, List, NamedTuple, Optional, Union

from .charsetprober import CharSetProber
from .enums import CharacterCategory, ProbingState, SequenceLikelihood


class SingleByteCharSetModel(NamedTuple):
    charset_name: str
    language: str
    char_to_order_map: Dict[int, int]
    language_model: Dict[int, Dict[int, int]]
    typical_positive_ratio: float
    keep_ascii_letters: bool
    alphabet: str


class SingleByteCharSetProber(CharSetProber):
    SAMPLE_SIZE = 64
    SB_ENOUGH_REL_THRESHOLD = 1024  # 0.25 * SAMPLE_SIZE^2
    POSITIVE_SHORTCUT_THRESHOLD = 0.95
    NEGATIVE_SHORTCUT_THRESHOLD = 0.05

    def __init__(
        self,
        model: SingleByteCharSetModel,
        is_reversed: bool = False,
        name_prober: Optional[CharSetProber] = None,
    ) -> None:
        super().__init__()
        self._model = model
        # TRUE if we need to reverse every pair in the model lookup
        self._reversed = is_reversed
        # Optional auxiliary prober for name decision
        self._name_prober = name_prober
        self._last_order = 255
        self._seq_counters: List[int] = []
        self._total_seqs = 0
        self._total_char = 0
        self._control_char = 0
        self._freq_char = 0
        self.reset()

    def reset(self) -> None:
        super().reset()
        # char order of last character
        self._last_order = 255
        self._seq_counters = [0] * SequenceLikelihood.get_num_categories()
        self._total_seqs = 0
        self._total_char = 0
        self._control_char = 0
        # characters that fall in our sampling range
        self._freq_char = 0

    @property
    def charset_name(self) -> Optional[str]:
        if self._name_prober:
            return self._name_prober.charset_name
        return self._model.charset_name

    @property
    def language(self) -> Optional[str]:
        if self._name_prober:
            return self._name_prober.language
        return self._model.language

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        # TODO: Make filter_international_words keep things in self.alphabet
        if not self._model.keep_ascii_letters:
            byte_str = self.filter_international_words(byte_str)
        else:
            byte_str = self.remove_xml_tags(byte_str)
        if not byte_str:
            return self.state
        char_to_order_map = self._model.char_to_order_map
        language_model = self._model.language_model
        for char in byte_str:
            order = char_to_order_map.get(char, CharacterCategory.UNDEFINED)
            # XXX: This was SYMBOL_CAT_ORDER before, with a value of 250, but
            #      CharacterCategory.SYMBOL is actually 253, so we use CONTROL
            #      to make it closer to the original intent. The only difference
            #      is whether or not we count digits and control characters for
            #      _total_char purposes.
            if order < CharacterCategory.CONTROL:
                self._total_char += 1
            if order < self.SAMPLE_SIZE:
                self._freq_char += 1
                if self._last_order < self.SAMPLE_SIZE:
                    self._total_seqs += 1
                    if not self._reversed:
                        lm_cat = language_model[self._last_order][order]
                    else:
                        lm_cat = language_model[order][self._last_order]
                    self._seq_counters[lm_cat] += 1
            self._last_order = order

        charset_name = self._model.charset_name
        if self.state == ProbingState.DETECTING:
            if self._total_seqs > self.SB_ENOUGH_REL_THRESHOLD:
                confidence = self.get_confidence()
                if confidence > self.POSITIVE_SHORTCUT_THRESHOLD:
                    self.logger.debug(
                        "%s confidence = %s, we have a winner", charset_name, confidence
                    )
                    self._state = ProbingState.FOUND_IT
                elif confidence < self.NEGATIVE_SHORTCUT_THRESHOLD:
                    self.logger.debug(
                        "%s confidence = %s, below negative shortcut threshold %s",
                        charset_name,
                        confidence,
                        self.NEGATIVE_SHORTCUT_THRESHOLD,
                    )
                    self._state = ProbingState.NOT_ME

        return self.state

    def get_confidence(self) -> float:
        r = 0.01
        if self._total_seqs > 0:
            r = (
                (
                    self._seq_counters[SequenceLikelihood.POSITIVE]
                    + 0.25 * self._seq_counters[SequenceLikelihood.LIKELY]
                )
                / self._total_seqs
                / self._model.typical_positive_ratio
            )
            # The more control characters (proportionnaly to the size
            # of the text), the less confident we become in the current
            # charset.
            r = r * (self._total_char - self._control_char) / self._total_char
            r = r * self._freq_char / self._total_char
            if r >= 1.0:
                r = 0.99
        return r


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/codingstatemachine.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

import logging

from .codingstatemachinedict import CodingStateMachineDict
from .enums import MachineState


class CodingStateMachine:
    """
    A state machine to verify a byte sequence for a particular encoding. For
    each byte the detector receives, it will feed that byte to every active
    state machine available, one byte at a time. The state machine changes its
    state based on its previous state and the byte it receives. There are 3
    states in a state machine that are of interest to an auto-detector:

    START state: This is the state to start with, or a legal byte sequence
                 (i.e. a valid code point) for character has been identified.

    ME state:  This indicates that the state machine identified a byte sequence
               that is specific to the charset it is designed for and that
               there is no other possible encoding which can contain this byte
               sequence. This will to lead to an immediate positive answer for
               the detector.

    ERROR state: This indicates the state machine identified an illegal byte
                 sequence for that encoding. This will lead to an immediate
                 negative answer for this encoding. Detector will exclude this
                 encoding from consideration from here on.
    """

    def __init__(self, sm: CodingStateMachineDict) -> None:
        self._model = sm
        self._curr_byte_pos = 0
        self._curr_char_len = 0
        self._curr_state = MachineState.START
        self.active = True
        self.logger = logging.getLogger(__name__)
        self.reset()

    def reset(self) -> None:
        self._curr_state = MachineState.START

    def next_state(self, c: int) -> int:
        # for each byte we get its class
        # if it is first byte, we also get byte length
        byte_class = self._model["class_table"][c]
        if self._curr_state == MachineState.START:
            self._curr_byte_pos = 0
            self._curr_char_len = self._model["char_len_table"][byte_class]
        # from byte's class and state_table, we get its next state
        curr_state = self._curr_state * self._model["class_factor"] + byte_class
        self._curr_state = self._model["state_table"][curr_state]
        self._curr_byte_pos += 1
        return self._curr_state

    def get_current_charlen(self) -> int:
        return self._curr_char_len

    def get_coding_state_machine(self) -> str:
        return self._model["name"]

    @property
    def language(self) -> str:
        return self._model["language"]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/gb2312freq.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# GB2312 most frequently used character table
#
# Char to FreqOrder table , from hz6763

# 512  --> 0.79  -- 0.79
# 1024 --> 0.92  -- 0.13
# 2048 --> 0.98  -- 0.06
# 6768 --> 1.00  -- 0.02
#
# Ideal Distribution Ratio = 0.79135/(1-0.79135) = 3.79
# Random Distribution Ration = 512 / (3755 - 512) = 0.157
#
# Typical Distribution Ratio about 25% of Ideal one, still much higher that RDR

GB2312_TYPICAL_DISTRIBUTION_RATIO = 0.9

GB2312_TABLE_SIZE = 3760

# fmt: off
GB2312_CHAR_TO_FREQ_ORDER = (
1671, 749,1443,2364,3924,3807,2330,3921,1704,3463,2691,1511,1515, 572,3191,2205,
2361, 224,2558, 479,1711, 963,3162, 440,4060,1905,2966,2947,3580,2647,3961,3842,
2204, 869,4207, 970,2678,5626,2944,2956,1479,4048, 514,3595, 588,1346,2820,3409,
 249,4088,1746,1873,2047,1774, 581,1813, 358,1174,3590,1014,1561,4844,2245, 670,
1636,3112, 889,1286, 953, 556,2327,3060,1290,3141, 613, 185,3477,1367, 850,3820,
1715,2428,2642,2303,2732,3041,2562,2648,3566,3946,1349, 388,3098,2091,1360,3585,
 152,1687,1539, 738,1559,  59,1232,2925,2267,1388,1249,1741,1679,2960, 151,1566,
1125,1352,4271, 924,4296, 385,3166,4459, 310,1245,2850,  70,3285,2729,3534,3575,
2398,3298,3466,1960,2265, 217,3647, 864,1909,2084,4401,2773,1010,3269,5152, 853,
3051,3121,1244,4251,1895, 364,1499,1540,2313,1180,3655,2268, 562, 715,2417,3061,
 544, 336,3768,2380,1752,4075, 950, 280,2425,4382, 183,2759,3272, 333,4297,2155,
1688,2356,1444,1039,4540, 736,1177,3349,2443,2368,2144,2225, 565, 196,1482,3406,
 927,1335,4147, 692, 878,1311,1653,3911,3622,1378,4200,1840,2969,3149,2126,1816,
2534,1546,2393,2760, 737,2494,  13, 447, 245,2747,  38,2765,2129,2589,1079, 606,
 360, 471,3755,2890, 404, 848, 699,1785,1236, 370,2221,1023,3746,2074,2026,2023,
2388,1581,2119, 812,1141,3091,2536,1519, 804,2053, 406,1596,1090, 784, 548,4414,
1806,2264,2936,1100, 343,4114,5096, 622,3358, 743,3668,1510,1626,5020,3567,2513,
3195,4115,5627,2489,2991,  24,2065,2697,1087,2719,  48,1634, 315,  68, 985,2052,
 198,2239,1347,1107,1439, 597,2366,2172, 871,3307, 919,2487,2790,1867, 236,2570,
1413,3794, 906,3365,3381,1701,1982,1818,1524,2924,1205, 616,2586,2072,2004, 575,
 253,3099,  32,1365,1182, 197,1714,2454,1201, 554,3388,3224,2748, 756,2587, 250,
2567,1507,1517,3529,1922,2761,2337,3416,1961,1677,2452,2238,3153, 615, 911,1506,
1474,2495,1265,1906,2749,3756,3280,2161, 898,2714,1759,3450,2243,2444, 563,  26,
3286,2266,3769,3344,2707,3677, 611,1402, 531,1028,2871,4548,1375, 261,2948, 835,
1190,4134, 353, 840,2684,1900,3082,1435,2109,1207,1674, 329,1872,2781,4055,2686,
2104, 608,3318,2423,2957,2768,1108,3739,3512,3271,3985,2203,1771,3520,1418,2054,
1681,1153, 225,1627,2929, 162,2050,2511,3687,1954, 124,1859,2431,1684,3032,2894,
 585,4805,3969,2869,2704,2088,2032,2095,3656,2635,4362,2209, 256, 518,2042,2105,
3777,3657, 643,2298,1148,1779, 190, 989,3544, 414,  11,2135,2063,2979,1471, 403,
3678, 126, 770,1563, 671,2499,3216,2877, 600,1179, 307,2805,4937,1268,1297,2694,
 252,4032,1448,1494,1331,1394, 127,2256, 222,1647,1035,1481,3056,1915,1048, 873,
3651, 210,  33,1608,2516, 200,1520, 415, 102,   0,3389,1287, 817,  91,3299,2940,
 836,1814, 549,2197,1396,1669,2987,3582,2297,2848,4528,1070, 687,  20,1819, 121,
1552,1364,1461,1968,2617,3540,2824,2083, 177, 948,4938,2291, 110,4549,2066, 648,
3359,1755,2110,2114,4642,4845,1693,3937,3308,1257,1869,2123, 208,1804,3159,2992,
2531,2549,3361,2418,1350,2347,2800,2568,1291,2036,2680,  72, 842,1990, 212,1233,
1154,1586,  75,2027,3410,4900,1823,1337,2710,2676, 728,2810,1522,3026,4995, 157,
 755,1050,4022, 710, 785,1936,2194,2085,1406,2777,2400, 150,1250,4049,1206, 807,
1910, 534, 529,3309,1721,1660, 274,  39,2827, 661,2670,1578, 925,3248,3815,1094,
4278,4901,4252,  41,1150,3747,2572,2227,4501,3658,4902,3813,3357,3617,2884,2258,
 887, 538,4187,3199,1294,2439,3042,2329,2343,2497,1255, 107, 543,1527, 521,3478,
3568, 194,5062,  15, 961,3870,1241,1192,2664,  66,5215,3260,2111,1295,1127,2152,
3805,4135, 901,1164,1976, 398,1278, 530,1460, 748, 904,1054,1966,1426,  53,2909,
 509, 523,2279,1534, 536,1019, 239,1685, 460,2353, 673,1065,2401,3600,4298,2272,
1272,2363, 284,1753,3679,4064,1695,  81, 815,2677,2757,2731,1386, 859, 500,4221,
2190,2566, 757,1006,2519,2068,1166,1455, 337,2654,3203,1863,1682,1914,3025,1252,
1409,1366, 847, 714,2834,2038,3209, 964,2970,1901, 885,2553,1078,1756,3049, 301,
1572,3326, 688,2130,1996,2429,1805,1648,2930,3421,2750,3652,3088, 262,1158,1254,
 389,1641,1812, 526,1719, 923,2073,1073,1902, 468, 489,4625,1140, 857,2375,3070,
3319,2863, 380, 116,1328,2693,1161,2244, 273,1212,1884,2769,3011,1775,1142, 461,
3066,1200,2147,2212, 790, 702,2695,4222,1601,1058, 434,2338,5153,3640,  67,2360,
4099,2502, 618,3472,1329, 416,1132, 830,2782,1807,2653,3211,3510,1662, 192,2124,
 296,3979,1739,1611,3684,  23, 118, 324, 446,1239,1225, 293,2520,3814,3795,2535,
3116,  17,1074, 467,2692,2201, 387,2922,  45,1326,3055,1645,3659,2817, 958, 243,
1903,2320,1339,2825,1784,3289, 356, 576, 865,2315,2381,3377,3916,1088,3122,1713,
1655, 935, 628,4689,1034,1327, 441, 800, 720, 894,1979,2183,1528,5289,2702,1071,
4046,3572,2399,1571,3281,  79, 761,1103, 327, 134, 758,1899,1371,1615, 879, 442,
 215,2605,2579, 173,2048,2485,1057,2975,3317,1097,2253,3801,4263,1403,1650,2946,
 814,4968,3487,1548,2644,1567,1285,   2, 295,2636,  97, 946,3576, 832, 141,4257,
3273, 760,3821,3521,3156,2607, 949,1024,1733,1516,1803,1920,2125,2283,2665,3180,
1501,2064,3560,2171,1592, 803,3518,1416, 732,3897,4258,1363,1362,2458, 119,1427,
 602,1525,2608,1605,1639,3175, 694,3064,  10, 465,  76,2000,4846,4208, 444,3781,
1619,3353,2206,1273,3796, 740,2483, 320,1723,2377,3660,2619,1359,1137,1762,1724,
2345,2842,1850,1862, 912, 821,1866, 612,2625,1735,2573,3369,1093, 844,  89, 937,
 930,1424,3564,2413,2972,1004,3046,3019,2011, 711,3171,1452,4178, 428, 801,1943,
 432, 445,2811, 206,4136,1472, 730, 349,  73, 397,2802,2547, 998,1637,1167, 789,
 396,3217, 154,1218, 716,1120,1780,2819,4826,1931,3334,3762,2139,1215,2627, 552,
3664,3628,3232,1405,2383,3111,1356,2652,3577,3320,3101,1703, 640,1045,1370,1246,
4996, 371,1575,2436,1621,2210, 984,4033,1734,2638,  16,4529, 663,2755,3255,1451,
3917,2257,1253,1955,2234,1263,2951, 214,1229, 617, 485, 359,1831,1969, 473,2310,
 750,2058, 165,  80,2864,2419, 361,4344,2416,2479,1134, 796,3726,1266,2943, 860,
2715, 938, 390,2734,1313,1384, 248, 202, 877,1064,2854, 522,3907, 279,1602, 297,
2357, 395,3740, 137,2075, 944,4089,2584,1267,3802,  62,1533,2285, 178, 176, 780,
2440, 201,3707, 590, 478,1560,4354,2117,1075,  30,  74,4643,4004,1635,1441,2745,
 776,2596, 238,1077,1692,1912,2844, 605, 499,1742,3947, 241,3053, 980,1749, 936,
2640,4511,2582, 515,1543,2162,5322,2892,2993, 890,2148,1924, 665,1827,3581,1032,
 968,3163, 339,1044,1896, 270, 583,1791,1720,4367,1194,3488,3669,  43,2523,1657,
 163,2167, 290,1209,1622,3378, 550, 634,2508,2510, 695,2634,2384,2512,1476,1414,
 220,1469,2341,2138,2852,3183,2900,4939,2865,3502,1211,3680, 854,3227,1299,2976,
3172, 186,2998,1459, 443,1067,3251,1495, 321,1932,3054, 909, 753,1410,1828, 436,
2441,1119,1587,3164,2186,1258, 227, 231,1425,1890,3200,3942, 247, 959, 725,5254,
2741, 577,2158,2079, 929, 120, 174, 838,2813, 591,1115, 417,2024,  40,3240,1536,
1037, 291,4151,2354, 632,1298,2406,2500,3535,1825,1846,3451, 205,1171, 345,4238,
  18,1163, 811, 685,2208,1217, 425,1312,1508,1175,4308,2552,1033, 587,1381,3059,
2984,3482, 340,1316,4023,3972, 792,3176, 519, 777,4690, 918, 933,4130,2981,3741,
  90,3360,2911,2200,5184,4550, 609,3079,2030, 272,3379,2736, 363,3881,1130,1447,
 286, 779, 357,1169,3350,3137,1630,1220,2687,2391, 747,1277,3688,2618,2682,2601,
1156,3196,5290,4034,3102,1689,3596,3128, 874, 219,2783, 798, 508,1843,2461, 269,
1658,1776,1392,1913,2983,3287,2866,2159,2372, 829,4076,  46,4253,2873,1889,1894,
 915,1834,1631,2181,2318, 298, 664,2818,3555,2735, 954,3228,3117, 527,3511,2173,
 681,2712,3033,2247,2346,3467,1652, 155,2164,3382, 113,1994, 450, 899, 494, 994,
1237,2958,1875,2336,1926,3727, 545,1577,1550, 633,3473, 204,1305,3072,2410,1956,
2471, 707,2134, 841,2195,2196,2663,3843,1026,4940, 990,3252,4997, 368,1092, 437,
3212,3258,1933,1829, 675,2977,2893, 412, 943,3723,4644,3294,3283,2230,2373,5154,
2389,2241,2661,2323,1404,2524, 593, 787, 677,3008,1275,2059, 438,2709,2609,2240,
2269,2246,1446,  36,1568,1373,3892,1574,2301,1456,3962, 693,2276,5216,2035,1143,
2720,1919,1797,1811,2763,4137,2597,1830,1699,1488,1198,2090, 424,1694, 312,3634,
3390,4179,3335,2252,1214, 561,1059,3243,2295,2561, 975,5155,2321,2751,3772, 472,
1537,3282,3398,1047,2077,2348,2878,1323,3340,3076, 690,2906,  51, 369, 170,3541,
1060,2187,2688,3670,2541,1083,1683, 928,3918, 459, 109,4427, 599,3744,4286, 143,
2101,2730,2490,  82,1588,3036,2121, 281,1860, 477,4035,1238,2812,3020,2716,3312,
1530,2188,2055,1317, 843, 636,1808,1173,3495, 649, 181,1002, 147,3641,1159,2414,
3750,2289,2795, 813,3123,2610,1136,4368,   5,3391,4541,2174, 420, 429,1728, 754,
1228,2115,2219, 347,2223,2733, 735,1518,3003,2355,3134,1764,3948,3329,1888,2424,
1001,1234,1972,3321,3363,1672,1021,1450,1584, 226, 765, 655,2526,3404,3244,2302,
3665, 731, 594,2184, 319,1576, 621, 658,2656,4299,2099,3864,1279,2071,2598,2739,
 795,3086,3699,3908,1707,2352,2402,1382,3136,2475,1465,4847,3496,3865,1085,3004,
2591,1084, 213,2287,1963,3565,2250, 822, 793,4574,3187,1772,1789,3050, 595,1484,
1959,2770,1080,2650, 456, 422,2996, 940,3322,4328,4345,3092,2742, 965,2784, 739,
4124, 952,1358,2498,2949,2565, 332,2698,2378, 660,2260,2473,4194,3856,2919, 535,
1260,2651,1208,1428,1300,1949,1303,2942, 433,2455,2450,1251,1946, 614,1269, 641,
1306,1810,2737,3078,2912, 564,2365,1419,1415,1497,4460,2367,2185,1379,3005,1307,
3218,2175,1897,3063, 682,1157,4040,4005,1712,1160,1941,1399, 394, 402,2952,1573,
1151,2986,2404, 862, 299,2033,1489,3006, 346, 171,2886,3401,1726,2932, 168,2533,
  47,2507,1030,3735,1145,3370,1395,1318,1579,3609,4560,2857,4116,1457,2529,1965,
 504,1036,2690,2988,2405, 745,5871, 849,2397,2056,3081, 863,2359,3857,2096,  99,
1397,1769,2300,4428,1643,3455,1978,1757,3718,1440,  35,4879,3742,1296,4228,2280,
 160,5063,1599,2013, 166, 520,3479,1646,3345,3012, 490,1937,1545,1264,2182,2505,
1096,1188,1369,1436,2421,1667,2792,2460,1270,2122, 727,3167,2143, 806,1706,1012,
1800,3037, 960,2218,1882, 805, 139,2456,1139,1521, 851,1052,3093,3089, 342,2039,
 744,5097,1468,1502,1585,2087, 223, 939, 326,2140,2577, 892,2481,1623,4077, 982,
3708, 135,2131,  87,2503,3114,2326,1106, 876,1616, 547,2997,2831,2093,3441,4530,
4314,   9,3256,4229,4148, 659,1462,1986,1710,2046,2913,2231,4090,4880,5255,3392,
3274,1368,3689,4645,1477, 705,3384,3635,1068,1529,2941,1458,3782,1509, 100,1656,
2548, 718,2339, 408,1590,2780,3548,1838,4117,3719,1345,3530, 717,3442,2778,3220,
2898,1892,4590,3614,3371,2043,1998,1224,3483, 891, 635, 584,2559,3355, 733,1766,
1729,1172,3789,1891,2307, 781,2982,2271,1957,1580,5773,2633,2005,4195,3097,1535,
3213,1189,1934,5693,3262, 586,3118,1324,1598, 517,1564,2217,1868,1893,4445,3728,
2703,3139,1526,1787,1992,3882,2875,1549,1199,1056,2224,1904,2711,5098,4287, 338,
1993,3129,3489,2689,1809,2815,1997, 957,1855,3898,2550,3275,3057,1105,1319, 627,
1505,1911,1883,3526, 698,3629,3456,1833,1431, 746,  77,1261,2017,2296,1977,1885,
 125,1334,1600, 525,1798,1109,2222,1470,1945, 559,2236,1186,3443,2476,1929,1411,
2411,3135,1777,3372,2621,1841,1613,3229, 668,1430,1839,2643,2916, 195,1989,2671,
2358,1387, 629,3205,2293,5256,4439, 123,1310, 888,1879,4300,3021,3605,1003,1162,
3192,2910,2010, 140,2395,2859,  55,1082,2012,2901, 662, 419,2081,1438, 680,2774,
4654,3912,1620,1731,1625,5035,4065,2328, 512,1344, 802,5443,2163,2311,2537, 524,
3399,  98,1155,2103,1918,2606,3925,2816,1393,2465,1504,3773,2177,3963,1478,4346,
 180,1113,4655,3461,2028,1698, 833,2696,1235,1322,1594,4408,3623,3013,3225,2040,
3022, 541,2881, 607,3632,2029,1665,1219, 639,1385,1686,1099,2803,3231,1938,3188,
2858, 427, 676,2772,1168,2025, 454,3253,2486,3556, 230,1950, 580, 791,1991,1280,
1086,1974,2034, 630, 257,3338,2788,4903,1017,  86,4790, 966,2789,1995,1696,1131,
 259,3095,4188,1308, 179,1463,5257, 289,4107,1248,  42,3413,1725,2288, 896,1947,
 774,4474,4254, 604,3430,4264, 392,2514,2588, 452, 237,1408,3018, 988,4531,1970,
3034,3310, 540,2370,1562,1288,2990, 502,4765,1147,   4,1853,2708, 207, 294,2814,
4078,2902,2509, 684,  34,3105,3532,2551, 644, 709,2801,2344, 573,1727,3573,3557,
2021,1081,3100,4315,2100,3681, 199,2263,1837,2385, 146,3484,1195,2776,3949, 997,
1939,3973,1008,1091,1202,1962,1847,1149,4209,5444,1076, 493, 117,5400,2521, 972,
1490,2934,1796,4542,2374,1512,2933,2657, 413,2888,1135,2762,2314,2156,1355,2369,
 766,2007,2527,2170,3124,2491,2593,2632,4757,2437, 234,3125,3591,1898,1750,1376,
1942,3468,3138, 570,2127,2145,3276,4131, 962, 132,1445,4196,  19, 941,3624,3480,
3366,1973,1374,4461,3431,2629, 283,2415,2275, 808,2887,3620,2112,2563,1353,3610,
 955,1089,3103,1053,  96,  88,4097, 823,3808,1583, 399, 292,4091,3313, 421,1128,
 642,4006, 903,2539,1877,2082, 596,  29,4066,1790, 722,2157, 130, 995,1569, 769,
1485, 464, 513,2213, 288,1923,1101,2453,4316, 133, 486,2445,  50, 625, 487,2207,
  57, 423, 481,2962, 159,3729,1558, 491, 303, 482, 501, 240,2837, 112,3648,2392,
1783, 362,   8,3433,3422, 610,2793,3277,1390,1284,1654,  21,3823, 734, 367, 623,
 193, 287, 374,1009,1483, 816, 476, 313,2255,2340,1262,2150,2899,1146,2581, 782,
2116,1659,2018,1880, 255,3586,3314,1110,2867,2137,2564, 986,2767,5185,2006, 650,
 158, 926, 762, 881,3157,2717,2362,3587, 306,3690,3245,1542,3077,2427,1691,2478,
2118,2985,3490,2438, 539,2305, 983, 129,1754, 355,4201,2386, 827,2923, 104,1773,
2838,2771, 411,2905,3919, 376, 767, 122,1114, 828,2422,1817,3506, 266,3460,1007,
1609,4998, 945,2612,4429,2274, 726,1247,1964,2914,2199,2070,4002,4108, 657,3323,
1422, 579, 455,2764,4737,1222,2895,1670, 824,1223,1487,2525, 558, 861,3080, 598,
2659,2515,1967, 752,2583,2376,2214,4180, 977, 704,2464,4999,2622,4109,1210,2961,
 819,1541, 142,2284,  44, 418, 457,1126,3730,4347,4626,1644,1876,3671,1864, 302,
1063,5694, 624, 723,1984,3745,1314,1676,2488,1610,1449,3558,3569,2166,2098, 409,
1011,2325,3704,2306, 818,1732,1383,1824,1844,3757, 999,2705,3497,1216,1423,2683,
2426,2954,2501,2726,2229,1475,2554,5064,1971,1794,1666,2014,1343, 783, 724, 191,
2434,1354,2220,5065,1763,2752,2472,4152, 131, 175,2885,3434,  92,1466,4920,2616,
3871,3872,3866, 128,1551,1632, 669,1854,3682,4691,4125,1230, 188,2973,3290,1302,
1213, 560,3266, 917, 763,3909,3249,1760, 868,1958, 764,1782,2097, 145,2277,3774,
4462,  64,1491,3062, 971,2132,3606,2442, 221,1226,1617, 218, 323,1185,3207,3147,
 571, 619,1473,1005,1744,2281, 449,1887,2396,3685, 275, 375,3816,1743,3844,3731,
 845,1983,2350,4210,1377, 773, 967,3499,3052,3743,2725,4007,1697,1022,3943,1464,
3264,2855,2722,1952,1029,2839,2467,  84,4383,2215, 820,1391,2015,2448,3672, 377,
1948,2168, 797,2545,3536,2578,2645,  94,2874,1678, 405,1259,3071, 771, 546,1315,
 470,1243,3083, 895,2468, 981, 969,2037, 846,4181, 653,1276,2928,  14,2594, 557,
3007,2474, 156, 902,1338,1740,2574, 537,2518, 973,2282,2216,2433,1928, 138,2903,
1293,2631,1612, 646,3457, 839,2935, 111, 496,2191,2847, 589,3186, 149,3994,2060,
4031,2641,4067,3145,1870,  37,3597,2136,1025,2051,3009,3383,3549,1121,1016,3261,
1301, 251,2446,2599,2153, 872,3246, 637, 334,3705, 831, 884, 921,3065,3140,4092,
2198,1944, 246,2964, 108,2045,1152,1921,2308,1031, 203,3173,4170,1907,3890, 810,
1401,2003,1690, 506, 647,1242,2828,1761,1649,3208,2249,1589,3709,2931,5156,1708,
 498, 666,2613, 834,3817,1231, 184,2851,1124, 883,3197,2261,3710,1765,1553,2658,
1178,2639,2351,  93,1193, 942,2538,2141,4402, 235,1821, 870,1591,2192,1709,1871,
3341,1618,4126,2595,2334, 603, 651,  69, 701, 268,2662,3411,2555,1380,1606, 503,
 448, 254,2371,2646, 574,1187,2309,1770, 322,2235,1292,1801, 305, 566,1133, 229,
2067,2057, 706, 167, 483,2002,2672,3295,1820,3561,3067, 316, 378,2746,3452,1112,
 136,1981, 507,1651,2917,1117, 285,4591, 182,2580,3522,1304, 335,3303,1835,2504,
1795,1792,2248, 674,1018,2106,2449,1857,2292,2845, 976,3047,1781,2600,2727,1389,
1281,  52,3152, 153, 265,3950, 672,3485,3951,4463, 430,1183, 365, 278,2169,  27,
1407,1336,2304, 209,1340,1730,2202,1852,2403,2883, 979,1737,1062, 631,2829,2542,
3876,2592, 825,2086,2226,3048,3625, 352,1417,3724, 542, 991, 431,1351,3938,1861,
2294, 826,1361,2927,3142,3503,1738, 463,2462,2723, 582,1916,1595,2808, 400,3845,
3891,2868,3621,2254,  58,2492,1123, 910,2160,2614,1372,1603,1196,1072,3385,1700,
3267,1980, 696, 480,2430, 920, 799,1570,2920,1951,2041,4047,2540,1321,4223,2469,
3562,2228,1271,2602, 401,2833,3351,2575,5157, 907,2312,1256, 410, 263,3507,1582,
 996, 678,1849,2316,1480, 908,3545,2237, 703,2322, 667,1826,2849,1531,2604,2999,
2407,3146,2151,2630,1786,3711, 469,3542, 497,3899,2409, 858, 837,4446,3393,1274,
 786, 620,1845,2001,3311, 484, 308,3367,1204,1815,3691,2332,1532,2557,1842,2020,
2724,1927,2333,4440, 567,  22,1673,2728,4475,1987,1858,1144,1597, 101,1832,3601,
  12, 974,3783,4391, 951,1412,   1,3720, 453,4608,4041, 528,1041,1027,3230,2628,
1129, 875,1051,3291,1203,2262,1069,2860,2799,2149,2615,3278, 144,1758,3040,  31,
 475,1680, 366,2685,3184, 311,1642,4008,2466,5036,1593,1493,2809, 216,1420,1668,
 233, 304,2128,3284, 232,1429,1768,1040,2008,3407,2740,2967,2543, 242,2133, 778,
1565,2022,2620, 505,2189,2756,1098,2273, 372,1614, 708, 553,2846,2094,2278, 169,
3626,2835,4161, 228,2674,3165, 809,1454,1309, 466,1705,1095, 900,3423, 880,2667,
3751,5258,2317,3109,2571,4317,2766,1503,1342, 866,4447,1118,  63,2076, 314,1881,
1348,1061, 172, 978,3515,1747, 532, 511,3970,   6, 601, 905,2699,3300,1751, 276,
1467,3725,2668,  65,4239,2544,2779,2556,1604, 578,2451,1802, 992,2331,2624,1320,
3446, 713,1513,1013, 103,2786,2447,1661, 886,1702, 916, 654,3574,2031,1556, 751,
2178,2821,2179,1498,1538,2176, 271, 914,2251,2080,1325, 638,1953,2937,3877,2432,
2754,  95,3265,1716, 260,1227,4083, 775, 106,1357,3254, 426,1607, 555,2480, 772,
1985, 244,2546, 474, 495,1046,2611,1851,2061,  71,2089,1675,2590, 742,3758,2843,
3222,1433, 267,2180,2576,2826,2233,2092,3913,2435, 956,1745,3075, 856,2113,1116,
 451,   3,1988,2896,1398, 993,2463,1878,2049,1341,2718,2721,2870,2108, 712,2904,
4363,2753,2324, 277,2872,2349,2649, 384, 987, 435, 691,3000, 922, 164,3939, 652,
1500,1184,4153,2482,3373,2165,4848,2335,3775,3508,3154,2806,2830,1554,2102,1664,
2530,1434,2408, 893,1547,2623,3447,2832,2242,2532,3169,2856,3223,2078,  49,3770,
3469, 462, 318, 656,2259,3250,3069, 679,1629,2758, 344,1138,1104,3120,1836,1283,
3115,2154,1437,4448, 934, 759,1999, 794,2862,1038, 533,2560,1722,2342, 855,2626,
1197,1663,4476,3127,  85,4240,2528,  25,1111,1181,3673, 407,3470,4561,2679,2713,
 768,1925,2841,3986,1544,1165, 932, 373,1240,2146,1930,2673, 721,4766, 354,4333,
 391,2963, 187,  61,3364,1442,1102, 330,1940,1767, 341,3809,4118, 393,2496,2062,
2211, 105, 331, 300, 439, 913,1332, 626, 379,3304,1557, 328, 689,3952, 309,1555,
 931, 317,2517,3027, 325, 569, 686,2107,3084,  60,1042,1333,2794, 264,3177,4014,
1628, 258,3712,   7,4464,1176,1043,1778, 683, 114,1975,  78,1492, 383,1886, 510,
 386, 645,5291,2891,2069,3305,4138,3867,2939,2603,2493,1935,1066,1848,3588,1015,
1282,1289,4609, 697,1453,3044,2666,3611,1856,2412,  54, 719,1330, 568,3778,2459,
1748, 788, 492, 551,1191,1000, 488,3394,3763, 282,1799, 348,2016,1523,3155,2390,
1049, 382,2019,1788,1170, 729,2968,3523, 897,3926,2785,2938,3292, 350,2319,3238,
1718,1717,2655,3453,3143,4465, 161,2889,2980,2009,1421,  56,1908,1640,2387,2232,
1917,1874,2477,4921, 148,  83,3438, 592,4245,2882,1822,1055, 741, 115,1496,1624,
 381,1638,4592,1020, 516,3214, 458, 947,4575,1432, 211,1514,2926,1865,2142, 189,
 852,1221,1400,1486, 882,2299,4036, 351,  28,1122, 700,6479,6480,6481,6482,6483,  #last 512
)
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langgreekmodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

GREEK_LANG_MODEL = {
    60: {  # 'e'
        60: 2,  # 'e'
        55: 1,  # 'o'
        58: 2,  # 't'
        36: 1,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 1,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    55: {  # 'o'
        60: 0,  # 'e'
        55: 2,  # 'o'
        58: 2,  # 't'
        36: 1,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 1,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 1,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    58: {  # 't'
        60: 2,  # 'e'
        55: 1,  # 'o'
        58: 1,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 1,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    36: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    61: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 1,  # ''
        21: 2,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 1,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    46: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 2,  # ''
        20: 2,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 2,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 1,  # ''
        2: 2,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 3,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    54: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 2,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 2,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 2,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    31: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 2,  # ''
        43: 2,  # ''
        41: 1,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 2,  # ''
        47: 2,  # ''
        44: 2,  # ''
        53: 2,  # ''
        38: 2,  # ''
        49: 2,  # ''
        59: 1,  # ''
        39: 0,  # ''
        35: 2,  # ''
        48: 2,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 2,  # ''
        56: 2,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 2,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 1,  # ''
        5: 0,  # ''
        11: 2,  # ''
        16: 3,  # ''
        10: 2,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 2,  # ''
        7: 2,  # ''
        2: 0,  # ''
        12: 3,  # ''
        28: 2,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 2,  # ''
        27: 0,  # ''
    },
    51: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        47: 1,  # ''
        44: 0,  # ''
        53: 1,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 2,  # ''
        15: 0,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    43: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 1,  # ''
        51: 0,  # ''
        43: 2,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 1,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 1,  # ''
        53: 1,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 1,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 2,  # ''
        56: 0,  # ''
        50: 1,  # ''
        57: 2,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    41: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 2,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 2,  # ''
        15: 2,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 1,  # ''
        26: 2,  # ''
        27: 2,  # ''
    },
    34: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 2,  # ''
        41: 2,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 2,  # ''
        53: 2,  # ''
        38: 2,  # ''
        49: 2,  # ''
        59: 1,  # ''
        39: 0,  # ''
        35: 2,  # ''
        48: 2,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 2,  # ''
        56: 0,  # ''
        50: 2,  # ''
        57: 2,  # ''
        17: 3,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 3,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 3,  # ''
        21: 2,  # ''
        3: 1,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 1,  # ''
        5: 2,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 2,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 0,  # ''
        9: 3,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 2,  # ''
        2: 2,  # ''
        12: 2,  # ''
        28: 2,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 1,  # ''
        27: 0,  # ''
    },
    40: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 1,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 2,  # ''
        47: 0,  # ''
        44: 2,  # ''
        53: 0,  # ''
        38: 2,  # ''
        49: 2,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 2,  # ''
        48: 2,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 1,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 1,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 1,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    52: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 1,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 1,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 2,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 2,  # ''
        27: 0,  # ''
    },
    47: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 1,  # ''
        43: 1,  # ''
        41: 2,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 2,  # ''
        53: 2,  # ''
        38: 2,  # ''
        49: 2,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 0,  # ''
        56: 2,  # ''
        50: 0,  # ''
        57: 2,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 2,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 1,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 2,  # ''
        2: 1,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 1,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    44: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 1,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 1,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 0,  # ''
        33: 1,  # ''
        45: 2,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 1,  # ''
        17: 3,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 2,  # ''
        26: 2,  # ''
        27: 2,  # ''
    },
    53: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 2,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 2,  # ''
        33: 0,  # ''
        45: 2,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 2,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 0,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 1,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 2,  # ''
        26: 2,  # ''
        27: 0,  # ''
    },
    38: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 2,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 2,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 2,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 2,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 3,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 2,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    49: {  # ''
        60: 2,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 2,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 2,  # ''
        17: 0,  # ''
        18: 2,  # ''
        22: 0,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 1,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 1,  # ''
        19: 2,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    59: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 1,  # ''
        40: 1,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 1,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 2,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    39: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 1,  # ''
        43: 2,  # ''
        41: 2,  # ''
        34: 2,  # ''
        40: 1,  # ''
        52: 2,  # ''
        47: 2,  # ''
        44: 2,  # ''
        53: 2,  # ''
        38: 2,  # ''
        49: 2,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 2,  # ''
        48: 2,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 2,  # ''
        56: 2,  # ''
        50: 2,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 2,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 2,  # ''
        16: 2,  # ''
        10: 2,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 2,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 2,  # ''
        12: 2,  # ''
        28: 1,  # ''
        23: 1,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 2,  # ''
        27: 0,  # ''
    },
    35: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 2,  # ''
        38: 1,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 0,  # ''
        33: 1,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 1,  # ''
        57: 2,  # ''
        17: 2,  # ''
        18: 1,  # ''
        22: 1,  # ''
        15: 2,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 2,  # ''
        26: 0,  # ''
        27: 3,  # ''
    },
    48: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 1,  # ''
        41: 1,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 2,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 0,  # ''
        33: 1,  # ''
        45: 1,  # ''
        56: 0,  # ''
        50: 1,  # ''
        57: 1,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 2,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 1,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 2,  # ''
        27: 0,  # ''
    },
    37: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 1,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 2,  # ''
        53: 0,  # ''
        38: 2,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 2,  # ''
        56: 0,  # ''
        50: 2,  # ''
        57: 2,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 2,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 2,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 2,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 2,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 2,  # ''
        27: 2,  # ''
    },
    33: {  # ''
        60: 0,  # 'e'
        55: 1,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 2,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 2,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 0,  # ''
        33: 1,  # ''
        45: 1,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 2,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 0,  # ''
        15: 2,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 2,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 2,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 2,  # ''
        26: 2,  # ''
        27: 3,  # ''
    },
    45: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 2,  # ''
        41: 0,  # ''
        34: 1,  # ''
        40: 2,  # ''
        52: 2,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 1,  # ''
        38: 2,  # ''
        49: 2,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 2,  # ''
        48: 1,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 1,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 3,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    56: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 1,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 1,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 2,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 2,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 1,  # ''
        27: 1,  # ''
    },
    50: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 1,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 2,  # ''
        40: 2,  # ''
        52: 0,  # ''
        47: 2,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 1,  # ''
        59: 0,  # ''
        39: 1,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 1,  # ''
        57: 1,  # ''
        17: 2,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 2,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 2,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    57: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 1,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 1,  # ''
        38: 0,  # ''
        49: 2,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 2,  # ''
        37: 2,  # ''
        33: 2,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 2,  # ''
        14: 2,  # ''
        7: 2,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 1,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    17: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 3,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 3,  # ''
        32: 3,  # ''
        13: 0,  # ''
        25: 3,  # ''
        5: 2,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 3,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    18: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 3,  # ''
        29: 2,  # ''
        20: 3,  # ''
        21: 2,  # ''
        3: 3,  # ''
        32: 2,  # ''
        13: 0,  # ''
        25: 3,  # ''
        5: 0,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 3,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    22: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 1,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 3,  # ''
        5: 0,  # ''
        11: 3,  # ''
        16: 2,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    15: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 3,  # ''
        29: 2,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 3,  # ''
        32: 3,  # ''
        13: 3,  # ''
        25: 3,  # ''
        5: 0,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 1,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 3,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    1: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 2,  # ''
        22: 0,  # ''
        15: 3,  # ''
        1: 0,  # ''
        29: 3,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 2,  # ''
        32: 3,  # ''
        13: 1,  # ''
        25: 3,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 2,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 0,  # ''
        19: 2,  # ''
        26: 2,  # ''
        27: 0,  # ''
    },
    29: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 2,  # ''
        22: 3,  # ''
        15: 2,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 2,  # ''
        21: 2,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 3,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 2,  # ''
        26: 2,  # ''
        27: 2,  # ''
    },
    20: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 3,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 3,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 2,  # ''
        27: 3,  # ''
    },
    21: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    3: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 3,  # ''
        1: 2,  # ''
        29: 3,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 2,  # ''
        32: 2,  # ''
        13: 0,  # ''
        25: 3,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 2,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 3,  # ''
        19: 2,  # ''
        26: 3,  # ''
        27: 2,  # ''
    },
    32: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 2,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 1,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 2,  # ''
        26: 0,  # ''
        27: 2,  # ''
    },
    13: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 3,  # ''
        21: 2,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 3,  # ''
        5: 0,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    25: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 2,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 1,  # ''
        10: 3,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    5: {  # ''
        60: 0,  # 'e'
        55: 1,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 1,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 0,  # ''
        1: 3,  # ''
        29: 3,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 3,  # ''
        32: 2,  # ''
        13: 3,  # ''
        25: 3,  # ''
        5: 0,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 0,  # ''
        27: 3,  # ''
    },
    11: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 3,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 2,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 2,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 2,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    16: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 1,  # ''
        20: 2,  # ''
        21: 1,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 2,  # ''
        5: 3,  # ''
        11: 2,  # ''
        16: 3,  # ''
        10: 2,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 2,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    10: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 1,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 3,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 2,  # ''
        28: 3,  # ''
        23: 0,  # ''
        42: 2,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 2,  # ''
        27: 2,  # ''
    },
    6: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 3,  # ''
        3: 3,  # ''
        32: 2,  # ''
        13: 3,  # ''
        25: 3,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 1,  # ''
        10: 0,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    30: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 2,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 2,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 2,  # ''
        26: 3,  # ''
        27: 1,  # ''
    },
    4: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 2,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 2,  # ''
        29: 3,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 3,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 2,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 2,  # ''
        19: 1,  # ''
        26: 3,  # ''
        27: 2,  # ''
    },
    9: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 3,  # ''
        10: 0,  # ''
        6: 2,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 2,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 2,  # ''
        27: 3,  # ''
    },
    8: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 2,  # ''
        20: 3,  # ''
        21: 2,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 3,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 1,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 3,  # ''
        9: 2,  # ''
        8: 2,  # ''
        14: 0,  # ''
        7: 2,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    14: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 2,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 0,  # ''
        5: 0,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 0,  # ''
        12: 0,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    7: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 3,  # ''
        20: 0,  # ''
        21: 2,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 3,  # ''
        5: 3,  # ''
        11: 3,  # ''
        16: 2,  # ''
        10: 3,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 3,  # ''
        23: 3,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 2,  # ''
    },
    2: {  # ''
        60: 0,  # 'e'
        55: 2,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 2,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 3,  # ''
        11: 2,  # ''
        16: 2,  # ''
        10: 3,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 2,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    12: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 3,  # ''
        15: 2,  # ''
        1: 3,  # ''
        29: 2,  # ''
        20: 3,  # ''
        21: 2,  # ''
        3: 2,  # ''
        32: 2,  # ''
        13: 2,  # ''
        25: 3,  # ''
        5: 2,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 3,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 2,  # ''
        19: 2,  # ''
        26: 0,  # ''
        27: 2,  # ''
    },
    28: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 3,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 2,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 0,  # ''
        6: 1,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 1,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 2,  # ''
        27: 2,  # ''
    },
    23: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 3,  # ''
        18: 2,  # ''
        22: 3,  # ''
        15: 3,  # ''
        1: 3,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 2,  # ''
        5: 3,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 2,  # ''
        6: 3,  # ''
        30: 0,  # ''
        4: 3,  # ''
        9: 0,  # ''
        8: 3,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 3,  # ''
        12: 3,  # ''
        28: 0,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 3,  # ''
        19: 3,  # ''
        26: 3,  # ''
        27: 3,  # ''
    },
    42: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 2,  # ''
        18: 2,  # ''
        22: 1,  # ''
        15: 2,  # ''
        1: 2,  # ''
        29: 0,  # ''
        20: 0,  # ''
        21: 0,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 3,  # ''
        25: 0,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 0,  # ''
        10: 0,  # ''
        6: 0,  # ''
        30: 0,  # ''
        4: 2,  # ''
        9: 0,  # ''
        8: 0,  # ''
        14: 0,  # ''
        7: 0,  # ''
        2: 2,  # ''
        12: 1,  # ''
        28: 0,  # ''
        23: 0,  # ''
        42: 0,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    24: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 1,  # ''
        18: 0,  # ''
        22: 2,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 2,  # ''
        20: 3,  # ''
        21: 2,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 0,  # ''
        25: 3,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 0,  # ''
        4: 0,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 2,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    19: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 3,  # ''
        20: 3,  # ''
        21: 3,  # ''
        3: 1,  # ''
        32: 2,  # ''
        13: 2,  # ''
        25: 2,  # ''
        5: 2,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 1,  # ''
        4: 2,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 3,  # ''
        42: 2,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    26: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 2,  # ''
        29: 2,  # ''
        20: 2,  # ''
        21: 1,  # ''
        3: 3,  # ''
        32: 0,  # ''
        13: 2,  # ''
        25: 3,  # ''
        5: 0,  # ''
        11: 3,  # ''
        16: 3,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 2,  # ''
        4: 3,  # ''
        9: 3,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 2,  # ''
        23: 2,  # ''
        42: 2,  # ''
        24: 2,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
    27: {  # ''
        60: 0,  # 'e'
        55: 0,  # 'o'
        58: 0,  # 't'
        36: 0,  # ''
        61: 0,  # ''
        46: 0,  # ''
        54: 0,  # ''
        31: 0,  # ''
        51: 0,  # ''
        43: 0,  # ''
        41: 0,  # ''
        34: 0,  # ''
        40: 0,  # ''
        52: 0,  # ''
        47: 0,  # ''
        44: 0,  # ''
        53: 0,  # ''
        38: 0,  # ''
        49: 0,  # ''
        59: 0,  # ''
        39: 0,  # ''
        35: 0,  # ''
        48: 0,  # ''
        37: 0,  # ''
        33: 0,  # ''
        45: 0,  # ''
        56: 0,  # ''
        50: 0,  # ''
        57: 0,  # ''
        17: 0,  # ''
        18: 0,  # ''
        22: 0,  # ''
        15: 0,  # ''
        1: 0,  # ''
        29: 1,  # ''
        20: 0,  # ''
        21: 3,  # ''
        3: 0,  # ''
        32: 0,  # ''
        13: 1,  # ''
        25: 2,  # ''
        5: 2,  # ''
        11: 0,  # ''
        16: 2,  # ''
        10: 3,  # ''
        6: 3,  # ''
        30: 1,  # ''
        4: 0,  # ''
        9: 2,  # ''
        8: 3,  # ''
        14: 3,  # ''
        7: 3,  # ''
        2: 3,  # ''
        12: 0,  # ''
        28: 1,  # ''
        23: 1,  # ''
        42: 0,  # ''
        24: 0,  # ''
        19: 0,  # ''
        26: 0,  # ''
        27: 0,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
WINDOWS_1253_GREEK_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 82,  # 'A'
    66: 100,  # 'B'
    67: 104,  # 'C'
    68: 94,  # 'D'
    69: 98,  # 'E'
    70: 101,  # 'F'
    71: 116,  # 'G'
    72: 102,  # 'H'
    73: 111,  # 'I'
    74: 187,  # 'J'
    75: 117,  # 'K'
    76: 92,  # 'L'
    77: 88,  # 'M'
    78: 113,  # 'N'
    79: 85,  # 'O'
    80: 79,  # 'P'
    81: 118,  # 'Q'
    82: 105,  # 'R'
    83: 83,  # 'S'
    84: 67,  # 'T'
    85: 114,  # 'U'
    86: 119,  # 'V'
    87: 95,  # 'W'
    88: 99,  # 'X'
    89: 109,  # 'Y'
    90: 188,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 72,  # 'a'
    98: 70,  # 'b'
    99: 80,  # 'c'
    100: 81,  # 'd'
    101: 60,  # 'e'
    102: 96,  # 'f'
    103: 93,  # 'g'
    104: 89,  # 'h'
    105: 68,  # 'i'
    106: 120,  # 'j'
    107: 97,  # 'k'
    108: 77,  # 'l'
    109: 86,  # 'm'
    110: 69,  # 'n'
    111: 55,  # 'o'
    112: 78,  # 'p'
    113: 115,  # 'q'
    114: 65,  # 'r'
    115: 66,  # 's'
    116: 58,  # 't'
    117: 76,  # 'u'
    118: 106,  # 'v'
    119: 103,  # 'w'
    120: 87,  # 'x'
    121: 107,  # 'y'
    122: 112,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 255,  # ''
    129: 255,  # None
    130: 255,  # ''
    131: 255,  # ''
    132: 255,  # ''
    133: 255,  # ''
    134: 255,  # ''
    135: 255,  # ''
    136: 255,  # None
    137: 255,  # ''
    138: 255,  # None
    139: 255,  # ''
    140: 255,  # None
    141: 255,  # None
    142: 255,  # None
    143: 255,  # None
    144: 255,  # None
    145: 255,  # ''
    146: 255,  # ''
    147: 255,  # ''
    148: 255,  # ''
    149: 255,  # ''
    150: 255,  # ''
    151: 255,  # ''
    152: 255,  # None
    153: 255,  # ''
    154: 255,  # None
    155: 255,  # ''
    156: 255,  # None
    157: 255,  # None
    158: 255,  # None
    159: 255,  # None
    160: 253,  # '\xa0'
    161: 233,  # ''
    162: 61,  # ''
    163: 253,  # ''
    164: 253,  # ''
    165: 253,  # ''
    166: 253,  # ''
    167: 253,  # ''
    168: 253,  # ''
    169: 253,  # ''
    170: 253,  # None
    171: 253,  # ''
    172: 253,  # ''
    173: 74,  # '\xad'
    174: 253,  # ''
    175: 253,  # ''
    176: 253,  # ''
    177: 253,  # ''
    178: 253,  # ''
    179: 253,  # ''
    180: 247,  # ''
    181: 253,  # ''
    182: 253,  # ''
    183: 36,  # ''
    184: 46,  # ''
    185: 71,  # ''
    186: 73,  # ''
    187: 253,  # ''
    188: 54,  # ''
    189: 253,  # ''
    190: 108,  # ''
    191: 123,  # ''
    192: 110,  # ''
    193: 31,  # ''
    194: 51,  # ''
    195: 43,  # ''
    196: 41,  # ''
    197: 34,  # ''
    198: 91,  # ''
    199: 40,  # ''
    200: 52,  # ''
    201: 47,  # ''
    202: 44,  # ''
    203: 53,  # ''
    204: 38,  # ''
    205: 49,  # ''
    206: 59,  # ''
    207: 39,  # ''
    208: 35,  # ''
    209: 48,  # ''
    210: 250,  # None
    211: 37,  # ''
    212: 33,  # ''
    213: 45,  # ''
    214: 56,  # ''
    215: 50,  # ''
    216: 84,  # ''
    217: 57,  # ''
    218: 120,  # ''
    219: 121,  # ''
    220: 17,  # ''
    221: 18,  # ''
    222: 22,  # ''
    223: 15,  # ''
    224: 124,  # ''
    225: 1,  # ''
    226: 29,  # ''
    227: 20,  # ''
    228: 21,  # ''
    229: 3,  # ''
    230: 32,  # ''
    231: 13,  # ''
    232: 25,  # ''
    233: 5,  # ''
    234: 11,  # ''
    235: 16,  # ''
    236: 10,  # ''
    237: 6,  # ''
    238: 30,  # ''
    239: 4,  # ''
    240: 9,  # ''
    241: 8,  # ''
    242: 14,  # ''
    243: 7,  # ''
    244: 2,  # ''
    245: 12,  # ''
    246: 28,  # ''
    247: 23,  # ''
    248: 42,  # ''
    249: 24,  # ''
    250: 64,  # ''
    251: 75,  # ''
    252: 19,  # ''
    253: 26,  # ''
    254: 27,  # ''
    255: 253,  # None
}

WINDOWS_1253_GREEK_MODEL = SingleByteCharSetModel(
    charset_name="windows-1253",
    language="Greek",
    char_to_order_map=WINDOWS_1253_GREEK_CHAR_TO_ORDER,
    language_model=GREEK_LANG_MODEL,
    typical_positive_ratio=0.982851,
    keep_ascii_letters=False,
    alphabet="",
)

ISO_8859_7_GREEK_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 82,  # 'A'
    66: 100,  # 'B'
    67: 104,  # 'C'
    68: 94,  # 'D'
    69: 98,  # 'E'
    70: 101,  # 'F'
    71: 116,  # 'G'
    72: 102,  # 'H'
    73: 111,  # 'I'
    74: 187,  # 'J'
    75: 117,  # 'K'
    76: 92,  # 'L'
    77: 88,  # 'M'
    78: 113,  # 'N'
    79: 85,  # 'O'
    80: 79,  # 'P'
    81: 118,  # 'Q'
    82: 105,  # 'R'
    83: 83,  # 'S'
    84: 67,  # 'T'
    85: 114,  # 'U'
    86: 119,  # 'V'
    87: 95,  # 'W'
    88: 99,  # 'X'
    89: 109,  # 'Y'
    90: 188,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 72,  # 'a'
    98: 70,  # 'b'
    99: 80,  # 'c'
    100: 81,  # 'd'
    101: 60,  # 'e'
    102: 96,  # 'f'
    103: 93,  # 'g'
    104: 89,  # 'h'
    105: 68,  # 'i'
    106: 120,  # 'j'
    107: 97,  # 'k'
    108: 77,  # 'l'
    109: 86,  # 'm'
    110: 69,  # 'n'
    111: 55,  # 'o'
    112: 78,  # 'p'
    113: 115,  # 'q'
    114: 65,  # 'r'
    115: 66,  # 's'
    116: 58,  # 't'
    117: 76,  # 'u'
    118: 106,  # 'v'
    119: 103,  # 'w'
    120: 87,  # 'x'
    121: 107,  # 'y'
    122: 112,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 255,  # '\x80'
    129: 255,  # '\x81'
    130: 255,  # '\x82'
    131: 255,  # '\x83'
    132: 255,  # '\x84'
    133: 255,  # '\x85'
    134: 255,  # '\x86'
    135: 255,  # '\x87'
    136: 255,  # '\x88'
    137: 255,  # '\x89'
    138: 255,  # '\x8a'
    139: 255,  # '\x8b'
    140: 255,  # '\x8c'
    141: 255,  # '\x8d'
    142: 255,  # '\x8e'
    143: 255,  # '\x8f'
    144: 255,  # '\x90'
    145: 255,  # '\x91'
    146: 255,  # '\x92'
    147: 255,  # '\x93'
    148: 255,  # '\x94'
    149: 255,  # '\x95'
    150: 255,  # '\x96'
    151: 255,  # '\x97'
    152: 255,  # '\x98'
    153: 255,  # '\x99'
    154: 255,  # '\x9a'
    155: 255,  # '\x9b'
    156: 255,  # '\x9c'
    157: 255,  # '\x9d'
    158: 255,  # '\x9e'
    159: 255,  # '\x9f'
    160: 253,  # '\xa0'
    161: 233,  # ''
    162: 90,  # ''
    163: 253,  # ''
    164: 253,  # ''
    165: 253,  # ''
    166: 253,  # ''
    167: 253,  # ''
    168: 253,  # ''
    169: 253,  # ''
    170: 253,  # ''
    171: 253,  # ''
    172: 253,  # ''
    173: 74,  # '\xad'
    174: 253,  # None
    175: 253,  # ''
    176: 253,  # ''
    177: 253,  # ''
    178: 253,  # ''
    179: 253,  # ''
    180: 247,  # ''
    181: 248,  # ''
    182: 61,  # ''
    183: 36,  # ''
    184: 46,  # ''
    185: 71,  # ''
    186: 73,  # ''
    187: 253,  # ''
    188: 54,  # ''
    189: 253,  # ''
    190: 108,  # ''
    191: 123,  # ''
    192: 110,  # ''
    193: 31,  # ''
    194: 51,  # ''
    195: 43,  # ''
    196: 41,  # ''
    197: 34,  # ''
    198: 91,  # ''
    199: 40,  # ''
    200: 52,  # ''
    201: 47,  # ''
    202: 44,  # ''
    203: 53,  # ''
    204: 38,  # ''
    205: 49,  # ''
    206: 59,  # ''
    207: 39,  # ''
    208: 35,  # ''
    209: 48,  # ''
    210: 250,  # None
    211: 37,  # ''
    212: 33,  # ''
    213: 45,  # ''
    214: 56,  # ''
    215: 50,  # ''
    216: 84,  # ''
    217: 57,  # ''
    218: 120,  # ''
    219: 121,  # ''
    220: 17,  # ''
    221: 18,  # ''
    222: 22,  # ''
    223: 15,  # ''
    224: 124,  # ''
    225: 1,  # ''
    226: 29,  # ''
    227: 20,  # ''
    228: 21,  # ''
    229: 3,  # ''
    230: 32,  # ''
    231: 13,  # ''
    232: 25,  # ''
    233: 5,  # ''
    234: 11,  # ''
    235: 16,  # ''
    236: 10,  # ''
    237: 6,  # ''
    238: 30,  # ''
    239: 4,  # ''
    240: 9,  # ''
    241: 8,  # ''
    242: 14,  # ''
    243: 7,  # ''
    244: 2,  # ''
    245: 12,  # ''
    246: 28,  # ''
    247: 23,  # ''
    248: 42,  # ''
    249: 24,  # ''
    250: 64,  # ''
    251: 75,  # ''
    252: 19,  # ''
    253: 26,  # ''
    254: 27,  # ''
    255: 253,  # None
}

ISO_8859_7_GREEK_MODEL = SingleByteCharSetModel(
    charset_name="ISO-8859-7",
    language="Greek",
    char_to_order_map=ISO_8859_7_GREEK_CHAR_TO_ORDER,
    language_model=GREEK_LANG_MODEL,
    typical_positive_ratio=0.982851,
    keep_ascii_letters=False,
    alphabet="",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langthaimodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

THAI_LANG_MODEL = {
    5: {  # ''
        5: 2,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 2,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 3,  # ''
        57: 2,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 2,  # ''
        20: 2,  # ''
        19: 3,  # ''
        44: 0,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 1,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 1,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 2,  # ''
        16: 1,  # ''
        2: 3,  # ''
        61: 2,  # ''
        15: 3,  # ''
        12: 3,  # ''
        42: 2,  # ''
        46: 3,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 3,  # ''
        63: 1,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 3,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 0,  # ''
        27: 2,  # ''
        32: 2,  # ''
        35: 1,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 1,  # ''
        33: 2,  # ''
        50: 1,  # ''
        37: 3,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    30: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 1,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 2,  # ''
        20: 0,  # ''
        19: 2,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 1,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 2,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 1,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 2,  # ''
        40: 3,  # ''
        27: 1,  # ''
        32: 1,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 2,  # ''
        7: 3,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    24: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 2,  # ''
        8: 2,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 2,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 0,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 2,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 3,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 2,  # ''
        36: 3,  # ''
        23: 3,  # ''
        13: 2,  # ''
        40: 0,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 2,  # ''
        11: 1,  # ''
        28: 0,  # ''
        41: 3,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    8: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 3,  # ''
        8: 2,  # ''
        26: 2,  # ''
        52: 1,  # ''
        34: 2,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 3,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 1,  # ''
        31: 2,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 2,  # ''
        16: 1,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 2,  # ''
        42: 2,  # ''
        46: 1,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 1,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 1,  # ''
        32: 1,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 3,  # ''
        37: 0,  # ''
        6: 2,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    26: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 0,  # ''
        8: 2,  # ''
        26: 3,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 1,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 1,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 1,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 1,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 3,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 3,  # ''
        23: 2,  # ''
        13: 1,  # ''
        40: 3,  # ''
        27: 1,  # ''
        32: 3,  # ''
        35: 2,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 2,  # ''
        7: 2,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    52: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 3,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 3,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 1,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 1,  # ''
        1: 1,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 1,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    34: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 1,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 1,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 1,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 2,  # ''
        1: 3,  # ''
        36: 1,  # ''
        23: 3,  # ''
        13: 2,  # ''
        40: 0,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 1,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    51: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 1,  # ''
        1: 1,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 2,  # ''
        40: 3,  # ''
        27: 2,  # ''
        32: 1,  # ''
        35: 1,  # ''
        11: 1,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 1,  # ''
        7: 2,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    47: {  # ''
        5: 1,  # ''
        30: 1,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 3,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 1,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 2,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 2,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 0,  # ''
        50: 1,  # ''
        37: 0,  # ''
        6: 2,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    58: {  # ''
        5: 2,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 1,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 2,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    57: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 3,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    49: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 2,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    53: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    55: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    43: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 3,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 3,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 1,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 3,  # ''
        10: 0,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 2,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    20: {  # ''
        5: 2,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 1,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 2,  # ''
        16: 3,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 3,  # ''
        1: 2,  # ''
        36: 2,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 1,  # ''
        27: 2,  # ''
        32: 3,  # ''
        35: 2,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 2,  # ''
        37: 2,  # ''
        6: 1,  # ''
        7: 3,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    19: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 1,  # ''
        44: 2,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 1,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 2,  # ''
        9: 1,  # ''
        16: 1,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 3,  # ''
        21: 0,  # ''
        4: 3,  # ''
        63: 1,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 2,  # ''
        23: 3,  # ''
        13: 2,  # ''
        40: 1,  # ''
        27: 1,  # ''
        32: 3,  # ''
        35: 2,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 1,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 2,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    44: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 2,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 2,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 1,  # ''
        40: 3,  # ''
        27: 2,  # ''
        32: 2,  # ''
        35: 3,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 2,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    14: {  # ''
        5: 1,  # ''
        30: 1,  # ''
        24: 3,  # ''
        8: 1,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 3,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 2,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 3,  # ''
        2: 3,  # ''
        61: 1,  # ''
        15: 1,  # ''
        12: 2,  # ''
        42: 3,  # ''
        46: 1,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 3,  # ''
        23: 2,  # ''
        13: 3,  # ''
        40: 2,  # ''
        27: 1,  # ''
        32: 3,  # ''
        35: 1,  # ''
        11: 0,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    48: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 1,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 2,  # ''
        36: 0,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 2,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    3: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 3,  # ''
        8: 1,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 2,  # ''
        14: 3,  # ''
        48: 3,  # ''
        3: 2,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 0,  # ''
        31: 2,  # ''
        54: 1,  # ''
        45: 1,  # ''
        9: 2,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 1,  # ''
        15: 2,  # ''
        12: 3,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 3,  # ''
        63: 1,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 3,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 3,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 2,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 3,  # ''
        29: 3,  # ''
        33: 3,  # ''
        50: 2,  # ''
        37: 1,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    17: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 1,  # ''
        26: 1,  # ''
        52: 1,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 3,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 1,  # ''
        45: 1,  # ''
        9: 1,  # ''
        16: 0,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 3,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 2,  # ''
        63: 1,  # ''
        22: 0,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 2,  # ''
        23: 2,  # ''
        13: 2,  # ''
        40: 0,  # ''
        27: 2,  # ''
        32: 3,  # ''
        35: 2,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 2,  # ''
        7: 2,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    25: {  # ''
        5: 2,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 1,  # ''
        57: 3,  # ''
        49: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 1,  # ''
        44: 1,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 0,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 1,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 0,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 1,  # ''
        18: 2,  # ''
        21: 1,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 3,  # ''
        1: 1,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 3,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 1,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 2,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 2,  # ''
        50: 0,  # ''
        37: 3,  # ''
        6: 1,  # ''
        7: 2,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    39: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 1,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 2,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 1,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 1,  # ''
        32: 0,  # ''
        35: 3,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 1,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    62: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 1,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 1,  # ''
        40: 2,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 2,  # ''
        7: 1,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    31: {  # ''
        5: 1,  # ''
        30: 1,  # ''
        24: 1,  # ''
        8: 1,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 1,  # ''
        20: 1,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 2,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 0,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 2,  # ''
        2: 3,  # ''
        61: 2,  # ''
        15: 2,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 1,  # ''
        4: 2,  # ''
        63: 1,  # ''
        22: 0,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 3,  # ''
        13: 2,  # ''
        40: 1,  # ''
        27: 3,  # ''
        32: 1,  # ''
        35: 2,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 1,  # ''
        6: 0,  # ''
        7: 1,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    54: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 2,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 2,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 1,  # ''
        32: 1,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 2,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    45: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 3,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 2,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    9: {  # ''
        5: 2,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 2,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 1,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 3,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 2,  # ''
        16: 1,  # ''
        2: 2,  # ''
        61: 2,  # ''
        15: 2,  # ''
        12: 2,  # ''
        42: 1,  # ''
        46: 1,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 0,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 3,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 2,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 1,  # ''
        37: 1,  # ''
        6: 3,  # ''
        7: 2,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    16: {  # ''
        5: 3,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 2,  # ''
        51: 0,  # ''
        47: 2,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 2,  # ''
        16: 0,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 3,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 1,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 3,  # ''
        40: 1,  # ''
        27: 2,  # ''
        32: 2,  # ''
        35: 3,  # ''
        11: 2,  # ''
        28: 1,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 2,  # ''
        37: 1,  # ''
        6: 3,  # ''
        7: 2,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    2: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 2,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 3,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 3,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 3,  # ''
        14: 3,  # ''
        48: 1,  # ''
        3: 2,  # ''
        17: 2,  # ''
        25: 3,  # ''
        39: 2,  # ''
        62: 1,  # ''
        31: 2,  # ''
        54: 1,  # ''
        45: 1,  # ''
        9: 3,  # ''
        16: 2,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 3,  # ''
        42: 2,  # ''
        46: 2,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 3,  # ''
        63: 1,  # ''
        22: 3,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 2,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 3,  # ''
        11: 3,  # ''
        28: 3,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 3,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 3,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    61: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 2,  # ''
        44: 0,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 2,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    15: {  # ''
        5: 2,  # ''
        30: 3,  # ''
        24: 1,  # ''
        8: 3,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 1,  # ''
        16: 3,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 1,  # ''
        4: 3,  # ''
        63: 2,  # ''
        22: 3,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 2,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 2,  # ''
        27: 3,  # ''
        32: 2,  # ''
        35: 3,  # ''
        11: 2,  # ''
        28: 1,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 2,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    12: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 1,  # ''
        8: 3,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 1,  # ''
        20: 2,  # ''
        19: 1,  # ''
        44: 1,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 1,  # ''
        45: 0,  # ''
        9: 3,  # ''
        16: 3,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 3,  # ''
        13: 2,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 2,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 1,  # ''
        33: 2,  # ''
        50: 1,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    42: {  # ''
        5: 1,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 1,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 2,  # ''
        42: 1,  # ''
        46: 2,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 2,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 2,  # ''
        13: 0,  # ''
        40: 3,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 2,  # ''
        11: 0,  # ''
        28: 1,  # ''
        41: 0,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    46: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 2,  # ''
        57: 1,  # ''
        49: 2,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 3,  # ''
        20: 0,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 1,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 2,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    18: {  # ''
        5: 2,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 2,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 3,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 2,  # ''
        9: 3,  # ''
        16: 1,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 2,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 3,  # ''
        23: 3,  # ''
        13: 3,  # ''
        40: 2,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 3,  # ''
        11: 2,  # ''
        28: 0,  # ''
        41: 1,  # ''
        29: 0,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 1,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    21: {  # ''
        5: 3,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 1,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 2,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 3,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 0,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        16: 2,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 0,  # ''
        23: 1,  # ''
        13: 1,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 1,  # ''
        35: 1,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 3,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    4: {  # ''
        5: 3,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 1,  # ''
        45: 1,  # ''
        9: 3,  # ''
        16: 3,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 2,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 2,  # ''
        10: 3,  # ''
        1: 3,  # ''
        36: 2,  # ''
        23: 2,  # ''
        13: 3,  # ''
        40: 0,  # ''
        27: 3,  # ''
        32: 3,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 1,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 1,  # ''
        37: 1,  # ''
        6: 2,  # ''
        7: 2,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    63: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    22: {  # ''
        5: 3,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 1,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 3,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 1,  # ''
        14: 3,  # ''
        48: 1,  # ''
        3: 2,  # ''
        17: 3,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 2,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 3,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 2,  # ''
        63: 1,  # ''
        22: 1,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    10: {  # ''
        5: 3,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 3,  # ''
        26: 3,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 3,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 2,  # ''
        53: 0,  # ''
        55: 3,  # ''
        43: 3,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 0,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 2,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        16: 3,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 3,  # ''
        42: 2,  # ''
        46: 0,  # ''
        18: 3,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    1: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 3,  # ''
        8: 3,  # ''
        26: 3,  # ''
        52: 0,  # ''
        34: 3,  # ''
        51: 1,  # ''
        47: 2,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 3,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 1,  # ''
        14: 3,  # ''
        48: 2,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 1,  # ''
        31: 3,  # ''
        54: 1,  # ''
        45: 1,  # ''
        9: 3,  # ''
        16: 3,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 3,  # ''
        42: 2,  # ''
        46: 3,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 2,  # ''
        63: 1,  # ''
        22: 3,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 1,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    36: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 3,  # ''
        8: 2,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 1,  # ''
        44: 1,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 1,  # ''
        25: 1,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 1,  # ''
        16: 0,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 3,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    23: {  # ''
        5: 3,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 3,  # ''
        52: 0,  # ''
        34: 3,  # ''
        51: 0,  # ''
        47: 2,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 1,  # ''
        14: 3,  # ''
        48: 3,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 0,  # ''
        31: 3,  # ''
        54: 1,  # ''
        45: 2,  # ''
        9: 3,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 3,  # ''
        42: 3,  # ''
        46: 2,  # ''
        18: 2,  # ''
        21: 3,  # ''
        4: 1,  # ''
        63: 1,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 1,  # ''
        41: 1,  # ''
        29: 1,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 2,  # ''
        38: 2,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    13: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 1,  # ''
        62: 0,  # ''
        31: 2,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 3,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 2,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 1,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 1,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    40: {  # ''
        5: 3,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 3,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 1,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    27: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 3,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    32: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 3,  # ''
        8: 3,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 2,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 1,  # ''
        43: 3,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 1,  # ''
        3: 2,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 1,  # ''
        9: 3,  # ''
        16: 1,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 1,  # ''
        42: 1,  # ''
        46: 2,  # ''
        18: 1,  # ''
        21: 1,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 0,  # ''
        41: 1,  # ''
        29: 0,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 2,  # ''
        38: 1,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    35: {  # ''
        5: 3,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 2,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 2,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 1,  # ''
        20: 2,  # ''
        19: 2,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 2,  # ''
        17: 0,  # ''
        25: 3,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 1,  # ''
        41: 1,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 3,  # ''
        7: 3,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    11: {  # ''
        5: 3,  # ''
        30: 3,  # ''
        24: 3,  # ''
        8: 2,  # ''
        26: 3,  # ''
        52: 3,  # ''
        34: 3,  # ''
        51: 2,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 1,  # ''
        20: 3,  # ''
        19: 3,  # ''
        44: 1,  # ''
        14: 3,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 3,  # ''
        39: 2,  # ''
        62: 1,  # ''
        31: 3,  # ''
        54: 1,  # ''
        45: 3,  # ''
        9: 3,  # ''
        16: 2,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 3,  # ''
        42: 2,  # ''
        46: 0,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    28: {  # ''
        5: 3,  # ''
        30: 2,  # ''
        24: 2,  # ''
        8: 1,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 3,  # ''
        44: 2,  # ''
        14: 3,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 2,  # ''
        39: 3,  # ''
        62: 0,  # ''
        31: 2,  # ''
        54: 2,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    41: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 1,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 2,  # ''
        44: 0,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 1,  # ''
        25: 3,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 1,  # ''
        45: 1,  # ''
        9: 1,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 3,  # ''
        12: 0,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 0,  # ''
        4: 2,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    29: {  # ''
        5: 2,  # ''
        30: 0,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 3,  # ''
        52: 0,  # ''
        34: 3,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 1,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 3,  # ''
        21: 3,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    33: {  # ''
        5: 1,  # ''
        30: 2,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 3,  # ''
        19: 1,  # ''
        44: 0,  # ''
        14: 3,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 1,  # ''
        25: 3,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 2,  # ''
        45: 0,  # ''
        9: 3,  # ''
        16: 0,  # ''
        2: 3,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 3,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 2,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    50: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    37: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 2,  # ''
        26: 3,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 1,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 2,  # ''
        44: 0,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 3,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 1,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 2,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 0,  # ''
        4: 1,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 1,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    6: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 1,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 1,  # ''
        3: 3,  # ''
        17: 1,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 1,  # ''
        31: 1,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 3,  # ''
        16: 3,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 2,  # ''
        12: 3,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 1,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 1,  # ''
        10: 0,  # ''
        1: 3,  # ''
        36: 2,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 3,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 1,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    7: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 2,  # ''
        8: 3,  # ''
        26: 2,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 1,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 1,  # ''
        19: 2,  # ''
        44: 1,  # ''
        14: 2,  # ''
        48: 0,  # ''
        3: 3,  # ''
        17: 2,  # ''
        25: 2,  # ''
        39: 2,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 1,  # ''
        45: 0,  # ''
        9: 3,  # ''
        16: 2,  # ''
        2: 2,  # ''
        61: 0,  # ''
        15: 1,  # ''
        12: 3,  # ''
        42: 1,  # ''
        46: 0,  # ''
        18: 2,  # ''
        21: 2,  # ''
        4: 3,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 3,  # ''
        36: 2,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 2,  # ''
        33: 2,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    38: {  # ''
        5: 2,  # ''
        30: 1,  # ''
        24: 1,  # ''
        8: 0,  # ''
        26: 1,  # ''
        52: 0,  # ''
        34: 1,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 2,  # ''
        19: 1,  # ''
        44: 1,  # ''
        14: 1,  # ''
        48: 0,  # ''
        3: 1,  # ''
        17: 1,  # ''
        25: 1,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 1,  # ''
        54: 1,  # ''
        45: 0,  # ''
        9: 2,  # ''
        16: 0,  # ''
        2: 1,  # ''
        61: 1,  # ''
        15: 1,  # ''
        12: 1,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 1,  # ''
        21: 1,  # ''
        4: 2,  # ''
        63: 1,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 2,  # ''
        28: 2,  # ''
        41: 1,  # ''
        29: 1,  # ''
        33: 1,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 0,  # ''
        59: 0,  # ''
        60: 0,  # ''
    },
    56: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 2,  # ''
        59: 1,  # ''
        60: 1,  # ''
    },
    59: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 1,  # ''
        59: 1,  # ''
        60: 3,  # ''
    },
    60: {  # ''
        5: 0,  # ''
        30: 0,  # ''
        24: 0,  # ''
        8: 0,  # ''
        26: 0,  # ''
        52: 0,  # ''
        34: 0,  # ''
        51: 0,  # ''
        47: 0,  # ''
        58: 0,  # ''
        57: 0,  # ''
        49: 0,  # ''
        53: 0,  # ''
        55: 0,  # ''
        43: 0,  # ''
        20: 0,  # ''
        19: 0,  # ''
        44: 0,  # ''
        14: 0,  # ''
        48: 0,  # ''
        3: 0,  # ''
        17: 0,  # ''
        25: 0,  # ''
        39: 0,  # ''
        62: 0,  # ''
        31: 0,  # ''
        54: 0,  # ''
        45: 0,  # ''
        9: 0,  # ''
        16: 0,  # ''
        2: 0,  # ''
        61: 0,  # ''
        15: 0,  # ''
        12: 0,  # ''
        42: 0,  # ''
        46: 0,  # ''
        18: 0,  # ''
        21: 0,  # ''
        4: 0,  # ''
        63: 0,  # ''
        22: 0,  # ''
        10: 0,  # ''
        1: 0,  # ''
        36: 0,  # ''
        23: 0,  # ''
        13: 0,  # ''
        40: 0,  # ''
        27: 0,  # ''
        32: 0,  # ''
        35: 0,  # ''
        11: 0,  # ''
        28: 0,  # ''
        41: 0,  # ''
        29: 0,  # ''
        33: 0,  # ''
        50: 0,  # ''
        37: 0,  # ''
        6: 0,  # ''
        7: 0,  # ''
        38: 0,  # ''
        56: 2,  # ''
        59: 1,  # ''
        60: 0,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
TIS_620_THAI_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 254,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 254,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 253,  # ' '
    33: 253,  # '!'
    34: 253,  # '"'
    35: 253,  # '#'
    36: 253,  # '$'
    37: 253,  # '%'
    38: 253,  # '&'
    39: 253,  # "'"
    40: 253,  # '('
    41: 253,  # ')'
    42: 253,  # '*'
    43: 253,  # '+'
    44: 253,  # ','
    45: 253,  # '-'
    46: 253,  # '.'
    47: 253,  # '/'
    48: 252,  # '0'
    49: 252,  # '1'
    50: 252,  # '2'
    51: 252,  # '3'
    52: 252,  # '4'
    53: 252,  # '5'
    54: 252,  # '6'
    55: 252,  # '7'
    56: 252,  # '8'
    57: 252,  # '9'
    58: 253,  # ':'
    59: 253,  # ';'
    60: 253,  # '<'
    61: 253,  # '='
    62: 253,  # '>'
    63: 253,  # '?'
    64: 253,  # '@'
    65: 182,  # 'A'
    66: 106,  # 'B'
    67: 107,  # 'C'
    68: 100,  # 'D'
    69: 183,  # 'E'
    70: 184,  # 'F'
    71: 185,  # 'G'
    72: 101,  # 'H'
    73: 94,  # 'I'
    74: 186,  # 'J'
    75: 187,  # 'K'
    76: 108,  # 'L'
    77: 109,  # 'M'
    78: 110,  # 'N'
    79: 111,  # 'O'
    80: 188,  # 'P'
    81: 189,  # 'Q'
    82: 190,  # 'R'
    83: 89,  # 'S'
    84: 95,  # 'T'
    85: 112,  # 'U'
    86: 113,  # 'V'
    87: 191,  # 'W'
    88: 192,  # 'X'
    89: 193,  # 'Y'
    90: 194,  # 'Z'
    91: 253,  # '['
    92: 253,  # '\\'
    93: 253,  # ']'
    94: 253,  # '^'
    95: 253,  # '_'
    96: 253,  # '`'
    97: 64,  # 'a'
    98: 72,  # 'b'
    99: 73,  # 'c'
    100: 114,  # 'd'
    101: 74,  # 'e'
    102: 115,  # 'f'
    103: 116,  # 'g'
    104: 102,  # 'h'
    105: 81,  # 'i'
    106: 201,  # 'j'
    107: 117,  # 'k'
    108: 90,  # 'l'
    109: 103,  # 'm'
    110: 78,  # 'n'
    111: 82,  # 'o'
    112: 96,  # 'p'
    113: 202,  # 'q'
    114: 91,  # 'r'
    115: 79,  # 's'
    116: 84,  # 't'
    117: 104,  # 'u'
    118: 105,  # 'v'
    119: 97,  # 'w'
    120: 98,  # 'x'
    121: 92,  # 'y'
    122: 203,  # 'z'
    123: 253,  # '{'
    124: 253,  # '|'
    125: 253,  # '}'
    126: 253,  # '~'
    127: 253,  # '\x7f'
    128: 209,  # '\x80'
    129: 210,  # '\x81'
    130: 211,  # '\x82'
    131: 212,  # '\x83'
    132: 213,  # '\x84'
    133: 88,  # '\x85'
    134: 214,  # '\x86'
    135: 215,  # '\x87'
    136: 216,  # '\x88'
    137: 217,  # '\x89'
    138: 218,  # '\x8a'
    139: 219,  # '\x8b'
    140: 220,  # '\x8c'
    141: 118,  # '\x8d'
    142: 221,  # '\x8e'
    143: 222,  # '\x8f'
    144: 223,  # '\x90'
    145: 224,  # '\x91'
    146: 99,  # '\x92'
    147: 85,  # '\x93'
    148: 83,  # '\x94'
    149: 225,  # '\x95'
    150: 226,  # '\x96'
    151: 227,  # '\x97'
    152: 228,  # '\x98'
    153: 229,  # '\x99'
    154: 230,  # '\x9a'
    155: 231,  # '\x9b'
    156: 232,  # '\x9c'
    157: 233,  # '\x9d'
    158: 234,  # '\x9e'
    159: 235,  # '\x9f'
    160: 236,  # None
    161: 5,  # ''
    162: 30,  # ''
    163: 237,  # ''
    164: 24,  # ''
    165: 238,  # ''
    166: 75,  # ''
    167: 8,  # ''
    168: 26,  # ''
    169: 52,  # ''
    170: 34,  # ''
    171: 51,  # ''
    172: 119,  # ''
    173: 47,  # ''
    174: 58,  # ''
    175: 57,  # ''
    176: 49,  # ''
    177: 53,  # ''
    178: 55,  # ''
    179: 43,  # ''
    180: 20,  # ''
    181: 19,  # ''
    182: 44,  # ''
    183: 14,  # ''
    184: 48,  # ''
    185: 3,  # ''
    186: 17,  # ''
    187: 25,  # ''
    188: 39,  # ''
    189: 62,  # ''
    190: 31,  # ''
    191: 54,  # ''
    192: 45,  # ''
    193: 9,  # ''
    194: 16,  # ''
    195: 2,  # ''
    196: 61,  # ''
    197: 15,  # ''
    198: 239,  # ''
    199: 12,  # ''
    200: 42,  # ''
    201: 46,  # ''
    202: 18,  # ''
    203: 21,  # ''
    204: 76,  # ''
    205: 4,  # ''
    206: 66,  # ''
    207: 63,  # ''
    208: 22,  # ''
    209: 10,  # ''
    210: 1,  # ''
    211: 36,  # ''
    212: 23,  # ''
    213: 13,  # ''
    214: 40,  # ''
    215: 27,  # ''
    216: 32,  # ''
    217: 35,  # ''
    218: 86,  # ''
    219: 240,  # None
    220: 241,  # None
    221: 242,  # None
    222: 243,  # None
    223: 244,  # ''
    224: 11,  # ''
    225: 28,  # ''
    226: 41,  # ''
    227: 29,  # ''
    228: 33,  # ''
    229: 245,  # ''
    230: 50,  # ''
    231: 37,  # ''
    232: 6,  # ''
    233: 7,  # ''
    234: 67,  # ''
    235: 77,  # ''
    236: 38,  # ''
    237: 93,  # ''
    238: 246,  # ''
    239: 247,  # ''
    240: 68,  # ''
    241: 56,  # ''
    242: 59,  # ''
    243: 65,  # ''
    244: 69,  # ''
    245: 60,  # ''
    246: 70,  # ''
    247: 80,  # ''
    248: 71,  # ''
    249: 87,  # ''
    250: 248,  # ''
    251: 249,  # ''
    252: 250,  # None
    253: 251,  # None
    254: 252,  # None
    255: 253,  # None
}

TIS_620_THAI_MODEL = SingleByteCharSetModel(
    charset_name="TIS-620",
    language="Thai",
    char_to_order_map=TIS_620_THAI_CHAR_TO_ORDER,
    language_model=THAI_LANG_MODEL,
    typical_positive_ratio=0.926386,
    keep_ascii_letters=False,
    alphabet="",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/langturkishmodel.py
# ========================================================
from pip._vendor.chardet.sbcharsetprober import SingleByteCharSetModel

# 3: Positive
# 2: Likely
# 1: Unlikely
# 0: Negative

TURKISH_LANG_MODEL = {
    23: {  # 'A'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 1,  # 'h'
        3: 1,  # 'i'
        24: 0,  # 'j'
        10: 2,  # 'k'
        5: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 1,  # 'r'
        8: 1,  # 's'
        9: 1,  # 't'
        14: 1,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    37: {  # 'B'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 2,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 1,  # ''
        19: 1,  # ''
    },
    47: {  # 'C'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 1,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 1,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 2,  # 'j'
        10: 1,  # 'k'
        5: 2,  # 'l'
        13: 2,  # 'm'
        4: 2,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 2,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 1,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    39: {  # 'D'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 1,  # 'l'
        13: 3,  # 'm'
        4: 0,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 1,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 1,  # ''
        19: 0,  # ''
    },
    29: {  # 'E'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 1,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 0,  # 'h'
        3: 1,  # 'i'
        24: 1,  # 'j'
        10: 0,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 1,  # 's'
        9: 1,  # 't'
        14: 1,  # 'u'
        32: 1,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    52: {  # 'F'
        23: 0,  # 'A'
        37: 1,  # 'B'
        47: 1,  # 'C'
        39: 1,  # 'D'
        29: 1,  # 'E'
        52: 2,  # 'F'
        36: 0,  # 'G'
        45: 2,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 1,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 1,  # 'b'
        28: 1,  # 'c'
        12: 1,  # 'd'
        2: 0,  # 'e'
        18: 1,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 2,  # 'i'
        24: 1,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 1,  # 'm'
        4: 2,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 2,  # 'r'
        8: 1,  # 's'
        9: 1,  # 't'
        14: 1,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 1,  # ''
        55: 2,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 2,  # ''
    },
    36: {  # 'G'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 2,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 2,  # 'N'
        42: 1,  # 'O'
        48: 1,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 1,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 1,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 0,  # 'r'
        8: 1,  # 's'
        9: 1,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 1,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 2,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    45: {  # 'H'
        23: 0,  # 'A'
        37: 1,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 2,  # 'G'
        45: 1,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 1,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 2,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 2,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 1,  # 'o'
        26: 1,  # 'p'
        7: 1,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 2,  # ''
        41: 1,  # ''
        6: 0,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    53: {  # 'I'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 2,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 1,  # ''
        19: 1,  # ''
    },
    60: {  # 'J'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 1,  # 'd'
        2: 0,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 1,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 1,  # 's'
        9: 0,  # 't'
        14: 0,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    16: {  # 'K'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 1,  # 'e'
        18: 3,  # 'f'
        27: 3,  # 'g'
        25: 3,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 0,  # 'u'
        32: 3,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 1,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    49: {  # 'L'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 2,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 2,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 0,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 2,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 2,  # 'n'
        15: 1,  # 'o'
        26: 1,  # 'p'
        7: 1,  # 'r'
        8: 1,  # 's'
        9: 1,  # 't'
        14: 0,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 2,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 1,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    20: {  # 'M'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 1,  # 'h'
        3: 2,  # 'i'
        24: 2,  # 'j'
        10: 2,  # 'k'
        5: 2,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 3,  # 'r'
        8: 0,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    46: {  # 'N'
        23: 0,  # 'A'
        37: 1,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 2,  # 'j'
        10: 1,  # 'k'
        5: 1,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 1,  # 'o'
        26: 1,  # 'p'
        7: 1,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 1,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 1,  # ''
        6: 2,  # ''
        40: 1,  # ''
        19: 1,  # ''
    },
    42: {  # 'O'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 1,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 0,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 2,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 2,  # ''
        6: 1,  # ''
        40: 1,  # ''
        19: 1,  # ''
    },
    48: {  # 'P'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 2,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 2,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 2,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 0,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    44: {  # 'R'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 1,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 2,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 1,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 1,  # ''
        19: 1,  # ''
    },
    35: {  # 'S'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 1,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 1,  # 'l'
        13: 2,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 1,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 2,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    31: {  # 'T'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 0,  # 'c'
        12: 1,  # 'd'
        2: 3,  # 'e'
        18: 2,  # 'f'
        27: 2,  # 'g'
        25: 0,  # 'h'
        3: 1,  # 'i'
        24: 1,  # 'j'
        10: 2,  # 'k'
        5: 2,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 2,  # 'r'
        8: 0,  # 's'
        9: 2,  # 't'
        14: 2,  # 'u'
        32: 1,  # 'v'
        57: 1,  # 'w'
        58: 1,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    51: {  # 'U'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 1,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 1,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    38: {  # 'V'
        23: 1,  # 'A'
        37: 1,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 1,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 2,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 2,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 1,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 1,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 3,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    62: {  # 'W'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 0,  # 'd'
        2: 0,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 0,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 0,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    43: {  # 'Y'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 0,  # 'G'
        45: 1,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 2,  # 'N'
        42: 0,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 1,  # 'j'
        10: 1,  # 'k'
        5: 1,  # 'l'
        13: 3,  # 'm'
        4: 0,  # 'n'
        15: 2,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 2,  # ''
        55: 1,  # ''
        59: 1,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 0,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    56: {  # 'Z'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 2,  # 'Z'
        1: 2,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 2,  # 'i'
        24: 1,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 1,  # 'r'
        8: 1,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    1: {  # 'a'
        23: 3,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 1,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 3,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 2,  # 'Z'
        1: 2,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 2,  # 'e'
        18: 3,  # 'f'
        27: 3,  # 'g'
        25: 3,  # 'h'
        3: 3,  # 'i'
        24: 3,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 3,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 3,  # 'v'
        57: 2,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 1,  # ''
        34: 1,  # ''
        17: 3,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    21: {  # 'b'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 3,  # 'g'
        25: 1,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 3,  # 'p'
        7: 1,  # 'r'
        8: 2,  # 's'
        9: 2,  # 't'
        14: 2,  # 'u'
        32: 1,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    28: {  # 'c'
        23: 0,  # 'A'
        37: 1,  # 'B'
        47: 1,  # 'C'
        39: 1,  # 'D'
        29: 2,  # 'E'
        52: 0,  # 'F'
        36: 2,  # 'G'
        45: 2,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 2,  # 'T'
        51: 2,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 3,  # 'Y'
        56: 0,  # 'Z'
        1: 1,  # 'a'
        21: 1,  # 'b'
        28: 2,  # 'c'
        12: 2,  # 'd'
        2: 1,  # 'e'
        18: 1,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 3,  # 'i'
        24: 1,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 2,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 1,  # 'u'
        32: 0,  # 'v'
        57: 1,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 1,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 1,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 1,  # ''
        34: 2,  # ''
        17: 2,  # ''
        30: 2,  # ''
        41: 1,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 2,  # ''
    },
    12: {  # 'd'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 2,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 1,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 1,  # 'f'
        27: 3,  # 'g'
        25: 3,  # 'h'
        3: 2,  # 'i'
        24: 3,  # 'j'
        10: 2,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 2,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 1,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 3,  # 'y'
        22: 1,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    2: {  # 'e'
        23: 2,  # 'A'
        37: 0,  # 'B'
        47: 2,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 1,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 1,  # 'R'
        35: 0,  # 'S'
        31: 3,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 2,  # 'e'
        18: 3,  # 'f'
        27: 3,  # 'g'
        25: 3,  # 'h'
        3: 3,  # 'i'
        24: 3,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 3,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 3,  # 'v'
        57: 2,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 1,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 3,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    18: {  # 'f'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 2,  # 'f'
        27: 1,  # 'g'
        25: 1,  # 'h'
        3: 1,  # 'i'
        24: 1,  # 'j'
        10: 1,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 1,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 1,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    27: {  # 'g'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 1,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 1,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 1,  # 'h'
        3: 2,  # 'i'
        24: 3,  # 'j'
        10: 2,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 2,  # 'r'
        8: 2,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 1,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    25: {  # 'h'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 2,  # 'h'
        3: 2,  # 'i'
        24: 3,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 1,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 1,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    3: {  # 'i'
        23: 2,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 0,  # 'N'
        42: 1,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 1,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 2,  # 'f'
        27: 3,  # 'g'
        25: 1,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 3,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 1,  # 'w'
        58: 1,  # 'x'
        11: 3,  # 'y'
        22: 1,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 1,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 3,  # ''
        30: 0,  # ''
        41: 1,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    24: {  # 'j'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 2,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 1,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 2,  # 'f'
        27: 1,  # 'g'
        25: 1,  # 'h'
        3: 2,  # 'i'
        24: 1,  # 'j'
        10: 2,  # 'k'
        5: 2,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 2,  # 'r'
        8: 3,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 2,  # 'x'
        11: 1,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    10: {  # 'k'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 3,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 3,  # 'e'
        18: 1,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 2,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 3,  # 'p'
        7: 2,  # 'r'
        8: 2,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 3,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    5: {  # 'l'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 1,  # 'e'
        18: 3,  # 'f'
        27: 3,  # 'g'
        25: 2,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 1,  # 'l'
        13: 1,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 2,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    13: {  # 'm'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 3,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 2,  # 'e'
        18: 3,  # 'f'
        27: 3,  # 'g'
        25: 3,  # 'h'
        3: 3,  # 'i'
        24: 3,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 2,  # 'u'
        32: 2,  # 'v'
        57: 1,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 3,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    4: {  # 'n'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 2,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 1,  # 'f'
        27: 2,  # 'g'
        25: 3,  # 'h'
        3: 2,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 3,  # 'p'
        7: 2,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 2,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    15: {  # 'o'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 2,  # 'L'
        20: 0,  # 'M'
        46: 2,  # 'N'
        42: 1,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 1,  # 'i'
        24: 2,  # 'j'
        10: 1,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 2,  # 'o'
        26: 0,  # 'p'
        7: 1,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 2,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 2,  # ''
        41: 2,  # ''
        6: 3,  # ''
        40: 2,  # ''
        19: 2,  # ''
    },
    26: {  # 'p'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 1,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 1,  # 'h'
        3: 2,  # 'i'
        24: 3,  # 'j'
        10: 1,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 2,  # 'r'
        8: 1,  # 's'
        9: 1,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 1,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    7: {  # 'r'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 1,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 2,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 1,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 3,  # 'h'
        3: 2,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 3,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    8: {  # 's'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 2,  # 'i'
        24: 3,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 3,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 2,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    9: {  # 't'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 2,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 2,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 3,  # 'v'
        57: 0,  # 'w'
        58: 2,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    14: {  # 'u'
        23: 3,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 2,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 3,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 2,  # 'Z'
        1: 2,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 2,  # 'e'
        18: 2,  # 'f'
        27: 3,  # 'g'
        25: 3,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 3,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 2,  # 'v'
        57: 2,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 3,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    32: {  # 'v'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 1,  # 'j'
        10: 1,  # 'k'
        5: 3,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 1,  # 'r'
        8: 2,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 1,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    57: {  # 'w'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 1,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 1,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 1,  # 's'
        9: 0,  # 't'
        14: 1,  # 'u'
        32: 0,  # 'v'
        57: 2,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    58: {  # 'x'
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 1,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 1,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 2,  # 'i'
        24: 2,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 2,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 1,  # 'r'
        8: 2,  # 's'
        9: 1,  # 't'
        14: 0,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    11: {  # 'y'
        23: 1,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 2,  # 'i'
        24: 1,  # 'j'
        10: 2,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 2,  # 'r'
        8: 1,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 1,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 3,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    22: {  # 'z'
        23: 2,  # 'A'
        37: 2,  # 'B'
        47: 1,  # 'C'
        39: 2,  # 'D'
        29: 3,  # 'E'
        52: 1,  # 'F'
        36: 2,  # 'G'
        45: 2,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 2,  # 'N'
        42: 2,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 3,  # 'T'
        51: 2,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 1,  # 'Z'
        1: 1,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 2,  # 'd'
        2: 2,  # 'e'
        18: 3,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 2,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 0,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 3,  # 'y'
        22: 2,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 2,  # ''
        59: 1,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 2,  # ''
        30: 2,  # ''
        41: 1,  # ''
        6: 3,  # ''
        40: 1,  # ''
        19: 2,  # ''
    },
    63: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 0,  # 'd'
        2: 1,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 0,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    54: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 1,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 1,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 1,  # 'b'
        28: 0,  # 'c'
        12: 1,  # 'd'
        2: 0,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 0,  # 'h'
        3: 3,  # 'i'
        24: 0,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 2,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 2,  # 'r'
        8: 0,  # 's'
        9: 1,  # 't'
        14: 0,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 2,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    50: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 1,  # 'D'
        29: 2,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 2,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 1,  # 'N'
        42: 2,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 2,  # 'd'
        2: 0,  # 'e'
        18: 1,  # 'f'
        27: 1,  # 'g'
        25: 1,  # 'h'
        3: 2,  # 'i'
        24: 0,  # 'j'
        10: 2,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 3,  # 'n'
        15: 2,  # 'o'
        26: 2,  # 'p'
        7: 3,  # 'r'
        8: 1,  # 's'
        9: 2,  # 't'
        14: 0,  # 'u'
        32: 1,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 2,  # ''
        30: 1,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    55: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 1,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 1,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 1,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 1,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 1,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 0,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    59: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 0,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 2,  # 'm'
        4: 0,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 2,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 1,  # ''
        19: 0,  # ''
    },
    33: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 3,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 0,  # 'Z'
        1: 0,  # 'a'
        21: 3,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 0,  # 'e'
        18: 2,  # 'f'
        27: 1,  # 'g'
        25: 3,  # 'h'
        3: 3,  # 'i'
        24: 0,  # 'j'
        10: 3,  # 'k'
        5: 0,  # 'l'
        13: 0,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 3,  # 'r'
        8: 2,  # 's'
        9: 3,  # 't'
        14: 0,  # 'u'
        32: 2,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 1,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    61: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 0,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 0,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 2,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 1,  # 'j'
        10: 0,  # 'k'
        5: 0,  # 'l'
        13: 1,  # 'm'
        4: 1,  # 'n'
        15: 0,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 1,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 1,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 1,  # ''
        34: 0,  # ''
        17: 0,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 1,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    34: {  # ''
        23: 0,  # 'A'
        37: 1,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 1,  # 'G'
        45: 1,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 1,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 2,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 1,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 2,  # 'c'
        12: 1,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 2,  # 'h'
        3: 1,  # 'i'
        24: 2,  # 'j'
        10: 1,  # 'k'
        5: 2,  # 'l'
        13: 3,  # 'm'
        4: 2,  # 'n'
        15: 2,  # 'o'
        26: 0,  # 'p'
        7: 0,  # 'r'
        8: 3,  # 's'
        9: 1,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 1,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 0,  # ''
        30: 2,  # ''
        41: 1,  # ''
        6: 1,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    17: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 0,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 1,  # 'J'
        16: 1,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 0,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 0,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 0,  # 'c'
        12: 1,  # 'd'
        2: 3,  # 'e'
        18: 1,  # 'f'
        27: 2,  # 'g'
        25: 0,  # 'h'
        3: 1,  # 'i'
        24: 1,  # 'j'
        10: 2,  # 'k'
        5: 3,  # 'l'
        13: 2,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 2,  # 'p'
        7: 2,  # 'r'
        8: 3,  # 's'
        9: 2,  # 't'
        14: 3,  # 'u'
        32: 1,  # 'v'
        57: 1,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 2,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    30: {  # ''
        23: 0,  # 'A'
        37: 2,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 1,  # 'M'
        46: 2,  # 'N'
        42: 2,  # 'O'
        48: 1,  # 'P'
        44: 1,  # 'R'
        35: 0,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 2,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 0,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 2,  # 'e'
        18: 0,  # 'f'
        27: 0,  # 'g'
        25: 0,  # 'h'
        3: 0,  # 'i'
        24: 3,  # 'j'
        10: 1,  # 'k'
        5: 2,  # 'l'
        13: 3,  # 'm'
        4: 0,  # 'n'
        15: 1,  # 'o'
        26: 0,  # 'p'
        7: 1,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 2,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 2,  # ''
        6: 2,  # ''
        40: 2,  # ''
        19: 1,  # ''
    },
    41: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 1,  # 'D'
        29: 1,  # 'E'
        52: 0,  # 'F'
        36: 2,  # 'G'
        45: 2,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 2,  # 'P'
        44: 0,  # 'R'
        35: 1,  # 'S'
        31: 1,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 0,  # 'Z'
        1: 1,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 2,  # 'd'
        2: 1,  # 'e'
        18: 0,  # 'f'
        27: 3,  # 'g'
        25: 2,  # 'h'
        3: 2,  # 'i'
        24: 2,  # 'j'
        10: 2,  # 'k'
        5: 0,  # 'l'
        13: 1,  # 'm'
        4: 3,  # 'n'
        15: 1,  # 'o'
        26: 1,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 2,  # 't'
        14: 0,  # 'u'
        32: 0,  # 'v'
        57: 1,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 1,  # ''
        59: 1,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 1,  # ''
        17: 1,  # ''
        30: 2,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 1,  # ''
    },
    6: {  # ''
        23: 2,  # 'A'
        37: 0,  # 'B'
        47: 0,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 2,  # 'J'
        16: 3,  # 'K'
        49: 0,  # 'L'
        20: 3,  # 'M'
        46: 1,  # 'N'
        42: 0,  # 'O'
        48: 0,  # 'P'
        44: 0,  # 'R'
        35: 0,  # 'S'
        31: 2,  # 'T'
        51: 0,  # 'U'
        38: 0,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 1,  # 'Z'
        1: 3,  # 'a'
        21: 2,  # 'b'
        28: 1,  # 'c'
        12: 3,  # 'd'
        2: 3,  # 'e'
        18: 3,  # 'f'
        27: 3,  # 'g'
        25: 2,  # 'h'
        3: 3,  # 'i'
        24: 3,  # 'j'
        10: 3,  # 'k'
        5: 3,  # 'l'
        13: 3,  # 'm'
        4: 3,  # 'n'
        15: 0,  # 'o'
        26: 3,  # 'p'
        7: 3,  # 'r'
        8: 3,  # 's'
        9: 3,  # 't'
        14: 3,  # 'u'
        32: 3,  # 'v'
        57: 1,  # 'w'
        58: 1,  # 'x'
        11: 3,  # 'y'
        22: 0,  # 'z'
        63: 1,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 2,  # ''
        61: 0,  # ''
        34: 0,  # ''
        17: 3,  # ''
        30: 0,  # ''
        41: 0,  # ''
        6: 3,  # ''
        40: 0,  # ''
        19: 0,  # ''
    },
    40: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 1,  # 'D'
        29: 1,  # 'E'
        52: 0,  # 'F'
        36: 1,  # 'G'
        45: 2,  # 'H'
        53: 1,  # 'I'
        60: 0,  # 'J'
        16: 0,  # 'K'
        49: 0,  # 'L'
        20: 2,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 2,  # 'P'
        44: 2,  # 'R'
        35: 1,  # 'S'
        31: 1,  # 'T'
        51: 0,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 2,  # 'Y'
        56: 1,  # 'Z'
        1: 0,  # 'a'
        21: 2,  # 'b'
        28: 0,  # 'c'
        12: 2,  # 'd'
        2: 0,  # 'e'
        18: 3,  # 'f'
        27: 0,  # 'g'
        25: 2,  # 'h'
        3: 3,  # 'i'
        24: 2,  # 'j'
        10: 1,  # 'k'
        5: 0,  # 'l'
        13: 1,  # 'm'
        4: 3,  # 'n'
        15: 2,  # 'o'
        26: 0,  # 'p'
        7: 3,  # 'r'
        8: 2,  # 's'
        9: 2,  # 't'
        14: 1,  # 'u'
        32: 3,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 2,  # 'y'
        22: 0,  # 'z'
        63: 0,  # ''
        54: 0,  # ''
        50: 0,  # ''
        55: 1,  # ''
        59: 0,  # ''
        33: 0,  # ''
        61: 0,  # ''
        34: 2,  # ''
        17: 1,  # ''
        30: 2,  # ''
        41: 0,  # ''
        6: 2,  # ''
        40: 1,  # ''
        19: 2,  # ''
    },
    19: {  # ''
        23: 0,  # 'A'
        37: 0,  # 'B'
        47: 1,  # 'C'
        39: 0,  # 'D'
        29: 0,  # 'E'
        52: 2,  # 'F'
        36: 1,  # 'G'
        45: 0,  # 'H'
        53: 0,  # 'I'
        60: 0,  # 'J'
        16: 3,  # 'K'
        49: 2,  # 'L'
        20: 0,  # 'M'
        46: 1,  # 'N'
        42: 1,  # 'O'
        48: 1,  # 'P'
        44: 1,  # 'R'
        35: 1,  # 'S'
        31: 0,  # 'T'
        51: 1,  # 'U'
        38: 1,  # 'V'
        62: 0,  # 'W'
        43: 1,  # 'Y'
        56: 0,  # 'Z'
        1: 3,  # 'a'
        21: 1,  # 'b'
        28: 2,  # 'c'
        12: 0,  # 'd'
        2: 3,  # 'e'
        18: 0,  # 'f'
        27: 2,  # 'g'
        25: 1,  # 'h'
        3: 1,  # 'i'
        24: 0,  # 'j'
        10: 2,  # 'k'
        5: 2,  # 'l'
        13: 3,  # 'm'
        4: 0,  # 'n'
        15: 0,  # 'o'
        26: 1,  # 'p'
        7: 3,  # 'r'
        8: 0,  # 's'
        9: 0,  # 't'
        14: 3,  # 'u'
        32: 0,  # 'v'
        57: 0,  # 'w'
        58: 0,  # 'x'
        11: 0,  # 'y'
        22: 2,  # 'z'
        63: 0,  # ''
        54: 1,  # ''
        50: 2,  # ''
        55: 0,  # ''
        59: 0,  # ''
        33: 1,  # ''
        61: 1,  # ''
        34: 2,  # ''
        17: 0,  # ''
        30: 1,  # ''
        41: 1,  # ''
        6: 1,  # ''
        40: 1,  # ''
        19: 1,  # ''
    },
}

# 255: Undefined characters that did not exist in training text
# 254: Carriage/Return
# 253: symbol (punctuation) that does not belong to word
# 252: 0 - 9
# 251: Control characters

# Character Mapping Table(s):
ISO_8859_9_TURKISH_CHAR_TO_ORDER = {
    0: 255,  # '\x00'
    1: 255,  # '\x01'
    2: 255,  # '\x02'
    3: 255,  # '\x03'
    4: 255,  # '\x04'
    5: 255,  # '\x05'
    6: 255,  # '\x06'
    7: 255,  # '\x07'
    8: 255,  # '\x08'
    9: 255,  # '\t'
    10: 255,  # '\n'
    11: 255,  # '\x0b'
    12: 255,  # '\x0c'
    13: 255,  # '\r'
    14: 255,  # '\x0e'
    15: 255,  # '\x0f'
    16: 255,  # '\x10'
    17: 255,  # '\x11'
    18: 255,  # '\x12'
    19: 255,  # '\x13'
    20: 255,  # '\x14'
    21: 255,  # '\x15'
    22: 255,  # '\x16'
    23: 255,  # '\x17'
    24: 255,  # '\x18'
    25: 255,  # '\x19'
    26: 255,  # '\x1a'
    27: 255,  # '\x1b'
    28: 255,  # '\x1c'
    29: 255,  # '\x1d'
    30: 255,  # '\x1e'
    31: 255,  # '\x1f'
    32: 255,  # ' '
    33: 255,  # '!'
    34: 255,  # '"'
    35: 255,  # '#'
    36: 255,  # '$'
    37: 255,  # '%'
    38: 255,  # '&'
    39: 255,  # "'"
    40: 255,  # '('
    41: 255,  # ')'
    42: 255,  # '*'
    43: 255,  # '+'
    44: 255,  # ','
    45: 255,  # '-'
    46: 255,  # '.'
    47: 255,  # '/'
    48: 255,  # '0'
    49: 255,  # '1'
    50: 255,  # '2'
    51: 255,  # '3'
    52: 255,  # '4'
    53: 255,  # '5'
    54: 255,  # '6'
    55: 255,  # '7'
    56: 255,  # '8'
    57: 255,  # '9'
    58: 255,  # ':'
    59: 255,  # ';'
    60: 255,  # '<'
    61: 255,  # '='
    62: 255,  # '>'
    63: 255,  # '?'
    64: 255,  # '@'
    65: 23,  # 'A'
    66: 37,  # 'B'
    67: 47,  # 'C'
    68: 39,  # 'D'
    69: 29,  # 'E'
    70: 52,  # 'F'
    71: 36,  # 'G'
    72: 45,  # 'H'
    73: 53,  # 'I'
    74: 60,  # 'J'
    75: 16,  # 'K'
    76: 49,  # 'L'
    77: 20,  # 'M'
    78: 46,  # 'N'
    79: 42,  # 'O'
    80: 48,  # 'P'
    81: 69,  # 'Q'
    82: 44,  # 'R'
    83: 35,  # 'S'
    84: 31,  # 'T'
    85: 51,  # 'U'
    86: 38,  # 'V'
    87: 62,  # 'W'
    88: 65,  # 'X'
    89: 43,  # 'Y'
    90: 56,  # 'Z'
    91: 255,  # '['
    92: 255,  # '\\'
    93: 255,  # ']'
    94: 255,  # '^'
    95: 255,  # '_'
    96: 255,  # '`'
    97: 1,  # 'a'
    98: 21,  # 'b'
    99: 28,  # 'c'
    100: 12,  # 'd'
    101: 2,  # 'e'
    102: 18,  # 'f'
    103: 27,  # 'g'
    104: 25,  # 'h'
    105: 3,  # 'i'
    106: 24,  # 'j'
    107: 10,  # 'k'
    108: 5,  # 'l'
    109: 13,  # 'm'
    110: 4,  # 'n'
    111: 15,  # 'o'
    112: 26,  # 'p'
    113: 64,  # 'q'
    114: 7,  # 'r'
    115: 8,  # 's'
    116: 9,  # 't'
    117: 14,  # 'u'
    118: 32,  # 'v'
    119: 57,  # 'w'
    120: 58,  # 'x'
    121: 11,  # 'y'
    122: 22,  # 'z'
    123: 255,  # '{'
    124: 255,  # '|'
    125: 255,  # '}'
    126: 255,  # '~'
    127: 255,  # '\x7f'
    128: 180,  # '\x80'
    129: 179,  # '\x81'
    130: 178,  # '\x82'
    131: 177,  # '\x83'
    132: 176,  # '\x84'
    133: 175,  # '\x85'
    134: 174,  # '\x86'
    135: 173,  # '\x87'
    136: 172,  # '\x88'
    137: 171,  # '\x89'
    138: 170,  # '\x8a'
    139: 169,  # '\x8b'
    140: 168,  # '\x8c'
    141: 167,  # '\x8d'
    142: 166,  # '\x8e'
    143: 165,  # '\x8f'
    144: 164,  # '\x90'
    145: 163,  # '\x91'
    146: 162,  # '\x92'
    147: 161,  # '\x93'
    148: 160,  # '\x94'
    149: 159,  # '\x95'
    150: 101,  # '\x96'
    151: 158,  # '\x97'
    152: 157,  # '\x98'
    153: 156,  # '\x99'
    154: 155,  # '\x9a'
    155: 154,  # '\x9b'
    156: 153,  # '\x9c'
    157: 152,  # '\x9d'
    158: 151,  # '\x9e'
    159: 106,  # '\x9f'
    160: 150,  # '\xa0'
    161: 149,  # ''
    162: 148,  # ''
    163: 147,  # ''
    164: 146,  # ''
    165: 145,  # ''
    166: 144,  # ''
    167: 100,  # ''
    168: 143,  # ''
    169: 142,  # ''
    170: 141,  # ''
    171: 140,  # ''
    172: 139,  # ''
    173: 138,  # '\xad'
    174: 137,  # ''
    175: 136,  # ''
    176: 94,  # ''
    177: 80,  # ''
    178: 93,  # ''
    179: 135,  # ''
    180: 105,  # ''
    181: 134,  # ''
    182: 133,  # ''
    183: 63,  # ''
    184: 132,  # ''
    185: 131,  # ''
    186: 130,  # ''
    187: 129,  # ''
    188: 128,  # ''
    189: 127,  # ''
    190: 126,  # ''
    191: 125,  # ''
    192: 124,  # ''
    193: 104,  # ''
    194: 73,  # ''
    195: 99,  # ''
    196: 79,  # ''
    197: 85,  # ''
    198: 123,  # ''
    199: 54,  # ''
    200: 122,  # ''
    201: 98,  # ''
    202: 92,  # ''
    203: 121,  # ''
    204: 120,  # ''
    205: 91,  # ''
    206: 103,  # ''
    207: 119,  # ''
    208: 68,  # ''
    209: 118,  # ''
    210: 117,  # ''
    211: 97,  # ''
    212: 116,  # ''
    213: 115,  # ''
    214: 50,  # ''
    215: 90,  # ''
    216: 114,  # ''
    217: 113,  # ''
    218: 112,  # ''
    219: 111,  # ''
    220: 55,  # ''
    221: 41,  # ''
    222: 40,  # ''
    223: 86,  # ''
    224: 89,  # ''
    225: 70,  # ''
    226: 59,  # ''
    227: 78,  # ''
    228: 71,  # ''
    229: 82,  # ''
    230: 88,  # ''
    231: 33,  # ''
    232: 77,  # ''
    233: 66,  # ''
    234: 84,  # ''
    235: 83,  # ''
    236: 110,  # ''
    237: 75,  # ''
    238: 61,  # ''
    239: 96,  # ''
    240: 30,  # ''
    241: 67,  # ''
    242: 109,  # ''
    243: 74,  # ''
    244: 87,  # ''
    245: 102,  # ''
    246: 34,  # ''
    247: 95,  # ''
    248: 81,  # ''
    249: 108,  # ''
    250: 76,  # ''
    251: 72,  # ''
    252: 17,  # ''
    253: 6,  # ''
    254: 19,  # ''
    255: 107,  # ''
}

ISO_8859_9_TURKISH_MODEL = SingleByteCharSetModel(
    charset_name="ISO-8859-9",
    language="Turkish",
    char_to_order_map=ISO_8859_9_TURKISH_CHAR_TO_ORDER,
    language_model=TURKISH_LANG_MODEL,
    typical_positive_ratio=0.97029,
    keep_ascii_letters=True,
    alphabet="ABCDEFGHIJKLMNOPRSTUVYZabcdefghijklmnoprstuvyz",
)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/johabprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import JOHABDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import JOHAB_SM_MODEL


class JOHABProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(JOHAB_SM_MODEL)
        self.distribution_analyzer = JOHABDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return "Johab"

    @property
    def language(self) -> str:
        return "Korean"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/euckrprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCKR_SM_MODEL


class EUCKRProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return "EUC-KR"

    @property
    def language(self) -> str:
        return "Korean"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/latin1prober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import List, Union

from .charsetprober import CharSetProber
from .enums import ProbingState

FREQ_CAT_NUM = 4

UDF = 0  # undefined
OTH = 1  # other
ASC = 2  # ascii capital letter
ASS = 3  # ascii small letter
ACV = 4  # accent capital vowel
ACO = 5  # accent capital other
ASV = 6  # accent small vowel
ASO = 7  # accent small other
CLASS_NUM = 8  # total classes

# fmt: off
Latin1_CharToClass = (
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 00 - 07
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 08 - 0F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 10 - 17
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 18 - 1F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 20 - 27
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 28 - 2F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 30 - 37
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 38 - 3F
    OTH, ASC, ASC, ASC, ASC, ASC, ASC, ASC,   # 40 - 47
    ASC, ASC, ASC, ASC, ASC, ASC, ASC, ASC,   # 48 - 4F
    ASC, ASC, ASC, ASC, ASC, ASC, ASC, ASC,   # 50 - 57
    ASC, ASC, ASC, OTH, OTH, OTH, OTH, OTH,   # 58 - 5F
    OTH, ASS, ASS, ASS, ASS, ASS, ASS, ASS,   # 60 - 67
    ASS, ASS, ASS, ASS, ASS, ASS, ASS, ASS,   # 68 - 6F
    ASS, ASS, ASS, ASS, ASS, ASS, ASS, ASS,   # 70 - 77
    ASS, ASS, ASS, OTH, OTH, OTH, OTH, OTH,   # 78 - 7F
    OTH, UDF, OTH, ASO, OTH, OTH, OTH, OTH,   # 80 - 87
    OTH, OTH, ACO, OTH, ACO, UDF, ACO, UDF,   # 88 - 8F
    UDF, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # 90 - 97
    OTH, OTH, ASO, OTH, ASO, UDF, ASO, ACO,   # 98 - 9F
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # A0 - A7
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # A8 - AF
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # B0 - B7
    OTH, OTH, OTH, OTH, OTH, OTH, OTH, OTH,   # B8 - BF
    ACV, ACV, ACV, ACV, ACV, ACV, ACO, ACO,   # C0 - C7
    ACV, ACV, ACV, ACV, ACV, ACV, ACV, ACV,   # C8 - CF
    ACO, ACO, ACV, ACV, ACV, ACV, ACV, OTH,   # D0 - D7
    ACV, ACV, ACV, ACV, ACV, ACO, ACO, ACO,   # D8 - DF
    ASV, ASV, ASV, ASV, ASV, ASV, ASO, ASO,   # E0 - E7
    ASV, ASV, ASV, ASV, ASV, ASV, ASV, ASV,   # E8 - EF
    ASO, ASO, ASV, ASV, ASV, ASV, ASV, OTH,   # F0 - F7
    ASV, ASV, ASV, ASV, ASV, ASO, ASO, ASO,   # F8 - FF
)

# 0 : illegal
# 1 : very unlikely
# 2 : normal
# 3 : very likely
Latin1ClassModel = (
# UDF OTH ASC ASS ACV ACO ASV ASO
    0,  0,  0,  0,  0,  0,  0,  0,  # UDF
    0,  3,  3,  3,  3,  3,  3,  3,  # OTH
    0,  3,  3,  3,  3,  3,  3,  3,  # ASC
    0,  3,  3,  3,  1,  1,  3,  3,  # ASS
    0,  3,  3,  3,  1,  2,  1,  2,  # ACV
    0,  3,  3,  3,  3,  3,  3,  3,  # ACO
    0,  3,  1,  3,  1,  1,  1,  3,  # ASV
    0,  3,  1,  3,  1,  1,  3,  3,  # ASO
)
# fmt: on


class Latin1Prober(CharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self._last_char_class = OTH
        self._freq_counter: List[int] = []
        self.reset()

    def reset(self) -> None:
        self._last_char_class = OTH
        self._freq_counter = [0] * FREQ_CAT_NUM
        super().reset()

    @property
    def charset_name(self) -> str:
        return "ISO-8859-1"

    @property
    def language(self) -> str:
        return ""

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        byte_str = self.remove_xml_tags(byte_str)
        for c in byte_str:
            char_class = Latin1_CharToClass[c]
            freq = Latin1ClassModel[(self._last_char_class * CLASS_NUM) + char_class]
            if freq == 0:
                self._state = ProbingState.NOT_ME
                break
            self._freq_counter[freq] += 1
            self._last_char_class = char_class

        return self.state

    def get_confidence(self) -> float:
        if self.state == ProbingState.NOT_ME:
            return 0.01

        total = sum(self._freq_counter)
        confidence = (
            0.0
            if total < 0.01
            else (self._freq_counter[3] - self._freq_counter[1] * 20.0) / total
        )
        confidence = max(confidence, 0.0)
        # lower the confidence of latin1 so that other more accurate
        # detector can take priority.
        confidence *= 0.73
        return confidence


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/big5freq.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# Big5 frequency table
# by Taiwan's Mandarin Promotion Council
# <http://www.edu.tw:81/mandr/>
#
# 128  --> 0.42261
# 256  --> 0.57851
# 512  --> 0.74851
# 1024 --> 0.89384
# 2048 --> 0.97583
#
# Ideal Distribution Ratio = 0.74851/(1-0.74851) =2.98
# Random Distribution Ration = 512/(5401-512)=0.105
#
# Typical Distribution Ratio about 25% of Ideal one, still much higher than RDR

BIG5_TYPICAL_DISTRIBUTION_RATIO = 0.75

# Char to FreqOrder table
BIG5_TABLE_SIZE = 5376
# fmt: off
BIG5_CHAR_TO_FREQ_ORDER = (
   1,1801,1506, 255,1431, 198,   9,  82,   6,5008, 177, 202,3681,1256,2821, 110, #   16
3814,  33,3274, 261,  76,  44,2114,  16,2946,2187,1176, 659,3971,  26,3451,2653, #   32
1198,3972,3350,4202, 410,2215, 302, 590, 361,1964,   8, 204,  58,4510,5009,1932, #   48
  63,5010,5011, 317,1614,  75, 222, 159,4203,2417,1480,5012,3555,3091, 224,2822, #   64
3682,   3,  10,3973,1471,  29,2787,1135,2866,1940, 873, 130,3275,1123, 312,5013, #   80
4511,2052, 507, 252, 682,5014, 142,1915, 124, 206,2947,  34,3556,3204,  64, 604, #   96
5015,2501,1977,1978, 155,1991, 645, 641,1606,5016,3452, 337,  72, 406,5017,  80, #  112
 630, 238,3205,1509, 263, 939,1092,2654, 756,1440,1094,3453, 449,  69,2987, 591, #  128
 179,2096, 471, 115,2035,1844,  60,  50,2988, 134, 806,1869, 734,2036,3454, 180, #  144
 995,1607, 156, 537,2907, 688,5018, 319,1305, 779,2145, 514,2379, 298,4512, 359, #  160
2502,  90,2716,1338, 663,  11, 906,1099,2553,  20,2441, 182, 532,1716,5019, 732, #  176
1376,4204,1311,1420,3206,  25,2317,1056, 113, 399, 382,1950, 242,3455,2474, 529, #  192
3276, 475,1447,3683,5020, 117,  21, 656, 810,1297,2300,2334,3557,5021, 126,4205, #  208
 706, 456, 150, 613,4513,  71,1118,2037,4206, 145,3092,  85, 835, 486,2115,1246, #  224
1426, 428, 727,1285,1015, 800, 106, 623, 303,1281,5022,2128,2359, 347,3815, 221, #  240
3558,3135,5023,1956,1153,4207,  83, 296,1199,3093, 192, 624,  93,5024, 822,1898, #  256
2823,3136, 795,2065, 991,1554,1542,1592,  27,  43,2867, 859, 139,1456, 860,4514, #  272
 437, 712,3974, 164,2397,3137, 695, 211,3037,2097, 195,3975,1608,3559,3560,3684, #  288
3976, 234, 811,2989,2098,3977,2233,1441,3561,1615,2380, 668,2077,1638, 305, 228, #  304
1664,4515, 467, 415,5025, 262,2099,1593, 239, 108, 300, 200,1033, 512,1247,2078, #  320
5026,5027,2176,3207,3685,2682, 593, 845,1062,3277,  88,1723,2038,3978,1951, 212, #  336
 266, 152, 149, 468,1899,4208,4516,  77, 187,5028,3038,  37,   5,2990,5029,3979, #  352
5030,5031,  39,2524,4517,2908,3208,2079,  55, 148,  74,4518, 545, 483,1474,1029, #  368
1665, 217,1870,1531,3138,1104,2655,4209,  24, 172,3562, 900,3980,3563,3564,4519, #  384
  32,1408,2824,1312, 329, 487,2360,2251,2717, 784,2683,   4,3039,3351,1427,1789, #  400
 188, 109, 499,5032,3686,1717,1790, 888,1217,3040,4520,5033,3565,5034,3352,1520, #  416
3687,3981, 196,1034, 775,5035,5036, 929,1816, 249, 439,  38,5037,1063,5038, 794, #  432
3982,1435,2301,  46, 178,3278,2066,5039,2381,5040, 214,1709,4521, 804,  35, 707, #  448
 324,3688,1601,2554, 140, 459,4210,5041,5042,1365, 839, 272, 978,2262,2580,3456, #  464
2129,1363,3689,1423, 697, 100,3094,  48,  70,1231, 495,3139,2196,5043,1294,5044, #  480
2080, 462, 586,1042,3279, 853, 256, 988, 185,2382,3457,1698, 434,1084,5045,3458, #  496
 314,2625,2788,4522,2335,2336, 569,2285, 637,1817,2525, 757,1162,1879,1616,3459, #  512
 287,1577,2116, 768,4523,1671,2868,3566,2526,1321,3816, 909,2418,5046,4211, 933, #  528
3817,4212,2053,2361,1222,4524, 765,2419,1322, 786,4525,5047,1920,1462,1677,2909, #  544
1699,5048,4526,1424,2442,3140,3690,2600,3353,1775,1941,3460,3983,4213, 309,1369, #  560
1130,2825, 364,2234,1653,1299,3984,3567,3985,3986,2656, 525,1085,3041, 902,2001, #  576
1475, 964,4527, 421,1845,1415,1057,2286, 940,1364,3141, 376,4528,4529,1381,   7, #  592
2527, 983,2383, 336,1710,2684,1846, 321,3461, 559,1131,3042,2752,1809,1132,1313, #  608
 265,1481,1858,5049, 352,1203,2826,3280, 167,1089, 420,2827, 776, 792,1724,3568, #  624
4214,2443,3281,5050,4215,5051, 446, 229, 333,2753, 901,3818,1200,1557,4530,2657, #  640
1921, 395,2754,2685,3819,4216,1836, 125, 916,3209,2626,4531,5052,5053,3820,5054, #  656
5055,5056,4532,3142,3691,1133,2555,1757,3462,1510,2318,1409,3569,5057,2146, 438, #  672
2601,2910,2384,3354,1068, 958,3043, 461, 311,2869,2686,4217,1916,3210,4218,1979, #  688
 383, 750,2755,2627,4219, 274, 539, 385,1278,1442,5058,1154,1965, 384, 561, 210, #  704
  98,1295,2556,3570,5059,1711,2420,1482,3463,3987,2911,1257, 129,5060,3821, 642, #  720
 523,2789,2790,2658,5061, 141,2235,1333,  68, 176, 441, 876, 907,4220, 603,2602, #  736
 710, 171,3464, 404, 549,  18,3143,2398,1410,3692,1666,5062,3571,4533,2912,4534, #  752
5063,2991, 368,5064, 146, 366,  99, 871,3693,1543, 748, 807,1586,1185,  22,2263, #  768
 379,3822,3211,5065,3212, 505,1942,2628,1992,1382,2319,5066, 380,2362, 218, 702, #  784
1818,1248,3465,3044,3572,3355,3282,5067,2992,3694, 930,3283,3823,5068,  59,5069, #  800
 585, 601,4221, 497,3466,1112,1314,4535,1802,5070,1223,1472,2177,5071, 749,1837, #  816
 690,1900,3824,1773,3988,1476, 429,1043,1791,2236,2117, 917,4222, 447,1086,1629, #  832
5072, 556,5073,5074,2021,1654, 844,1090, 105, 550, 966,1758,2828,1008,1783, 686, #  848
1095,5075,2287, 793,1602,5076,3573,2603,4536,4223,2948,2302,4537,3825, 980,2503, #  864
 544, 353, 527,4538, 908,2687,2913,5077, 381,2629,1943,1348,5078,1341,1252, 560, #  880
3095,5079,3467,2870,5080,2054, 973, 886,2081, 143,4539,5081,5082, 157,3989, 496, #  896
4224,  57, 840, 540,2039,4540,4541,3468,2118,1445, 970,2264,1748,1966,2082,4225, #  912
3144,1234,1776,3284,2829,3695, 773,1206,2130,1066,2040,1326,3990,1738,1725,4226, #  928
 279,3145,  51,1544,2604, 423,1578,2131,2067, 173,4542,1880,5083,5084,1583, 264, #  944
 610,3696,4543,2444, 280, 154,5085,5086,5087,1739, 338,1282,3096, 693,2871,1411, #  960
1074,3826,2445,5088,4544,5089,5090,1240, 952,2399,5091,2914,1538,2688, 685,1483, #  976
4227,2475,1436, 953,4228,2055,4545, 671,2400,  79,4229,2446,3285, 608, 567,2689, #  992
3469,4230,4231,1691, 393,1261,1792,2401,5092,4546,5093,5094,5095,5096,1383,1672, # 1008
3827,3213,1464, 522,1119, 661,1150, 216, 675,4547,3991,1432,3574, 609,4548,2690, # 1024
2402,5097,5098,5099,4232,3045,   0,5100,2476, 315, 231,2447, 301,3356,4549,2385, # 1040
5101, 233,4233,3697,1819,4550,4551,5102,  96,1777,1315,2083,5103, 257,5104,1810, # 1056
3698,2718,1139,1820,4234,2022,1124,2164,2791,1778,2659,5105,3097, 363,1655,3214, # 1072
5106,2993,5107,5108,5109,3992,1567,3993, 718, 103,3215, 849,1443, 341,3357,2949, # 1088
1484,5110,1712, 127,  67, 339,4235,2403, 679,1412, 821,5111,5112, 834, 738, 351, # 1104
2994,2147, 846, 235,1497,1881, 418,1993,3828,2719, 186,1100,2148,2756,3575,1545, # 1120
1355,2950,2872,1377, 583,3994,4236,2581,2995,5113,1298,3699,1078,2557,3700,2363, # 1136
  78,3829,3830, 267,1289,2100,2002,1594,4237, 348, 369,1274,2197,2178,1838,4552, # 1152
1821,2830,3701,2757,2288,2003,4553,2951,2758, 144,3358, 882,4554,3995,2759,3470, # 1168
4555,2915,5114,4238,1726, 320,5115,3996,3046, 788,2996,5116,2831,1774,1327,2873, # 1184
3997,2832,5117,1306,4556,2004,1700,3831,3576,2364,2660, 787,2023, 506, 824,3702, # 1200
 534, 323,4557,1044,3359,2024,1901, 946,3471,5118,1779,1500,1678,5119,1882,4558, # 1216
 165, 243,4559,3703,2528, 123, 683,4239, 764,4560,  36,3998,1793, 589,2916, 816, # 1232
 626,1667,3047,2237,1639,1555,1622,3832,3999,5120,4000,2874,1370,1228,1933, 891, # 1248
2084,2917, 304,4240,5121, 292,2997,2720,3577, 691,2101,4241,1115,4561, 118, 662, # 1264
5122, 611,1156, 854,2386,1316,2875,   2, 386, 515,2918,5123,5124,3286, 868,2238, # 1280
1486, 855,2661, 785,2216,3048,5125,1040,3216,3578,5126,3146, 448,5127,1525,5128, # 1296
2165,4562,5129,3833,5130,4242,2833,3579,3147, 503, 818,4001,3148,1568, 814, 676, # 1312
1444, 306,1749,5131,3834,1416,1030, 197,1428, 805,2834,1501,4563,5132,5133,5134, # 1328
1994,5135,4564,5136,5137,2198,  13,2792,3704,2998,3149,1229,1917,5138,3835,2132, # 1344
5139,4243,4565,2404,3580,5140,2217,1511,1727,1120,5141,5142, 646,3836,2448, 307, # 1360
5143,5144,1595,3217,5145,5146,5147,3705,1113,1356,4002,1465,2529,2530,5148, 519, # 1376
5149, 128,2133,  92,2289,1980,5150,4003,1512, 342,3150,2199,5151,2793,2218,1981, # 1392
3360,4244, 290,1656,1317, 789, 827,2365,5152,3837,4566, 562, 581,4004,5153, 401, # 1408
4567,2252,  94,4568,5154,1399,2794,5155,1463,2025,4569,3218,1944,5156, 828,1105, # 1424
4245,1262,1394,5157,4246, 605,4570,5158,1784,2876,5159,2835, 819,2102, 578,2200, # 1440
2952,5160,1502, 436,3287,4247,3288,2836,4005,2919,3472,3473,5161,2721,2320,5162, # 1456
5163,2337,2068,  23,4571, 193, 826,3838,2103, 699,1630,4248,3098, 390,1794,1064, # 1472
3581,5164,1579,3099,3100,1400,5165,4249,1839,1640,2877,5166,4572,4573, 137,4250, # 1488
 598,3101,1967, 780, 104, 974,2953,5167, 278, 899, 253, 402, 572, 504, 493,1339, # 1504
5168,4006,1275,4574,2582,2558,5169,3706,3049,3102,2253, 565,1334,2722, 863,  41, # 1520
5170,5171,4575,5172,1657,2338,  19, 463,2760,4251, 606,5173,2999,3289,1087,2085, # 1536
1323,2662,3000,5174,1631,1623,1750,4252,2691,5175,2878, 791,2723,2663,2339, 232, # 1552
2421,5176,3001,1498,5177,2664,2630, 755,1366,3707,3290,3151,2026,1609, 119,1918, # 1568
3474, 862,1026,4253,5178,4007,3839,4576,4008,4577,2265,1952,2477,5179,1125, 817, # 1584
4254,4255,4009,1513,1766,2041,1487,4256,3050,3291,2837,3840,3152,5180,5181,1507, # 1600
5182,2692, 733,  40,1632,1106,2879, 345,4257, 841,2531, 230,4578,3002,1847,3292, # 1616
3475,5183,1263, 986,3476,5184, 735, 879, 254,1137, 857, 622,1300,1180,1388,1562, # 1632
4010,4011,2954, 967,2761,2665,1349, 592,2134,1692,3361,3003,1995,4258,1679,4012, # 1648
1902,2188,5185, 739,3708,2724,1296,1290,5186,4259,2201,2202,1922,1563,2605,2559, # 1664
1871,2762,3004,5187, 435,5188, 343,1108, 596,  17,1751,4579,2239,3477,3709,5189, # 1680
4580, 294,3582,2955,1693, 477, 979, 281,2042,3583, 643,2043,3710,2631,2795,2266, # 1696
1031,2340,2135,2303,3584,4581, 367,1249,2560,5190,3585,5191,4582,1283,3362,2005, # 1712
 240,1762,3363,4583,4584, 836,1069,3153, 474,5192,2149,2532, 268,3586,5193,3219, # 1728
1521,1284,5194,1658,1546,4260,5195,3587,3588,5196,4261,3364,2693,1685,4262, 961, # 1744
1673,2632, 190,2006,2203,3841,4585,4586,5197, 570,2504,3711,1490,5198,4587,2633, # 1760
3293,1957,4588, 584,1514, 396,1045,1945,5199,4589,1968,2449,5200,5201,4590,4013, # 1776
 619,5202,3154,3294, 215,2007,2796,2561,3220,4591,3221,4592, 763,4263,3842,4593, # 1792
5203,5204,1958,1767,2956,3365,3712,1174, 452,1477,4594,3366,3155,5205,2838,1253, # 1808
2387,2189,1091,2290,4264, 492,5206, 638,1169,1825,2136,1752,4014, 648, 926,1021, # 1824
1324,4595, 520,4596, 997, 847,1007, 892,4597,3843,2267,1872,3713,2405,1785,4598, # 1840
1953,2957,3103,3222,1728,4265,2044,3714,4599,2008,1701,3156,1551,  30,2268,4266, # 1856
5207,2027,4600,3589,5208, 501,5209,4267, 594,3478,2166,1822,3590,3479,3591,3223, # 1872
 829,2839,4268,5210,1680,3157,1225,4269,5211,3295,4601,4270,3158,2341,5212,4602, # 1888
4271,5213,4015,4016,5214,1848,2388,2606,3367,5215,4603, 374,4017, 652,4272,4273, # 1904
 375,1140, 798,5216,5217,5218,2366,4604,2269, 546,1659, 138,3051,2450,4605,5219, # 1920
2254, 612,1849, 910, 796,3844,1740,1371, 825,3845,3846,5220,2920,2562,5221, 692, # 1936
 444,3052,2634, 801,4606,4274,5222,1491, 244,1053,3053,4275,4276, 340,5223,4018, # 1952
1041,3005, 293,1168,  87,1357,5224,1539, 959,5225,2240, 721, 694,4277,3847, 219, # 1968
1478, 644,1417,3368,2666,1413,1401,1335,1389,4019,5226,5227,3006,2367,3159,1826, # 1984
 730,1515, 184,2840,  66,4607,5228,1660,2958, 246,3369, 378,1457, 226,3480, 975, # 2000
4020,2959,1264,3592, 674, 696,5229, 163,5230,1141,2422,2167, 713,3593,3370,4608, # 2016
4021,5231,5232,1186,  15,5233,1079,1070,5234,1522,3224,3594, 276,1050,2725, 758, # 2032
1126, 653,2960,3296,5235,2342, 889,3595,4022,3104,3007, 903,1250,4609,4023,3481, # 2048
3596,1342,1681,1718, 766,3297, 286,  89,2961,3715,5236,1713,5237,2607,3371,3008, # 2064
5238,2962,2219,3225,2880,5239,4610,2505,2533, 181, 387,1075,4024, 731,2190,3372, # 2080
5240,3298, 310, 313,3482,2304, 770,4278,  54,3054, 189,4611,3105,3848,4025,5241, # 2096
1230,1617,1850, 355,3597,4279,4612,3373, 111,4280,3716,1350,3160,3483,3055,4281, # 2112
2150,3299,3598,5242,2797,4026,4027,3009, 722,2009,5243,1071, 247,1207,2343,2478, # 2128
1378,4613,2010, 864,1437,1214,4614, 373,3849,1142,2220, 667,4615, 442,2763,2563, # 2144
3850,4028,1969,4282,3300,1840, 837, 170,1107, 934,1336,1883,5244,5245,2119,4283, # 2160
2841, 743,1569,5246,4616,4284, 582,2389,1418,3484,5247,1803,5248, 357,1395,1729, # 2176
3717,3301,2423,1564,2241,5249,3106,3851,1633,4617,1114,2086,4285,1532,5250, 482, # 2192
2451,4618,5251,5252,1492, 833,1466,5253,2726,3599,1641,2842,5254,1526,1272,3718, # 2208
4286,1686,1795, 416,2564,1903,1954,1804,5255,3852,2798,3853,1159,2321,5256,2881, # 2224
4619,1610,1584,3056,2424,2764, 443,3302,1163,3161,5257,5258,4029,5259,4287,2506, # 2240
3057,4620,4030,3162,2104,1647,3600,2011,1873,4288,5260,4289, 431,3485,5261, 250, # 2256
  97,  81,4290,5262,1648,1851,1558, 160, 848,5263, 866, 740,1694,5264,2204,2843, # 2272
3226,4291,4621,3719,1687, 950,2479, 426, 469,3227,3720,3721,4031,5265,5266,1188, # 2288
 424,1996, 861,3601,4292,3854,2205,2694, 168,1235,3602,4293,5267,2087,1674,4622, # 2304
3374,3303, 220,2565,1009,5268,3855, 670,3010, 332,1208, 717,5269,5270,3603,2452, # 2320
4032,3375,5271, 513,5272,1209,2882,3376,3163,4623,1080,5273,5274,5275,5276,2534, # 2336
3722,3604, 815,1587,4033,4034,5277,3605,3486,3856,1254,4624,1328,3058,1390,4035, # 2352
1741,4036,3857,4037,5278, 236,3858,2453,3304,5279,5280,3723,3859,1273,3860,4625, # 2368
5281, 308,5282,4626, 245,4627,1852,2480,1307,2583, 430, 715,2137,2454,5283, 270, # 2384
 199,2883,4038,5284,3606,2727,1753, 761,1754, 725,1661,1841,4628,3487,3724,5285, # 2400
5286, 587,  14,3305, 227,2608, 326, 480,2270, 943,2765,3607, 291, 650,1884,5287, # 2416
1702,1226, 102,1547,  62,3488, 904,4629,3489,1164,4294,5288,5289,1224,1548,2766, # 2432
 391, 498,1493,5290,1386,1419,5291,2056,1177,4630, 813, 880,1081,2368, 566,1145, # 2448
4631,2291,1001,1035,2566,2609,2242, 394,1286,5292,5293,2069,5294,  86,1494,1730, # 2464
4039, 491,1588, 745, 897,2963, 843,3377,4040,2767,2884,3306,1768, 998,2221,2070, # 2480
 397,1827,1195,1970,3725,3011,3378, 284,5295,3861,2507,2138,2120,1904,5296,4041, # 2496
2151,4042,4295,1036,3490,1905, 114,2567,4296, 209,1527,5297,5298,2964,2844,2635, # 2512
2390,2728,3164, 812,2568,5299,3307,5300,1559, 737,1885,3726,1210, 885,  28,2695, # 2528
3608,3862,5301,4297,1004,1780,4632,5302, 346,1982,2222,2696,4633,3863,1742, 797, # 2544
1642,4043,1934,1072,1384,2152, 896,4044,3308,3727,3228,2885,3609,5303,2569,1959, # 2560
4634,2455,1786,5304,5305,5306,4045,4298,1005,1308,3728,4299,2729,4635,4636,1528, # 2576
2610, 161,1178,4300,1983, 987,4637,1101,4301, 631,4046,1157,3229,2425,1343,1241, # 2592
1016,2243,2570, 372, 877,2344,2508,1160, 555,1935, 911,4047,5307, 466,1170, 169, # 2608
1051,2921,2697,3729,2481,3012,1182,2012,2571,1251,2636,5308, 992,2345,3491,1540, # 2624
2730,1201,2071,2406,1997,2482,5309,4638, 528,1923,2191,1503,1874,1570,2369,3379, # 2640
3309,5310, 557,1073,5311,1828,3492,2088,2271,3165,3059,3107, 767,3108,2799,4639, # 2656
1006,4302,4640,2346,1267,2179,3730,3230, 778,4048,3231,2731,1597,2667,5312,4641, # 2672
5313,3493,5314,5315,5316,3310,2698,1433,3311, 131,  95,1504,4049, 723,4303,3166, # 2688
1842,3610,2768,2192,4050,2028,2105,3731,5317,3013,4051,1218,5318,3380,3232,4052, # 2704
4304,2584, 248,1634,3864, 912,5319,2845,3732,3060,3865, 654,  53,5320,3014,5321, # 2720
1688,4642, 777,3494,1032,4053,1425,5322, 191, 820,2121,2846, 971,4643, 931,3233, # 2736
 135, 664, 783,3866,1998, 772,2922,1936,4054,3867,4644,2923,3234, 282,2732, 640, # 2752
1372,3495,1127, 922, 325,3381,5323,5324, 711,2045,5325,5326,4055,2223,2800,1937, # 2768
4056,3382,2224,2255,3868,2305,5327,4645,3869,1258,3312,4057,3235,2139,2965,4058, # 2784
4059,5328,2225, 258,3236,4646, 101,1227,5329,3313,1755,5330,1391,3314,5331,2924, # 2800
2057, 893,5332,5333,5334,1402,4305,2347,5335,5336,3237,3611,5337,5338, 878,1325, # 2816
1781,2801,4647, 259,1385,2585, 744,1183,2272,4648,5339,4060,2509,5340, 684,1024, # 2832
4306,5341, 472,3612,3496,1165,3315,4061,4062, 322,2153, 881, 455,1695,1152,1340, # 2848
 660, 554,2154,4649,1058,4650,4307, 830,1065,3383,4063,4651,1924,5342,1703,1919, # 2864
5343, 932,2273, 122,5344,4652, 947, 677,5345,3870,2637, 297,1906,1925,2274,4653, # 2880
2322,3316,5346,5347,4308,5348,4309,  84,4310, 112, 989,5349, 547,1059,4064, 701, # 2896
3613,1019,5350,4311,5351,3497, 942, 639, 457,2306,2456, 993,2966, 407, 851, 494, # 2912
4654,3384, 927,5352,1237,5353,2426,3385, 573,4312, 680, 921,2925,1279,1875, 285, # 2928
 790,1448,1984, 719,2168,5354,5355,4655,4065,4066,1649,5356,1541, 563,5357,1077, # 2944
5358,3386,3061,3498, 511,3015,4067,4068,3733,4069,1268,2572,3387,3238,4656,4657, # 2960
5359, 535,1048,1276,1189,2926,2029,3167,1438,1373,2847,2967,1134,2013,5360,4313, # 2976
1238,2586,3109,1259,5361, 700,5362,2968,3168,3734,4314,5363,4315,1146,1876,1907, # 2992
4658,2611,4070, 781,2427, 132,1589, 203, 147, 273,2802,2407, 898,1787,2155,4071, # 3008
4072,5364,3871,2803,5365,5366,4659,4660,5367,3239,5368,1635,3872, 965,5369,1805, # 3024
2699,1516,3614,1121,1082,1329,3317,4073,1449,3873,  65,1128,2848,2927,2769,1590, # 3040
3874,5370,5371,  12,2668,  45, 976,2587,3169,4661, 517,2535,1013,1037,3240,5372, # 3056
3875,2849,5373,3876,5374,3499,5375,2612, 614,1999,2323,3877,3110,2733,2638,5376, # 3072
2588,4316, 599,1269,5377,1811,3735,5378,2700,3111, 759,1060, 489,1806,3388,3318, # 3088
1358,5379,5380,2391,1387,1215,2639,2256, 490,5381,5382,4317,1759,2392,2348,5383, # 3104
4662,3878,1908,4074,2640,1807,3241,4663,3500,3319,2770,2349, 874,5384,5385,3501, # 3120
3736,1859,  91,2928,3737,3062,3879,4664,5386,3170,4075,2669,5387,3502,1202,1403, # 3136
3880,2969,2536,1517,2510,4665,3503,2511,5388,4666,5389,2701,1886,1495,1731,4076, # 3152
2370,4667,5390,2030,5391,5392,4077,2702,1216, 237,2589,4318,2324,4078,3881,4668, # 3168
4669,2703,3615,3504, 445,4670,5393,5394,5395,5396,2771,  61,4079,3738,1823,4080, # 3184
5397, 687,2046, 935, 925, 405,2670, 703,1096,1860,2734,4671,4081,1877,1367,2704, # 3200
3389, 918,2106,1782,2483, 334,3320,1611,1093,4672, 564,3171,3505,3739,3390, 945, # 3216
2641,2058,4673,5398,1926, 872,4319,5399,3506,2705,3112, 349,4320,3740,4082,4674, # 3232
3882,4321,3741,2156,4083,4675,4676,4322,4677,2408,2047, 782,4084, 400, 251,4323, # 3248
1624,5400,5401, 277,3742, 299,1265, 476,1191,3883,2122,4324,4325,1109, 205,5402, # 3264
2590,1000,2157,3616,1861,5403,5404,5405,4678,5406,4679,2573, 107,2484,2158,4085, # 3280
3507,3172,5407,1533, 541,1301, 158, 753,4326,2886,3617,5408,1696, 370,1088,4327, # 3296
4680,3618, 579, 327, 440, 162,2244, 269,1938,1374,3508, 968,3063,  56,1396,3113, # 3312
2107,3321,3391,5409,1927,2159,4681,3016,5410,3619,5411,5412,3743,4682,2485,5413, # 3328
2804,5414,1650,4683,5415,2613,5416,5417,4086,2671,3392,1149,3393,4087,3884,4088, # 3344
5418,1076,  49,5419, 951,3242,3322,3323, 450,2850, 920,5420,1812,2805,2371,4328, # 3360
1909,1138,2372,3885,3509,5421,3243,4684,1910,1147,1518,2428,4685,3886,5422,4686, # 3376
2393,2614, 260,1796,3244,5423,5424,3887,3324, 708,5425,3620,1704,5426,3621,1351, # 3392
1618,3394,3017,1887, 944,4329,3395,4330,3064,3396,4331,5427,3744, 422, 413,1714, # 3408
3325, 500,2059,2350,4332,2486,5428,1344,1911, 954,5429,1668,5430,5431,4089,2409, # 3424
4333,3622,3888,4334,5432,2307,1318,2512,3114, 133,3115,2887,4687, 629,  31,2851, # 3440
2706,3889,4688, 850, 949,4689,4090,2970,1732,2089,4335,1496,1853,5433,4091, 620, # 3456
3245, 981,1242,3745,3397,1619,3746,1643,3326,2140,2457,1971,1719,3510,2169,5434, # 3472
3246,5435,5436,3398,1829,5437,1277,4690,1565,2048,5438,1636,3623,3116,5439, 869, # 3488
2852, 655,3890,3891,3117,4092,3018,3892,1310,3624,4691,5440,5441,5442,1733, 558, # 3504
4692,3747, 335,1549,3065,1756,4336,3748,1946,3511,1830,1291,1192, 470,2735,2108, # 3520
2806, 913,1054,4093,5443,1027,5444,3066,4094,4693, 982,2672,3399,3173,3512,3247, # 3536
3248,1947,2807,5445, 571,4694,5446,1831,5447,3625,2591,1523,2429,5448,2090, 984, # 3552
4695,3749,1960,5449,3750, 852, 923,2808,3513,3751, 969,1519, 999,2049,2325,1705, # 3568
5450,3118, 615,1662, 151, 597,4095,2410,2326,1049, 275,4696,3752,4337, 568,3753, # 3584
3626,2487,4338,3754,5451,2430,2275, 409,3249,5452,1566,2888,3514,1002, 769,2853, # 3600
 194,2091,3174,3755,2226,3327,4339, 628,1505,5453,5454,1763,2180,3019,4096, 521, # 3616
1161,2592,1788,2206,2411,4697,4097,1625,4340,4341, 412,  42,3119, 464,5455,2642, # 3632
4698,3400,1760,1571,2889,3515,2537,1219,2207,3893,2643,2141,2373,4699,4700,3328, # 3648
1651,3401,3627,5456,5457,3628,2488,3516,5458,3756,5459,5460,2276,2092, 460,5461, # 3664
4701,5462,3020, 962, 588,3629, 289,3250,2644,1116,  52,5463,3067,1797,5464,5465, # 3680
5466,1467,5467,1598,1143,3757,4342,1985,1734,1067,4702,1280,3402, 465,4703,1572, # 3696
 510,5468,1928,2245,1813,1644,3630,5469,4704,3758,5470,5471,2673,1573,1534,5472, # 3712
5473, 536,1808,1761,3517,3894,3175,2645,5474,5475,5476,4705,3518,2929,1912,2809, # 3728
5477,3329,1122, 377,3251,5478, 360,5479,5480,4343,1529, 551,5481,2060,3759,1769, # 3744
2431,5482,2930,4344,3330,3120,2327,2109,2031,4706,1404, 136,1468,1479, 672,1171, # 3760
3252,2308, 271,3176,5483,2772,5484,2050, 678,2736, 865,1948,4707,5485,2014,4098, # 3776
2971,5486,2737,2227,1397,3068,3760,4708,4709,1735,2931,3403,3631,5487,3895, 509, # 3792
2854,2458,2890,3896,5488,5489,3177,3178,4710,4345,2538,4711,2309,1166,1010, 552, # 3808
 681,1888,5490,5491,2972,2973,4099,1287,1596,1862,3179, 358, 453, 736, 175, 478, # 3824
1117, 905,1167,1097,5492,1854,1530,5493,1706,5494,2181,3519,2292,3761,3520,3632, # 3840
4346,2093,4347,5495,3404,1193,2489,4348,1458,2193,2208,1863,1889,1421,3331,2932, # 3856
3069,2182,3521, 595,2123,5496,4100,5497,5498,4349,1707,2646, 223,3762,1359, 751, # 3872
3121, 183,3522,5499,2810,3021, 419,2374, 633, 704,3897,2394, 241,5500,5501,5502, # 3888
 838,3022,3763,2277,2773,2459,3898,1939,2051,4101,1309,3122,2246,1181,5503,1136, # 3904
2209,3899,2375,1446,4350,2310,4712,5504,5505,4351,1055,2615, 484,3764,5506,4102, # 3920
 625,4352,2278,3405,1499,4353,4103,5507,4104,4354,3253,2279,2280,3523,5508,5509, # 3936
2774, 808,2616,3765,3406,4105,4355,3123,2539, 526,3407,3900,4356, 955,5510,1620, # 3952
4357,2647,2432,5511,1429,3766,1669,1832, 994, 928,5512,3633,1260,5513,5514,5515, # 3968
1949,2293, 741,2933,1626,4358,2738,2460, 867,1184, 362,3408,1392,5516,5517,4106, # 3984
4359,1770,1736,3254,2934,4713,4714,1929,2707,1459,1158,5518,3070,3409,2891,1292, # 4000
1930,2513,2855,3767,1986,1187,2072,2015,2617,4360,5519,2574,2514,2170,3768,2490, # 4016
3332,5520,3769,4715,5521,5522, 666,1003,3023,1022,3634,4361,5523,4716,1814,2257, # 4032
 574,3901,1603, 295,1535, 705,3902,4362, 283, 858, 417,5524,5525,3255,4717,4718, # 4048
3071,1220,1890,1046,2281,2461,4107,1393,1599, 689,2575, 388,4363,5526,2491, 802, # 4064
5527,2811,3903,2061,1405,2258,5528,4719,3904,2110,1052,1345,3256,1585,5529, 809, # 4080
5530,5531,5532, 575,2739,3524, 956,1552,1469,1144,2328,5533,2329,1560,2462,3635, # 4096
3257,4108, 616,2210,4364,3180,2183,2294,5534,1833,5535,3525,4720,5536,1319,3770, # 4112
3771,1211,3636,1023,3258,1293,2812,5537,5538,5539,3905, 607,2311,3906, 762,2892, # 4128
1439,4365,1360,4721,1485,3072,5540,4722,1038,4366,1450,2062,2648,4367,1379,4723, # 4144
2593,5541,5542,4368,1352,1414,2330,2935,1172,5543,5544,3907,3908,4724,1798,1451, # 4160
5545,5546,5547,5548,2936,4109,4110,2492,2351, 411,4111,4112,3637,3333,3124,4725, # 4176
1561,2674,1452,4113,1375,5549,5550,  47,2974, 316,5551,1406,1591,2937,3181,5552, # 4192
1025,2142,3125,3182, 354,2740, 884,2228,4369,2412, 508,3772, 726,3638, 996,2433, # 4208
3639, 729,5553, 392,2194,1453,4114,4726,3773,5554,5555,2463,3640,2618,1675,2813, # 4224
 919,2352,2975,2353,1270,4727,4115,  73,5556,5557, 647,5558,3259,2856,2259,1550, # 4240
1346,3024,5559,1332, 883,3526,5560,5561,5562,5563,3334,2775,5564,1212, 831,1347, # 4256
4370,4728,2331,3909,1864,3073, 720,3910,4729,4730,3911,5565,4371,5566,5567,4731, # 4272
5568,5569,1799,4732,3774,2619,4733,3641,1645,2376,4734,5570,2938, 669,2211,2675, # 4288
2434,5571,2893,5572,5573,1028,3260,5574,4372,2413,5575,2260,1353,5576,5577,4735, # 4304
3183, 518,5578,4116,5579,4373,1961,5580,2143,4374,5581,5582,3025,2354,2355,3912, # 4320
 516,1834,1454,4117,2708,4375,4736,2229,2620,1972,1129,3642,5583,2776,5584,2976, # 4336
1422, 577,1470,3026,1524,3410,5585,5586, 432,4376,3074,3527,5587,2594,1455,2515, # 4352
2230,1973,1175,5588,1020,2741,4118,3528,4737,5589,2742,5590,1743,1361,3075,3529, # 4368
2649,4119,4377,4738,2295, 895, 924,4378,2171, 331,2247,3076, 166,1627,3077,1098, # 4384
5591,1232,2894,2231,3411,4739, 657, 403,1196,2377, 542,3775,3412,1600,4379,3530, # 4400
5592,4740,2777,3261, 576, 530,1362,4741,4742,2540,2676,3776,4120,5593, 842,3913, # 4416
5594,2814,2032,1014,4121, 213,2709,3413, 665, 621,4380,5595,3777,2939,2435,5596, # 4432
2436,3335,3643,3414,4743,4381,2541,4382,4744,3644,1682,4383,3531,1380,5597, 724, # 4448
2282, 600,1670,5598,1337,1233,4745,3126,2248,5599,1621,4746,5600, 651,4384,5601, # 4464
1612,4385,2621,5602,2857,5603,2743,2312,3078,5604, 716,2464,3079, 174,1255,2710, # 4480
4122,3645, 548,1320,1398, 728,4123,1574,5605,1891,1197,3080,4124,5606,3081,3082, # 4496
3778,3646,3779, 747,5607, 635,4386,4747,5608,5609,5610,4387,5611,5612,4748,5613, # 4512
3415,4749,2437, 451,5614,3780,2542,2073,4388,2744,4389,4125,5615,1764,4750,5616, # 4528
4390, 350,4751,2283,2395,2493,5617,4391,4126,2249,1434,4127, 488,4752, 458,4392, # 4544
4128,3781, 771,1330,2396,3914,2576,3184,2160,2414,1553,2677,3185,4393,5618,2494, # 4560
2895,2622,1720,2711,4394,3416,4753,5619,2543,4395,5620,3262,4396,2778,5621,2016, # 4576
2745,5622,1155,1017,3782,3915,5623,3336,2313, 201,1865,4397,1430,5624,4129,5625, # 4592
5626,5627,5628,5629,4398,1604,5630, 414,1866, 371,2595,4754,4755,3532,2017,3127, # 4608
4756,1708, 960,4399, 887, 389,2172,1536,1663,1721,5631,2232,4130,2356,2940,1580, # 4624
5632,5633,1744,4757,2544,4758,4759,5634,4760,5635,2074,5636,4761,3647,3417,2896, # 4640
4400,5637,4401,2650,3418,2815, 673,2712,2465, 709,3533,4131,3648,4402,5638,1148, # 4656
 502, 634,5639,5640,1204,4762,3649,1575,4763,2623,3783,5641,3784,3128, 948,3263, # 4672
 121,1745,3916,1110,5642,4403,3083,2516,3027,4132,3785,1151,1771,3917,1488,4133, # 4688
1987,5643,2438,3534,5644,5645,2094,5646,4404,3918,1213,1407,2816, 531,2746,2545, # 4704
3264,1011,1537,4764,2779,4405,3129,1061,5647,3786,3787,1867,2897,5648,2018, 120, # 4720
4406,4407,2063,3650,3265,2314,3919,2678,3419,1955,4765,4134,5649,3535,1047,2713, # 4736
1266,5650,1368,4766,2858, 649,3420,3920,2546,2747,1102,2859,2679,5651,5652,2000, # 4752
5653,1111,3651,2977,5654,2495,3921,3652,2817,1855,3421,3788,5655,5656,3422,2415, # 4768
2898,3337,3266,3653,5657,2577,5658,3654,2818,4135,1460, 856,5659,3655,5660,2899, # 4784
2978,5661,2900,3922,5662,4408, 632,2517, 875,3923,1697,3924,2296,5663,5664,4767, # 4800
3028,1239, 580,4768,4409,5665, 914, 936,2075,1190,4136,1039,2124,5666,5667,5668, # 4816
5669,3423,1473,5670,1354,4410,3925,4769,2173,3084,4137, 915,3338,4411,4412,3339, # 4832
1605,1835,5671,2748, 398,3656,4413,3926,4138, 328,1913,2860,4139,3927,1331,4414, # 4848
3029, 937,4415,5672,3657,4140,4141,3424,2161,4770,3425, 524, 742, 538,3085,1012, # 4864
5673,5674,3928,2466,5675, 658,1103, 225,3929,5676,5677,4771,5678,4772,5679,3267, # 4880
1243,5680,4142, 963,2250,4773,5681,2714,3658,3186,5682,5683,2596,2332,5684,4774, # 4896
5685,5686,5687,3536, 957,3426,2547,2033,1931,2941,2467, 870,2019,3659,1746,2780, # 4912
2781,2439,2468,5688,3930,5689,3789,3130,3790,3537,3427,3791,5690,1179,3086,5691, # 4928
3187,2378,4416,3792,2548,3188,3131,2749,4143,5692,3428,1556,2549,2297, 977,2901, # 4944
2034,4144,1205,3429,5693,1765,3430,3189,2125,1271, 714,1689,4775,3538,5694,2333, # 4960
3931, 533,4417,3660,2184, 617,5695,2469,3340,3539,2315,5696,5697,3190,5698,5699, # 4976
3932,1988, 618, 427,2651,3540,3431,5700,5701,1244,1690,5702,2819,4418,4776,5703, # 4992
3541,4777,5704,2284,1576, 473,3661,4419,3432, 972,5705,3662,5706,3087,5707,5708, # 5008
4778,4779,5709,3793,4145,4146,5710, 153,4780, 356,5711,1892,2902,4420,2144, 408, # 5024
 803,2357,5712,3933,5713,4421,1646,2578,2518,4781,4782,3934,5714,3935,4422,5715, # 5040
2416,3433, 752,5716,5717,1962,3341,2979,5718, 746,3030,2470,4783,4423,3794, 698, # 5056
4784,1893,4424,3663,2550,4785,3664,3936,5719,3191,3434,5720,1824,1302,4147,2715, # 5072
3937,1974,4425,5721,4426,3192, 823,1303,1288,1236,2861,3542,4148,3435, 774,3938, # 5088
5722,1581,4786,1304,2862,3939,4787,5723,2440,2162,1083,3268,4427,4149,4428, 344, # 5104
1173, 288,2316, 454,1683,5724,5725,1461,4788,4150,2597,5726,5727,4789, 985, 894, # 5120
5728,3436,3193,5729,1914,2942,3795,1989,5730,2111,1975,5731,4151,5732,2579,1194, # 5136
 425,5733,4790,3194,1245,3796,4429,5734,5735,2863,5736, 636,4791,1856,3940, 760, # 5152
1800,5737,4430,2212,1508,4792,4152,1894,1684,2298,5738,5739,4793,4431,4432,2213, # 5168
 479,5740,5741, 832,5742,4153,2496,5743,2980,2497,3797, 990,3132, 627,1815,2652, # 5184
4433,1582,4434,2126,2112,3543,4794,5744, 799,4435,3195,5745,4795,2113,1737,3031, # 5200
1018, 543, 754,4436,3342,1676,4796,4797,4154,4798,1489,5746,3544,5747,2624,2903, # 5216
4155,5748,5749,2981,5750,5751,5752,5753,3196,4799,4800,2185,1722,5754,3269,3270, # 5232
1843,3665,1715, 481, 365,1976,1857,5755,5756,1963,2498,4801,5757,2127,3666,3271, # 5248
 433,1895,2064,2076,5758, 602,2750,5759,5760,5761,5762,5763,3032,1628,3437,5764, # 5264
3197,4802,4156,2904,4803,2519,5765,2551,2782,5766,5767,5768,3343,4804,2905,5769, # 5280
4805,5770,2864,4806,4807,1221,2982,4157,2520,5771,5772,5773,1868,1990,5774,5775, # 5296
5776,1896,5777,5778,4808,1897,4158, 318,5779,2095,4159,4437,5780,5781, 485,5782, # 5312
 938,3941, 553,2680, 116,5783,3942,3667,5784,3545,2681,2783,3438,3344,2820,5785, # 5328
3668,2943,4160,1747,2944,2983,5786,5787, 207,5788,4809,5789,4810,2521,5790,3033, # 5344
 890,3669,3943,5791,1878,3798,3439,5792,2186,2358,3440,1652,5793,5794,5795, 941, # 5360
2299, 208,3546,4161,2020, 330,4438,3944,2906,2499,3799,4439,4811,5796,5797,5798, # 5376
)
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/cp949prober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return "CP949"

    @property
    def language(self) -> str:
        return "Korean"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/codingstatemachinedict.py
# ========================================================
from typing import TYPE_CHECKING, Tuple

if TYPE_CHECKING:
    # TypedDict was introduced in Python 3.8.
    #
    # TODO: Remove the else block and TYPE_CHECKING check when dropping support
    # for Python 3.7.
    from typing import TypedDict

    class CodingStateMachineDict(TypedDict, total=False):
        class_table: Tuple[int, ...]
        class_factor: int
        state_table: Tuple[int, ...]
        char_len_table: Tuple[int, ...]
        name: str
        language: str  # Optional key

else:
    CodingStateMachineDict = dict


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/mbcsgroupprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .enums import LanguageFilter
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/mbcharsetprober.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from typing import Optional, Union

from .chardistribution import CharDistributionAnalysis
from .charsetprober import CharSetProber
from .codingstatemachine import CodingStateMachine
from .enums import LanguageFilter, MachineState, ProbingState


class MultiByteCharSetProber(CharSetProber):
    """
    MultiByteCharSetProber
    """

    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.distribution_analyzer: Optional[CharDistributionAnalysis] = None
        self.coding_sm: Optional[CodingStateMachine] = None
        self._last_char = bytearray(b"\0\0")

    def reset(self) -> None:
        super().reset()
        if self.coding_sm:
            self.coding_sm.reset()
        if self.distribution_analyzer:
            self.distribution_analyzer.reset()
        self._last_char = bytearray(b"\0\0")

    def feed(self, byte_str: Union[bytes, bytearray]) -> ProbingState:
        assert self.coding_sm is not None
        assert self.distribution_analyzer is not None

        for i, byte in enumerate(byte_str):
            coding_state = self.coding_sm.next_state(byte)
            if coding_state == MachineState.ERROR:
                self.logger.debug(
                    "%s %s prober hit error at byte %s",
                    self.charset_name,
                    self.language,
                    i,
                )
                self._state = ProbingState.NOT_ME
                break
            if coding_state == MachineState.ITS_ME:
                self._state = ProbingState.FOUND_IT
                break
            if coding_state == MachineState.START:
                char_len = self.coding_sm.get_current_charlen()
                if i == 0:
                    self._last_char[1] = byte
                    self.distribution_analyzer.feed(self._last_char, char_len)
                else:
                    self.distribution_analyzer.feed(byte_str[i - 1 : i + 1], char_len)

        self._last_char[0] = byte_str[-1]

        if self.state == ProbingState.DETECTING:
            if self.distribution_analyzer.got_enough_data() and (
                self.get_confidence() > self.SHORTCUT_THRESHOLD
            ):
                self._state = ProbingState.FOUND_IT

        return self.state

    def get_confidence(self) -> float:
        assert self.distribution_analyzer is not None
        return self.distribution_analyzer.get_confidence()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/jisfreq.py
# ========================================================
######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# Sampling from about 20M text materials include literature and computer technology
#
# Japanese frequency table, applied to both S-JIS and EUC-JP
# They are sorted in order.

# 128  --> 0.77094
# 256  --> 0.85710
# 512  --> 0.92635
# 1024 --> 0.97130
# 2048 --> 0.99431
#
# Ideal Distribution Ratio = 0.92635 / (1-0.92635) = 12.58
# Random Distribution Ration = 512 / (2965+62+83+86-512) = 0.191
#
# Typical Distribution Ratio, 25% of IDR

JIS_TYPICAL_DISTRIBUTION_RATIO = 3.0

# Char to FreqOrder table ,
JIS_TABLE_SIZE = 4368

# fmt: off
JIS_CHAR_TO_FREQ_ORDER = (
  40,   1,   6, 182, 152, 180, 295,2127, 285, 381,3295,4304,3068,4606,3165,3510, #   16
3511,1822,2785,4607,1193,2226,5070,4608, 171,2996,1247,  18, 179,5071, 856,1661, #   32
1262,5072, 619, 127,3431,3512,3230,1899,1700, 232, 228,1294,1298, 284, 283,2041, #   48
2042,1061,1062,  48,  49,  44,  45, 433, 434,1040,1041, 996, 787,2997,1255,4305, #   64
2108,4609,1684,1648,5073,5074,5075,5076,5077,5078,3687,5079,4610,5080,3927,3928, #   80
5081,3296,3432, 290,2285,1471,2187,5082,2580,2825,1303,2140,1739,1445,2691,3375, #   96
1691,3297,4306,4307,4611, 452,3376,1182,2713,3688,3069,4308,5083,5084,5085,5086, #  112
5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102, #  128
5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,4097,5113,5114,5115,5116,5117, #  144
5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133, #  160
5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149, #  176
5150,5151,5152,4612,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164, #  192
5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,1472, 598, 618, 820,1205, #  208
1309,1412,1858,1307,1692,5176,5177,5178,5179,5180,5181,5182,1142,1452,1234,1172, #  224
1875,2043,2149,1793,1382,2973, 925,2404,1067,1241, 960,1377,2935,1491, 919,1217, #  240
1865,2030,1406,1499,2749,4098,5183,5184,5185,5186,5187,5188,2561,4099,3117,1804, #  256
2049,3689,4309,3513,1663,5189,3166,3118,3298,1587,1561,3433,5190,3119,1625,2998, #  272
3299,4613,1766,3690,2786,4614,5191,5192,5193,5194,2161,  26,3377,   2,3929,  20, #  288
3691,  47,4100,  50,  17,  16,  35, 268,  27, 243,  42, 155,  24, 154,  29, 184, #  304
   4,  91,  14,  92,  53, 396,  33, 289,   9,  37,  64, 620,  21,  39, 321,   5, #  320
  12,  11,  52,  13,   3, 208, 138,   0,   7,  60, 526, 141, 151,1069, 181, 275, #  336
1591,  83, 132,1475, 126, 331, 829,  15,  69, 160,  59,  22, 157,  55,1079, 312, #  352
 109,  38,  23,  25,  10,  19,  79,5195,  61, 382,1124,   8,  30,5196,5197,5198, #  368
5199,5200,5201,5202,5203,5204,5205,5206,  89,  62,  74,  34,2416, 112, 139, 196, #  384
 271, 149,  84, 607, 131, 765,  46,  88, 153, 683,  76, 874, 101, 258,  57,  80, #  400
  32, 364, 121,1508, 169,1547,  68, 235, 145,2999,  41, 360,3027,  70,  63,  31, #  416
  43, 259, 262,1383,  99, 533, 194,  66,  93, 846, 217, 192,  56, 106,  58, 565, #  432
 280, 272, 311, 256, 146,  82, 308,  71, 100, 128, 214, 655, 110, 261, 104,1140, #  448
  54,  51,  36,  87,  67,3070, 185,2618,2936,2020,  28,1066,2390,2059,5207,5208, #  464
5209,5210,5211,5212,5213,5214,5215,5216,4615,5217,5218,5219,5220,5221,5222,5223, #  480
5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,3514,5237,5238, #  496
5239,5240,5241,5242,5243,5244,2297,2031,4616,4310,3692,5245,3071,5246,3598,5247, #  512
4617,3231,3515,5248,4101,4311,4618,3808,4312,4102,5249,4103,4104,3599,5250,5251, #  528
5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267, #  544
5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283, #  560
5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299, #  576
5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315, #  592
5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331, #  608
5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347, #  624
5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363, #  640
5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379, #  656
5380,5381, 363, 642,2787,2878,2788,2789,2316,3232,2317,3434,2011, 165,1942,3930, #  672
3931,3932,3933,5382,4619,5383,4620,5384,5385,5386,5387,5388,5389,5390,5391,5392, #  688
5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408, #  704
5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424, #  720
5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440, #  736
5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456, #  752
5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472, #  768
5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488, #  784
5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504, #  800
5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520, #  816
5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536, #  832
5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552, #  848
5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568, #  864
5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584, #  880
5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600, #  896
5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616, #  912
5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632, #  928
5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648, #  944
5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664, #  960
5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680, #  976
5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696, #  992
5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712, # 1008
5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728, # 1024
5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744, # 1040
5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760, # 1056
5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776, # 1072
5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792, # 1088
5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808, # 1104
5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824, # 1120
5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840, # 1136
5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856, # 1152
5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872, # 1168
5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888, # 1184
5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904, # 1200
5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920, # 1216
5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936, # 1232
5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952, # 1248
5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968, # 1264
5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984, # 1280
5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000, # 1296
6001,6002,6003,6004,6005,6006,6007,6008,6009,6010,6011,6012,6013,6014,6015,6016, # 1312
6017,6018,6019,6020,6021,6022,6023,6024,6025,6026,6027,6028,6029,6030,6031,6032, # 1328
6033,6034,6035,6036,6037,6038,6039,6040,6041,6042,6043,6044,6045,6046,6047,6048, # 1344
6049,6050,6051,6052,6053,6054,6055,6056,6057,6058,6059,6060,6061,6062,6063,6064, # 1360
6065,6066,6067,6068,6069,6070,6071,6072,6073,6074,6075,6076,6077,6078,6079,6080, # 1376
6081,6082,6083,6084,6085,6086,6087,6088,6089,6090,6091,6092,6093,6094,6095,6096, # 1392
6097,6098,6099,6100,6101,6102,6103,6104,6105,6106,6107,6108,6109,6110,6111,6112, # 1408
6113,6114,2044,2060,4621, 997,1235, 473,1186,4622, 920,3378,6115,6116, 379,1108, # 1424
4313,2657,2735,3934,6117,3809, 636,3233, 573,1026,3693,3435,2974,3300,2298,4105, # 1440
 854,2937,2463, 393,2581,2417, 539, 752,1280,2750,2480, 140,1161, 440, 708,1569, # 1456
 665,2497,1746,1291,1523,3000, 164,1603, 847,1331, 537,1997, 486, 508,1693,2418, # 1472
1970,2227, 878,1220, 299,1030, 969, 652,2751, 624,1137,3301,2619,  65,3302,2045, # 1488
1761,1859,3120,1930,3694,3516, 663,1767, 852, 835,3695, 269, 767,2826,2339,1305, # 1504
 896,1150, 770,1616,6118, 506,1502,2075,1012,2519, 775,2520,2975,2340,2938,4314, # 1520
3028,2086,1224,1943,2286,6119,3072,4315,2240,1273,1987,3935,1557, 175, 597, 985, # 1536
3517,2419,2521,1416,3029, 585, 938,1931,1007,1052,1932,1685,6120,3379,4316,4623, # 1552
 804, 599,3121,1333,2128,2539,1159,1554,2032,3810, 687,2033,2904, 952, 675,1467, # 1568
3436,6121,2241,1096,1786,2440,1543,1924, 980,1813,2228, 781,2692,1879, 728,1918, # 1584
3696,4624, 548,1950,4625,1809,1088,1356,3303,2522,1944, 502, 972, 373, 513,2827, # 1600
 586,2377,2391,1003,1976,1631,6122,2464,1084, 648,1776,4626,2141, 324, 962,2012, # 1616
2177,2076,1384, 742,2178,1448,1173,1810, 222, 102, 301, 445, 125,2420, 662,2498, # 1632
 277, 200,1476,1165,1068, 224,2562,1378,1446, 450,1880, 659, 791, 582,4627,2939, # 1648
3936,1516,1274, 555,2099,3697,1020,1389,1526,3380,1762,1723,1787,2229, 412,2114, # 1664
1900,2392,3518, 512,2597, 427,1925,2341,3122,1653,1686,2465,2499, 697, 330, 273, # 1680
 380,2162, 951, 832, 780, 991,1301,3073, 965,2270,3519, 668,2523,2636,1286, 535, # 1696
1407, 518, 671, 957,2658,2378, 267, 611,2197,3030,6123, 248,2299, 967,1799,2356, # 1712
 850,1418,3437,1876,1256,1480,2828,1718,6124,6125,1755,1664,2405,6126,4628,2879, # 1728
2829, 499,2179, 676,4629, 557,2329,2214,2090, 325,3234, 464, 811,3001, 992,2342, # 1744
2481,1232,1469, 303,2242, 466,1070,2163, 603,1777,2091,4630,2752,4631,2714, 322, # 1760
2659,1964,1768, 481,2188,1463,2330,2857,3600,2092,3031,2421,4632,2318,2070,1849, # 1776
2598,4633,1302,2254,1668,1701,2422,3811,2905,3032,3123,2046,4106,1763,1694,4634, # 1792
1604, 943,1724,1454, 917, 868,2215,1169,2940, 552,1145,1800,1228,1823,1955, 316, # 1808
1080,2510, 361,1807,2830,4107,2660,3381,1346,1423,1134,4108,6127, 541,1263,1229, # 1824
1148,2540, 545, 465,1833,2880,3438,1901,3074,2482, 816,3937, 713,1788,2500, 122, # 1840
1575, 195,1451,2501,1111,6128, 859, 374,1225,2243,2483,4317, 390,1033,3439,3075, # 1856
2524,1687, 266, 793,1440,2599, 946, 779, 802, 507, 897,1081, 528,2189,1292, 711, # 1872
1866,1725,1167,1640, 753, 398,2661,1053, 246, 348,4318, 137,1024,3440,1600,2077, # 1888
2129, 825,4319, 698, 238, 521, 187,2300,1157,2423,1641,1605,1464,1610,1097,2541, # 1904
1260,1436, 759,2255,1814,2150, 705,3235, 409,2563,3304, 561,3033,2005,2564, 726, # 1920
1956,2343,3698,4109, 949,3812,3813,3520,1669, 653,1379,2525, 881,2198, 632,2256, # 1936
1027, 778,1074, 733,1957, 514,1481,2466, 554,2180, 702,3938,1606,1017,1398,6129, # 1952
1380,3521, 921, 993,1313, 594, 449,1489,1617,1166, 768,1426,1360, 495,1794,3601, # 1968
1177,3602,1170,4320,2344, 476, 425,3167,4635,3168,1424, 401,2662,1171,3382,1998, # 1984
1089,4110, 477,3169, 474,6130,1909, 596,2831,1842, 494, 693,1051,1028,1207,3076, # 2000
 606,2115, 727,2790,1473,1115, 743,3522, 630, 805,1532,4321,2021, 366,1057, 838, # 2016
 684,1114,2142,4322,2050,1492,1892,1808,2271,3814,2424,1971,1447,1373,3305,1090, # 2032
1536,3939,3523,3306,1455,2199, 336, 369,2331,1035, 584,2393, 902, 718,2600,6131, # 2048
2753, 463,2151,1149,1611,2467, 715,1308,3124,1268, 343,1413,3236,1517,1347,2663, # 2064
2093,3940,2022,1131,1553,2100,2941,1427,3441,2942,1323,2484,6132,1980, 872,2368, # 2080
2441,2943, 320,2369,2116,1082, 679,1933,3941,2791,3815, 625,1143,2023, 422,2200, # 2096
3816,6133, 730,1695, 356,2257,1626,2301,2858,2637,1627,1778, 937, 883,2906,2693, # 2112
3002,1769,1086, 400,1063,1325,3307,2792,4111,3077, 456,2345,1046, 747,6134,1524, # 2128
 884,1094,3383,1474,2164,1059, 974,1688,2181,2258,1047, 345,1665,1187, 358, 875, # 2144
3170, 305, 660,3524,2190,1334,1135,3171,1540,1649,2542,1527, 927, 968,2793, 885, # 2160
1972,1850, 482, 500,2638,1218,1109,1085,2543,1654,2034, 876,  78,2287,1482,1277, # 2176
 861,1675,1083,1779, 724,2754, 454, 397,1132,1612,2332, 893, 672,1237, 257,2259, # 2192
2370, 135,3384, 337,2244, 547, 352, 340, 709,2485,1400, 788,1138,2511, 540, 772, # 2208
1682,2260,2272,2544,2013,1843,1902,4636,1999,1562,2288,4637,2201,1403,1533, 407, # 2224
 576,3308,1254,2071, 978,3385, 170, 136,1201,3125,2664,3172,2394, 213, 912, 873, # 2240
3603,1713,2202, 699,3604,3699, 813,3442, 493, 531,1054, 468,2907,1483, 304, 281, # 2256
4112,1726,1252,2094, 339,2319,2130,2639, 756,1563,2944, 748, 571,2976,1588,2425, # 2272
2715,1851,1460,2426,1528,1392,1973,3237, 288,3309, 685,3386, 296, 892,2716,2216, # 2288
1570,2245, 722,1747,2217, 905,3238,1103,6135,1893,1441,1965, 251,1805,2371,3700, # 2304
2601,1919,1078,  75,2182,1509,1592,1270,2640,4638,2152,6136,3310,3817, 524, 706, # 2320
1075, 292,3818,1756,2602, 317,  98,3173,3605,3525,1844,2218,3819,2502, 814, 567, # 2336
 385,2908,1534,6137, 534,1642,3239, 797,6138,1670,1529, 953,4323, 188,1071, 538, # 2352
 178, 729,3240,2109,1226,1374,2000,2357,2977, 731,2468,1116,2014,2051,6139,1261, # 2368
1593, 803,2859,2736,3443, 556, 682, 823,1541,6140,1369,2289,1706,2794, 845, 462, # 2384
2603,2665,1361, 387, 162,2358,1740, 739,1770,1720,1304,1401,3241,1049, 627,1571, # 2400
2427,3526,1877,3942,1852,1500, 431,1910,1503, 677, 297,2795, 286,1433,1038,1198, # 2416
2290,1133,1596,4113,4639,2469,1510,1484,3943,6141,2442, 108, 712,4640,2372, 866, # 2432
3701,2755,3242,1348, 834,1945,1408,3527,2395,3243,1811, 824, 994,1179,2110,1548, # 2448
1453, 790,3003, 690,4324,4325,2832,2909,3820,1860,3821, 225,1748, 310, 346,1780, # 2464
2470, 821,1993,2717,2796, 828, 877,3528,2860,2471,1702,2165,2910,2486,1789, 453, # 2480
 359,2291,1676,  73,1164,1461,1127,3311, 421, 604, 314,1037, 589, 116,2487, 737, # 2496
 837,1180, 111, 244, 735,6142,2261,1861,1362, 986, 523, 418, 581,2666,3822, 103, # 2512
 855, 503,1414,1867,2488,1091, 657,1597, 979, 605,1316,4641,1021,2443,2078,2001, # 2528
1209,  96, 587,2166,1032, 260,1072,2153, 173,  94, 226,3244, 819,2006,4642,4114, # 2544
2203, 231,1744, 782,  97,2667, 786,3387, 887, 391, 442,2219,4326,1425,6143,2694, # 2560
 633,1544,1202, 483,2015, 592,2052,1958,2472,1655, 419, 129,4327,3444,3312,1714, # 2576
1257,3078,4328,1518,1098, 865,1310,1019,1885,1512,1734, 469,2444, 148, 773, 436, # 2592
1815,1868,1128,1055,4329,1245,2756,3445,2154,1934,1039,4643, 579,1238, 932,2320, # 2608
 353, 205, 801, 115,2428, 944,2321,1881, 399,2565,1211, 678, 766,3944, 335,2101, # 2624
1459,1781,1402,3945,2737,2131,1010, 844, 981,1326,1013, 550,1816,1545,2620,1335, # 2640
1008, 371,2881, 936,1419,1613,3529,1456,1395,2273,1834,2604,1317,2738,2503, 416, # 2656
1643,4330, 806,1126, 229, 591,3946,1314,1981,1576,1837,1666, 347,1790, 977,3313, # 2672
 764,2861,1853, 688,2429,1920,1462,  77, 595, 415,2002,3034, 798,1192,4115,6144, # 2688
2978,4331,3035,2695,2582,2072,2566, 430,2430,1727, 842,1396,3947,3702, 613, 377, # 2704
 278, 236,1417,3388,3314,3174, 757,1869, 107,3530,6145,1194, 623,2262, 207,1253, # 2720
2167,3446,3948, 492,1117,1935, 536,1838,2757,1246,4332, 696,2095,2406,1393,1572, # 2736
3175,1782, 583, 190, 253,1390,2230, 830,3126,3389, 934,3245,1703,1749,2979,1870, # 2752
2545,1656,2204, 869,2346,4116,3176,1817, 496,1764,4644, 942,1504, 404,1903,1122, # 2768
1580,3606,2945,1022, 515, 372,1735, 955,2431,3036,6146,2797,1110,2302,2798, 617, # 2784
6147, 441, 762,1771,3447,3607,3608,1904, 840,3037,  86, 939,1385, 572,1370,2445, # 2800
1336, 114,3703, 898, 294, 203,3315, 703,1583,2274, 429, 961,4333,1854,1951,3390, # 2816
2373,3704,4334,1318,1381, 966,1911,2322,1006,1155, 309, 989, 458,2718,1795,1372, # 2832
1203, 252,1689,1363,3177, 517,1936, 168,1490, 562, 193,3823,1042,4117,1835, 551, # 2848
 470,4645, 395, 489,3448,1871,1465,2583,2641, 417,1493, 279,1295, 511,1236,1119, # 2864
  72,1231,1982,1812,3004, 871,1564, 984,3449,1667,2696,2096,4646,2347,2833,1673, # 2880
3609, 695,3246,2668, 807,1183,4647, 890, 388,2333,1801,1457,2911,1765,1477,1031, # 2896
3316,3317,1278,3391,2799,2292,2526, 163,3450,4335,2669,1404,1802,6148,2323,2407, # 2912
1584,1728,1494,1824,1269, 298, 909,3318,1034,1632, 375, 776,1683,2061, 291, 210, # 2928
1123, 809,1249,1002,2642,3038, 206,1011,2132, 144, 975, 882,1565, 342, 667, 754, # 2944
1442,2143,1299,2303,2062, 447, 626,2205,1221,2739,2912,1144,1214,2206,2584, 760, # 2960
1715, 614, 950,1281,2670,2621, 810, 577,1287,2546,4648, 242,2168, 250,2643, 691, # 2976
 123,2644, 647, 313,1029, 689,1357,2946,1650, 216, 771,1339,1306, 808,2063, 549, # 2992
 913,1371,2913,2914,6149,1466,1092,1174,1196,1311,2605,2396,1783,1796,3079, 406, # 3008
2671,2117,3949,4649, 487,1825,2220,6150,2915, 448,2348,1073,6151,2397,1707, 130, # 3024
 900,1598, 329, 176,1959,2527,1620,6152,2275,4336,3319,1983,2191,3705,3610,2155, # 3040
3706,1912,1513,1614,6153,1988, 646, 392,2304,1589,3320,3039,1826,1239,1352,1340, # 3056
2916, 505,2567,1709,1437,2408,2547, 906,6154,2672, 384,1458,1594,1100,1329, 710, # 3072
 423,3531,2064,2231,2622,1989,2673,1087,1882, 333, 841,3005,1296,2882,2379, 580, # 3088
1937,1827,1293,2585, 601, 574, 249,1772,4118,2079,1120, 645, 901,1176,1690, 795, # 3104
2207, 478,1434, 516,1190,1530, 761,2080, 930,1264, 355, 435,1552, 644,1791, 987, # 3120
 220,1364,1163,1121,1538, 306,2169,1327,1222, 546,2645, 218, 241, 610,1704,3321, # 3136
1984,1839,1966,2528, 451,6155,2586,3707,2568, 907,3178, 254,2947, 186,1845,4650, # 3152
 745, 432,1757, 428,1633, 888,2246,2221,2489,3611,2118,1258,1265, 956,3127,1784, # 3168
4337,2490, 319, 510, 119, 457,3612, 274,2035,2007,4651,1409,3128, 970,2758, 590, # 3184
2800, 661,2247,4652,2008,3950,1420,1549,3080,3322,3951,1651,1375,2111, 485,2491, # 3200
1429,1156,6156,2548,2183,1495, 831,1840,2529,2446, 501,1657, 307,1894,3247,1341, # 3216
 666, 899,2156,1539,2549,1559, 886, 349,2208,3081,2305,1736,3824,2170,2759,1014, # 3232
1913,1386, 542,1397,2948, 490, 368, 716, 362, 159, 282,2569,1129,1658,1288,1750, # 3248
2674, 276, 649,2016, 751,1496, 658,1818,1284,1862,2209,2087,2512,3451, 622,2834, # 3264
 376, 117,1060,2053,1208,1721,1101,1443, 247,1250,3179,1792,3952,2760,2398,3953, # 3280
6157,2144,3708, 446,2432,1151,2570,3452,2447,2761,2835,1210,2448,3082, 424,2222, # 3296
1251,2449,2119,2836, 504,1581,4338, 602, 817, 857,3825,2349,2306, 357,3826,1470, # 3312
1883,2883, 255, 958, 929,2917,3248, 302,4653,1050,1271,1751,2307,1952,1430,2697, # 3328
2719,2359, 354,3180, 777, 158,2036,4339,1659,4340,4654,2308,2949,2248,1146,2232, # 3344
3532,2720,1696,2623,3827,6158,3129,1550,2698,1485,1297,1428, 637, 931,2721,2145, # 3360
 914,2550,2587,  81,2450, 612, 827,2646,1242,4655,1118,2884, 472,1855,3181,3533, # 3376
3534, 569,1353,2699,1244,1758,2588,4119,2009,2762,2171,3709,1312,1531,6159,1152, # 3392
1938, 134,1830, 471,3710,2276,1112,1535,3323,3453,3535, 982,1337,2950, 488, 826, # 3408
 674,1058,1628,4120,2017, 522,2399, 211, 568,1367,3454, 350, 293,1872,1139,3249, # 3424
1399,1946,3006,1300,2360,3324, 588, 736,6160,2606, 744, 669,3536,3828,6161,1358, # 3440
 199, 723, 848, 933, 851,1939,1505,1514,1338,1618,1831,4656,1634,3613, 443,2740, # 3456
3829, 717,1947, 491,1914,6162,2551,1542,4121,1025,6163,1099,1223, 198,3040,2722, # 3472
 370, 410,1905,2589, 998,1248,3182,2380, 519,1449,4122,1710, 947, 928,1153,4341, # 3488
2277, 344,2624,1511, 615, 105, 161,1212,1076,1960,3130,2054,1926,1175,1906,2473, # 3504
 414,1873,2801,6164,2309, 315,1319,3325, 318,2018,2146,2157, 963, 631, 223,4342, # 3520
4343,2675, 479,3711,1197,2625,3712,2676,2361,6165,4344,4123,6166,2451,3183,1886, # 3536
2184,1674,1330,1711,1635,1506, 799, 219,3250,3083,3954,1677,3713,3326,2081,3614, # 3552
1652,2073,4657,1147,3041,1752, 643,1961, 147,1974,3955,6167,1716,2037, 918,3007, # 3568
1994, 120,1537, 118, 609,3184,4345, 740,3455,1219, 332,1615,3830,6168,1621,2980, # 3584
1582, 783, 212, 553,2350,3714,1349,2433,2082,4124, 889,6169,2310,1275,1410, 973, # 3600
 166,1320,3456,1797,1215,3185,2885,1846,2590,2763,4658, 629, 822,3008, 763, 940, # 3616
1990,2862, 439,2409,1566,1240,1622, 926,1282,1907,2764, 654,2210,1607, 327,1130, # 3632
3956,1678,1623,6170,2434,2192, 686, 608,3831,3715, 903,3957,3042,6171,2741,1522, # 3648
1915,1105,1555,2552,1359, 323,3251,4346,3457, 738,1354,2553,2311,2334,1828,2003, # 3664
3832,1753,2351,1227,6172,1887,4125,1478,6173,2410,1874,1712,1847, 520,1204,2607, # 3680
 264,4659, 836,2677,2102, 600,4660,3833,2278,3084,6174,4347,3615,1342, 640, 532, # 3696
 543,2608,1888,2400,2591,1009,4348,1497, 341,1737,3616,2723,1394, 529,3252,1321, # 3712
 983,4661,1515,2120, 971,2592, 924, 287,1662,3186,4349,2700,4350,1519, 908,1948, # 3728
2452, 156, 796,1629,1486,2223,2055, 694,4126,1259,1036,3392,1213,2249,2742,1889, # 3744
1230,3958,1015, 910, 408, 559,3617,4662, 746, 725, 935,4663,3959,3009,1289, 563, # 3760
 867,4664,3960,1567,2981,2038,2626, 988,2263,2381,4351, 143,2374, 704,1895,6175, # 3776
1188,3716,2088, 673,3085,2362,4352, 484,1608,1921,2765,2918, 215, 904,3618,3537, # 3792
 894, 509, 976,3043,2701,3961,4353,2837,2982, 498,6176,6177,1102,3538,1332,3393, # 3808
1487,1636,1637, 233, 245,3962, 383, 650, 995,3044, 460,1520,1206,2352, 749,3327, # 3824
 530, 700, 389,1438,1560,1773,3963,2264, 719,2951,2724,3834, 870,1832,1644,1000, # 3840
 839,2474,3717, 197,1630,3394, 365,2886,3964,1285,2133, 734, 922, 818,1106, 732, # 3856
 480,2083,1774,3458, 923,2279,1350, 221,3086,  85,2233,2234,3835,1585,3010,2147, # 3872
1387,1705,2382,1619,2475, 133, 239,2802,1991,1016,2084,2383, 411,2838,1113, 651, # 3888
1985,1160,3328, 990,1863,3087,1048,1276,2647, 265,2627,1599,3253,2056, 150, 638, # 3904
2019, 656, 853, 326,1479, 680,1439,4354,1001,1759, 413,3459,3395,2492,1431, 459, # 3920
4355,1125,3329,2265,1953,1450,2065,2863, 849, 351,2678,3131,3254,3255,1104,1577, # 3936
 227,1351,1645,2453,2193,1421,2887, 812,2121, 634,  95,2435, 201,2312,4665,1646, # 3952
1671,2743,1601,2554,2702,2648,2280,1315,1366,2089,3132,1573,3718,3965,1729,1189, # 3968
 328,2679,1077,1940,1136, 558,1283, 964,1195, 621,2074,1199,1743,3460,3619,1896, # 3984
1916,1890,3836,2952,1154,2112,1064, 862, 378,3011,2066,2113,2803,1568,2839,6178, # 4000
3088,2919,1941,1660,2004,1992,2194, 142, 707,1590,1708,1624,1922,1023,1836,1233, # 4016
1004,2313, 789, 741,3620,6179,1609,2411,1200,4127,3719,3720,4666,2057,3721, 593, # 4032
2840, 367,2920,1878,6180,3461,1521, 628,1168, 692,2211,2649, 300, 720,2067,2571, # 4048
2953,3396, 959,2504,3966,3539,3462,1977, 701,6181, 954,1043, 800, 681, 183,3722, # 4064
1803,1730,3540,4128,2103, 815,2314, 174, 467, 230,2454,1093,2134, 755,3541,3397, # 4080
1141,1162,6182,1738,2039, 270,3256,2513,1005,1647,2185,3837, 858,1679,1897,1719, # 4096
2954,2324,1806, 402, 670, 167,4129,1498,2158,2104, 750,6183, 915, 189,1680,1551, # 4112
 455,4356,1501,2455, 405,1095,2955, 338,1586,1266,1819, 570, 641,1324, 237,1556, # 4128
2650,1388,3723,6184,1368,2384,1343,1978,3089,2436, 879,3724, 792,1191, 758,3012, # 4144
1411,2135,1322,4357, 240,4667,1848,3725,1574,6185, 420,3045,1546,1391, 714,4358, # 4160
1967, 941,1864, 863, 664, 426, 560,1731,2680,1785,2864,1949,2363, 403,3330,1415, # 4176
1279,2136,1697,2335, 204, 721,2097,3838,  90,6186,2085,2505, 191,3967, 124,2148, # 4192
1376,1798,1178,1107,1898,1405, 860,4359,1243,1272,2375,2983,1558,2456,1638, 113, # 4208
3621, 578,1923,2609, 880, 386,4130, 784,2186,2266,1422,2956,2172,1722, 497, 263, # 4224
2514,1267,2412,2610, 177,2703,3542, 774,1927,1344, 616,1432,1595,1018, 172,4360, # 4240
2325, 911,4361, 438,1468,3622, 794,3968,2024,2173,1681,1829,2957, 945, 895,3090, # 4256
 575,2212,2476, 475,2401,2681, 785,2744,1745,2293,2555,1975,3133,2865, 394,4668, # 4272
3839, 635,4131, 639, 202,1507,2195,2766,1345,1435,2572,3726,1908,1184,1181,2457, # 4288
3727,3134,4362, 843,2611, 437, 916,4669, 234, 769,1884,3046,3047,3623, 833,6187, # 4304
1639,2250,2402,1355,1185,2010,2047, 999, 525,1732,1290,1488,2612, 948,1578,3728, # 4320
2413,2477,1216,2725,2159, 334,3840,1328,3624,2921,1525,4132, 564,1056, 891,4363, # 4336
1444,1698,2385,2251,3729,1365,2281,2235,1717,6188, 864,3841,2515, 444, 527,2767, # 4352
2922,3625, 544, 461,6189, 566, 209,2437,3398,2098,1065,2068,3331,3626,3257,2137, # 4368  #last 512
)
# fmt: on


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/cli/chardetect.py
# ========================================================
"""
Script which takes one or more file paths and reports on their detected
encodings

Example::

    % chardetect somefile someotherfile
    somefile: windows-1252 with confidence 0.5
    someotherfile: ascii with confidence 1.0

If no paths are provided, it takes its input from stdin.

"""


import argparse
import sys
from typing import Iterable, List, Optional

from .. import __version__
from ..universaldetector import UniversalDetector


def description_of(
    lines: Iterable[bytes],
    name: str = "stdin",
    minimal: bool = False,
    should_rename_legacy: bool = False,
) -> Optional[str]:
    """
    Return a string describing the probable encoding of a file or
    list of strings.

    :param lines: The lines to get the encoding of.
    :type lines: Iterable of bytes
    :param name: Name of file or collection of lines
    :type name: str
    :param should_rename_legacy:  Should we rename legacy encodings to
                                  their more modern equivalents?
    :type should_rename_legacy:   ``bool``
    """
    u = UniversalDetector(should_rename_legacy=should_rename_legacy)
    for line in lines:
        line = bytearray(line)
        u.feed(line)
        # shortcut out of the loop to save reading further - particularly useful if we read a BOM.
        if u.done:
            break
    u.close()
    result = u.result
    if minimal:
        return result["encoding"]
    if result["encoding"]:
        return f'{name}: {result["encoding"]} with confidence {result["confidence"]}'
    return f"{name}: no result"


def main(argv: Optional[List[str]] = None) -> None:
    """
    Handles command line arguments and gets things started.

    :param argv: List of arguments, as if specified on the command-line.
                 If None, ``sys.argv[1:]`` is used instead.
    :type argv: list of str
    """
    # Get command line arguments
    parser = argparse.ArgumentParser(
        description=(
            "Takes one or more file paths and reports their detected encodings"
        )
    )
    parser.add_argument(
        "input",
        help="File whose encoding we would like to determine. (default: stdin)",
        type=argparse.FileType("rb"),
        nargs="*",
        default=[sys.stdin.buffer],
    )
    parser.add_argument(
        "--minimal",
        help="Print only the encoding to standard output",
        action="store_true",
    )
    parser.add_argument(
        "-l",
        "--legacy",
        help="Rename legacy encodings to more modern ones.",
        action="store_true",
    )
    parser.add_argument(
        "--version", action="version", version=f"%(prog)s {__version__}"
    )
    args = parser.parse_args(argv)

    for f in args.input:
        if f.isatty():
            print(
                "You are running chardetect interactively. Press "
                "CTRL-D twice at the start of a blank line to signal the "
                "end of your input. If you want help, run chardetect "
                "--help\n",
                file=sys.stderr,
            )
        print(
            description_of(
                f, f.name, minimal=args.minimal, should_rename_legacy=args.legacy
            )
        )


if __name__ == "__main__":
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/cli/__init__.py
# ========================================================


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/chardet/enums.py
# ========================================================
"""
All of the Enums that are used throughout the chardet package.

:author: Dan Blanchard (dan.blanchard@gmail.com)
"""

from enum import Enum, Flag


class InputState:
    """
    This enum represents the different states a universal detector can be in.
    """

    PURE_ASCII = 0
    ESC_ASCII = 1
    HIGH_BYTE = 2


class LanguageFilter(Flag):
    """
    This enum represents the different language filters we can apply to a
    ``UniversalDetector``.
    """

    NONE = 0x00
    CHINESE_SIMPLIFIED = 0x01
    CHINESE_TRADITIONAL = 0x02
    JAPANESE = 0x04
    KOREAN = 0x08
    NON_CJK = 0x10
    ALL = 0x1F
    CHINESE = CHINESE_SIMPLIFIED | CHINESE_TRADITIONAL
    CJK = CHINESE | JAPANESE | KOREAN


class ProbingState(Enum):
    """
    This enum represents the different states a prober can be in.
    """

    DETECTING = 0
    FOUND_IT = 1
    NOT_ME = 2


class MachineState:
    """
    This enum represents the different states a state machine can be in.
    """

    START = 0
    ERROR = 1
    ITS_ME = 2


class SequenceLikelihood:
    """
    This enum represents the likelihood of a character following the previous one.
    """

    NEGATIVE = 0
    UNLIKELY = 1
    LIKELY = 2
    POSITIVE = 3

    @classmethod
    def get_num_categories(cls) -> int:
        """:returns: The number of likelihood categories in the enum."""
        return 4


class CharacterCategory:
    """
    This enum represents the different categories language models for
    ``SingleByteCharsetProber`` put characters into.

    Anything less than CONTROL is considered a letter.
    """

    UNDEFINED = 255
    LINE_BREAK = 254
    SYMBOL = 253
    DIGIT = 252
    CONTROL = 251


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/utils.py
# ========================================================
"""
requests.utils
~~~~~~~~~~~~~~

This module provides utility functions that are used within Requests
that are also useful for external consumption.
"""

import codecs
import contextlib
import io
import os
import re
import socket
import struct
import sys
import tempfile
import warnings
import zipfile
from collections import OrderedDict

from pip._vendor.urllib3.util import make_headers, parse_url

from . import certs
from .__version__ import __version__

# to_native_string is unused here, but imported here for backwards compatibility
from ._internal_utils import (  # noqa: F401
    _HEADER_VALIDATORS_BYTE,
    _HEADER_VALIDATORS_STR,
    HEADER_VALIDATORS,
    to_native_string,
)
from .compat import (
    Mapping,
    basestring,
    bytes,
    getproxies,
    getproxies_environment,
    integer_types,
)
from .compat import parse_http_list as _parse_list_header
from .compat import (
    proxy_bypass,
    proxy_bypass_environment,
    quote,
    str,
    unquote,
    urlparse,
    urlunparse,
)
from .cookies import cookiejar_from_dict
from .exceptions import (
    FileModeWarning,
    InvalidHeader,
    InvalidURL,
    UnrewindableBodyError,
)
from .structures import CaseInsensitiveDict

NETRC_FILES = (".netrc", "_netrc")

DEFAULT_CA_BUNDLE_PATH = certs.where()

DEFAULT_PORTS = {"http": 80, "https": 443}

# Ensure that ', ' is used to preserve previous delimiter behavior.
DEFAULT_ACCEPT_ENCODING = ", ".join(
    re.split(r",\s*", make_headers(accept_encoding=True)["accept-encoding"])
)


if sys.platform == "win32":
    # provide a proxy_bypass version on Windows without DNS lookups

    def proxy_bypass_registry(host):
        try:
            import winreg
        except ImportError:
            return False

        try:
            internetSettings = winreg.OpenKey(
                winreg.HKEY_CURRENT_USER,
                r"Software\Microsoft\Windows\CurrentVersion\Internet Settings",
            )
            # ProxyEnable could be REG_SZ or REG_DWORD, normalizing it
            proxyEnable = int(winreg.QueryValueEx(internetSettings, "ProxyEnable")[0])
            # ProxyOverride is almost always a string
            proxyOverride = winreg.QueryValueEx(internetSettings, "ProxyOverride")[0]
        except (OSError, ValueError):
            return False
        if not proxyEnable or not proxyOverride:
            return False

        # make a check value list from the registry entry: replace the
        # '<local>' string by the localhost entry and the corresponding
        # canonical entry.
        proxyOverride = proxyOverride.split(";")
        # now check if we match one of the registry values.
        for test in proxyOverride:
            if test == "<local>":
                if "." not in host:
                    return True
            test = test.replace(".", r"\.")  # mask dots
            test = test.replace("*", r".*")  # change glob sequence
            test = test.replace("?", r".")  # change glob char
            if re.match(test, host, re.I):
                return True
        return False

    def proxy_bypass(host):  # noqa
        """Return True, if the host should be bypassed.

        Checks proxy settings gathered from the environment, if specified,
        or the registry.
        """
        if getproxies_environment():
            return proxy_bypass_environment(host)
        else:
            return proxy_bypass_registry(host)


def dict_to_sequence(d):
    """Returns an internal sequence dictionary update."""

    if hasattr(d, "items"):
        d = d.items()

    return d


def super_len(o):
    total_length = None
    current_position = 0

    if hasattr(o, "__len__"):
        total_length = len(o)

    elif hasattr(o, "len"):
        total_length = o.len

    elif hasattr(o, "fileno"):
        try:
            fileno = o.fileno()
        except (io.UnsupportedOperation, AttributeError):
            # AttributeError is a surprising exception, seeing as how we've just checked
            # that `hasattr(o, 'fileno')`.  It happens for objects obtained via
            # `Tarfile.extractfile()`, per issue 5229.
            pass
        else:
            total_length = os.fstat(fileno).st_size

            # Having used fstat to determine the file length, we need to
            # confirm that this file was opened up in binary mode.
            if "b" not in o.mode:
                warnings.warn(
                    (
                        "Requests has determined the content-length for this "
                        "request using the binary size of the file: however, the "
                        "file has been opened in text mode (i.e. without the 'b' "
                        "flag in the mode). This may lead to an incorrect "
                        "content-length. In Requests 3.0, support will be removed "
                        "for files in text mode."
                    ),
                    FileModeWarning,
                )

    if hasattr(o, "tell"):
        try:
            current_position = o.tell()
        except OSError:
            # This can happen in some weird situations, such as when the file
            # is actually a special file descriptor like stdin. In this
            # instance, we don't know what the length is, so set it to zero and
            # let requests chunk it instead.
            if total_length is not None:
                current_position = total_length
        else:
            if hasattr(o, "seek") and total_length is None:
                # StringIO and BytesIO have seek but no usable fileno
                try:
                    # seek to end of file
                    o.seek(0, 2)
                    total_length = o.tell()

                    # seek back to current position to support
                    # partially read file-like objects
                    o.seek(current_position or 0)
                except OSError:
                    total_length = 0

    if total_length is None:
        total_length = 0

    return max(0, total_length - current_position)


def get_netrc_auth(url, raise_errors=False):
    """Returns the Requests tuple auth for a given url from netrc."""

    netrc_file = os.environ.get("NETRC")
    if netrc_file is not None:
        netrc_locations = (netrc_file,)
    else:
        netrc_locations = (f"~/{f}" for f in NETRC_FILES)

    try:
        from netrc import NetrcParseError, netrc

        netrc_path = None

        for f in netrc_locations:
            try:
                loc = os.path.expanduser(f)
            except KeyError:
                # os.path.expanduser can fail when $HOME is undefined and
                # getpwuid fails. See https://bugs.python.org/issue20164 &
                # https://github.com/psf/requests/issues/1846
                return

            if os.path.exists(loc):
                netrc_path = loc
                break

        # Abort early if there isn't one.
        if netrc_path is None:
            return

        ri = urlparse(url)
        host = ri.hostname

        try:
            _netrc = netrc(netrc_path).authenticators(host)
            if _netrc:
                # Return with login / password
                login_i = 0 if _netrc[0] else 1
                return (_netrc[login_i], _netrc[2])
        except (NetrcParseError, OSError):
            # If there was a parsing error or a permissions issue reading the file,
            # we'll just skip netrc auth unless explicitly asked to raise errors.
            if raise_errors:
                raise

    # App Engine hackiness.
    except (ImportError, AttributeError):
        pass


def guess_filename(obj):
    """Tries to guess the filename of the given object."""
    name = getattr(obj, "name", None)
    if name and isinstance(name, basestring) and name[0] != "<" and name[-1] != ">":
        return os.path.basename(name)


def extract_zipped_paths(path):
    """Replace nonexistent paths that look like they refer to a member of a zip
    archive with the location of an extracted copy of the target, or else
    just return the provided path unchanged.
    """
    if os.path.exists(path):
        # this is already a valid path, no need to do anything further
        return path

    # find the first valid part of the provided path and treat that as a zip archive
    # assume the rest of the path is the name of a member in the archive
    archive, member = os.path.split(path)
    while archive and not os.path.exists(archive):
        archive, prefix = os.path.split(archive)
        if not prefix:
            # If we don't check for an empty prefix after the split (in other words, archive remains unchanged after the split),
            # we _can_ end up in an infinite loop on a rare corner case affecting a small number of users
            break
        member = "/".join([prefix, member])

    if not zipfile.is_zipfile(archive):
        return path

    zip_file = zipfile.ZipFile(archive)
    if member not in zip_file.namelist():
        return path

    # we have a valid zip archive and a valid member of that archive
    tmp = tempfile.gettempdir()
    extracted_path = os.path.join(tmp, member.split("/")[-1])
    if not os.path.exists(extracted_path):
        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition
        with atomic_open(extracted_path) as file_handler:
            file_handler.write(zip_file.read(member))
    return extracted_path


@contextlib.contextmanager
def atomic_open(filename):
    """Write a file to the disk in an atomic fashion"""
    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))
    try:
        with os.fdopen(tmp_descriptor, "wb") as tmp_handler:
            yield tmp_handler
        os.replace(tmp_name, filename)
    except BaseException:
        os.remove(tmp_name)
        raise


def from_key_val_list(value):
    """Take an object and test to see if it can be represented as a
    dictionary. Unless it can not be represented as such, return an
    OrderedDict, e.g.,

    ::

        >>> from_key_val_list([('key', 'val')])
        OrderedDict([('key', 'val')])
        >>> from_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples
        >>> from_key_val_list({'key': 'val'})
        OrderedDict([('key', 'val')])

    :rtype: OrderedDict
    """
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError("cannot encode objects that are not 2-tuples")

    return OrderedDict(value)


def to_key_val_list(value):
    """Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> to_key_val_list([('key', 'val')])
        [('key', 'val')]
        >>> to_key_val_list({'key': 'val'})
        [('key', 'val')]
        >>> to_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are not 2-tuples

    :rtype: list
    """
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int)):
        raise ValueError("cannot encode objects that are not 2-tuples")

    if isinstance(value, Mapping):
        value = value.items()

    return list(value)


# From mitsuhiko/werkzeug (used with permission).
def parse_list_header(value):
    """Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> parse_list_header('token, "quoted value"')
    ['token', 'quoted value']

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    """
    result = []
    for item in _parse_list_header(value):
        if item[:1] == item[-1:] == '"':
            item = unquote_header_value(item[1:-1])
        result.append(item)
    return result


# From mitsuhiko/werkzeug (used with permission).
def parse_dict_header(value):
    """Parse lists of key, value pairs as described by RFC 2068 Section 2 and
    convert them into a python dict:

    >>> d = parse_dict_header('foo="is a fish", bar="as well"')
    >>> type(d) is dict
    True
    >>> sorted(d.items())
    [('bar', 'as well'), ('foo', 'is a fish')]

    If there is no value for a key it will be `None`:

    >>> parse_dict_header('key_without_value')
    {'key_without_value': None}

    To create a header from the :class:`dict` again, use the
    :func:`dump_header` function.

    :param value: a string with a dict header.
    :return: :class:`dict`
    :rtype: dict
    """
    result = {}
    for item in _parse_list_header(value):
        if "=" not in item:
            result[item] = None
            continue
        name, value = item.split("=", 1)
        if value[:1] == value[-1:] == '"':
            value = unquote_header_value(value[1:-1])
        result[name] = value
    return result


# From mitsuhiko/werkzeug (used with permission).
def unquote_header_value(value, is_filename=False):
    r"""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    """
    if value and value[0] == value[-1] == '"':
        # this is not the real unquoting, but fixing this so that the
        # RFC is met will result in bugs with internet explorer and
        # probably some other browsers as well.  IE for example is
        # uploading files with "C:\foo\bar.txt" as filename
        value = value[1:-1]

        # if this is a filename and the starting characters look like
        # a UNC path, then just return the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash and then
        # _fix_ie_filename() doesn't work correctly.  See #458.
        if not is_filename or value[:2] != "\\\\":
            return value.replace("\\\\", "\\").replace('\\"', '"')
    return value


def dict_from_cookiejar(cj):
    """Returns a key/value dictionary from a CookieJar.

    :param cj: CookieJar object to extract cookies from.
    :rtype: dict
    """

    cookie_dict = {}

    for cookie in cj:
        cookie_dict[cookie.name] = cookie.value

    return cookie_dict


def add_dict_to_cookiejar(cj, cookie_dict):
    """Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    """

    return cookiejar_from_dict(cookie_dict, cj)


def get_encodings_from_content(content):
    """Returns encodings from given content string.

    :param content: bytestring to extract encodings from.
    """
    warnings.warn(
        (
            "In requests 3.0, get_encodings_from_content will be removed. For "
            "more information, please see the discussion on issue #2266. (This"
            " warning should only appear once.)"
        ),
        DeprecationWarning,
    )

    charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
    pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
    xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')

    return (
        charset_re.findall(content)
        + pragma_re.findall(content)
        + xml_re.findall(content)
    )


def _parse_content_type_header(header):
    """Returns content type and parameters from given header

    :param header: string
    :return: tuple containing content type and dictionary of
         parameters
    """

    tokens = header.split(";")
    content_type, params = tokens[0].strip(), tokens[1:]
    params_dict = {}
    items_to_strip = "\"' "

    for param in params:
        param = param.strip()
        if param:
            key, value = param, True
            index_of_equals = param.find("=")
            if index_of_equals != -1:
                key = param[:index_of_equals].strip(items_to_strip)
                value = param[index_of_equals + 1 :].strip(items_to_strip)
            params_dict[key.lower()] = value
    return content_type, params_dict


def get_encoding_from_headers(headers):
    """Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    """

    content_type = headers.get("content-type")

    if not content_type:
        return None

    content_type, params = _parse_content_type_header(content_type)

    if "charset" in params:
        return params["charset"].strip("'\"")

    if "text" in content_type:
        return "ISO-8859-1"

    if "application/json" in content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        return "utf-8"


def stream_decode_response_unicode(iterator, r):
    """Stream decodes an iterator."""

    if r.encoding is None:
        yield from iterator
        return

    decoder = codecs.getincrementaldecoder(r.encoding)(errors="replace")
    for chunk in iterator:
        rv = decoder.decode(chunk)
        if rv:
            yield rv
    rv = decoder.decode(b"", final=True)
    if rv:
        yield rv


def iter_slices(string, slice_length):
    """Iterate over slices of a string."""
    pos = 0
    if slice_length is None or slice_length <= 0:
        slice_length = len(string)
    while pos < len(string):
        yield string[pos : pos + slice_length]
        pos += slice_length


def get_unicode_from_response(r):
    """Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    """
    warnings.warn(
        (
            "In requests 3.0, get_unicode_from_response will be removed. For "
            "more information, please see the discussion on issue #2266. (This"
            " warning should only appear once.)"
        ),
        DeprecationWarning,
    )

    tried_encodings = []

    # Try charset from content-type
    encoding = get_encoding_from_headers(r.headers)

    if encoding:
        try:
            return str(r.content, encoding)
        except UnicodeError:
            tried_encodings.append(encoding)

    # Fall back:
    try:
        return str(r.content, encoding, errors="replace")
    except TypeError:
        return r.content


# The unreserved URI characters (RFC 3986)
UNRESERVED_SET = frozenset(
    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz" + "0123456789-._~"
)


def unquote_unreserved(uri):
    """Un-escape any percent-escape sequences in a URI that are unreserved
    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.

    :rtype: str
    """
    parts = uri.split("%")
    for i in range(1, len(parts)):
        h = parts[i][0:2]
        if len(h) == 2 and h.isalnum():
            try:
                c = chr(int(h, 16))
            except ValueError:
                raise InvalidURL(f"Invalid percent-escape sequence: '{h}'")

            if c in UNRESERVED_SET:
                parts[i] = c + parts[i][2:]
            else:
                parts[i] = f"%{parts[i]}"
        else:
            parts[i] = f"%{parts[i]}"
    return "".join(parts)


def requote_uri(uri):
    """Re-quote the given URI.

    This function passes the given URI through an unquote/quote cycle to
    ensure that it is fully and consistently quoted.

    :rtype: str
    """
    safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
    safe_without_percent = "!#$&'()*+,/:;=?@[]~"
    try:
        # Unquote only the unreserved characters
        # Then quote only illegal characters (do not quote reserved,
        # unreserved, or '%')
        return quote(unquote_unreserved(uri), safe=safe_with_percent)
    except InvalidURL:
        # We couldn't unquote the given URI, so let's try quoting it, but
        # there may be unquoted '%'s in the URI. We need to make sure they're
        # properly quoted so they do not cause issues elsewhere.
        return quote(uri, safe=safe_without_percent)


def address_in_network(ip, net):
    """This function allows you to check if an IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    """
    ipaddr = struct.unpack("=L", socket.inet_aton(ip))[0]
    netaddr, bits = net.split("/")
    netmask = struct.unpack("=L", socket.inet_aton(dotted_netmask(int(bits))))[0]
    network = struct.unpack("=L", socket.inet_aton(netaddr))[0] & netmask
    return (ipaddr & netmask) == (network & netmask)


def dotted_netmask(mask):
    """Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    """
    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1
    return socket.inet_ntoa(struct.pack(">I", bits))


def is_ipv4_address(string_ip):
    """
    :rtype: bool
    """
    try:
        socket.inet_aton(string_ip)
    except OSError:
        return False
    return True


def is_valid_cidr(string_network):
    """
    Very simple check of the cidr format in no_proxy variable.

    :rtype: bool
    """
    if string_network.count("/") == 1:
        try:
            mask = int(string_network.split("/")[1])
        except ValueError:
            return False

        if mask < 1 or mask > 32:
            return False

        try:
            socket.inet_aton(string_network.split("/")[0])
        except OSError:
            return False
    else:
        return False
    return True


@contextlib.contextmanager
def set_environ(env_name, value):
    """Set the environment variable 'env_name' to 'value'

    Save previous value, yield, and then restore the previous value stored in
    the environment variable 'env_name'.

    If 'value' is None, do nothing"""
    value_changed = value is not None
    if value_changed:
        old_value = os.environ.get(env_name)
        os.environ[env_name] = value
    try:
        yield
    finally:
        if value_changed:
            if old_value is None:
                del os.environ[env_name]
            else:
                os.environ[env_name] = old_value


def should_bypass_proxies(url, no_proxy):
    """
    Returns whether we should bypass proxies or not.

    :rtype: bool
    """
    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour with other http projects (curl, wget).
    def get_proxy(key):
        return os.environ.get(key) or os.environ.get(key.upper())

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy_arg = no_proxy
    if no_proxy is None:
        no_proxy = get_proxy("no_proxy")
    parsed = urlparse(url)

    if parsed.hostname is None:
        # URLs don't always have hostnames, e.g. file:/// urls.
        return True

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the hostname, both with and without the port.
        no_proxy = (host for host in no_proxy.replace(" ", "").split(",") if host)

        if is_ipv4_address(parsed.hostname):
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip):
                    if address_in_network(parsed.hostname, proxy_ip):
                        return True
                elif parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            host_with_port = parsed.hostname
            if parsed.port:
                host_with_port += f":{parsed.port}"

            for host in no_proxy:
                if parsed.hostname.endswith(host) or host_with_port.endswith(host):
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    with set_environ("no_proxy", no_proxy_arg):
        # parsed.hostname can be `None` in cases such as a file URI.
        try:
            bypass = proxy_bypass(parsed.hostname)
        except (TypeError, socket.gaierror):
            bypass = False

    if bypass:
        return True

    return False


def get_environ_proxies(url, no_proxy=None):
    """
    Return a dict of environment proxies.

    :rtype: dict
    """
    if should_bypass_proxies(url, no_proxy=no_proxy):
        return {}
    else:
        return getproxies()


def select_proxy(url, proxies):
    """Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    """
    proxies = proxies or {}
    urlparts = urlparse(url)
    if urlparts.hostname is None:
        return proxies.get(urlparts.scheme, proxies.get("all"))

    proxy_keys = [
        urlparts.scheme + "://" + urlparts.hostname,
        urlparts.scheme,
        "all://" + urlparts.hostname,
        "all",
    ]
    proxy = None
    for proxy_key in proxy_keys:
        if proxy_key in proxies:
            proxy = proxies[proxy_key]
            break

    return proxy


def resolve_proxies(request, proxies, trust_env=True):
    """This method takes proxy information from a request and configuration
    input to resolve a mapping of target proxies. This will consider settings
    such a NO_PROXY to strip proxy configurations.

    :param request: Request or PreparedRequest
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    """
    proxies = proxies if proxies is not None else {}
    url = request.url
    scheme = urlparse(url).scheme
    no_proxy = proxies.get("no_proxy")
    new_proxies = proxies.copy()

    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):
        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)

        proxy = environ_proxies.get(scheme, environ_proxies.get("all"))

        if proxy:
            new_proxies.setdefault(scheme, proxy)
    return new_proxies


def default_user_agent(name="python-requests"):
    """
    Return a string representing the default user agent.

    :rtype: str
    """
    return f"{name}/{__version__}"


def default_headers():
    """
    :rtype: requests.structures.CaseInsensitiveDict
    """
    return CaseInsensitiveDict(
        {
            "User-Agent": default_user_agent(),
            "Accept-Encoding": DEFAULT_ACCEPT_ENCODING,
            "Accept": "*/*",
            "Connection": "keep-alive",
        }
    )


def parse_header_links(value):
    """Return a list of parsed link headers proxies.

    i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"

    :rtype: list
    """

    links = []

    replace_chars = " '\""

    value = value.strip(replace_chars)
    if not value:
        return links

    for val in re.split(", *<", value):
        try:
            url, params = val.split(";", 1)
        except ValueError:
            url, params = val, ""

        link = {"url": url.strip("<> '\"")}

        for param in params.split(";"):
            try:
                key, value = param.split("=")
            except ValueError:
                break

            link[key.strip(replace_chars)] = value.strip(replace_chars)

        links.append(link)

    return links


# Null bytes; no need to recreate these on each call to guess_json_utf
_null = "\x00".encode("ascii")  # encoding to ASCII for Python 3
_null2 = _null * 2
_null3 = _null * 3


def guess_json_utf(data):
    """
    :rtype: str
    """
    # JSON always starts with two ASCII characters, so detection is as
    # easy as counting the nulls and from their location and count
    # determine the encoding. Also detect a BOM, if present.
    sample = data[:4]
    if sample in (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):
        return "utf-32"  # BOM included
    if sample[:3] == codecs.BOM_UTF8:
        return "utf-8-sig"  # BOM included, MS style (discouraged)
    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
        return "utf-16"  # BOM included
    nullcount = sample.count(_null)
    if nullcount == 0:
        return "utf-8"
    if nullcount == 2:
        if sample[::2] == _null2:  # 1st and 3rd are null
            return "utf-16-be"
        if sample[1::2] == _null2:  # 2nd and 4th are null
            return "utf-16-le"
        # Did not detect 2 valid UTF-16 ascii-range characters
    if nullcount == 3:
        if sample[:3] == _null3:
            return "utf-32-be"
        if sample[1:] == _null3:
            return "utf-32-le"
        # Did not detect a valid UTF-32 ascii-range character
    return None


def prepend_scheme_if_needed(url, new_scheme):
    """Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    """
    parsed = parse_url(url)
    scheme, auth, host, port, path, query, fragment = parsed

    # A defect in urlparse determines that there isn't a netloc present in some
    # urls. We previously assumed parsing was overly cautious, and swapped the
    # netloc and path. Due to a lack of tests on the original defect, this is
    # maintained with parse_url for backwards compatibility.
    netloc = parsed.netloc
    if not netloc:
        netloc, path = path, netloc

    if auth:
        # parse_url doesn't provide the netloc with auth
        # so we'll add it ourselves.
        netloc = "@".join([auth, netloc])
    if scheme is None:
        scheme = new_scheme
    if path is None:
        path = ""

    return urlunparse((scheme, netloc, path, "", query, fragment))


def get_auth_from_url(url):
    """Given a url with authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str)
    """
    parsed = urlparse(url)

    try:
        auth = (unquote(parsed.username), unquote(parsed.password))
    except (AttributeError, TypeError):
        auth = ("", "")

    return auth


def check_header_validity(header):
    """Verifies that header parts don't contain leading whitespace
    reserved characters, or return characters.

    :param header: tuple, in the format (name, value).
    """
    name, value = header
    _validate_header_part(header, name, 0)
    _validate_header_part(header, value, 1)


def _validate_header_part(header, header_part, header_validator_index):
    if isinstance(header_part, str):
        validator = _HEADER_VALIDATORS_STR[header_validator_index]
    elif isinstance(header_part, bytes):
        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]
    else:
        raise InvalidHeader(
            f"Header part ({header_part!r}) from {header} "
            f"must be of type str or bytes, not {type(header_part)}"
        )

    if not validator.match(header_part):
        header_kind = "name" if header_validator_index == 0 else "value"
        raise InvalidHeader(
            f"Invalid leading whitespace, reserved character(s), or return"
            f"character(s) in header {header_kind}: {header_part!r}"
        )


def urldefragauth(url):
    """
    Given a url remove the fragment and the authentication part.

    :rtype: str
    """
    scheme, netloc, path, params, query, fragment = urlparse(url)

    # see func:`prepend_scheme_if_needed`
    if not netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit("@", 1)[-1]

    return urlunparse((scheme, netloc, path, params, query, ""))


def rewind_body(prepared_request):
    """Move file pointer back to its recorded starting position
    so it can be read again on redirect.
    """
    body_seek = getattr(prepared_request.body, "seek", None)
    if body_seek is not None and isinstance(
        prepared_request._body_position, integer_types
    ):
        try:
            body_seek(prepared_request._body_position)
        except OSError:
            raise UnrewindableBodyError(
                "An error occurred when rewinding request body for redirect."
            )
    else:
        raise UnrewindableBodyError("Unable to rewind request body for redirect.")


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/exceptions.py
# ========================================================
"""
requests.exceptions
~~~~~~~~~~~~~~~~~~~

This module contains the set of Requests' exceptions.
"""
from pip._vendor.urllib3.exceptions import HTTPError as BaseHTTPError

from .compat import JSONDecodeError as CompatJSONDecodeError


class RequestException(IOError):
    """There was an ambiguous exception that occurred while handling your
    request.
    """

    def __init__(self, *args, **kwargs):
        """Initialize RequestException with `request` and `response` objects."""
        response = kwargs.pop("response", None)
        self.response = response
        self.request = kwargs.pop("request", None)
        if response is not None and not self.request and hasattr(response, "request"):
            self.request = self.response.request
        super().__init__(*args, **kwargs)


class InvalidJSONError(RequestException):
    """A JSON error occurred."""


class JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):
    """Couldn't decode the text into json"""

    def __init__(self, *args, **kwargs):
        """
        Construct the JSONDecodeError instance first with all
        args. Then use it's args to construct the IOError so that
        the json specific args aren't used as IOError specific args
        and the error message from JSONDecodeError is preserved.
        """
        CompatJSONDecodeError.__init__(self, *args)
        InvalidJSONError.__init__(self, *self.args, **kwargs)


class HTTPError(RequestException):
    """An HTTP error occurred."""


class ConnectionError(RequestException):
    """A Connection error occurred."""


class ProxyError(ConnectionError):
    """A proxy error occurred."""


class SSLError(ConnectionError):
    """An SSL error occurred."""


class Timeout(RequestException):
    """The request timed out.

    Catching this error will catch both
    :exc:`~requests.exceptions.ConnectTimeout` and
    :exc:`~requests.exceptions.ReadTimeout` errors.
    """


class ConnectTimeout(ConnectionError, Timeout):
    """The request timed out while trying to connect to the remote server.

    Requests that produced this error are safe to retry.
    """


class ReadTimeout(Timeout):
    """The server did not send any data in the allotted amount of time."""


class URLRequired(RequestException):
    """A valid URL is required to make a request."""


class TooManyRedirects(RequestException):
    """Too many redirects."""


class MissingSchema(RequestException, ValueError):
    """The URL scheme (e.g. http or https) is missing."""


class InvalidSchema(RequestException, ValueError):
    """The URL scheme provided is either invalid or unsupported."""


class InvalidURL(RequestException, ValueError):
    """The URL provided was somehow invalid."""


class InvalidHeader(RequestException, ValueError):
    """The header value provided was somehow invalid."""


class InvalidProxyURL(InvalidURL):
    """The proxy URL provided is invalid."""


class ChunkedEncodingError(RequestException):
    """The server declared chunked encoding but sent an invalid chunk."""


class ContentDecodingError(RequestException, BaseHTTPError):
    """Failed to decode response content."""


class StreamConsumedError(RequestException, TypeError):
    """The content for this response was already consumed."""


class RetryError(RequestException):
    """Custom retries logic failed"""


class UnrewindableBodyError(RequestException):
    """Requests encountered an error when trying to rewind a body."""


# Warnings


class RequestsWarning(Warning):
    """Base warning for Requests."""


class FileModeWarning(RequestsWarning, DeprecationWarning):
    """A file was opened in text mode, but Requests determined its binary length."""


class RequestsDependencyWarning(RequestsWarning):
    """An imported dependency doesn't match the expected version range."""


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/__init__.py
# ========================================================
#   __
#  /__)  _  _     _   _ _/   _
# / (   (- (/ (/ (- _)  /  _)
#          /

"""
Requests HTTP Library
~~~~~~~~~~~~~~~~~~~~~

Requests is an HTTP library, written in Python, for human beings.
Basic GET usage:

   >>> import requests
   >>> r = requests.get('https://www.python.org')
   >>> r.status_code
   200
   >>> b'Python is a programming language' in r.content
   True

... or POST:

   >>> payload = dict(key1='value1', key2='value2')
   >>> r = requests.post('https://httpbin.org/post', data=payload)
   >>> print(r.text)
   {
     ...
     "form": {
       "key1": "value1",
       "key2": "value2"
     },
     ...
   }

The other HTTP methods are supported - see `requests.api`. Full documentation
is at <https://requests.readthedocs.io>.

:copyright: (c) 2017 by Kenneth Reitz.
:license: Apache 2.0, see LICENSE for more details.
"""

import warnings

from pip._vendor import urllib3

from .exceptions import RequestsDependencyWarning

charset_normalizer_version = None

try:
    from pip._vendor.chardet import __version__ as chardet_version
except ImportError:
    chardet_version = None


def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):
    urllib3_version = urllib3_version.split(".")
    assert urllib3_version != ["dev"]  # Verify urllib3 isn't installed from git.

    # Sometimes, urllib3 only reports its version as 16.1.
    if len(urllib3_version) == 2:
        urllib3_version.append("0")

    # Check urllib3 for compatibility.
    major, minor, patch = urllib3_version  # noqa: F811
    major, minor, patch = int(major), int(minor), int(patch)
    # urllib3 >= 1.21.1
    assert major >= 1
    if major == 1:
        assert minor >= 21

    # Check charset_normalizer for compatibility.
    if chardet_version:
        major, minor, patch = chardet_version.split(".")[:3]
        major, minor, patch = int(major), int(minor), int(patch)
        # chardet_version >= 3.0.2, < 6.0.0
        assert (3, 0, 2) <= (major, minor, patch) < (6, 0, 0)
    elif charset_normalizer_version:
        major, minor, patch = charset_normalizer_version.split(".")[:3]
        major, minor, patch = int(major), int(minor), int(patch)
        # charset_normalizer >= 2.0.0 < 4.0.0
        assert (2, 0, 0) <= (major, minor, patch) < (4, 0, 0)
    else:
        raise Exception("You need either charset_normalizer or chardet installed")


def _check_cryptography(cryptography_version):
    # cryptography < 1.3.4
    try:
        cryptography_version = list(map(int, cryptography_version.split(".")))
    except ValueError:
        return

    if cryptography_version < [1, 3, 4]:
        warning = "Old version of cryptography ({}) may cause slowdown.".format(
            cryptography_version
        )
        warnings.warn(warning, RequestsDependencyWarning)


# Check imported dependencies for compatibility.
try:
    check_compatibility(
        urllib3.__version__, chardet_version, charset_normalizer_version
    )
except (AssertionError, ValueError):
    warnings.warn(
        "urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
        "version!".format(
            urllib3.__version__, chardet_version, charset_normalizer_version
        ),
        RequestsDependencyWarning,
    )

# Attempt to enable urllib3's fallback for SNI support
# if the standard library doesn't support SNI or the
# 'ssl' library isn't available.
try:
    # Note: This logic prevents upgrading cryptography on Windows, if imported
    #       as part of pip.
    from pip._internal.utils.compat import WINDOWS
    if not WINDOWS:
        raise ImportError("pip internals: don't import cryptography on Windows")
    try:
        import ssl
    except ImportError:
        ssl = None

    if not getattr(ssl, "HAS_SNI", False):
        from pip._vendor.urllib3.contrib import pyopenssl

        pyopenssl.inject_into_urllib3()

        # Check cryptography version
        from cryptography import __version__ as cryptography_version

        _check_cryptography(cryptography_version)
except ImportError:
    pass

# urllib3's DependencyWarnings should be silenced.
from pip._vendor.urllib3.exceptions import DependencyWarning

warnings.simplefilter("ignore", DependencyWarning)

# Set default logging handler to avoid "No handler found" warnings.
import logging
from logging import NullHandler

from . import packages, utils
from .__version__ import (
    __author__,
    __author_email__,
    __build__,
    __cake__,
    __copyright__,
    __description__,
    __license__,
    __title__,
    __url__,
    __version__,
)
from .api import delete, get, head, options, patch, post, put, request
from .exceptions import (
    ConnectionError,
    ConnectTimeout,
    FileModeWarning,
    HTTPError,
    JSONDecodeError,
    ReadTimeout,
    RequestException,
    Timeout,
    TooManyRedirects,
    URLRequired,
)
from .models import PreparedRequest, Request, Response
from .sessions import Session, session
from .status_codes import codes

logging.getLogger(__name__).addHandler(NullHandler())

# FileModeWarnings go off per the default.
warnings.simplefilter("default", FileModeWarning, append=True)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/models.py
# ========================================================
"""
requests.models
~~~~~~~~~~~~~~~

This module contains the primary objects that power Requests.
"""

import datetime

# Import encoding now, to avoid implicit import later.
# Implicit import within threads may cause LookupError when standard library is in a ZIP,
# such as in Embedded Python. See https://github.com/psf/requests/issues/3578.
import encodings.idna  # noqa: F401
from io import UnsupportedOperation

from pip._vendor.urllib3.exceptions import (
    DecodeError,
    LocationParseError,
    ProtocolError,
    ReadTimeoutError,
    SSLError,
)
from pip._vendor.urllib3.fields import RequestField
from pip._vendor.urllib3.filepost import encode_multipart_formdata
from pip._vendor.urllib3.util import parse_url

from ._internal_utils import to_native_string, unicode_is_ascii
from .auth import HTTPBasicAuth
from .compat import (
    Callable,
    JSONDecodeError,
    Mapping,
    basestring,
    builtin_str,
    chardet,
    cookielib,
)
from .compat import json as complexjson
from .compat import urlencode, urlsplit, urlunparse
from .cookies import _copy_cookie_jar, cookiejar_from_dict, get_cookie_header
from .exceptions import (
    ChunkedEncodingError,
    ConnectionError,
    ContentDecodingError,
    HTTPError,
    InvalidJSONError,
    InvalidURL,
)
from .exceptions import JSONDecodeError as RequestsJSONDecodeError
from .exceptions import MissingSchema
from .exceptions import SSLError as RequestsSSLError
from .exceptions import StreamConsumedError
from .hooks import default_hooks
from .status_codes import codes
from .structures import CaseInsensitiveDict
from .utils import (
    check_header_validity,
    get_auth_from_url,
    guess_filename,
    guess_json_utf,
    iter_slices,
    parse_header_links,
    requote_uri,
    stream_decode_response_unicode,
    super_len,
    to_key_val_list,
)

#: The set of HTTP status codes that indicate an automatically
#: processable redirect.
REDIRECT_STATI = (
    codes.moved,  # 301
    codes.found,  # 302
    codes.other,  # 303
    codes.temporary_redirect,  # 307
    codes.permanent_redirect,  # 308
)

DEFAULT_REDIRECT_LIMIT = 30
CONTENT_CHUNK_SIZE = 10 * 1024
ITER_CHUNK_SIZE = 512


class RequestEncodingMixin:
    @property
    def path_url(self):
        """Build the path URL to use."""

        url = []

        p = urlsplit(self.url)

        path = p.path
        if not path:
            path = "/"

        url.append(path)

        query = p.query
        if query:
            url.append("?")
            url.append(query)

        return "".join(url)

    @staticmethod
    def _encode_params(data):
        """Encode parameters in a piece of data.

        Will successfully encode parameters when passed as a dict or a list of
        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
        if parameters are supplied as a dict.
        """

        if isinstance(data, (str, bytes)):
            return data
        elif hasattr(data, "read"):
            return data
        elif hasattr(data, "__iter__"):
            result = []
            for k, vs in to_key_val_list(data):
                if isinstance(vs, basestring) or not hasattr(vs, "__iter__"):
                    vs = [vs]
                for v in vs:
                    if v is not None:
                        result.append(
                            (
                                k.encode("utf-8") if isinstance(k, str) else k,
                                v.encode("utf-8") if isinstance(v, str) else v,
                            )
                        )
            return urlencode(result, doseq=True)
        else:
            return data

    @staticmethod
    def _encode_files(files, data):
        """Build the body for a multipart/form-data request.

        Will successfully encode files when passed as a dict or a list of
        tuples. Order is retained if data is a list of tuples but arbitrary
        if parameters are supplied as a dict.
        The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)
        or 4-tuples (filename, fileobj, contentype, custom_headers).
        """
        if not files:
            raise ValueError("Files must be provided.")
        elif isinstance(data, basestring):
            raise ValueError("Data must not be a string.")

        new_fields = []
        fields = to_key_val_list(data or {})
        files = to_key_val_list(files or {})

        for field, val in fields:
            if isinstance(val, basestring) or not hasattr(val, "__iter__"):
                val = [val]
            for v in val:
                if v is not None:
                    # Don't call str() on bytestrings: in Py3 it all goes wrong.
                    if not isinstance(v, bytes):
                        v = str(v)

                    new_fields.append(
                        (
                            field.decode("utf-8")
                            if isinstance(field, bytes)
                            else field,
                            v.encode("utf-8") if isinstance(v, str) else v,
                        )
                    )

        for (k, v) in files:
            # support for explicit filename
            ft = None
            fh = None
            if isinstance(v, (tuple, list)):
                if len(v) == 2:
                    fn, fp = v
                elif len(v) == 3:
                    fn, fp, ft = v
                else:
                    fn, fp, ft, fh = v
            else:
                fn = guess_filename(v) or k
                fp = v

            if isinstance(fp, (str, bytes, bytearray)):
                fdata = fp
            elif hasattr(fp, "read"):
                fdata = fp.read()
            elif fp is None:
                continue
            else:
                fdata = fp

            rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)
            rf.make_multipart(content_type=ft)
            new_fields.append(rf)

        body, content_type = encode_multipart_formdata(new_fields)

        return body, content_type


class RequestHooksMixin:
    def register_hook(self, event, hook):
        """Properly register a hook."""

        if event not in self.hooks:
            raise ValueError(f'Unsupported event specified, with event name "{event}"')

        if isinstance(hook, Callable):
            self.hooks[event].append(hook)
        elif hasattr(hook, "__iter__"):
            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))

    def deregister_hook(self, event, hook):
        """Deregister a previously registered hook.
        Returns True if the hook existed, False if not.
        """

        try:
            self.hooks[event].remove(hook)
            return True
        except ValueError:
            return False


class Request(RequestHooksMixin):
    """A user-created :class:`Request <Request>` object.

    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.

    :param method: HTTP method to use.
    :param url: URL to send.
    :param headers: dictionary of headers to send.
    :param files: dictionary of {filename: fileobject} files to multipart upload.
    :param data: the body to attach to the request. If a dictionary or
        list of tuples ``[(key, value)]`` is provided, form-encoding will
        take place.
    :param json: json for the body to attach to the request (if files or data is not specified).
    :param params: URL parameters to append to the URL. If a dictionary or
        list of tuples ``[(key, value)]`` is provided, form-encoding will
        take place.
    :param auth: Auth handler or (user, pass) tuple.
    :param cookies: dictionary or CookieJar of cookies to attach to this request.
    :param hooks: dictionary of callback hooks, for internal usage.

    Usage::

      >>> import requests
      >>> req = requests.Request('GET', 'https://httpbin.org/get')
      >>> req.prepare()
      <PreparedRequest [GET]>
    """

    def __init__(
        self,
        method=None,
        url=None,
        headers=None,
        files=None,
        data=None,
        params=None,
        auth=None,
        cookies=None,
        hooks=None,
        json=None,
    ):

        # Default empty dicts for dict params.
        data = [] if data is None else data
        files = [] if files is None else files
        headers = {} if headers is None else headers
        params = {} if params is None else params
        hooks = {} if hooks is None else hooks

        self.hooks = default_hooks()
        for (k, v) in list(hooks.items()):
            self.register_hook(event=k, hook=v)

        self.method = method
        self.url = url
        self.headers = headers
        self.files = files
        self.data = data
        self.json = json
        self.params = params
        self.auth = auth
        self.cookies = cookies

    def __repr__(self):
        return f"<Request [{self.method}]>"

    def prepare(self):
        """Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it."""
        p = PreparedRequest()
        p.prepare(
            method=self.method,
            url=self.url,
            headers=self.headers,
            files=self.files,
            data=self.data,
            json=self.json,
            params=self.params,
            auth=self.auth,
            cookies=self.cookies,
            hooks=self.hooks,
        )
        return p


class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):
    """The fully mutable :class:`PreparedRequest <PreparedRequest>` object,
    containing the exact bytes that will be sent to the server.

    Instances are generated from a :class:`Request <Request>` object, and
    should not be instantiated manually; doing so may produce undesirable
    effects.

    Usage::

      >>> import requests
      >>> req = requests.Request('GET', 'https://httpbin.org/get')
      >>> r = req.prepare()
      >>> r
      <PreparedRequest [GET]>

      >>> s = requests.Session()
      >>> s.send(r)
      <Response [200]>
    """

    def __init__(self):
        #: HTTP verb to send to the server.
        self.method = None
        #: HTTP URL to send the request to.
        self.url = None
        #: dictionary of HTTP headers.
        self.headers = None
        # The `CookieJar` used to create the Cookie header will be stored here
        # after prepare_cookies is called
        self._cookies = None
        #: request body to send to the server.
        self.body = None
        #: dictionary of callback hooks, for internal usage.
        self.hooks = default_hooks()
        #: integer denoting starting position of a readable file-like body.
        self._body_position = None

    def prepare(
        self,
        method=None,
        url=None,
        headers=None,
        files=None,
        data=None,
        params=None,
        auth=None,
        cookies=None,
        hooks=None,
        json=None,
    ):
        """Prepares the entire request with the given parameters."""

        self.prepare_method(method)
        self.prepare_url(url, params)
        self.prepare_headers(headers)
        self.prepare_cookies(cookies)
        self.prepare_body(data, files, json)
        self.prepare_auth(auth, url)

        # Note that prepare_auth must be last to enable authentication schemes
        # such as OAuth to work on a fully prepared request.

        # This MUST go after prepare_auth. Authenticators could add a hook
        self.prepare_hooks(hooks)

    def __repr__(self):
        return f"<PreparedRequest [{self.method}]>"

    def copy(self):
        p = PreparedRequest()
        p.method = self.method
        p.url = self.url
        p.headers = self.headers.copy() if self.headers is not None else None
        p._cookies = _copy_cookie_jar(self._cookies)
        p.body = self.body
        p.hooks = self.hooks
        p._body_position = self._body_position
        return p

    def prepare_method(self, method):
        """Prepares the given HTTP method."""
        self.method = method
        if self.method is not None:
            self.method = to_native_string(self.method.upper())

    @staticmethod
    def _get_idna_encoded_host(host):
        from pip._vendor import idna

        try:
            host = idna.encode(host, uts46=True).decode("utf-8")
        except idna.IDNAError:
            raise UnicodeError
        return host

    def prepare_url(self, url, params):
        """Prepares the given HTTP URL."""
        #: Accept objects that have string representations.
        #: We're unable to blindly call unicode/str functions
        #: as this will include the bytestring indicator (b'')
        #: on python 3.x.
        #: https://github.com/psf/requests/pull/2238
        if isinstance(url, bytes):
            url = url.decode("utf8")
        else:
            url = str(url)

        # Remove leading whitespaces from url
        url = url.lstrip()

        # Don't do any URL preparation for non-HTTP schemes like `mailto`,
        # `data` etc to work around exceptions from `url_parse`, which
        # handles RFC 3986 only.
        if ":" in url and not url.lower().startswith("http"):
            self.url = url
            return

        # Support for unicode domain names and paths.
        try:
            scheme, auth, host, port, path, query, fragment = parse_url(url)
        except LocationParseError as e:
            raise InvalidURL(*e.args)

        if not scheme:
            raise MissingSchema(
                f"Invalid URL {url!r}: No scheme supplied. "
                f"Perhaps you meant https://{url}?"
            )

        if not host:
            raise InvalidURL(f"Invalid URL {url!r}: No host supplied")

        # In general, we want to try IDNA encoding the hostname if the string contains
        # non-ASCII characters. This allows users to automatically get the correct IDNA
        # behaviour. For strings containing only ASCII characters, we need to also verify
        # it doesn't start with a wildcard (*), before allowing the unencoded hostname.
        if not unicode_is_ascii(host):
            try:
                host = self._get_idna_encoded_host(host)
            except UnicodeError:
                raise InvalidURL("URL has an invalid label.")
        elif host.startswith(("*", ".")):
            raise InvalidURL("URL has an invalid label.")

        # Carefully reconstruct the network location
        netloc = auth or ""
        if netloc:
            netloc += "@"
        netloc += host
        if port:
            netloc += f":{port}"

        # Bare domains aren't valid URLs.
        if not path:
            path = "/"

        if isinstance(params, (str, bytes)):
            params = to_native_string(params)

        enc_params = self._encode_params(params)
        if enc_params:
            if query:
                query = f"{query}&{enc_params}"
            else:
                query = enc_params

        url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))
        self.url = url

    def prepare_headers(self, headers):
        """Prepares the given HTTP headers."""

        self.headers = CaseInsensitiveDict()
        if headers:
            for header in headers.items():
                # Raise exception on invalid header value.
                check_header_validity(header)
                name, value = header
                self.headers[to_native_string(name)] = value

    def prepare_body(self, data, files, json=None):
        """Prepares the given HTTP body data."""

        # Check if file, fo, generator, iterator.
        # If not, run through normal process.

        # Nottin' on you.
        body = None
        content_type = None

        if not data and json is not None:
            # urllib3 requires a bytes-like body. Python 2's json.dumps
            # provides this natively, but Python 3 gives a Unicode string.
            content_type = "application/json"

            try:
                body = complexjson.dumps(json, allow_nan=False)
            except ValueError as ve:
                raise InvalidJSONError(ve, request=self)

            if not isinstance(body, bytes):
                body = body.encode("utf-8")

        is_stream = all(
            [
                hasattr(data, "__iter__"),
                not isinstance(data, (basestring, list, tuple, Mapping)),
            ]
        )

        if is_stream:
            try:
                length = super_len(data)
            except (TypeError, AttributeError, UnsupportedOperation):
                length = None

            body = data

            if getattr(body, "tell", None) is not None:
                # Record the current file position before reading.
                # This will allow us to rewind a file in the event
                # of a redirect.
                try:
                    self._body_position = body.tell()
                except OSError:
                    # This differentiates from None, allowing us to catch
                    # a failed `tell()` later when trying to rewind the body
                    self._body_position = object()

            if files:
                raise NotImplementedError(
                    "Streamed bodies and files are mutually exclusive."
                )

            if length:
                self.headers["Content-Length"] = builtin_str(length)
            else:
                self.headers["Transfer-Encoding"] = "chunked"
        else:
            # Multi-part file uploads.
            if files:
                (body, content_type) = self._encode_files(files, data)
            else:
                if data:
                    body = self._encode_params(data)
                    if isinstance(data, basestring) or hasattr(data, "read"):
                        content_type = None
                    else:
                        content_type = "application/x-www-form-urlencoded"

            self.prepare_content_length(body)

            # Add content-type if it wasn't explicitly provided.
            if content_type and ("content-type" not in self.headers):
                self.headers["Content-Type"] = content_type

        self.body = body

    def prepare_content_length(self, body):
        """Prepare Content-Length header based on request method and body"""
        if body is not None:
            length = super_len(body)
            if length:
                # If length exists, set it. Otherwise, we fallback
                # to Transfer-Encoding: chunked.
                self.headers["Content-Length"] = builtin_str(length)
        elif (
            self.method not in ("GET", "HEAD")
            and self.headers.get("Content-Length") is None
        ):
            # Set Content-Length to 0 for methods that can have a body
            # but don't provide one. (i.e. not GET or HEAD)
            self.headers["Content-Length"] = "0"

    def prepare_auth(self, auth, url=""):
        """Prepares the given HTTP auth data."""

        # If no Auth is explicitly provided, extract it from the URL first.
        if auth is None:
            url_auth = get_auth_from_url(self.url)
            auth = url_auth if any(url_auth) else None

        if auth:
            if isinstance(auth, tuple) and len(auth) == 2:
                # special-case basic HTTP auth
                auth = HTTPBasicAuth(*auth)

            # Allow auth to make its changes.
            r = auth(self)

            # Update self to reflect the auth changes.
            self.__dict__.update(r.__dict__)

            # Recompute Content-Length
            self.prepare_content_length(self.body)

    def prepare_cookies(self, cookies):
        """Prepares the given HTTP cookie data.

        This function eventually generates a ``Cookie`` header from the
        given cookies using cookielib. Due to cookielib's design, the header
        will not be regenerated if it already exists, meaning this function
        can only be called once for the life of the
        :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
        to ``prepare_cookies`` will have no actual effect, unless the "Cookie"
        header is removed beforehand.
        """
        if isinstance(cookies, cookielib.CookieJar):
            self._cookies = cookies
        else:
            self._cookies = cookiejar_from_dict(cookies)

        cookie_header = get_cookie_header(self._cookies, self)
        if cookie_header is not None:
            self.headers["Cookie"] = cookie_header

    def prepare_hooks(self, hooks):
        """Prepares the given hooks."""
        # hooks can be passed as None to the prepare method and to this
        # method. To prevent iterating over None, simply use an empty list
        # if hooks is False-y
        hooks = hooks or []
        for event in hooks:
            self.register_hook(event, hooks[event])


class Response:
    """The :class:`Response <Response>` object, which contains a
    server's response to an HTTP request.
    """

    __attrs__ = [
        "_content",
        "status_code",
        "headers",
        "url",
        "history",
        "encoding",
        "reason",
        "cookies",
        "elapsed",
        "request",
    ]

    def __init__(self):
        self._content = False
        self._content_consumed = False
        self._next = None

        #: Integer Code of responded HTTP Status, e.g. 404 or 200.
        self.status_code = None

        #: Case-insensitive Dictionary of Response Headers.
        #: For example, ``headers['content-encoding']`` will return the
        #: value of a ``'Content-Encoding'`` response header.
        self.headers = CaseInsensitiveDict()

        #: File-like object representation of response (for advanced usage).
        #: Use of ``raw`` requires that ``stream=True`` be set on the request.
        #: This requirement does not apply for use internally to Requests.
        self.raw = None

        #: Final URL location of Response.
        self.url = None

        #: Encoding to decode with when accessing r.text.
        self.encoding = None

        #: A list of :class:`Response <Response>` objects from
        #: the history of the Request. Any redirect responses will end
        #: up here. The list is sorted from the oldest to the most recent request.
        self.history = []

        #: Textual reason of responded HTTP Status, e.g. "Not Found" or "OK".
        self.reason = None

        #: A CookieJar of Cookies the server sent back.
        self.cookies = cookiejar_from_dict({})

        #: The amount of time elapsed between sending the request
        #: and the arrival of the response (as a timedelta).
        #: This property specifically measures the time taken between sending
        #: the first byte of the request and finishing parsing the headers. It
        #: is therefore unaffected by consuming the response content or the
        #: value of the ``stream`` keyword argument.
        self.elapsed = datetime.timedelta(0)

        #: The :class:`PreparedRequest <PreparedRequest>` object to which this
        #: is a response.
        self.request = None

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

    def __getstate__(self):
        # Consume everything; accessing the content attribute makes
        # sure the content has been fully read.
        if not self._content_consumed:
            self.content

        return {attr: getattr(self, attr, None) for attr in self.__attrs__}

    def __setstate__(self, state):
        for name, value in state.items():
            setattr(self, name, value)

        # pickled objects do not have .raw
        setattr(self, "_content_consumed", True)
        setattr(self, "raw", None)

    def __repr__(self):
        return f"<Response [{self.status_code}]>"

    def __bool__(self):
        """Returns True if :attr:`status_code` is less than 400.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code, is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        """
        return self.ok

    def __nonzero__(self):
        """Returns True if :attr:`status_code` is less than 400.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code, is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        """
        return self.ok

    def __iter__(self):
        """Allows you to use a response as an iterator."""
        return self.iter_content(128)

    @property
    def ok(self):
        """Returns True if :attr:`status_code` is less than 400, False if not.

        This attribute checks if the status code of the response is between
        400 and 600 to see if there was a client error or a server error. If
        the status code is between 200 and 400, this will return True. This
        is **not** a check to see if the response code is ``200 OK``.
        """
        try:
            self.raise_for_status()
        except HTTPError:
            return False
        return True

    @property
    def is_redirect(self):
        """True if this Response is a well-formed HTTP redirect that could have
        been processed automatically (by :meth:`Session.resolve_redirects`).
        """
        return "location" in self.headers and self.status_code in REDIRECT_STATI

    @property
    def is_permanent_redirect(self):
        """True if this Response one of the permanent versions of redirect."""
        return "location" in self.headers and self.status_code in (
            codes.moved_permanently,
            codes.permanent_redirect,
        )

    @property
    def next(self):
        """Returns a PreparedRequest for the next request in a redirect chain, if there is one."""
        return self._next

    @property
    def apparent_encoding(self):
        """The apparent encoding, provided by the charset_normalizer or chardet libraries."""
        return chardet.detect(self.content)["encoding"]

    def iter_content(self, chunk_size=1, decode_unicode=False):
        """Iterates over the response data.  When stream=True is set on the
        request, this avoids reading the content at once into memory for
        large responses.  The chunk size is the number of bytes it should
        read into memory.  This is not necessarily the length of each item
        returned as decoding can take place.

        chunk_size must be of type int or None. A value of None will
        function differently depending on the value of `stream`.
        stream=True will read data as it arrives in whatever size the
        chunks are received. If stream=False, data is returned as
        a single chunk.

        If decode_unicode is True, content will be decoded using the best
        available encoding based on the response.
        """

        def generate():
            # Special case for urllib3.
            if hasattr(self.raw, "stream"):
                try:
                    yield from self.raw.stream(chunk_size, decode_content=True)
                except ProtocolError as e:
                    raise ChunkedEncodingError(e)
                except DecodeError as e:
                    raise ContentDecodingError(e)
                except ReadTimeoutError as e:
                    raise ConnectionError(e)
                except SSLError as e:
                    raise RequestsSSLError(e)
            else:
                # Standard file-like object.
                while True:
                    chunk = self.raw.read(chunk_size)
                    if not chunk:
                        break
                    yield chunk

            self._content_consumed = True

        if self._content_consumed and isinstance(self._content, bool):
            raise StreamConsumedError()
        elif chunk_size is not None and not isinstance(chunk_size, int):
            raise TypeError(
                f"chunk_size must be an int, it is instead a {type(chunk_size)}."
            )
        # simulate reading small chunks of the content
        reused_chunks = iter_slices(self._content, chunk_size)

        stream_chunks = generate()

        chunks = reused_chunks if self._content_consumed else stream_chunks

        if decode_unicode:
            chunks = stream_decode_response_unicode(chunks, self)

        return chunks

    def iter_lines(
        self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=False, delimiter=None
    ):
        """Iterates over the response data, one line at a time.  When
        stream=True is set on the request, this avoids reading the
        content at once into memory for large responses.

        .. note:: This method is not reentrant safe.
        """

        pending = None

        for chunk in self.iter_content(
            chunk_size=chunk_size, decode_unicode=decode_unicode
        ):

            if pending is not None:
                chunk = pending + chunk

            if delimiter:
                lines = chunk.split(delimiter)
            else:
                lines = chunk.splitlines()

            if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:
                pending = lines.pop()
            else:
                pending = None

            yield from lines

        if pending is not None:
            yield pending

    @property
    def content(self):
        """Content of the response, in bytes."""

        if self._content is False:
            # Read the contents.
            if self._content_consumed:
                raise RuntimeError("The content for this response was already consumed")

            if self.status_code == 0 or self.raw is None:
                self._content = None
            else:
                self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""

        self._content_consumed = True
        # don't need to release the connection; that's been handled by urllib3
        # since we exhausted the data.
        return self._content

    @property
    def text(self):
        """Content of the response, in unicode.

        If Response.encoding is None, encoding will be guessed using
        ``charset_normalizer`` or ``chardet``.

        The encoding of the response content is determined based solely on HTTP
        headers, following RFC 2616 to the letter. If you can take advantage of
        non-HTTP knowledge to make a better guess at the encoding, you should
        set ``r.encoding`` appropriately before accessing this property.
        """

        # Try charset from content-type
        content = None
        encoding = self.encoding

        if not self.content:
            return ""

        # Fallback to auto-detected encoding.
        if self.encoding is None:
            encoding = self.apparent_encoding

        # Decode unicode from given encoding.
        try:
            content = str(self.content, encoding, errors="replace")
        except (LookupError, TypeError):
            # A LookupError is raised if the encoding was not found which could
            # indicate a misspelling or similar mistake.
            #
            # A TypeError can be raised if encoding is None
            #
            # So we try blindly encoding.
            content = str(self.content, errors="replace")

        return content

    def json(self, **kwargs):
        r"""Returns the json-encoded content of a response, if any.

        :param \*\*kwargs: Optional arguments that ``json.loads`` takes.
        :raises requests.exceptions.JSONDecodeError: If the response body does not
            contain valid json.
        """

        if not self.encoding and self.content and len(self.content) > 3:
            # No encoding set. JSON RFC 4627 section 3 states we should expect
            # UTF-8, -16 or -32. Detect which one to use; If the detection or
            # decoding fails, fall back to `self.text` (using charset_normalizer to make
            # a best guess).
            encoding = guess_json_utf(self.content)
            if encoding is not None:
                try:
                    return complexjson.loads(self.content.decode(encoding), **kwargs)
                except UnicodeDecodeError:
                    # Wrong UTF codec detected; usually because it's not UTF-8
                    # but some other 8-bit codec.  This is an RFC violation,
                    # and the server didn't bother to tell us what codec *was*
                    # used.
                    pass
                except JSONDecodeError as e:
                    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)

        try:
            return complexjson.loads(self.text, **kwargs)
        except JSONDecodeError as e:
            # Catch JSON-related errors and raise as requests.JSONDecodeError
            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError
            raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)

    @property
    def links(self):
        """Returns the parsed header links of the response, if any."""

        header = self.headers.get("link")

        resolved_links = {}

        if header:
            links = parse_header_links(header)

            for link in links:
                key = link.get("rel") or link.get("url")
                resolved_links[key] = link

        return resolved_links

    def raise_for_status(self):
        """Raises :class:`HTTPError`, if one occurred."""

        http_error_msg = ""
        if isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = self.reason.decode("utf-8")
            except UnicodeDecodeError:
                reason = self.reason.decode("iso-8859-1")
        else:
            reason = self.reason

        if 400 <= self.status_code < 500:
            http_error_msg = (
                f"{self.status_code} Client Error: {reason} for url: {self.url}"
            )

        elif 500 <= self.status_code < 600:
            http_error_msg = (
                f"{self.status_code} Server Error: {reason} for url: {self.url}"
            )

        if http_error_msg:
            raise HTTPError(http_error_msg, response=self)

    def close(self):
        """Releases the connection back to the pool. Once this method has been
        called the underlying ``raw`` object must not be accessed again.

        *Note: Should not normally need to be called explicitly.*
        """
        if not self._content_consumed:
            self.raw.close()

        release_conn = getattr(self.raw, "release_conn", None)
        if release_conn is not None:
            release_conn()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/api.py
# ========================================================
"""
requests.api
~~~~~~~~~~~~

This module implements the Requests API.

:copyright: (c) 2012 by Kenneth Reitz.
:license: Apache2, see LICENSE for more details.
"""

from . import sessions


def request(method, url, **kwargs):
    """Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      >>> req
      <Response [200]>
    """

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)


def get(url, params=None, **kwargs):
    r"""Sends a GET request.

    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the query string for the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request("get", url, params=params, **kwargs)


def options(url, **kwargs):
    r"""Sends an OPTIONS request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request("options", url, **kwargs)


def head(url, **kwargs):
    r"""Sends a HEAD request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes. If
        `allow_redirects` is not provided, it will be set to `False` (as
        opposed to the default :meth:`request` behavior).
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    kwargs.setdefault("allow_redirects", False)
    return request("head", url, **kwargs)


def post(url, data=None, json=None, **kwargs):
    r"""Sends a POST request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request("post", url, data=data, json=json, **kwargs)


def put(url, data=None, **kwargs):
    r"""Sends a PUT request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request("put", url, data=data, **kwargs)


def patch(url, data=None, **kwargs):
    r"""Sends a PATCH request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request("patch", url, data=data, **kwargs)


def delete(url, **kwargs):
    r"""Sends a DELETE request.

    :param url: URL for the new :class:`Request` object.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request("delete", url, **kwargs)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/sessions.py
# ========================================================
"""
requests.sessions
~~~~~~~~~~~~~~~~~

This module provides a Session object to manage and persist settings across
requests (cookies, auth, proxies).
"""
import os
import sys
import time
from collections import OrderedDict
from datetime import timedelta

from ._internal_utils import to_native_string
from .adapters import HTTPAdapter
from .auth import _basic_auth_str
from .compat import Mapping, cookielib, urljoin, urlparse
from .cookies import (
    RequestsCookieJar,
    cookiejar_from_dict,
    extract_cookies_to_jar,
    merge_cookies,
)
from .exceptions import (
    ChunkedEncodingError,
    ContentDecodingError,
    InvalidSchema,
    TooManyRedirects,
)
from .hooks import default_hooks, dispatch_hook

# formerly defined here, reexposed here for backward compatibility
from .models import (  # noqa: F401
    DEFAULT_REDIRECT_LIMIT,
    REDIRECT_STATI,
    PreparedRequest,
    Request,
)
from .status_codes import codes
from .structures import CaseInsensitiveDict
from .utils import (  # noqa: F401
    DEFAULT_PORTS,
    default_headers,
    get_auth_from_url,
    get_environ_proxies,
    get_netrc_auth,
    requote_uri,
    resolve_proxies,
    rewind_body,
    should_bypass_proxies,
    to_key_val_list,
)

# Preferred clock, based on which one is more accurate on a given system.
if sys.platform == "win32":
    preferred_clock = time.perf_counter
else:
    preferred_clock = time.time


def merge_setting(request_setting, session_setting, dict_class=OrderedDict):
    """Determines appropriate setting for a given request, taking into account
    the explicit setting on that request, and the setting in the session. If a
    setting is a dictionary, they will be merged together using `dict_class`
    """

    if session_setting is None:
        return request_setting

    if request_setting is None:
        return session_setting

    # Bypass if not a dictionary (e.g. verify)
    if not (
        isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping)
    ):
        return request_setting

    merged_setting = dict_class(to_key_val_list(session_setting))
    merged_setting.update(to_key_val_list(request_setting))

    # Remove keys that are set to None. Extract keys first to avoid altering
    # the dictionary during iteration.
    none_keys = [k for (k, v) in merged_setting.items() if v is None]
    for key in none_keys:
        del merged_setting[key]

    return merged_setting


def merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):
    """Properly merges both requests and session hooks.

    This is necessary because when request_hooks == {'response': []}, the
    merge breaks Session hooks entirely.
    """
    if session_hooks is None or session_hooks.get("response") == []:
        return request_hooks

    if request_hooks is None or request_hooks.get("response") == []:
        return session_hooks

    return merge_setting(request_hooks, session_hooks, dict_class)


class SessionRedirectMixin:
    def get_redirect_target(self, resp):
        """Receives a Response. Returns a redirect URI or ``None``"""
        # Due to the nature of how requests processes redirects this method will
        # be called at least once upon the original response and at least twice
        # on each subsequent redirect response (if any).
        # If a custom mixin is used to handle this logic, it may be advantageous
        # to cache the redirect location onto the response object as a private
        # attribute.
        if resp.is_redirect:
            location = resp.headers["location"]
            # Currently the underlying http module on py3 decode headers
            # in latin1, but empirical evidence suggests that latin1 is very
            # rarely used with non-ASCII characters in HTTP headers.
            # It is more likely to get UTF8 header rather than latin1.
            # This causes incorrect handling of UTF8 encoded location headers.
            # To solve this, we re-encode the location in latin1.
            location = location.encode("latin1")
            return to_native_string(location, "utf8")
        return None

    def should_strip_auth(self, old_url, new_url):
        """Decide whether Authorization header should be removed when redirecting"""
        old_parsed = urlparse(old_url)
        new_parsed = urlparse(new_url)
        if old_parsed.hostname != new_parsed.hostname:
            return True
        # Special case: allow http -> https redirect when using the standard
        # ports. This isn't specified by RFC 7235, but is kept to avoid
        # breaking backwards compatibility with older versions of requests
        # that allowed any redirects on the same host.
        if (
            old_parsed.scheme == "http"
            and old_parsed.port in (80, None)
            and new_parsed.scheme == "https"
            and new_parsed.port in (443, None)
        ):
            return False

        # Handle default port usage corresponding to scheme.
        changed_port = old_parsed.port != new_parsed.port
        changed_scheme = old_parsed.scheme != new_parsed.scheme
        default_port = (DEFAULT_PORTS.get(old_parsed.scheme, None), None)
        if (
            not changed_scheme
            and old_parsed.port in default_port
            and new_parsed.port in default_port
        ):
            return False

        # Standard case: root URI must match
        return changed_port or changed_scheme

    def resolve_redirects(
        self,
        resp,
        req,
        stream=False,
        timeout=None,
        verify=True,
        cert=None,
        proxies=None,
        yield_requests=False,
        **adapter_kwargs,
    ):
        """Receives a Response. Returns a generator of Responses or Requests."""

        hist = []  # keep track of history

        url = self.get_redirect_target(resp)
        previous_fragment = urlparse(req.url).fragment
        while url:
            prepared_request = req.copy()

            # Update history and keep track of redirects.
            # resp.history must ignore the original request in this loop
            hist.append(resp)
            resp.history = hist[1:]

            try:
                resp.content  # Consume socket so it can be released
            except (ChunkedEncodingError, ContentDecodingError, RuntimeError):
                resp.raw.read(decode_content=False)

            if len(resp.history) >= self.max_redirects:
                raise TooManyRedirects(
                    f"Exceeded {self.max_redirects} redirects.", response=resp
                )

            # Release the connection back into the pool.
            resp.close()

            # Handle redirection without scheme (see: RFC 1808 Section 4)
            if url.startswith("//"):
                parsed_rurl = urlparse(resp.url)
                url = ":".join([to_native_string(parsed_rurl.scheme), url])

            # Normalize url case and attach previous fragment if needed (RFC 7231 7.1.2)
            parsed = urlparse(url)
            if parsed.fragment == "" and previous_fragment:
                parsed = parsed._replace(fragment=previous_fragment)
            elif parsed.fragment:
                previous_fragment = parsed.fragment
            url = parsed.geturl()

            # Facilitate relative 'location' headers, as allowed by RFC 7231.
            # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')
            # Compliant with RFC3986, we percent encode the url.
            if not parsed.netloc:
                url = urljoin(resp.url, requote_uri(url))
            else:
                url = requote_uri(url)

            prepared_request.url = to_native_string(url)

            self.rebuild_method(prepared_request, resp)

            # https://github.com/psf/requests/issues/1084
            if resp.status_code not in (
                codes.temporary_redirect,
                codes.permanent_redirect,
            ):
                # https://github.com/psf/requests/issues/3490
                purged_headers = ("Content-Length", "Content-Type", "Transfer-Encoding")
                for header in purged_headers:
                    prepared_request.headers.pop(header, None)
                prepared_request.body = None

            headers = prepared_request.headers
            headers.pop("Cookie", None)

            # Extract any cookies sent on the response to the cookiejar
            # in the new request. Because we've mutated our copied prepared
            # request, use the old one that we haven't yet touched.
            extract_cookies_to_jar(prepared_request._cookies, req, resp.raw)
            merge_cookies(prepared_request._cookies, self.cookies)
            prepared_request.prepare_cookies(prepared_request._cookies)

            # Rebuild auth and proxy information.
            proxies = self.rebuild_proxies(prepared_request, proxies)
            self.rebuild_auth(prepared_request, resp)

            # A failed tell() sets `_body_position` to `object()`. This non-None
            # value ensures `rewindable` will be True, allowing us to raise an
            # UnrewindableBodyError, instead of hanging the connection.
            rewindable = prepared_request._body_position is not None and (
                "Content-Length" in headers or "Transfer-Encoding" in headers
            )

            # Attempt to rewind consumed file-like object.
            if rewindable:
                rewind_body(prepared_request)

            # Override the original request.
            req = prepared_request

            if yield_requests:
                yield req
            else:

                resp = self.send(
                    req,
                    stream=stream,
                    timeout=timeout,
                    verify=verify,
                    cert=cert,
                    proxies=proxies,
                    allow_redirects=False,
                    **adapter_kwargs,
                )

                extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)

                # extract redirect url, if any, for the next loop
                url = self.get_redirect_target(resp)
                yield resp

    def rebuild_auth(self, prepared_request, response):
        """When being redirected we may want to strip authentication from the
        request to avoid leaking credentials. This method intelligently removes
        and reapplies authentication where possible to avoid credential loss.
        """
        headers = prepared_request.headers
        url = prepared_request.url

        if "Authorization" in headers and self.should_strip_auth(
            response.request.url, url
        ):
            # If we get redirected to a new host, we should strip out any
            # authentication headers.
            del headers["Authorization"]

        # .netrc might have more auth for us on our new host.
        new_auth = get_netrc_auth(url) if self.trust_env else None
        if new_auth is not None:
            prepared_request.prepare_auth(new_auth)

    def rebuild_proxies(self, prepared_request, proxies):
        """This method re-evaluates the proxy configuration by considering the
        environment variables. If we are redirected to a URL covered by
        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing
        proxy keys for this URL (in case they were stripped by a previous
        redirect).

        This method also replaces the Proxy-Authorization header where
        necessary.

        :rtype: dict
        """
        headers = prepared_request.headers
        scheme = urlparse(prepared_request.url).scheme
        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)

        if "Proxy-Authorization" in headers:
            del headers["Proxy-Authorization"]

        try:
            username, password = get_auth_from_url(new_proxies[scheme])
        except KeyError:
            username, password = None, None

        # urllib3 handles proxy authorization for us in the standard adapter.
        # Avoid appending this to TLS tunneled requests where it may be leaked.
        if not scheme.startswith('https') and username and password:
            headers["Proxy-Authorization"] = _basic_auth_str(username, password)

        return new_proxies

    def rebuild_method(self, prepared_request, response):
        """When being redirected we may want to change the method of the request
        based on certain specs or browser behavior.
        """
        method = prepared_request.method

        # https://tools.ietf.org/html/rfc7231#section-6.4.4
        if response.status_code == codes.see_other and method != "HEAD":
            method = "GET"

        # Do what the browsers do, despite standards...
        # First, turn 302s into GETs.
        if response.status_code == codes.found and method != "HEAD":
            method = "GET"

        # Second, if a POST is responded to with a 301, turn it into a GET.
        # This bizarre behaviour is explained in Issue 1704.
        if response.status_code == codes.moved and method == "POST":
            method = "GET"

        prepared_request.method = method


class Session(SessionRedirectMixin):
    """A Requests session.

    Provides cookie persistence, connection-pooling, and configuration.

    Basic Usage::

      >>> import requests
      >>> s = requests.Session()
      >>> s.get('https://httpbin.org/get')
      <Response [200]>

    Or as a context manager::

      >>> with requests.Session() as s:
      ...     s.get('https://httpbin.org/get')
      <Response [200]>
    """

    __attrs__ = [
        "headers",
        "cookies",
        "auth",
        "proxies",
        "hooks",
        "params",
        "verify",
        "cert",
        "adapters",
        "stream",
        "trust_env",
        "max_redirects",
    ]

    def __init__(self):

        #: A case-insensitive dictionary of headers to be sent on each
        #: :class:`Request <Request>` sent from this
        #: :class:`Session <Session>`.
        self.headers = default_headers()

        #: Default Authentication tuple or object to attach to
        #: :class:`Request <Request>`.
        self.auth = None

        #: Dictionary mapping protocol or protocol and host to the URL of the proxy
        #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to
        #: be used on each :class:`Request <Request>`.
        self.proxies = {}

        #: Event-handling hooks.
        self.hooks = default_hooks()

        #: Dictionary of querystring data to attach to each
        #: :class:`Request <Request>`. The dictionary values may be lists for
        #: representing multivalued query parameters.
        self.params = {}

        #: Stream response content default.
        self.stream = False

        #: SSL Verification default.
        #: Defaults to `True`, requiring requests to verify the TLS certificate at the
        #: remote end.
        #: If verify is set to `False`, requests will accept any TLS certificate
        #: presented by the server, and will ignore hostname mismatches and/or
        #: expired certificates, which will make your application vulnerable to
        #: man-in-the-middle (MitM) attacks.
        #: Only set this to `False` for testing.
        self.verify = True

        #: SSL client certificate default, if String, path to ssl client
        #: cert file (.pem). If Tuple, ('cert', 'key') pair.
        self.cert = None

        #: Maximum number of redirects allowed. If the request exceeds this
        #: limit, a :class:`TooManyRedirects` exception is raised.
        #: This defaults to requests.models.DEFAULT_REDIRECT_LIMIT, which is
        #: 30.
        self.max_redirects = DEFAULT_REDIRECT_LIMIT

        #: Trust environment settings for proxy configuration, default
        #: authentication and similar.
        self.trust_env = True

        #: A CookieJar containing all currently outstanding cookies set on this
        #: session. By default it is a
        #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but
        #: may be any other ``cookielib.CookieJar`` compatible object.
        self.cookies = cookiejar_from_dict({})

        # Default connection adapters.
        self.adapters = OrderedDict()
        self.mount("https://", HTTPAdapter())
        self.mount("http://", HTTPAdapter())

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()

    def prepare_request(self, request):
        """Constructs a :class:`PreparedRequest <PreparedRequest>` for
        transmission and returns it. The :class:`PreparedRequest` has settings
        merged from the :class:`Request <Request>` instance and those of the
        :class:`Session`.

        :param request: :class:`Request` instance to prepare with this
            session's settings.
        :rtype: requests.PreparedRequest
        """
        cookies = request.cookies or {}

        # Bootstrap CookieJar.
        if not isinstance(cookies, cookielib.CookieJar):
            cookies = cookiejar_from_dict(cookies)

        # Merge with session cookies
        merged_cookies = merge_cookies(
            merge_cookies(RequestsCookieJar(), self.cookies), cookies
        )

        # Set environment's basic authentication if not explicitly set.
        auth = request.auth
        if self.trust_env and not auth and not self.auth:
            auth = get_netrc_auth(request.url)

        p = PreparedRequest()
        p.prepare(
            method=request.method.upper(),
            url=request.url,
            files=request.files,
            data=request.data,
            json=request.json,
            headers=merge_setting(
                request.headers, self.headers, dict_class=CaseInsensitiveDict
            ),
            params=merge_setting(request.params, self.params),
            auth=merge_setting(auth, self.auth),
            cookies=merged_cookies,
            hooks=merge_hooks(request.hooks, self.hooks),
        )
        return p

    def request(
        self,
        method,
        url,
        params=None,
        data=None,
        headers=None,
        cookies=None,
        files=None,
        auth=None,
        timeout=None,
        allow_redirects=True,
        proxies=None,
        hooks=None,
        stream=None,
        verify=None,
        cert=None,
        json=None,
    ):
        """Constructs a :class:`Request <Request>`, prepares it and sends it.
        Returns :class:`Response <Response>` object.

        :param method: method for the new :class:`Request` object.
        :param url: URL for the new :class:`Request` object.
        :param params: (optional) Dictionary or bytes to be sent in the query
            string for the :class:`Request`.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param json: (optional) json to send in the body of the
            :class:`Request`.
        :param headers: (optional) Dictionary of HTTP Headers to send with the
            :class:`Request`.
        :param cookies: (optional) Dict or CookieJar object to send with the
            :class:`Request`.
        :param files: (optional) Dictionary of ``'filename': file-like-objects``
            for multipart encoding upload.
        :param auth: (optional) Auth tuple or callable to enable
            Basic/Digest/Custom HTTP Auth.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple
        :param allow_redirects: (optional) Set to True by default.
        :type allow_redirects: bool
        :param proxies: (optional) Dictionary mapping protocol or protocol and
            hostname to the URL of the proxy.
        :param stream: (optional) whether to immediately download the response
            content. Defaults to ``False``.
        :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``. When set to
            ``False``, requests will accept any TLS certificate presented by
            the server, and will ignore hostname mismatches and/or expired
            certificates, which will make your application vulnerable to
            man-in-the-middle (MitM) attacks. Setting verify to ``False``
            may be useful during local development or testing.
        :param cert: (optional) if String, path to ssl client cert file (.pem).
            If Tuple, ('cert', 'key') pair.
        :rtype: requests.Response
        """
        # Create the Request.
        req = Request(
            method=method.upper(),
            url=url,
            headers=headers,
            files=files,
            data=data or {},
            json=json,
            params=params or {},
            auth=auth,
            cookies=cookies,
            hooks=hooks,
        )
        prep = self.prepare_request(req)

        proxies = proxies or {}

        settings = self.merge_environment_settings(
            prep.url, proxies, stream, verify, cert
        )

        # Send the request.
        send_kwargs = {
            "timeout": timeout,
            "allow_redirects": allow_redirects,
        }
        send_kwargs.update(settings)
        resp = self.send(prep, **send_kwargs)

        return resp

    def get(self, url, **kwargs):
        r"""Sends a GET request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        kwargs.setdefault("allow_redirects", True)
        return self.request("GET", url, **kwargs)

    def options(self, url, **kwargs):
        r"""Sends a OPTIONS request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        kwargs.setdefault("allow_redirects", True)
        return self.request("OPTIONS", url, **kwargs)

    def head(self, url, **kwargs):
        r"""Sends a HEAD request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        kwargs.setdefault("allow_redirects", False)
        return self.request("HEAD", url, **kwargs)

    def post(self, url, data=None, json=None, **kwargs):
        r"""Sends a POST request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param json: (optional) json to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        return self.request("POST", url, data=data, json=json, **kwargs)

    def put(self, url, data=None, **kwargs):
        r"""Sends a PUT request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        return self.request("PUT", url, data=data, **kwargs)

    def patch(self, url, data=None, **kwargs):
        r"""Sends a PATCH request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param data: (optional) Dictionary, list of tuples, bytes, or file-like
            object to send in the body of the :class:`Request`.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        return self.request("PATCH", url, data=data, **kwargs)

    def delete(self, url, **kwargs):
        r"""Sends a DELETE request. Returns :class:`Response` object.

        :param url: URL for the new :class:`Request` object.
        :param \*\*kwargs: Optional arguments that ``request`` takes.
        :rtype: requests.Response
        """

        return self.request("DELETE", url, **kwargs)

    def send(self, request, **kwargs):
        """Send a given PreparedRequest.

        :rtype: requests.Response
        """
        # Set defaults that the hooks can utilize to ensure they always have
        # the correct parameters to reproduce the previous request.
        kwargs.setdefault("stream", self.stream)
        kwargs.setdefault("verify", self.verify)
        kwargs.setdefault("cert", self.cert)
        if "proxies" not in kwargs:
            kwargs["proxies"] = resolve_proxies(request, self.proxies, self.trust_env)

        # It's possible that users might accidentally send a Request object.
        # Guard against that specific failure case.
        if isinstance(request, Request):
            raise ValueError("You can only send PreparedRequests.")

        # Set up variables needed for resolve_redirects and dispatching of hooks
        allow_redirects = kwargs.pop("allow_redirects", True)
        stream = kwargs.get("stream")
        hooks = request.hooks

        # Get the appropriate adapter to use
        adapter = self.get_adapter(url=request.url)

        # Start time (approximately) of the request
        start = preferred_clock()

        # Send the request
        r = adapter.send(request, **kwargs)

        # Total elapsed time of the request (approximately)
        elapsed = preferred_clock() - start
        r.elapsed = timedelta(seconds=elapsed)

        # Response manipulation hooks
        r = dispatch_hook("response", hooks, r, **kwargs)

        # Persist cookies
        if r.history:

            # If the hooks create history then we want those cookies too
            for resp in r.history:
                extract_cookies_to_jar(self.cookies, resp.request, resp.raw)

        extract_cookies_to_jar(self.cookies, request, r.raw)

        # Resolve redirects if allowed.
        if allow_redirects:
            # Redirect resolving generator.
            gen = self.resolve_redirects(r, request, **kwargs)
            history = [resp for resp in gen]
        else:
            history = []

        # Shuffle things around if there's history.
        if history:
            # Insert the first (original) request at the start
            history.insert(0, r)
            # Get the last request made
            r = history.pop()
            r.history = history

        # If redirects aren't being followed, store the response on the Request for Response.next().
        if not allow_redirects:
            try:
                r._next = next(
                    self.resolve_redirects(r, request, yield_requests=True, **kwargs)
                )
            except StopIteration:
                pass

        if not stream:
            r.content

        return r

    def merge_environment_settings(self, url, proxies, stream, verify, cert):
        """
        Check the environment and merge it with some settings.

        :rtype: dict
        """
        # Gather clues from the surrounding environment.
        if self.trust_env:
            # Set environment's proxies.
            no_proxy = proxies.get("no_proxy") if proxies is not None else None
            env_proxies = get_environ_proxies(url, no_proxy=no_proxy)
            for (k, v) in env_proxies.items():
                proxies.setdefault(k, v)

            # Look for requests environment configuration
            # and be compatible with cURL.
            if verify is True or verify is None:
                verify = (
                    os.environ.get("REQUESTS_CA_BUNDLE")
                    or os.environ.get("CURL_CA_BUNDLE")
                    or verify
                )

        # Merge all the kwargs.
        proxies = merge_setting(proxies, self.proxies)
        stream = merge_setting(stream, self.stream)
        verify = merge_setting(verify, self.verify)
        cert = merge_setting(cert, self.cert)

        return {"proxies": proxies, "stream": stream, "verify": verify, "cert": cert}

    def get_adapter(self, url):
        """
        Returns the appropriate connection adapter for the given URL.

        :rtype: requests.adapters.BaseAdapter
        """
        for (prefix, adapter) in self.adapters.items():

            if url.lower().startswith(prefix.lower()):
                return adapter

        # Nothing matches :-/
        raise InvalidSchema(f"No connection adapters were found for {url!r}")

    def close(self):
        """Closes all adapters and as such the session"""
        for v in self.adapters.values():
            v.close()

    def mount(self, prefix, adapter):
        """Registers a connection adapter to a prefix.

        Adapters are sorted in descending order by prefix length.
        """
        self.adapters[prefix] = adapter
        keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]

        for key in keys_to_move:
            self.adapters[key] = self.adapters.pop(key)

    def __getstate__(self):
        state = {attr: getattr(self, attr, None) for attr in self.__attrs__}
        return state

    def __setstate__(self, state):
        for attr, value in state.items():
            setattr(self, attr, value)


def session():
    """
    Returns a :class:`Session` for context-management.

    .. deprecated:: 1.0.0

        This method has been deprecated since version 1.0.0 and is only kept for
        backwards compatibility. New code should use :class:`~requests.sessions.Session`
        to create a session. This may be removed at a future date.

    :rtype: Session
    """
    return Session()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/status_codes.py
# ========================================================
r"""
The ``codes`` object defines a mapping from common names for HTTP statuses
to their numerical codes, accessible either as attributes or as dictionary
items.

Example::

    >>> import requests
    >>> requests.codes['temporary_redirect']
    307
    >>> requests.codes.teapot
    418
    >>> requests.codes['\o/']
    200

Some codes have multiple names, and both upper- and lower-case versions of
the names are allowed. For example, ``codes.ok``, ``codes.OK``, and
``codes.okay`` all correspond to the HTTP status code 200.
"""

from .structures import LookupDict

_codes = {
    # Informational.
    100: ("continue",),
    101: ("switching_protocols",),
    102: ("processing",),
    103: ("checkpoint",),
    122: ("uri_too_long", "request_uri_too_long"),
    200: ("ok", "okay", "all_ok", "all_okay", "all_good", "\\o/", ""),
    201: ("created",),
    202: ("accepted",),
    203: ("non_authoritative_info", "non_authoritative_information"),
    204: ("no_content",),
    205: ("reset_content", "reset"),
    206: ("partial_content", "partial"),
    207: ("multi_status", "multiple_status", "multi_stati", "multiple_stati"),
    208: ("already_reported",),
    226: ("im_used",),
    # Redirection.
    300: ("multiple_choices",),
    301: ("moved_permanently", "moved", "\\o-"),
    302: ("found",),
    303: ("see_other", "other"),
    304: ("not_modified",),
    305: ("use_proxy",),
    306: ("switch_proxy",),
    307: ("temporary_redirect", "temporary_moved", "temporary"),
    308: (
        "permanent_redirect",
        "resume_incomplete",
        "resume",
    ),  # "resume" and "resume_incomplete" to be removed in 3.0
    # Client Error.
    400: ("bad_request", "bad"),
    401: ("unauthorized",),
    402: ("payment_required", "payment"),
    403: ("forbidden",),
    404: ("not_found", "-o-"),
    405: ("method_not_allowed", "not_allowed"),
    406: ("not_acceptable",),
    407: ("proxy_authentication_required", "proxy_auth", "proxy_authentication"),
    408: ("request_timeout", "timeout"),
    409: ("conflict",),
    410: ("gone",),
    411: ("length_required",),
    412: ("precondition_failed", "precondition"),
    413: ("request_entity_too_large",),
    414: ("request_uri_too_large",),
    415: ("unsupported_media_type", "unsupported_media", "media_type"),
    416: (
        "requested_range_not_satisfiable",
        "requested_range",
        "range_not_satisfiable",
    ),
    417: ("expectation_failed",),
    418: ("im_a_teapot", "teapot", "i_am_a_teapot"),
    421: ("misdirected_request",),
    422: ("unprocessable_entity", "unprocessable"),
    423: ("locked",),
    424: ("failed_dependency", "dependency"),
    425: ("unordered_collection", "unordered"),
    426: ("upgrade_required", "upgrade"),
    428: ("precondition_required", "precondition"),
    429: ("too_many_requests", "too_many"),
    431: ("header_fields_too_large", "fields_too_large"),
    444: ("no_response", "none"),
    449: ("retry_with", "retry"),
    450: ("blocked_by_windows_parental_controls", "parental_controls"),
    451: ("unavailable_for_legal_reasons", "legal_reasons"),
    499: ("client_closed_request",),
    # Server Error.
    500: ("internal_server_error", "server_error", "/o\\", ""),
    501: ("not_implemented",),
    502: ("bad_gateway",),
    503: ("service_unavailable", "unavailable"),
    504: ("gateway_timeout",),
    505: ("http_version_not_supported", "http_version"),
    506: ("variant_also_negotiates",),
    507: ("insufficient_storage",),
    509: ("bandwidth_limit_exceeded", "bandwidth"),
    510: ("not_extended",),
    511: ("network_authentication_required", "network_auth", "network_authentication"),
}

codes = LookupDict(name="status_codes")


def _init():
    for code, titles in _codes.items():
        for title in titles:
            setattr(codes, title, code)
            if not title.startswith(("\\", "/")):
                setattr(codes, title.upper(), code)

    def doc(code):
        names = ", ".join(f"``{n}``" for n in _codes[code])
        return "* %d: %s" % (code, names)

    global __doc__
    __doc__ = (
        __doc__ + "\n" + "\n".join(doc(code) for code in sorted(_codes))
        if __doc__ is not None
        else None
    )


_init()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/packages.py
# ========================================================
import sys

# This code exists for backwards compatibility reasons.
# I don't like it either. Just look the other way. :)

for package in ('urllib3', 'idna', 'chardet'):
    vendored_package = "pip._vendor." + package
    locals()[package] = __import__(vendored_package)
    # This traversal is apparently necessary such that the identities are
    # preserved (requests.packages.urllib3.* is urllib3.*)
    for mod in list(sys.modules):
        if mod == vendored_package or mod.startswith(vendored_package + '.'):
            unprefixed_mod = mod[len("pip._vendor."):]
            sys.modules['pip._vendor.requests.packages.' + unprefixed_mod] = sys.modules[mod]

# Kinda cool, though, right?


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/_internal_utils.py
# ========================================================
"""
requests._internal_utils
~~~~~~~~~~~~~~

Provides utility functions that are consumed internally by Requests
which depend on extremely few external helpers (such as compat)
"""
import re

from .compat import builtin_str

_VALID_HEADER_NAME_RE_BYTE = re.compile(rb"^[^:\s][^:\r\n]*$")
_VALID_HEADER_NAME_RE_STR = re.compile(r"^[^:\s][^:\r\n]*$")
_VALID_HEADER_VALUE_RE_BYTE = re.compile(rb"^\S[^\r\n]*$|^$")
_VALID_HEADER_VALUE_RE_STR = re.compile(r"^\S[^\r\n]*$|^$")

_HEADER_VALIDATORS_STR = (_VALID_HEADER_NAME_RE_STR, _VALID_HEADER_VALUE_RE_STR)
_HEADER_VALIDATORS_BYTE = (_VALID_HEADER_NAME_RE_BYTE, _VALID_HEADER_VALUE_RE_BYTE)
HEADER_VALIDATORS = {
    bytes: _HEADER_VALIDATORS_BYTE,
    str: _HEADER_VALIDATORS_STR,
}


def to_native_string(string, encoding="ascii"):
    """Given a string object, regardless of type, returns a representation of
    that string in the native string type, encoding and decoding where
    necessary. This assumes ASCII unless told otherwise.
    """
    if isinstance(string, builtin_str):
        out = string
    else:
        out = string.decode(encoding)

    return out


def unicode_is_ascii(u_string):
    """Determine if unicode string only contains ASCII characters.

    :param str u_string: unicode string to check. Must be unicode
        and not Python 2 `str`.
    :rtype: bool
    """
    assert isinstance(u_string, str)
    try:
        u_string.encode("ascii")
        return True
    except UnicodeEncodeError:
        return False


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/auth.py
# ========================================================
"""
requests.auth
~~~~~~~~~~~~~

This module contains the authentication handlers for Requests.
"""

import hashlib
import os
import re
import threading
import time
import warnings
from base64 import b64encode

from ._internal_utils import to_native_string
from .compat import basestring, str, urlparse
from .cookies import extract_cookies_to_jar
from .utils import parse_dict_header

CONTENT_TYPE_FORM_URLENCODED = "application/x-www-form-urlencoded"
CONTENT_TYPE_MULTI_PART = "multipart/form-data"


def _basic_auth_str(username, password):
    """Returns a Basic Auth string."""

    # "I want us to put a big-ol' comment on top of it that
    # says that this behaviour is dumb but we need to preserve
    # it because people are relying on it."
    #    - Lukasa
    #
    # These are here solely to maintain backwards compatibility
    # for things like ints. This will be removed in 3.0.0.
    if not isinstance(username, basestring):
        warnings.warn(
            "Non-string usernames will no longer be supported in Requests "
            "3.0.0. Please convert the object you've passed in ({!r}) to "
            "a string or bytes object in the near future to avoid "
            "problems.".format(username),
            category=DeprecationWarning,
        )
        username = str(username)

    if not isinstance(password, basestring):
        warnings.warn(
            "Non-string passwords will no longer be supported in Requests "
            "3.0.0. Please convert the object you've passed in ({!r}) to "
            "a string or bytes object in the near future to avoid "
            "problems.".format(type(password)),
            category=DeprecationWarning,
        )
        password = str(password)
    # -- End Removal --

    if isinstance(username, str):
        username = username.encode("latin1")

    if isinstance(password, str):
        password = password.encode("latin1")

    authstr = "Basic " + to_native_string(
        b64encode(b":".join((username, password))).strip()
    )

    return authstr


class AuthBase:
    """Base class that all auth implementations derive from"""

    def __call__(self, r):
        raise NotImplementedError("Auth hooks must be callable.")


class HTTPBasicAuth(AuthBase):
    """Attaches HTTP Basic Authentication to the given Request object."""

    def __init__(self, username, password):
        self.username = username
        self.password = password

    def __eq__(self, other):
        return all(
            [
                self.username == getattr(other, "username", None),
                self.password == getattr(other, "password", None),
            ]
        )

    def __ne__(self, other):
        return not self == other

    def __call__(self, r):
        r.headers["Authorization"] = _basic_auth_str(self.username, self.password)
        return r


class HTTPProxyAuth(HTTPBasicAuth):
    """Attaches HTTP Proxy Authentication to a given Request object."""

    def __call__(self, r):
        r.headers["Proxy-Authorization"] = _basic_auth_str(self.username, self.password)
        return r


class HTTPDigestAuth(AuthBase):
    """Attaches HTTP Digest Authentication to the given Request object."""

    def __init__(self, username, password):
        self.username = username
        self.password = password
        # Keep state in per-thread local storage
        self._thread_local = threading.local()

    def init_per_thread_state(self):
        # Ensure state is initialized just once per-thread
        if not hasattr(self._thread_local, "init"):
            self._thread_local.init = True
            self._thread_local.last_nonce = ""
            self._thread_local.nonce_count = 0
            self._thread_local.chal = {}
            self._thread_local.pos = None
            self._thread_local.num_401_calls = None

    def build_digest_header(self, method, url):
        """
        :rtype: str
        """

        realm = self._thread_local.chal["realm"]
        nonce = self._thread_local.chal["nonce"]
        qop = self._thread_local.chal.get("qop")
        algorithm = self._thread_local.chal.get("algorithm")
        opaque = self._thread_local.chal.get("opaque")
        hash_utf8 = None

        if algorithm is None:
            _algorithm = "MD5"
        else:
            _algorithm = algorithm.upper()
        # lambdas assume digest modules are imported at the top level
        if _algorithm == "MD5" or _algorithm == "MD5-SESS":

            def md5_utf8(x):
                if isinstance(x, str):
                    x = x.encode("utf-8")
                return hashlib.md5(x).hexdigest()

            hash_utf8 = md5_utf8
        elif _algorithm == "SHA":

            def sha_utf8(x):
                if isinstance(x, str):
                    x = x.encode("utf-8")
                return hashlib.sha1(x).hexdigest()

            hash_utf8 = sha_utf8
        elif _algorithm == "SHA-256":

            def sha256_utf8(x):
                if isinstance(x, str):
                    x = x.encode("utf-8")
                return hashlib.sha256(x).hexdigest()

            hash_utf8 = sha256_utf8
        elif _algorithm == "SHA-512":

            def sha512_utf8(x):
                if isinstance(x, str):
                    x = x.encode("utf-8")
                return hashlib.sha512(x).hexdigest()

            hash_utf8 = sha512_utf8

        KD = lambda s, d: hash_utf8(f"{s}:{d}")  # noqa:E731

        if hash_utf8 is None:
            return None

        # XXX not implemented yet
        entdig = None
        p_parsed = urlparse(url)
        #: path is request-uri defined in RFC 2616 which should not be empty
        path = p_parsed.path or "/"
        if p_parsed.query:
            path += f"?{p_parsed.query}"

        A1 = f"{self.username}:{realm}:{self.password}"
        A2 = f"{method}:{path}"

        HA1 = hash_utf8(A1)
        HA2 = hash_utf8(A2)

        if nonce == self._thread_local.last_nonce:
            self._thread_local.nonce_count += 1
        else:
            self._thread_local.nonce_count = 1
        ncvalue = f"{self._thread_local.nonce_count:08x}"
        s = str(self._thread_local.nonce_count).encode("utf-8")
        s += nonce.encode("utf-8")
        s += time.ctime().encode("utf-8")
        s += os.urandom(8)

        cnonce = hashlib.sha1(s).hexdigest()[:16]
        if _algorithm == "MD5-SESS":
            HA1 = hash_utf8(f"{HA1}:{nonce}:{cnonce}")

        if not qop:
            respdig = KD(HA1, f"{nonce}:{HA2}")
        elif qop == "auth" or "auth" in qop.split(","):
            noncebit = f"{nonce}:{ncvalue}:{cnonce}:auth:{HA2}"
            respdig = KD(HA1, noncebit)
        else:
            # XXX handle auth-int.
            return None

        self._thread_local.last_nonce = nonce

        # XXX should the partial digests be encoded too?
        base = (
            f'username="{self.username}", realm="{realm}", nonce="{nonce}", '
            f'uri="{path}", response="{respdig}"'
        )
        if opaque:
            base += f', opaque="{opaque}"'
        if algorithm:
            base += f', algorithm="{algorithm}"'
        if entdig:
            base += f', digest="{entdig}"'
        if qop:
            base += f', qop="auth", nc={ncvalue}, cnonce="{cnonce}"'

        return f"Digest {base}"

    def handle_redirect(self, r, **kwargs):
        """Reset num_401_calls counter on redirects."""
        if r.is_redirect:
            self._thread_local.num_401_calls = 1

    def handle_401(self, r, **kwargs):
        """
        Takes the given response and tries digest-auth, if needed.

        :rtype: requests.Response
        """

        # If response is not 4xx, do not auth
        # See https://github.com/psf/requests/issues/3772
        if not 400 <= r.status_code < 500:
            self._thread_local.num_401_calls = 1
            return r

        if self._thread_local.pos is not None:
            # Rewind the file position indicator of the body to where
            # it was to resend the request.
            r.request.body.seek(self._thread_local.pos)
        s_auth = r.headers.get("www-authenticate", "")

        if "digest" in s_auth.lower() and self._thread_local.num_401_calls < 2:

            self._thread_local.num_401_calls += 1
            pat = re.compile(r"digest ", flags=re.IGNORECASE)
            self._thread_local.chal = parse_dict_header(pat.sub("", s_auth, count=1))

            # Consume content and release the original connection
            # to allow our new request to reuse the same one.
            r.content
            r.close()
            prep = r.request.copy()
            extract_cookies_to_jar(prep._cookies, r.request, r.raw)
            prep.prepare_cookies(prep._cookies)

            prep.headers["Authorization"] = self.build_digest_header(
                prep.method, prep.url
            )
            _r = r.connection.send(prep, **kwargs)
            _r.history.append(r)
            _r.request = prep

            return _r

        self._thread_local.num_401_calls = 1
        return r

    def __call__(self, r):
        # Initialize per-thread state, if needed
        self.init_per_thread_state()
        # If we have a saved nonce, skip the 401
        if self._thread_local.last_nonce:
            r.headers["Authorization"] = self.build_digest_header(r.method, r.url)
        try:
            self._thread_local.pos = r.body.tell()
        except AttributeError:
            # In the case of HTTPDigestAuth being reused and the body of
            # the previous request was a file-like object, pos has the
            # file position of the previous body. Ensure it's set to
            # None.
            self._thread_local.pos = None
        r.register_hook("response", self.handle_401)
        r.register_hook("response", self.handle_redirect)
        self._thread_local.num_401_calls = 1

        return r

    def __eq__(self, other):
        return all(
            [
                self.username == getattr(other, "username", None),
                self.password == getattr(other, "password", None),
            ]
        )

    def __ne__(self, other):
        return not self == other


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/adapters.py
# ========================================================
"""
requests.adapters
~~~~~~~~~~~~~~~~~

This module contains the transport adapters that Requests uses to define
and maintain connections.
"""

import os.path
import socket  # noqa: F401

from pip._vendor.urllib3.exceptions import ClosedPoolError, ConnectTimeoutError
from pip._vendor.urllib3.exceptions import HTTPError as _HTTPError
from pip._vendor.urllib3.exceptions import InvalidHeader as _InvalidHeader
from pip._vendor.urllib3.exceptions import (
    LocationValueError,
    MaxRetryError,
    NewConnectionError,
    ProtocolError,
)
from pip._vendor.urllib3.exceptions import ProxyError as _ProxyError
from pip._vendor.urllib3.exceptions import ReadTimeoutError, ResponseError
from pip._vendor.urllib3.exceptions import SSLError as _SSLError
from pip._vendor.urllib3.poolmanager import PoolManager, proxy_from_url
from pip._vendor.urllib3.util import Timeout as TimeoutSauce
from pip._vendor.urllib3.util import parse_url
from pip._vendor.urllib3.util.retry import Retry

from .auth import _basic_auth_str
from .compat import basestring, urlparse
from .cookies import extract_cookies_to_jar
from .exceptions import (
    ConnectionError,
    ConnectTimeout,
    InvalidHeader,
    InvalidProxyURL,
    InvalidSchema,
    InvalidURL,
    ProxyError,
    ReadTimeout,
    RetryError,
    SSLError,
)
from .models import Response
from .structures import CaseInsensitiveDict
from .utils import (
    DEFAULT_CA_BUNDLE_PATH,
    extract_zipped_paths,
    get_auth_from_url,
    get_encoding_from_headers,
    prepend_scheme_if_needed,
    select_proxy,
    urldefragauth,
)

try:
    from pip._vendor.urllib3.contrib.socks import SOCKSProxyManager
except ImportError:

    def SOCKSProxyManager(*args, **kwargs):
        raise InvalidSchema("Missing dependencies for SOCKS support.")


DEFAULT_POOLBLOCK = False
DEFAULT_POOLSIZE = 10
DEFAULT_RETRIES = 0
DEFAULT_POOL_TIMEOUT = None


class BaseAdapter:
    """The Base Transport Adapter"""

    def __init__(self):
        super().__init__()

    def send(
        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
    ):
        """Sends PreparedRequest object. Returns Response object.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple
        :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        """
        raise NotImplementedError

    def close(self):
        """Cleans up adapter specific items."""
        raise NotImplementedError


class HTTPAdapter(BaseAdapter):
    """The built-in HTTP Adapter for urllib3.

    Provides a general-case interface for Requests sessions to contact HTTP and
    HTTPS urls by implementing the Transport Adapter interface. This class will
    usually be created by the :class:`Session <Session>` class under the
    covers.

    :param pool_connections: The number of urllib3 connection pools to cache.
    :param pool_maxsize: The maximum number of connections to save in the pool.
    :param max_retries: The maximum number of retries each connection
        should attempt. Note, this applies only to failed DNS lookups, socket
        connections and connection timeouts, never to requests where data has
        made it to the server. By default, Requests does not retry failed
        connections. If you need granular control over the conditions under
        which we retry a request, import urllib3's ``Retry`` class and pass
        that instead.
    :param pool_block: Whether the connection pool should block for connections.

    Usage::

      >>> import requests
      >>> s = requests.Session()
      >>> a = requests.adapters.HTTPAdapter(max_retries=3)
      >>> s.mount('http://', a)
    """

    __attrs__ = [
        "max_retries",
        "config",
        "_pool_connections",
        "_pool_maxsize",
        "_pool_block",
    ]

    def __init__(
        self,
        pool_connections=DEFAULT_POOLSIZE,
        pool_maxsize=DEFAULT_POOLSIZE,
        max_retries=DEFAULT_RETRIES,
        pool_block=DEFAULT_POOLBLOCK,
    ):
        if max_retries == DEFAULT_RETRIES:
            self.max_retries = Retry(0, read=False)
        else:
            self.max_retries = Retry.from_int(max_retries)
        self.config = {}
        self.proxy_manager = {}

        super().__init__()

        self._pool_connections = pool_connections
        self._pool_maxsize = pool_maxsize
        self._pool_block = pool_block

        self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)

    def __getstate__(self):
        return {attr: getattr(self, attr, None) for attr in self.__attrs__}

    def __setstate__(self, state):
        # Can't handle by adding 'proxy_manager' to self.__attrs__ because
        # self.poolmanager uses a lambda function, which isn't pickleable.
        self.proxy_manager = {}
        self.config = {}

        for attr, value in state.items():
            setattr(self, attr, value)

        self.init_poolmanager(
            self._pool_connections, self._pool_maxsize, block=self._pool_block
        )

    def init_poolmanager(
        self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs
    ):
        """Initializes a urllib3 PoolManager.

        This method should not be called from user code, and is only
        exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param connections: The number of urllib3 connection pools to cache.
        :param maxsize: The maximum number of connections to save in the pool.
        :param block: Block when no free connections are available.
        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
        """
        # save these values for pickling
        self._pool_connections = connections
        self._pool_maxsize = maxsize
        self._pool_block = block

        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            **pool_kwargs,
        )

    def proxy_manager_for(self, proxy, **proxy_kwargs):
        """Return urllib3 ProxyManager for the given proxy.

        This method should not be called from user code, and is only
        exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param proxy: The proxy to return a urllib3 ProxyManager for.
        :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
        :returns: ProxyManager
        :rtype: urllib3.ProxyManager
        """
        if proxy in self.proxy_manager:
            manager = self.proxy_manager[proxy]
        elif proxy.lower().startswith("socks"):
            username, password = get_auth_from_url(proxy)
            manager = self.proxy_manager[proxy] = SOCKSProxyManager(
                proxy,
                username=username,
                password=password,
                num_pools=self._pool_connections,
                maxsize=self._pool_maxsize,
                block=self._pool_block,
                **proxy_kwargs,
            )
        else:
            proxy_headers = self.proxy_headers(proxy)
            manager = self.proxy_manager[proxy] = proxy_from_url(
                proxy,
                proxy_headers=proxy_headers,
                num_pools=self._pool_connections,
                maxsize=self._pool_maxsize,
                block=self._pool_block,
                **proxy_kwargs,
            )

        return manager

    def cert_verify(self, conn, url, verify, cert):
        """Verify a SSL certificate. This method should not be called from user
        code, and is only exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param conn: The urllib3 connection object associated with the cert.
        :param url: The requested URL.
        :param verify: Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use
        :param cert: The SSL certificate to verify.
        """
        if url.lower().startswith("https") and verify:

            cert_loc = None

            # Allow self-specified cert location.
            if verify is not True:
                cert_loc = verify

            if not cert_loc:
                cert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)

            if not cert_loc or not os.path.exists(cert_loc):
                raise OSError(
                    f"Could not find a suitable TLS CA certificate bundle, "
                    f"invalid path: {cert_loc}"
                )

            conn.cert_reqs = "CERT_REQUIRED"

            if not os.path.isdir(cert_loc):
                conn.ca_certs = cert_loc
            else:
                conn.ca_cert_dir = cert_loc
        else:
            conn.cert_reqs = "CERT_NONE"
            conn.ca_certs = None
            conn.ca_cert_dir = None

        if cert:
            if not isinstance(cert, basestring):
                conn.cert_file = cert[0]
                conn.key_file = cert[1]
            else:
                conn.cert_file = cert
                conn.key_file = None
            if conn.cert_file and not os.path.exists(conn.cert_file):
                raise OSError(
                    f"Could not find the TLS certificate file, "
                    f"invalid path: {conn.cert_file}"
                )
            if conn.key_file and not os.path.exists(conn.key_file):
                raise OSError(
                    f"Could not find the TLS key file, invalid path: {conn.key_file}"
                )

    def build_response(self, req, resp):
        """Builds a :class:`Response <requests.Response>` object from a urllib3
        response. This should not be called from user code, and is only exposed
        for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`

        :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
        :param resp: The urllib3 response object.
        :rtype: requests.Response
        """
        response = Response()

        # Fallback to None if there's no status_code, for whatever reason.
        response.status_code = getattr(resp, "status", None)

        # Make headers case-insensitive.
        response.headers = CaseInsensitiveDict(getattr(resp, "headers", {}))

        # Set encoding.
        response.encoding = get_encoding_from_headers(response.headers)
        response.raw = resp
        response.reason = response.raw.reason

        if isinstance(req.url, bytes):
            response.url = req.url.decode("utf-8")
        else:
            response.url = req.url

        # Add new cookies from the server.
        extract_cookies_to_jar(response.cookies, req, resp)

        # Give the Response some context.
        response.request = req
        response.connection = self

        return response

    def get_connection(self, url, proxies=None):
        """Returns a urllib3 connection for the given URL. This should not be
        called from user code, and is only exposed for use when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param url: The URL to connect to.
        :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
        :rtype: urllib3.ConnectionPool
        """
        proxy = select_proxy(url, proxies)

        if proxy:
            proxy = prepend_scheme_if_needed(proxy, "http")
            proxy_url = parse_url(proxy)
            if not proxy_url.host:
                raise InvalidProxyURL(
                    "Please check proxy URL. It is malformed "
                    "and could be missing the host."
                )
            proxy_manager = self.proxy_manager_for(proxy)
            conn = proxy_manager.connection_from_url(url)
        else:
            # Only scheme should be lower case
            parsed = urlparse(url)
            url = parsed.geturl()
            conn = self.poolmanager.connection_from_url(url)

        return conn

    def close(self):
        """Disposes of any internal state.

        Currently, this closes the PoolManager and any active ProxyManager,
        which closes any pooled connections.
        """
        self.poolmanager.clear()
        for proxy in self.proxy_manager.values():
            proxy.clear()

    def request_url(self, request, proxies):
        """Obtain the url to use when making the final request.

        If the message is being sent through a HTTP proxy, the full URL has to
        be used. Otherwise, we should only use the path portion of the URL.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.
        :rtype: str
        """
        proxy = select_proxy(request.url, proxies)
        scheme = urlparse(request.url).scheme

        is_proxied_http_request = proxy and scheme != "https"
        using_socks_proxy = False
        if proxy:
            proxy_scheme = urlparse(proxy).scheme.lower()
            using_socks_proxy = proxy_scheme.startswith("socks")

        url = request.path_url
        if is_proxied_http_request and not using_socks_proxy:
            url = urldefragauth(request.url)

        return url

    def add_headers(self, request, **kwargs):
        """Add any headers needed by the connection. As of v2.0 this does
        nothing by default, but is left for overriding by users that subclass
        the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
        :param kwargs: The keyword arguments from the call to send().
        """
        pass

    def proxy_headers(self, proxy):
        """Returns a dictionary of the headers to add to any request sent
        through a proxy. This works with urllib3 magic to ensure that they are
        correctly sent to the proxy, rather than in a tunnelled request if
        CONNECT is being used.

        This should not be called from user code, and is only exposed for use
        when subclassing the
        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

        :param proxy: The url of the proxy being used for this request.
        :rtype: dict
        """
        headers = {}
        username, password = get_auth_from_url(proxy)

        if username:
            headers["Proxy-Authorization"] = _basic_auth_str(username, password)

        return headers

    def send(
        self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
    ):
        """Sends PreparedRequest object. Returns Response object.

        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
        :param stream: (optional) Whether to stream the request content.
        :param timeout: (optional) How long to wait for the server to send
            data before giving up, as a float, or a :ref:`(connect timeout,
            read timeout) <timeouts>` tuple.
        :type timeout: float or tuple or urllib3 Timeout object
        :param verify: (optional) Either a boolean, in which case it controls whether
            we verify the server's TLS certificate, or a string, in which case it
            must be a path to a CA bundle to use
        :param cert: (optional) Any user-provided SSL certificate to be trusted.
        :param proxies: (optional) The proxies dictionary to apply to the request.
        :rtype: requests.Response
        """

        try:
            conn = self.get_connection(request.url, proxies)
        except LocationValueError as e:
            raise InvalidURL(e, request=request)

        self.cert_verify(conn, request.url, verify, cert)
        url = self.request_url(request, proxies)
        self.add_headers(
            request,
            stream=stream,
            timeout=timeout,
            verify=verify,
            cert=cert,
            proxies=proxies,
        )

        chunked = not (request.body is None or "Content-Length" in request.headers)

        if isinstance(timeout, tuple):
            try:
                connect, read = timeout
                timeout = TimeoutSauce(connect=connect, read=read)
            except ValueError:
                raise ValueError(
                    f"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
                    f"or a single float to set both timeouts to the same value."
                )
        elif isinstance(timeout, TimeoutSauce):
            pass
        else:
            timeout = TimeoutSauce(connect=timeout, read=timeout)

        try:
            resp = conn.urlopen(
                method=request.method,
                url=url,
                body=request.body,
                headers=request.headers,
                redirect=False,
                assert_same_host=False,
                preload_content=False,
                decode_content=False,
                retries=self.max_retries,
                timeout=timeout,
                chunked=chunked,
            )

        except (ProtocolError, OSError) as err:
            raise ConnectionError(err, request=request)

        except MaxRetryError as e:
            if isinstance(e.reason, ConnectTimeoutError):
                # TODO: Remove this in 3.0.0: see #2811
                if not isinstance(e.reason, NewConnectionError):
                    raise ConnectTimeout(e, request=request)

            if isinstance(e.reason, ResponseError):
                raise RetryError(e, request=request)

            if isinstance(e.reason, _ProxyError):
                raise ProxyError(e, request=request)

            if isinstance(e.reason, _SSLError):
                # This branch is for urllib3 v1.22 and later.
                raise SSLError(e, request=request)

            raise ConnectionError(e, request=request)

        except ClosedPoolError as e:
            raise ConnectionError(e, request=request)

        except _ProxyError as e:
            raise ProxyError(e)

        except (_SSLError, _HTTPError) as e:
            if isinstance(e, _SSLError):
                # This branch is for urllib3 versions earlier than v1.22
                raise SSLError(e, request=request)
            elif isinstance(e, ReadTimeoutError):
                raise ReadTimeout(e, request=request)
            elif isinstance(e, _InvalidHeader):
                raise InvalidHeader(e, request=request)
            else:
                raise

        return self.build_response(request, resp)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/compat.py
# ========================================================
"""
requests.compat
~~~~~~~~~~~~~~~

This module previously handled import compatibility issues
between Python 2 and Python 3. It remains for backwards
compatibility until the next major version.
"""

from pip._vendor import chardet

import sys

# -------
# Pythons
# -------

# Syntax sugar.
_ver = sys.version_info

#: Python 2.x?
is_py2 = _ver[0] == 2

#: Python 3.x?
is_py3 = _ver[0] == 3

# Note: We've patched out simplejson support in pip because it prevents
#       upgrading simplejson on Windows.
import json
from json import JSONDecodeError

# Keep OrderedDict for backwards compatibility.
from collections import OrderedDict
from collections.abc import Callable, Mapping, MutableMapping
from http import cookiejar as cookielib
from http.cookies import Morsel
from io import StringIO

# --------------
# Legacy Imports
# --------------
from urllib.parse import (
    quote,
    quote_plus,
    unquote,
    unquote_plus,
    urldefrag,
    urlencode,
    urljoin,
    urlparse,
    urlsplit,
    urlunparse,
)
from urllib.request import (
    getproxies,
    getproxies_environment,
    parse_http_list,
    proxy_bypass,
    proxy_bypass_environment,
)

builtin_str = str
str = str
bytes = bytes
basestring = (str, bytes)
numeric_types = (int, float)
integer_types = (int,)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/cookies.py
# ========================================================
"""
requests.cookies
~~~~~~~~~~~~~~~~

Compatibility code to be able to use `cookielib.CookieJar` with requests.

requests.utils imports from here, so be careful with imports.
"""

import calendar
import copy
import time

from ._internal_utils import to_native_string
from .compat import Morsel, MutableMapping, cookielib, urlparse, urlunparse

try:
    import threading
except ImportError:
    import dummy_threading as threading


class MockRequest:
    """Wraps a `requests.Request` to mimic a `urllib2.Request`.

    The code in `cookielib.CookieJar` expects this interface in order to correctly
    manage cookie policies, i.e., determine whether a cookie can be set, given the
    domains of the request and the cookie.

    The original request object is read-only. The client is responsible for collecting
    the new headers via `get_new_headers()` and interpreting them appropriately. You
    probably want `get_cookie_header`, defined below.
    """

    def __init__(self, request):
        self._r = request
        self._new_headers = {}
        self.type = urlparse(self._r.url).scheme

    def get_type(self):
        return self.type

    def get_host(self):
        return urlparse(self._r.url).netloc

    def get_origin_req_host(self):
        return self.get_host()

    def get_full_url(self):
        # Only return the response's URL if the user hadn't set the Host
        # header
        if not self._r.headers.get("Host"):
            return self._r.url
        # If they did set it, retrieve it and reconstruct the expected domain
        host = to_native_string(self._r.headers["Host"], encoding="utf-8")
        parsed = urlparse(self._r.url)
        # Reconstruct the URL as we expect it
        return urlunparse(
            [
                parsed.scheme,
                host,
                parsed.path,
                parsed.params,
                parsed.query,
                parsed.fragment,
            ]
        )

    def is_unverifiable(self):
        return True

    def has_header(self, name):
        return name in self._r.headers or name in self._new_headers

    def get_header(self, name, default=None):
        return self._r.headers.get(name, self._new_headers.get(name, default))

    def add_header(self, key, val):
        """cookielib has no legitimate use for this method; add it back if you find one."""
        raise NotImplementedError(
            "Cookie headers should be added with add_unredirected_header()"
        )

    def add_unredirected_header(self, name, value):
        self._new_headers[name] = value

    def get_new_headers(self):
        return self._new_headers

    @property
    def unverifiable(self):
        return self.is_unverifiable()

    @property
    def origin_req_host(self):
        return self.get_origin_req_host()

    @property
    def host(self):
        return self.get_host()


class MockResponse:
    """Wraps a `httplib.HTTPMessage` to mimic a `urllib.addinfourl`.

    ...what? Basically, expose the parsed HTTP headers from the server response
    the way `cookielib` expects to see them.
    """

    def __init__(self, headers):
        """Make a MockResponse for `cookielib` to read.

        :param headers: a httplib.HTTPMessage or analogous carrying the headers
        """
        self._headers = headers

    def info(self):
        return self._headers

    def getheaders(self, name):
        self._headers.getheaders(name)


def extract_cookies_to_jar(jar, request, response):
    """Extract the cookies from the response into a CookieJar.

    :param jar: cookielib.CookieJar (not necessarily a RequestsCookieJar)
    :param request: our own requests.Request object
    :param response: urllib3.HTTPResponse object
    """
    if not (hasattr(response, "_original_response") and response._original_response):
        return
    # the _original_response field is the wrapped httplib.HTTPResponse object,
    req = MockRequest(request)
    # pull out the HTTPMessage with the headers and put it in the mock:
    res = MockResponse(response._original_response.msg)
    jar.extract_cookies(res, req)


def get_cookie_header(jar, request):
    """
    Produce an appropriate Cookie header string to be sent with `request`, or None.

    :rtype: str
    """
    r = MockRequest(request)
    jar.add_cookie_header(r)
    return r.get_new_headers().get("Cookie")


def remove_cookie_by_name(cookiejar, name, domain=None, path=None):
    """Unsets a cookie by name, by default over all domains and paths.

    Wraps CookieJar.clear(), is O(n).
    """
    clearables = []
    for cookie in cookiejar:
        if cookie.name != name:
            continue
        if domain is not None and domain != cookie.domain:
            continue
        if path is not None and path != cookie.path:
            continue
        clearables.append((cookie.domain, cookie.path, cookie.name))

    for domain, path, name in clearables:
        cookiejar.clear(domain, path, name)


class CookieConflictError(RuntimeError):
    """There are two cookies that meet the criteria specified in the cookie jar.
    Use .get and .set and include domain and path args in order to be more specific.
    """


class RequestsCookieJar(cookielib.CookieJar, MutableMapping):
    """Compatibility class; is a cookielib.CookieJar, but exposes a dict
    interface.

    This is the CookieJar we create by default for requests and sessions that
    don't specify one, since some clients may expect response.cookies and
    session.cookies to support dict operations.

    Requests does not use the dict interface internally; it's just for
    compatibility with external client code. All requests code should work
    out of the box with externally provided instances of ``CookieJar``, e.g.
    ``LWPCookieJar`` and ``FileCookieJar``.

    Unlike a regular CookieJar, this class is pickleable.

    .. warning:: dictionary operations that are normally O(1) may be O(n).
    """

    def get(self, name, default=None, domain=None, path=None):
        """Dict-like get() that also supports optional domain and path args in
        order to resolve naming collisions from using one cookie jar over
        multiple domains.

        .. warning:: operation is O(n), not O(1).
        """
        try:
            return self._find_no_duplicates(name, domain, path)
        except KeyError:
            return default

    def set(self, name, value, **kwargs):
        """Dict-like set() that also supports optional domain and path args in
        order to resolve naming collisions from using one cookie jar over
        multiple domains.
        """
        # support client code that unsets cookies by assignment of a None value:
        if value is None:
            remove_cookie_by_name(
                self, name, domain=kwargs.get("domain"), path=kwargs.get("path")
            )
            return

        if isinstance(value, Morsel):
            c = morsel_to_cookie(value)
        else:
            c = create_cookie(name, value, **kwargs)
        self.set_cookie(c)
        return c

    def iterkeys(self):
        """Dict-like iterkeys() that returns an iterator of names of cookies
        from the jar.

        .. seealso:: itervalues() and iteritems().
        """
        for cookie in iter(self):
            yield cookie.name

    def keys(self):
        """Dict-like keys() that returns a list of names of cookies from the
        jar.

        .. seealso:: values() and items().
        """
        return list(self.iterkeys())

    def itervalues(self):
        """Dict-like itervalues() that returns an iterator of values of cookies
        from the jar.

        .. seealso:: iterkeys() and iteritems().
        """
        for cookie in iter(self):
            yield cookie.value

    def values(self):
        """Dict-like values() that returns a list of values of cookies from the
        jar.

        .. seealso:: keys() and items().
        """
        return list(self.itervalues())

    def iteritems(self):
        """Dict-like iteritems() that returns an iterator of name-value tuples
        from the jar.

        .. seealso:: iterkeys() and itervalues().
        """
        for cookie in iter(self):
            yield cookie.name, cookie.value

    def items(self):
        """Dict-like items() that returns a list of name-value tuples from the
        jar. Allows client-code to call ``dict(RequestsCookieJar)`` and get a
        vanilla python dict of key value pairs.

        .. seealso:: keys() and values().
        """
        return list(self.iteritems())

    def list_domains(self):
        """Utility method to list all the domains in the jar."""
        domains = []
        for cookie in iter(self):
            if cookie.domain not in domains:
                domains.append(cookie.domain)
        return domains

    def list_paths(self):
        """Utility method to list all the paths in the jar."""
        paths = []
        for cookie in iter(self):
            if cookie.path not in paths:
                paths.append(cookie.path)
        return paths

    def multiple_domains(self):
        """Returns True if there are multiple domains in the jar.
        Returns False otherwise.

        :rtype: bool
        """
        domains = []
        for cookie in iter(self):
            if cookie.domain is not None and cookie.domain in domains:
                return True
            domains.append(cookie.domain)
        return False  # there is only one domain in jar

    def get_dict(self, domain=None, path=None):
        """Takes as an argument an optional domain and path and returns a plain
        old Python dict of name-value pairs of cookies that meet the
        requirements.

        :rtype: dict
        """
        dictionary = {}
        for cookie in iter(self):
            if (domain is None or cookie.domain == domain) and (
                path is None or cookie.path == path
            ):
                dictionary[cookie.name] = cookie.value
        return dictionary

    def __contains__(self, name):
        try:
            return super().__contains__(name)
        except CookieConflictError:
            return True

    def __getitem__(self, name):
        """Dict-like __getitem__() for compatibility with client code. Throws
        exception if there are more than one cookie with name. In that case,
        use the more explicit get() method instead.

        .. warning:: operation is O(n), not O(1).
        """
        return self._find_no_duplicates(name)

    def __setitem__(self, name, value):
        """Dict-like __setitem__ for compatibility with client code. Throws
        exception if there is already a cookie of that name in the jar. In that
        case, use the more explicit set() method instead.
        """
        self.set(name, value)

    def __delitem__(self, name):
        """Deletes a cookie given a name. Wraps ``cookielib.CookieJar``'s
        ``remove_cookie_by_name()``.
        """
        remove_cookie_by_name(self, name)

    def set_cookie(self, cookie, *args, **kwargs):
        if (
            hasattr(cookie.value, "startswith")
            and cookie.value.startswith('"')
            and cookie.value.endswith('"')
        ):
            cookie.value = cookie.value.replace('\\"', "")
        return super().set_cookie(cookie, *args, **kwargs)

    def update(self, other):
        """Updates this jar with cookies from another CookieJar or dict-like"""
        if isinstance(other, cookielib.CookieJar):
            for cookie in other:
                self.set_cookie(copy.copy(cookie))
        else:
            super().update(other)

    def _find(self, name, domain=None, path=None):
        """Requests uses this method internally to get cookie values.

        If there are conflicting cookies, _find arbitrarily chooses one.
        See _find_no_duplicates if you want an exception thrown if there are
        conflicting cookies.

        :param name: a string containing name of cookie
        :param domain: (optional) string containing domain of cookie
        :param path: (optional) string containing path of cookie
        :return: cookie.value
        """
        for cookie in iter(self):
            if cookie.name == name:
                if domain is None or cookie.domain == domain:
                    if path is None or cookie.path == path:
                        return cookie.value

        raise KeyError(f"name={name!r}, domain={domain!r}, path={path!r}")

    def _find_no_duplicates(self, name, domain=None, path=None):
        """Both ``__get_item__`` and ``get`` call this function: it's never
        used elsewhere in Requests.

        :param name: a string containing name of cookie
        :param domain: (optional) string containing domain of cookie
        :param path: (optional) string containing path of cookie
        :raises KeyError: if cookie is not found
        :raises CookieConflictError: if there are multiple cookies
            that match name and optionally domain and path
        :return: cookie.value
        """
        toReturn = None
        for cookie in iter(self):
            if cookie.name == name:
                if domain is None or cookie.domain == domain:
                    if path is None or cookie.path == path:
                        if toReturn is not None:
                            # if there are multiple cookies that meet passed in criteria
                            raise CookieConflictError(
                                f"There are multiple cookies with name, {name!r}"
                            )
                        # we will eventually return this as long as no cookie conflict
                        toReturn = cookie.value

        if toReturn:
            return toReturn
        raise KeyError(f"name={name!r}, domain={domain!r}, path={path!r}")

    def __getstate__(self):
        """Unlike a normal CookieJar, this class is pickleable."""
        state = self.__dict__.copy()
        # remove the unpickleable RLock object
        state.pop("_cookies_lock")
        return state

    def __setstate__(self, state):
        """Unlike a normal CookieJar, this class is pickleable."""
        self.__dict__.update(state)
        if "_cookies_lock" not in self.__dict__:
            self._cookies_lock = threading.RLock()

    def copy(self):
        """Return a copy of this RequestsCookieJar."""
        new_cj = RequestsCookieJar()
        new_cj.set_policy(self.get_policy())
        new_cj.update(self)
        return new_cj

    def get_policy(self):
        """Return the CookiePolicy instance used."""
        return self._policy


def _copy_cookie_jar(jar):
    if jar is None:
        return None

    if hasattr(jar, "copy"):
        # We're dealing with an instance of RequestsCookieJar
        return jar.copy()
    # We're dealing with a generic CookieJar instance
    new_jar = copy.copy(jar)
    new_jar.clear()
    for cookie in jar:
        new_jar.set_cookie(copy.copy(cookie))
    return new_jar


def create_cookie(name, value, **kwargs):
    """Make a cookie from underspecified parameters.

    By default, the pair of `name` and `value` will be set for the domain ''
    and sent on every request (this is sometimes called a "supercookie").
    """
    result = {
        "version": 0,
        "name": name,
        "value": value,
        "port": None,
        "domain": "",
        "path": "/",
        "secure": False,
        "expires": None,
        "discard": True,
        "comment": None,
        "comment_url": None,
        "rest": {"HttpOnly": None},
        "rfc2109": False,
    }

    badargs = set(kwargs) - set(result)
    if badargs:
        raise TypeError(
            f"create_cookie() got unexpected keyword arguments: {list(badargs)}"
        )

    result.update(kwargs)
    result["port_specified"] = bool(result["port"])
    result["domain_specified"] = bool(result["domain"])
    result["domain_initial_dot"] = result["domain"].startswith(".")
    result["path_specified"] = bool(result["path"])

    return cookielib.Cookie(**result)


def morsel_to_cookie(morsel):
    """Convert a Morsel object into a Cookie containing the one k/v pair."""

    expires = None
    if morsel["max-age"]:
        try:
            expires = int(time.time() + int(morsel["max-age"]))
        except ValueError:
            raise TypeError(f"max-age: {morsel['max-age']} must be integer")
    elif morsel["expires"]:
        time_template = "%a, %d-%b-%Y %H:%M:%S GMT"
        expires = calendar.timegm(time.strptime(morsel["expires"], time_template))
    return create_cookie(
        comment=morsel["comment"],
        comment_url=bool(morsel["comment"]),
        discard=False,
        domain=morsel["domain"],
        expires=expires,
        name=morsel.key,
        path=morsel["path"],
        port=None,
        rest={"HttpOnly": morsel["httponly"]},
        rfc2109=False,
        secure=bool(morsel["secure"]),
        value=morsel.value,
        version=morsel["version"] or 0,
    )


def cookiejar_from_dict(cookie_dict, cookiejar=None, overwrite=True):
    """Returns a CookieJar from a key/value dictionary.

    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :param cookiejar: (optional) A cookiejar to add the cookies to.
    :param overwrite: (optional) If False, will not replace cookies
        already in the jar with new ones.
    :rtype: CookieJar
    """
    if cookiejar is None:
        cookiejar = RequestsCookieJar()

    if cookie_dict is not None:
        names_from_jar = [cookie.name for cookie in cookiejar]
        for name in cookie_dict:
            if overwrite or (name not in names_from_jar):
                cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))

    return cookiejar


def merge_cookies(cookiejar, cookies):
    """Add cookies to cookiejar and returns a merged CookieJar.

    :param cookiejar: CookieJar object to add the cookies to.
    :param cookies: Dictionary or CookieJar object to be added.
    :rtype: CookieJar
    """
    if not isinstance(cookiejar, cookielib.CookieJar):
        raise ValueError("You can only merge into CookieJar")

    if isinstance(cookies, dict):
        cookiejar = cookiejar_from_dict(cookies, cookiejar=cookiejar, overwrite=False)
    elif isinstance(cookies, cookielib.CookieJar):
        try:
            cookiejar.update(cookies)
        except AttributeError:
            for cookie_in_jar in cookies:
                cookiejar.set_cookie(cookie_in_jar)

    return cookiejar


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/structures.py
# ========================================================
"""
requests.structures
~~~~~~~~~~~~~~~~~~~

Data structures that power Requests.
"""

from collections import OrderedDict

from .compat import Mapping, MutableMapping


class CaseInsensitiveDict(MutableMapping):
    """A case-insensitive ``dict``-like object.

    Implements all methods and operations of
    ``MutableMapping`` as well as dict's ``copy``. Also
    provides ``lower_items``.

    All keys are expected to be strings. The structure remembers the
    case of the last key to be set, and ``iter(instance)``,
    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``
    will contain case-sensitive keys. However, querying and contains
    testing is case insensitive::

        cid = CaseInsensitiveDict()
        cid['Accept'] = 'application/json'
        cid['aCCEPT'] == 'application/json'  # True
        list(cid) == ['Accept']  # True

    For example, ``headers['content-encoding']`` will return the
    value of a ``'Content-Encoding'`` response header, regardless
    of how the header name was originally stored.

    If the constructor, ``.update``, or equality comparison
    operations are given keys that have equal ``.lower()``s, the
    behavior is undefined.
    """

    def __init__(self, data=None, **kwargs):
        self._store = OrderedDict()
        if data is None:
            data = {}
        self.update(data, **kwargs)

    def __setitem__(self, key, value):
        # Use the lowercased key for lookups, but store the actual
        # key alongside the value.
        self._store[key.lower()] = (key, value)

    def __getitem__(self, key):
        return self._store[key.lower()][1]

    def __delitem__(self, key):
        del self._store[key.lower()]

    def __iter__(self):
        return (casedkey for casedkey, mappedvalue in self._store.values())

    def __len__(self):
        return len(self._store)

    def lower_items(self):
        """Like iteritems(), but with all lowercase keys."""
        return ((lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items())

    def __eq__(self, other):
        if isinstance(other, Mapping):
            other = CaseInsensitiveDict(other)
        else:
            return NotImplemented
        # Compare insensitively
        return dict(self.lower_items()) == dict(other.lower_items())

    # Copy is required
    def copy(self):
        return CaseInsensitiveDict(self._store.values())

    def __repr__(self):
        return str(dict(self.items()))


class LookupDict(dict):
    """Dictionary lookup object."""

    def __init__(self, name=None):
        self.name = name
        super().__init__()

    def __repr__(self):
        return f"<lookup '{self.name}'>"

    def __getitem__(self, key):
        # We allow fall-through here, so values default to None

        return self.__dict__.get(key, None)

    def get(self, key, default=None):
        return self.__dict__.get(key, default)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/help.py
# ========================================================
"""Module containing bug report helper(s)."""

import json
import platform
import ssl
import sys

from pip._vendor import idna
from pip._vendor import urllib3

from . import __version__ as requests_version

charset_normalizer = None

try:
    from pip._vendor import chardet
except ImportError:
    chardet = None

try:
    from pip._vendor.urllib3.contrib import pyopenssl
except ImportError:
    pyopenssl = None
    OpenSSL = None
    cryptography = None
else:
    import cryptography
    import OpenSSL


def _implementation():
    """Return a dict with the Python implementation and version.

    Provide both the name and the version of the Python implementation
    currently running. For example, on CPython 3.10.3 it will return
    {'name': 'CPython', 'version': '3.10.3'}.

    This function works best on CPython and PyPy: in particular, it probably
    doesn't work for Jython or IronPython. Future investigation should be done
    to work out the correct shape of the code for those platforms.
    """
    implementation = platform.python_implementation()

    if implementation == "CPython":
        implementation_version = platform.python_version()
    elif implementation == "PyPy":
        implementation_version = "{}.{}.{}".format(
            sys.pypy_version_info.major,
            sys.pypy_version_info.minor,
            sys.pypy_version_info.micro,
        )
        if sys.pypy_version_info.releaselevel != "final":
            implementation_version = "".join(
                [implementation_version, sys.pypy_version_info.releaselevel]
            )
    elif implementation == "Jython":
        implementation_version = platform.python_version()  # Complete Guess
    elif implementation == "IronPython":
        implementation_version = platform.python_version()  # Complete Guess
    else:
        implementation_version = "Unknown"

    return {"name": implementation, "version": implementation_version}


def info():
    """Generate information for a bug report."""
    try:
        platform_info = {
            "system": platform.system(),
            "release": platform.release(),
        }
    except OSError:
        platform_info = {
            "system": "Unknown",
            "release": "Unknown",
        }

    implementation_info = _implementation()
    urllib3_info = {"version": urllib3.__version__}
    charset_normalizer_info = {"version": None}
    chardet_info = {"version": None}
    if charset_normalizer:
        charset_normalizer_info = {"version": charset_normalizer.__version__}
    if chardet:
        chardet_info = {"version": chardet.__version__}

    pyopenssl_info = {
        "version": None,
        "openssl_version": "",
    }
    if OpenSSL:
        pyopenssl_info = {
            "version": OpenSSL.__version__,
            "openssl_version": f"{OpenSSL.SSL.OPENSSL_VERSION_NUMBER:x}",
        }
    cryptography_info = {
        "version": getattr(cryptography, "__version__", ""),
    }
    idna_info = {
        "version": getattr(idna, "__version__", ""),
    }

    system_ssl = ssl.OPENSSL_VERSION_NUMBER
    system_ssl_info = {"version": f"{system_ssl:x}" if system_ssl is not None else ""}

    return {
        "platform": platform_info,
        "implementation": implementation_info,
        "system_ssl": system_ssl_info,
        "using_pyopenssl": pyopenssl is not None,
        "using_charset_normalizer": chardet is None,
        "pyOpenSSL": pyopenssl_info,
        "urllib3": urllib3_info,
        "chardet": chardet_info,
        "charset_normalizer": charset_normalizer_info,
        "cryptography": cryptography_info,
        "idna": idna_info,
        "requests": {
            "version": requests_version,
        },
    }


def main():
    """Pretty-print the bug information as JSON."""
    print(json.dumps(info(), sort_keys=True, indent=2))


if __name__ == "__main__":
    main()


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/hooks.py
# ========================================================
"""
requests.hooks
~~~~~~~~~~~~~~

This module provides the capabilities for the Requests hooks system.

Available hooks:

``response``:
    The response generated from a Request.
"""
HOOKS = ["response"]


def default_hooks():
    return {event: [] for event in HOOKS}


# TODO: response is the only one


def dispatch_hook(key, hooks, hook_data, **kwargs):
    """Dispatches a hook dictionary on a given piece of data."""
    hooks = hooks or {}
    hooks = hooks.get(key)
    if hooks:
        if hasattr(hooks, "__call__"):
            hooks = [hooks]
        for hook in hooks:
            _hook_data = hook(hook_data, **kwargs)
            if _hook_data is not None:
                hook_data = _hook_data
    return hook_data


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/certs.py
# ========================================================
#!/usr/bin/env python

"""
requests.certs
~~~~~~~~~~~~~~

This module returns the preferred default CA certificate bundle. There is
only one  the one from the certifi package.

If you are packaging Requests, e.g., for a Linux distribution or a managed
environment, you can change the definition of where() to return a separately
packaged CA bundle.
"""

import os

if "_PIP_STANDALONE_CERT" not in os.environ:
    from pip._vendor.certifi import where
else:
    def where():
        return os.environ["_PIP_STANDALONE_CERT"]

if __name__ == "__main__":
    print(where())


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/requests/__version__.py
# ========================================================
# .-. .-. .-. . . .-. .-. .-. .-.
# |(  |-  |.| | | |-  `-.  |  `-.
# ' ' `-' `-`.`-' `-' `-'  '  `-'

__title__ = "requests"
__description__ = "Python HTTP for Humans."
__url__ = "https://requests.readthedocs.io"
__version__ = "2.31.0"
__build__ = 0x023100
__author__ = "Kenneth Reitz"
__author_email__ = "me@kennethreitz.org"
__license__ = "Apache 2.0"
__copyright__ = "Copyright Kenneth Reitz"
__cake__ = "\u2728 \U0001f370 \u2728"


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/tenacity/stop.py
# ========================================================
# Copyright 20162021 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import abc
import typing

from pip._vendor.tenacity import _utils

if typing.TYPE_CHECKING:
    import threading

    from pip._vendor.tenacity import RetryCallState


class stop_base(abc.ABC):
    """Abstract base class for stop strategies."""

    @abc.abstractmethod
    def __call__(self, retry_state: "RetryCallState") -> bool:
        pass

    def __and__(self, other: "stop_base") -> "stop_all":
        return stop_all(self, other)

    def __or__(self, other: "stop_base") -> "stop_any":
        return stop_any(self, other)


StopBaseT = typing.Union[stop_base, typing.Callable[["RetryCallState"], bool]]


class stop_any(stop_base):
    """Stop if any of the stop condition is valid."""

    def __init__(self, *stops: stop_base) -> None:
        self.stops = stops

    def __call__(self, retry_state: "RetryCallState") -> bool:
        return any(x(retry_state) for x in self.stops)


class stop_all(stop_base):
    """Stop if all the stop conditions are valid."""

    def __init__(self, *stops: stop_base) -> None:
        self.stops = stops

    def __call__(self, retry_state: "RetryCallState") -> bool:
        return all(x(retry_state) for x in self.stops)


class _stop_never(stop_base):
    """Never stop."""

    def __call__(self, retry_state: "RetryCallState") -> bool:
        return False


stop_never = _stop_never()


class stop_when_event_set(stop_base):
    """Stop when the given event is set."""

    def __init__(self, event: "threading.Event") -> None:
        self.event = event

    def __call__(self, retry_state: "RetryCallState") -> bool:
        return self.event.is_set()


class stop_after_attempt(stop_base):
    """Stop when the previous attempt >= max_attempt."""

    def __init__(self, max_attempt_number: int) -> None:
        self.max_attempt_number = max_attempt_number

    def __call__(self, retry_state: "RetryCallState") -> bool:
        return retry_state.attempt_number >= self.max_attempt_number


class stop_after_delay(stop_base):
    """Stop when the time from the first attempt >= limit."""

    def __init__(self, max_delay: _utils.time_unit_type) -> None:
        self.max_delay = _utils.to_seconds(max_delay)

    def __call__(self, retry_state: "RetryCallState") -> bool:
        if retry_state.seconds_since_start is None:
            raise RuntimeError("__call__() called but seconds_since_start is not set")
        return retry_state.seconds_since_start >= self.max_delay


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/tenacity/before.py
# ========================================================
# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import typing

from pip._vendor.tenacity import _utils

if typing.TYPE_CHECKING:
    import logging

    from pip._vendor.tenacity import RetryCallState


def before_nothing(retry_state: "RetryCallState") -> None:
    """Before call strategy that does nothing."""


def before_log(logger: "logging.Logger", log_level: int) -> typing.Callable[["RetryCallState"], None]:
    """Before call strategy that logs to some logger the attempt."""

    def log_it(retry_state: "RetryCallState") -> None:
        if retry_state.fn is None:
            # NOTE(sileht): can't really happen, but we must please mypy
            fn_name = "<unknown>"
        else:
            fn_name = _utils.get_callback_name(retry_state.fn)
        logger.log(
            log_level,
            f"Starting call to '{fn_name}', "
            f"this is the {_utils.to_ordinal(retry_state.attempt_number)} time calling it.",
        )

    return log_it


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/tenacity/tornadoweb.py
# ========================================================
# Copyright 2017 Elisey Zanko
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing

from pip._vendor.tenacity import BaseRetrying
from pip._vendor.tenacity import DoAttempt
from pip._vendor.tenacity import DoSleep
from pip._vendor.tenacity import RetryCallState

from tornado import gen

if typing.TYPE_CHECKING:
    from tornado.concurrent import Future

_RetValT = typing.TypeVar("_RetValT")


class TornadoRetrying(BaseRetrying):
    def __init__(self, sleep: "typing.Callable[[float], Future[None]]" = gen.sleep, **kwargs: typing.Any) -> None:
        super().__init__(**kwargs)
        self.sleep = sleep

    @gen.coroutine  # type: ignore[misc]
    def __call__(
        self,
        fn: "typing.Callable[..., typing.Union[typing.Generator[typing.Any, typing.Any, _RetValT], Future[_RetValT]]]",
        *args: typing.Any,
        **kwargs: typing.Any,
    ) -> "typing.Generator[typing.Any, typing.Any, _RetValT]":
        self.begin()

        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
            do = self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                try:
                    result = yield fn(*args, **kwargs)
                except BaseException:  # noqa: B902
                    retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]
                else:
                    retry_state.set_result(result)
            elif isinstance(do, DoSleep):
                retry_state.prepare_for_next_attempt()
                yield self.sleep(do)
            else:
                raise gen.Return(do)


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/tenacity/__init__.py
# ========================================================
# Copyright 2016-2018 Julien Danjou
# Copyright 2017 Elisey Zanko
# Copyright 2016 tienne Bersac
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import functools
import sys
import threading
import time
import typing as t
import warnings
from abc import ABC, abstractmethod
from concurrent import futures
from inspect import iscoroutinefunction

# Import all built-in retry strategies for easier usage.
from .retry import retry_base  # noqa
from .retry import retry_all  # noqa
from .retry import retry_always  # noqa
from .retry import retry_any  # noqa
from .retry import retry_if_exception  # noqa
from .retry import retry_if_exception_type  # noqa
from .retry import retry_if_exception_cause_type  # noqa
from .retry import retry_if_not_exception_type  # noqa
from .retry import retry_if_not_result  # noqa
from .retry import retry_if_result  # noqa
from .retry import retry_never  # noqa
from .retry import retry_unless_exception_type  # noqa
from .retry import retry_if_exception_message  # noqa
from .retry import retry_if_not_exception_message  # noqa

# Import all nap strategies for easier usage.
from .nap import sleep  # noqa
from .nap import sleep_using_event  # noqa

# Import all built-in stop strategies for easier usage.
from .stop import stop_after_attempt  # noqa
from .stop import stop_after_delay  # noqa
from .stop import stop_all  # noqa
from .stop import stop_any  # noqa
from .stop import stop_never  # noqa
from .stop import stop_when_event_set  # noqa

# Import all built-in wait strategies for easier usage.
from .wait import wait_chain  # noqa
from .wait import wait_combine  # noqa
from .wait import wait_exponential  # noqa
from .wait import wait_fixed  # noqa
from .wait import wait_incrementing  # noqa
from .wait import wait_none  # noqa
from .wait import wait_random  # noqa
from .wait import wait_random_exponential  # noqa
from .wait import wait_random_exponential as wait_full_jitter  # noqa
from .wait import wait_exponential_jitter  # noqa

# Import all built-in before strategies for easier usage.
from .before import before_log  # noqa
from .before import before_nothing  # noqa

# Import all built-in after strategies for easier usage.
from .after import after_log  # noqa
from .after import after_nothing  # noqa

# Import all built-in after strategies for easier usage.
from .before_sleep import before_sleep_log  # noqa
from .before_sleep import before_sleep_nothing  # noqa

# Replace a conditional import with a hard-coded None so that pip does
# not attempt to use tornado even if it is present in the environment.
# If tornado is non-None, tenacity will attempt to execute some code
# that is sensitive to the version of tornado, which could break pip
# if an old version is found.
tornado = None  # type: ignore

if t.TYPE_CHECKING:
    import types

    from .retry import RetryBaseT
    from .stop import StopBaseT
    from .wait import WaitBaseT


WrappedFnReturnT = t.TypeVar("WrappedFnReturnT")
WrappedFn = t.TypeVar("WrappedFn", bound=t.Callable[..., t.Any])


class TryAgain(Exception):
    """Always retry the executed function when raised."""


NO_RESULT = object()


class DoAttempt:
    pass


class DoSleep(float):
    pass


class BaseAction:
    """Base class for representing actions to take by retry object.

    Concrete implementations must define:
    - __init__: to initialize all necessary fields
    - REPR_FIELDS: class variable specifying attributes to include in repr(self)
    - NAME: for identification in retry object methods and callbacks
    """

    REPR_FIELDS: t.Sequence[str] = ()
    NAME: t.Optional[str] = None

    def __repr__(self) -> str:
        state_str = ", ".join(f"{field}={getattr(self, field)!r}" for field in self.REPR_FIELDS)
        return f"{self.__class__.__name__}({state_str})"

    def __str__(self) -> str:
        return repr(self)


class RetryAction(BaseAction):
    REPR_FIELDS = ("sleep",)
    NAME = "retry"

    def __init__(self, sleep: t.SupportsFloat) -> None:
        self.sleep = float(sleep)


_unset = object()


def _first_set(first: t.Union[t.Any, object], second: t.Any) -> t.Any:
    return second if first is _unset else first


class RetryError(Exception):
    """Encapsulates the last attempt instance right before giving up."""

    def __init__(self, last_attempt: "Future") -> None:
        self.last_attempt = last_attempt
        super().__init__(last_attempt)

    def reraise(self) -> "t.NoReturn":
        if self.last_attempt.failed:
            raise self.last_attempt.result()
        raise self

    def __str__(self) -> str:
        return f"{self.__class__.__name__}[{self.last_attempt}]"


class AttemptManager:
    """Manage attempt context."""

    def __init__(self, retry_state: "RetryCallState"):
        self.retry_state = retry_state

    def __enter__(self) -> None:
        pass

    def __exit__(
        self,
        exc_type: t.Optional[t.Type[BaseException]],
        exc_value: t.Optional[BaseException],
        traceback: t.Optional["types.TracebackType"],
    ) -> t.Optional[bool]:
        if exc_type is not None and exc_value is not None:
            self.retry_state.set_exception((exc_type, exc_value, traceback))
            return True  # Swallow exception.
        else:
            # We don't have the result, actually.
            self.retry_state.set_result(None)
            return None


class BaseRetrying(ABC):
    def __init__(
        self,
        sleep: t.Callable[[t.Union[int, float]], None] = sleep,
        stop: "StopBaseT" = stop_never,
        wait: "WaitBaseT" = wait_none(),
        retry: "RetryBaseT" = retry_if_exception_type(),
        before: t.Callable[["RetryCallState"], None] = before_nothing,
        after: t.Callable[["RetryCallState"], None] = after_nothing,
        before_sleep: t.Optional[t.Callable[["RetryCallState"], None]] = None,
        reraise: bool = False,
        retry_error_cls: t.Type[RetryError] = RetryError,
        retry_error_callback: t.Optional[t.Callable[["RetryCallState"], t.Any]] = None,
    ):
        self.sleep = sleep
        self.stop = stop
        self.wait = wait
        self.retry = retry
        self.before = before
        self.after = after
        self.before_sleep = before_sleep
        self.reraise = reraise
        self._local = threading.local()
        self.retry_error_cls = retry_error_cls
        self.retry_error_callback = retry_error_callback

    def copy(
        self,
        sleep: t.Union[t.Callable[[t.Union[int, float]], None], object] = _unset,
        stop: t.Union["StopBaseT", object] = _unset,
        wait: t.Union["WaitBaseT", object] = _unset,
        retry: t.Union[retry_base, object] = _unset,
        before: t.Union[t.Callable[["RetryCallState"], None], object] = _unset,
        after: t.Union[t.Callable[["RetryCallState"], None], object] = _unset,
        before_sleep: t.Union[t.Optional[t.Callable[["RetryCallState"], None]], object] = _unset,
        reraise: t.Union[bool, object] = _unset,
        retry_error_cls: t.Union[t.Type[RetryError], object] = _unset,
        retry_error_callback: t.Union[t.Optional[t.Callable[["RetryCallState"], t.Any]], object] = _unset,
    ) -> "BaseRetrying":
        """Copy this object with some parameters changed if needed."""
        return self.__class__(
            sleep=_first_set(sleep, self.sleep),
            stop=_first_set(stop, self.stop),
            wait=_first_set(wait, self.wait),
            retry=_first_set(retry, self.retry),
            before=_first_set(before, self.before),
            after=_first_set(after, self.after),
            before_sleep=_first_set(before_sleep, self.before_sleep),
            reraise=_first_set(reraise, self.reraise),
            retry_error_cls=_first_set(retry_error_cls, self.retry_error_cls),
            retry_error_callback=_first_set(retry_error_callback, self.retry_error_callback),
        )

    def __repr__(self) -> str:
        return (
            f"<{self.__class__.__name__} object at 0x{id(self):x} ("
            f"stop={self.stop}, "
            f"wait={self.wait}, "
            f"sleep={self.sleep}, "
            f"retry={self.retry}, "
            f"before={self.before}, "
            f"after={self.after})>"
        )

    @property
    def statistics(self) -> t.Dict[str, t.Any]:
        """Return a dictionary of runtime statistics.

        This dictionary will be empty when the controller has never been
        ran. When it is running or has ran previously it should have (but
        may not) have useful and/or informational keys and values when
        running is underway and/or completed.

        .. warning:: The keys in this dictionary **should** be some what
                     stable (not changing), but there existence **may**
                     change between major releases as new statistics are
                     gathered or removed so before accessing keys ensure that
                     they actually exist and handle when they do not.

        .. note:: The values in this dictionary are local to the thread
                  running call (so if multiple threads share the same retrying
                  object - either directly or indirectly) they will each have
                  there own view of statistics they have collected (in the
                  future we may provide a way to aggregate the various
                  statistics from each thread).
        """
        try:
            return self._local.statistics  # type: ignore[no-any-return]
        except AttributeError:
            self._local.statistics = t.cast(t.Dict[str, t.Any], {})
            return self._local.statistics

    def wraps(self, f: WrappedFn) -> WrappedFn:
        """Wrap a function for retrying.

        :param f: A function to wraps for retrying.
        """

        @functools.wraps(f)
        def wrapped_f(*args: t.Any, **kw: t.Any) -> t.Any:
            return self(f, *args, **kw)

        def retry_with(*args: t.Any, **kwargs: t.Any) -> WrappedFn:
            return self.copy(*args, **kwargs).wraps(f)

        wrapped_f.retry = self  # type: ignore[attr-defined]
        wrapped_f.retry_with = retry_with  # type: ignore[attr-defined]

        return wrapped_f  # type: ignore[return-value]

    def begin(self) -> None:
        self.statistics.clear()
        self.statistics["start_time"] = time.monotonic()
        self.statistics["attempt_number"] = 1
        self.statistics["idle_for"] = 0

    def iter(self, retry_state: "RetryCallState") -> t.Union[DoAttempt, DoSleep, t.Any]:  # noqa
        fut = retry_state.outcome
        if fut is None:
            if self.before is not None:
                self.before(retry_state)
            return DoAttempt()

        is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)
        if not (is_explicit_retry or self.retry(retry_state)):
            return fut.result()

        if self.after is not None:
            self.after(retry_state)

        self.statistics["delay_since_first_attempt"] = retry_state.seconds_since_start
        if self.stop(retry_state):
            if self.retry_error_callback:
                return self.retry_error_callback(retry_state)
            retry_exc = self.retry_error_cls(fut)
            if self.reraise:
                raise retry_exc.reraise()
            raise retry_exc from fut.exception()

        if self.wait:
            sleep = self.wait(retry_state)
        else:
            sleep = 0.0
        retry_state.next_action = RetryAction(sleep)
        retry_state.idle_for += sleep
        self.statistics["idle_for"] += sleep
        self.statistics["attempt_number"] += 1

        if self.before_sleep is not None:
            self.before_sleep(retry_state)

        return DoSleep(sleep)

    def __iter__(self) -> t.Generator[AttemptManager, None, None]:
        self.begin()

        retry_state = RetryCallState(self, fn=None, args=(), kwargs={})
        while True:
            do = self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                yield AttemptManager(retry_state=retry_state)
            elif isinstance(do, DoSleep):
                retry_state.prepare_for_next_attempt()
                self.sleep(do)
            else:
                break

    @abstractmethod
    def __call__(
        self,
        fn: t.Callable[..., WrappedFnReturnT],
        *args: t.Any,
        **kwargs: t.Any,
    ) -> WrappedFnReturnT:
        pass


class Retrying(BaseRetrying):
    """Retrying controller."""

    def __call__(
        self,
        fn: t.Callable[..., WrappedFnReturnT],
        *args: t.Any,
        **kwargs: t.Any,
    ) -> WrappedFnReturnT:
        self.begin()

        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
            do = self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                try:
                    result = fn(*args, **kwargs)
                except BaseException:  # noqa: B902
                    retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]
                else:
                    retry_state.set_result(result)
            elif isinstance(do, DoSleep):
                retry_state.prepare_for_next_attempt()
                self.sleep(do)
            else:
                return do  # type: ignore[no-any-return]


if sys.version_info[1] >= 9:
    FutureGenericT = futures.Future[t.Any]
else:
    FutureGenericT = futures.Future


class Future(FutureGenericT):
    """Encapsulates a (future or past) attempted call to a target function."""

    def __init__(self, attempt_number: int) -> None:
        super().__init__()
        self.attempt_number = attempt_number

    @property
    def failed(self) -> bool:
        """Return whether a exception is being held in this future."""
        return self.exception() is not None

    @classmethod
    def construct(cls, attempt_number: int, value: t.Any, has_exception: bool) -> "Future":
        """Construct a new Future object."""
        fut = cls(attempt_number)
        if has_exception:
            fut.set_exception(value)
        else:
            fut.set_result(value)
        return fut


class RetryCallState:
    """State related to a single call wrapped with Retrying."""

    def __init__(
        self,
        retry_object: BaseRetrying,
        fn: t.Optional[WrappedFn],
        args: t.Any,
        kwargs: t.Any,
    ) -> None:
        #: Retry call start timestamp
        self.start_time = time.monotonic()
        #: Retry manager object
        self.retry_object = retry_object
        #: Function wrapped by this retry call
        self.fn = fn
        #: Arguments of the function wrapped by this retry call
        self.args = args
        #: Keyword arguments of the function wrapped by this retry call
        self.kwargs = kwargs

        #: The number of the current attempt
        self.attempt_number: int = 1
        #: Last outcome (result or exception) produced by the function
        self.outcome: t.Optional[Future] = None
        #: Timestamp of the last outcome
        self.outcome_timestamp: t.Optional[float] = None
        #: Time spent sleeping in retries
        self.idle_for: float = 0.0
        #: Next action as decided by the retry manager
        self.next_action: t.Optional[RetryAction] = None

    @property
    def seconds_since_start(self) -> t.Optional[float]:
        if self.outcome_timestamp is None:
            return None
        return self.outcome_timestamp - self.start_time

    def prepare_for_next_attempt(self) -> None:
        self.outcome = None
        self.outcome_timestamp = None
        self.attempt_number += 1
        self.next_action = None

    def set_result(self, val: t.Any) -> None:
        ts = time.monotonic()
        fut = Future(self.attempt_number)
        fut.set_result(val)
        self.outcome, self.outcome_timestamp = fut, ts

    def set_exception(
        self, exc_info: t.Tuple[t.Type[BaseException], BaseException, "types.TracebackType| None"]
    ) -> None:
        ts = time.monotonic()
        fut = Future(self.attempt_number)
        fut.set_exception(exc_info[1])
        self.outcome, self.outcome_timestamp = fut, ts

    def __repr__(self) -> str:
        if self.outcome is None:
            result = "none yet"
        elif self.outcome.failed:
            exception = self.outcome.exception()
            result = f"failed ({exception.__class__.__name__} {exception})"
        else:
            result = f"returned {self.outcome.result()}"

        slept = float(round(self.idle_for, 2))
        clsname = self.__class__.__name__
        return f"<{clsname} {id(self)}: attempt #{self.attempt_number}; slept for {slept}; last result: {result}>"


@t.overload
def retry(func: WrappedFn) -> WrappedFn:
    ...


@t.overload
def retry(
    sleep: t.Callable[[t.Union[int, float]], None] = sleep,
    stop: "StopBaseT" = stop_never,
    wait: "WaitBaseT" = wait_none(),
    retry: "RetryBaseT" = retry_if_exception_type(),
    before: t.Callable[["RetryCallState"], None] = before_nothing,
    after: t.Callable[["RetryCallState"], None] = after_nothing,
    before_sleep: t.Optional[t.Callable[["RetryCallState"], None]] = None,
    reraise: bool = False,
    retry_error_cls: t.Type["RetryError"] = RetryError,
    retry_error_callback: t.Optional[t.Callable[["RetryCallState"], t.Any]] = None,
) -> t.Callable[[WrappedFn], WrappedFn]:
    ...


def retry(*dargs: t.Any, **dkw: t.Any) -> t.Any:
    """Wrap a function with a new `Retrying` object.

    :param dargs: positional arguments passed to Retrying object
    :param dkw: keyword arguments passed to the Retrying object
    """
    # support both @retry and @retry() as valid syntax
    if len(dargs) == 1 and callable(dargs[0]):
        return retry()(dargs[0])
    else:

        def wrap(f: WrappedFn) -> WrappedFn:
            if isinstance(f, retry_base):
                warnings.warn(
                    f"Got retry_base instance ({f.__class__.__name__}) as callable argument, "
                    f"this will probably hang indefinitely (did you mean retry={f.__class__.__name__}(...)?)"
                )
            r: "BaseRetrying"
            if iscoroutinefunction(f):
                r = AsyncRetrying(*dargs, **dkw)
            elif tornado and hasattr(tornado.gen, "is_coroutine_function") and tornado.gen.is_coroutine_function(f):
                r = TornadoRetrying(*dargs, **dkw)
            else:
                r = Retrying(*dargs, **dkw)

            return r.wraps(f)

        return wrap


from pip._vendor.tenacity._asyncio import AsyncRetrying  # noqa:E402,I100

if tornado:
    from pip._vendor.tenacity.tornadoweb import TornadoRetrying


__all__ = [
    "retry_base",
    "retry_all",
    "retry_always",
    "retry_any",
    "retry_if_exception",
    "retry_if_exception_type",
    "retry_if_exception_cause_type",
    "retry_if_not_exception_type",
    "retry_if_not_result",
    "retry_if_result",
    "retry_never",
    "retry_unless_exception_type",
    "retry_if_exception_message",
    "retry_if_not_exception_message",
    "sleep",
    "sleep_using_event",
    "stop_after_attempt",
    "stop_after_delay",
    "stop_all",
    "stop_any",
    "stop_never",
    "stop_when_event_set",
    "wait_chain",
    "wait_combine",
    "wait_exponential",
    "wait_fixed",
    "wait_incrementing",
    "wait_none",
    "wait_random",
    "wait_random_exponential",
    "wait_full_jitter",
    "wait_exponential_jitter",
    "before_log",
    "before_nothing",
    "after_log",
    "after_nothing",
    "before_sleep_log",
    "before_sleep_nothing",
    "retry",
    "WrappedFn",
    "TryAgain",
    "NO_RESULT",
    "DoAttempt",
    "DoSleep",
    "BaseAction",
    "RetryAction",
    "RetryError",
    "AttemptManager",
    "BaseRetrying",
    "Retrying",
    "Future",
    "RetryCallState",
    "AsyncRetrying",
]


# ========================================================
# FILE: ./rust_core/.venv/lib/python3.12/site-packages/pip/_vendor/tenacity/wait.py
# ========================================================

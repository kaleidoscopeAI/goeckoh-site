-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./config.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Backward compatibility config shim."""
from __future__ import annotations

from dataclasses import dataclass, field

from core.paths import PathRegistry as Paths
from core.settings import (
    AudioSettings,
    BehaviorSettings,
    HeartSettings,
    SpeechSettings as SpeechModelSettings,
    SystemSettings,
    load_settings,
)


# Provide original name for downstream modules
EchoHeartConfig = HeartSettings


@dataclass(slots=True)
class CompanionConfig(SystemSettings):
    """Alias around SystemSettings to avoid refactoring every import immediately."""

    heart: HeartSettings = field(default_factory=HeartSettings)

    def __post_init__(self) -> None:
        # ensure directories exist
        self.paths.ensure()


def _load_config() -> CompanionConfig:
    settings = load_settings()
    return CompanionConfig(
        child_id=settings.child_id,
        child_name=settings.child_name,
        device=settings.device,
        audio=settings.audio,
        speech=settings.speech,
        llm=settings.llm,
        behavior=settings.behavior,
        paths=settings.paths,
        heart=settings.heart,
    )


CONFIG = _load_config()-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./speech_loop.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import asyncio
import tempfile
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import AsyncGenerator, Iterable, Optional

import numpy as np

from audio.io import AudioIO
from audio.vad import VAD
from emotion.heart import EchoCrystallineHeart
from emotion.llm import LocalLLM
from core.logging import GuidanceLogger, MetricsLogger
from core.models import AttemptRecord, BehaviorEvent
from core.settings import SystemSettings, load_settings
from speech.stt import STT
from speech.text import TextProcessor
from voice.mimic import VoiceMimic
from voice.profile import VoiceProfile
from aba.engine import AbaEngine
from behavior_monitor import BehaviorMonitor
from loop.decision import AgentDecision, Mode
from loop.expressions import ExpressionGear, AudioData, Information, apply_prosody_to_tts


@dataclass(slots=True)
class SpeechLoop:
    settings: SystemSettings = field(default_factory=load_settings)
    _q: asyncio.Queue[np.ndarray] = field(init=False)
    _current_utterance: list[np.ndarray] = field(default_factory=list)
    _last_speech_end_time: float = field(default_factory=float)

    def __post_init__(self) -> None:
        self.audio_io = AudioIO(self.settings.audio)
        self.vad = VAD(self.settings.audio)
        self.stt_engine = STT(self.settings.speech)
        self.text_processor = TextProcessor(self.settings.speech)
        self.heart = EchoCrystallineHeart(self.settings.heart)
        self.llm = LocalLLM(self.settings.heart)
        self.metrics_logger = MetricsLogger(self.settings.paths.metrics_csv)
        self.guidance_logger = GuidanceLogger(self.settings.paths.guidance_csv)
        self.behavior_monitor = BehaviorMonitor(self.settings)

        self.voice_mimic = VoiceMimic(self.settings.speech)
        self.voice_profile = VoiceProfile(
            audio_cfg=self.settings.audio, paths=self.settings.paths
        )
        self.expression_gear = ExpressionGear(
            tts_engine=self.voice_mimic,
            audio_cfg=self.settings.audio,
            voice_profile=self.voice_profile,
        )
        self.aba_engine = AbaEngine(
            settings=self.settings,
            voice_profile=self.voice_profile,
            guidance_logger=self.guidance_logger,
        )
        self._q = asyncio.Queue()

    async def run(self) -> None:
        self.audio_io.listen()
        print("[SpeechLoop] Listening for speech...")
        
        try:
            while True:
                audio_chunk = await self._q.get()
                vad_result = self.vad.process(audio_chunk)

                if vad_result and "start" in vad_result:
                    self._current_utterance = [audio_chunk]
                    self.vad.reset() # Reset VAD state for next utterance
                elif vad_result and "end" in vad_result:
                    if self._current_utterance:
                        self._current_utterance.append(audio_chunk)
                        full_utterance_audio = np.concatenate(self._current_utterance)
                        self._current_utterance = []
                        await self._handle_utterance(full_utterance_audio)
                elif self._current_utterance:
                    self._current_utterance.append(audio_chunk)
        except asyncio.CancelledError:
            print("[SpeechLoop] Shutting down...")
        finally:
            self.audio_io.stop()

    async def _handle_utterance(self, audio_chunk: np.ndarray) -> None:
        if audio_chunk.size == 0:
            return

        rms = np.sqrt(np.mean(audio_chunk**2))
        if rms < self.settings.audio.silence_rms_threshold:
            return

        print(f"[SpeechLoop] Processing utterance (RMS: {rms:.4f})...")
        
        attempt_path = self._persist_attempt(audio_chunk)
        
        # STT and Text Processing
        raw_text = self.stt_engine.transcribe(audio_chunk)
        corrected_text = self.text_processor.normalize(raw_text)
        normalized_text = LocalLLM.enforce_first_person(corrected_text or raw_text or "")
        needs_correction = (corrected_text or "").strip() != (raw_text or "").strip()

        print(f"[SpeechLoop] Raw: '{raw_text}' | Corrected: '{corrected_text}' | Normalized: '{normalized_text}'")

        # Heart and LLM Integration
        heart_result = self.heart.step(audio_chunk)
        llm_output = None
        if self.settings.heart.use_llm and normalized_text.strip():
            arousal_mean = float(heart_result["emotions"].mean(dim=0)[0].item())
            valence_mean = float(heart_result["emotions"].mean(dim=0)[1].item())

            prompt = self.heart._build_prompt( # Accessing protected member for now, consider refactor
                transcript=normalized_text,
                arousal=arousal_mean,
                valence=valence_mean,
                T_val=heart_result["T"],
                coherence=heart_result["coherence"],
            )
            llm_output = self.llm.generate(
                prompt=prompt,
                temperature=heart_result["T"] * self.settings.heart.llm_temperature_scale,
                top_p=self.settings.heart.llm_top_p_base + self.settings.heart.llm_top_p_spread * (1.0 - heart_result["coherence"]),
            )
            # Update heart with LLM embedding
            emb = self.llm.embed(llm_output)
            emb_t = torch.from_numpy(emb).to(self.heart.device, dtype=torch.float32)
            if self.settings.heart.num_nodes <= self.settings.heart.embedding_dim:
                proj = emb_t[: self.settings.heart.num_nodes]
            else:
                reps = math.ceil(self.settings.heart.num_nodes / self.settings.heart.embedding_dim)
                tiled = emb_t.repeat(reps)
                proj = tiled[: self.settings.heart.num_nodes]
            proj = proj.view(self.settings.heart.num_nodes, 1)
            ch = self.settings.heart.embedding_channel
            if 0 <= ch < self.settings.heart.num_channels:
                self.heart.emotions[:, ch: ch + 1].add_(self.settings.heart.embedding_gain * proj)
                self.heart.emotions.clamp_(-self.settings.heart.max_abs, self.settings.heart.max_abs)


        # ABA Engine Integration
        event = self.behavior_monitor.register(normalized_text=normalized_text, needs_correction=needs_correction, rms=rms)
        
        if not needs_correction:
            self.aba_engine.reinforce_success(normalized_text)
            self.aba_engine.track_skill_progress(normalized_text, success=True)
        else:
            self.aba_engine.track_skill_progress(normalized_text, success=False)
        
        aba_event_response = None
        if event:
            aba_event_response = self.aba_engine.intervene(event, normalized_text)
            if aba_event_response and aba_event_response.category == "inner_echo":
                llm_output = aba_event_response.message # Override LLM output with ABA suggestion

            print(f"[SpeechLoop] Behavior Event: {event} -> ABA Response: {llm_output or 'None'}")


        # Decide what to speak
        text_to_speak = ""
        if llm_output:
            text_to_speak = llm_output
        elif needs_correction and self.settings.behavior.correction_echo_enabled:
            text_to_speak = normalized_text
        
        if text_to_speak:
            self._speak_inner(text_to_speak)

        # Logging
        record = AttemptRecord(
            timestamp=datetime.utcnow(),
            phrase_text=normalized_text,
            raw_text=raw_text,
            corrected_text=corrected_text,
            needs_correction=needs_correction,
            audio_file=attempt_path,
            similarity=1.0 if not needs_correction else 0.0, # Placeholder, will be updated with actual similarity score
        )
        self.metrics_logger.append(record)
        
        if event:
            # Log the original behavior event that triggered ABA
            self.guidance_logger.append(
                BehaviorEvent(
                    timestamp=datetime.utcnow(),
                    level="warning" if event != "encouragement" else "info",
                    category=event,  # type: ignore[arg-type]
                    title=f"{event.replace('_', ' ').title()} detected",
                    message=text_to_speak, # The response from the system
                )
            )

    def _persist_attempt(self, audio_chunk: np.ndarray) -> Path:
        attempt_dir = self.settings.paths.attempts_dir
        attempt_dir.mkdir(parents=True, exist_ok=True)
        # Use a more robust filename to avoid conflicts and ensure uniqueness
        filename = f"attempt_{datetime.utcnow().strftime('%Y%m%d%H%M%S%f')}.wav"
        path = attempt_dir / filename
        # Ensure the audio chunk is in the correct format (e.g., int16) before saving
        # Assuming audio_chunk is float32 from sounddevice, convert to int16
        audio_int16 = (audio_chunk * 32767).astype(np.int16)
        self.audio_io.save_wav(audio_int16, path)
        return path

    def _speak_inner(self, text: str) -> None:
        decision = AgentDecision(target_text=text, mode="inner")
        info = self.expression_gear.express(decision, None) # No audio_info for now, needs to be from child's speech
        if info and isinstance(info.payload, AudioData):
            self.audio_io.play(info.payload.waveform, self.settings.audio.sample_rate)


async def _audio_stream_consumer(audio_io: AudioIO, q: asyncio.Queue[np.ndarray]):
    """Consumes audio chunks from AudioIO and puts them into an asyncio Queue."""
    for chunk in audio_io.chunk_generator():
        await q.put(chunk)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./unified_system_agi_core.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
agi_core.py

Advanced unified cognitive substrate: hardware interface, relational matrix,
multi-engine cognition, emotional chemistry, memory, and planning. Designed
to be run as a production-capable module integrated with the Opportunity
Synthesis service.

Notes:
- Deterministic, high-quality local embeddings via random-projection + hashing.
- Safe, deterministic components (no external ML downloads required).
- Exposes a clear programmatic API for stepping the agent and querying state.
"""

import os
import time
import math
import random
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from threading import Lock

# -------------------------
# Utilities
# -------------------------

def now_seconds() -> float:
    return time.time()

def stable_hash_bytes(s: str, length: int = 256) -> bytes:
    # deterministic hashing into bytes array using SHAKE-like approach without external libs
    import hashlib
    h = hashlib.blake2b(digest_size=32)
    h.update(s.encode("utf8"))
    base = h.digest()
    out = bytearray()
    i = 0
    while len(out) < length:
        h2 = hashlib.blake2b(digest_size=32)
        h2.update(base)
        h2.update(bytes([i]))
        out.extend(h2.digest())
        i += 1
    return bytes(out[:length])

# -------------------------
# Hardware Abstraction
# -------------------------

class BitRegister:
    def __init__(self, name: str, size: int = 128):
        self.name = name
        self.size = size
        self.bits = np.zeros(size, dtype=np.uint8)
        self.noise_rate = 2e-6
        self.lock = Lock()

    def write_int(self, value: int):
        with self.lock:
            for i in range(self.size):
                self.bits[i] = (value >> i) & 1

    def read_int(self) -> int:
        with self.lock:
            if random.random() < self.noise_rate:
                idx = random.randrange(self.size)
                self.bits[idx] ^= 1
            out = 0
            for i in range(self.size):
                out |= int(self.bits[i]) << i
            return out

    def set_bit(self, idx: int, val: int):
        with self.lock:
            self.bits[idx % self.size] = 1 if val else 0

    def get_bit(self, idx: int) -> int:
        with self.lock:
            return int(self.bits[idx % self.size])

    def as_bitstring(self) -> str:
        with self.lock:
            return ''.join(str(int(b)) for b in self.bits[::-1])

class SimulatedHardware:
    def __init__(self, adc_channels: int = 16):
        self.cpu_register = BitRegister("CPU_REG", size=256)
        self.gpio = BitRegister("GPIO", size=64)
        self.adc = np.zeros(adc_channels, dtype=float)
        self.temp_C = 35.0
        self.freq_GHz = 1.2

    def poll_sensors(self):
        # realistic sensor noise and drift
        self.adc += np.random.randn(len(self.adc)) * 0.005
        self.adc = np.clip(self.adc, -5.0, 5.0)
        self.temp_C += (self.freq_GHz - 1.0) * 0.02 + np.random.randn() * 0.01

    def set_frequency(self, ghz: float):
        self.freq_GHz = float(max(0.2, min(ghz, 5.0)))

    def as_status(self):
        return {
            "freq_GHz": round(self.freq_GHz, 3),
            "temp_C": round(self.temp_C, 3),
            "cpu_reg": self.cpu_register.as_bitstring(),
            "gpio": self.gpio.as_bitstring(),
            "adc": [round(float(x), 4) for x in self.adc.tolist()],
        }

# -------------------------
# Relational Matrix
# -------------------------

class RelationalMatrix:
    def __init__(self, n_system: int, n_apparatus: int):
        self.n_system = n_system
        self.n_apparatus = n_apparatus
        # complex amplitudes
        self.R = (np.random.randn(n_system, n_apparatus) + 1j * np.random.randn(n_system, n_apparatus)) * 0.02
        self.normalize_rows()

    def normalize_rows(self):
        mags = np.linalg.norm(self.R, axis=1, keepdims=True)
        mags[mags == 0] = 1.0
        self.R = self.R / mags

    def bidirectional_weight(self, i: int, j: int) -> complex:
        # map apparatus j back to some system index deterministically
        reverse_i = j % self.n_system
        reverse_j = i % self.n_apparatus
        return self.R[i, j] * np.conj(self.R[reverse_i, reverse_j])

    def probability_for_system(self, i: int) -> float:
        weights = np.array([abs(self.bidirectional_weight(i, j)) for j in range(self.n_apparatus)])
        s = np.sum(weights)
        return float(s / (np.sum(weights) + 1e-12))

    def update_hebbian(self, pre_idx: int, post_idx: int, lr: float = 1e-3):
        i = pre_idx % self.n_system
        j = post_idx % self.n_apparatus
        self.R[i, j] += lr * (1.0 + 0.05j)
        self.normalize_rows()

# -------------------------
# Thought Engines
# -------------------------

class ThoughtEngines:
    def __init__(self, n_nodes: int):
        self.n = n_nodes
        self.b = np.zeros(n_nodes)
        self.h = np.zeros(n_nodes)
        self.kappa = np.zeros(n_nodes)
        self.mu = np.zeros(n_nodes)
        # stateful noise seeds for reproducibility
        self._rng = np.random.RandomState(42)

    def step(self, relational: RelationalMatrix, inputs: np.ndarray, dt: float = 0.1):
        n = self.n
        if inputs is None:
            inputs = np.zeros(n)
        # coupling matrix from relational matrix (n x n)
        R = relational.R
        affin = np.real(R @ R.conj().T)
        maxval = np.max(np.abs(affin)) if np.max(np.abs(affin)) > 0 else 1.0
        W = affin / maxval

        # perspective
        db = 0.12 * (inputs * np.tanh(inputs)) - 0.05 * self.b + 0.03 * (W @ self.b - np.sum(W, axis=1) * self.b)
        self.b += db * dt

        # speculation with structured stochasticity
        eps = self._rng.randn(n) * 0.02
        dh = 0.10 * (inputs + eps) - 0.06 * self.h + 0.03 * (W @ self.h - np.sum(W, axis=1) * self.h)
        self.h += dh * dt

        # kaleidoscope
        dk = 0.08 * (self.b + 0.5 * self.h) - 0.04 * self.kappa + 0.02 * (W @ self.kappa - np.sum(W, axis=1) * self.kappa)
        self.kappa += dk * dt

        # mirror
        mismatch = np.abs(self.b - np.mean(self.b))
        dmu = -0.07 * mismatch + 0.05 * np.std(self.h) + 0.03 * (W @ self.mu - np.sum(W, axis=1) * self.mu)
        self.mu += dmu * dt

        # clip numeric stability
        for arr in (self.b, self.h, self.kappa, self.mu):
            np.clip(arr, -12.0, 12.0, out=arr)

# -------------------------
# Emotional Chemistry
# -------------------------

class EmotionalChemistry:
    def __init__(self):
        self.DA = 0.5
        self.Ser = 0.5
        self.NE = 0.5

    def step(self, reward: float, mood_signal: float, arousal: float, dt: float = 0.1):
        self.DA += (0.9 * reward - 0.12 * self.DA) * dt
        self.Ser += (0.4 * mood_signal - 0.06 * self.Ser) * dt
        self.NE += (0.65 * arousal - 0.08 * self.NE) * dt
        self.DA = float(np.clip(self.DA, 0.0, 1.0))
        self.Ser = float(np.clip(self.Ser, 0.0, 1.0))
        self.NE = float(np.clip(self.NE, 0.0, 1.0))

    def vector(self) -> List[float]:
        return [self.DA, self.Ser, self.NE]

# -------------------------
# Memory
# -------------------------

class MemorySystem:
    def __init__(self, embedding_dim: int = 128, capacity: int = 10000):
        self.embedding_dim = embedding_dim
        self.capacity = capacity
        self.episodic = []  # list of (ts, emb, text)
        self.semantic = {}  # key -> emb
        self._rng = np.random.RandomState(12345)
        # We'll derive a deterministic random projection matrix seeded by a constant
        self.random_proj = self._rng.randn(self.embedding_dim, 256) * 0.01

    def embed(self, text: str) -> np.ndarray:
        b = stable_hash_bytes(text, length=256)
        arr = np.frombuffer(b, dtype=np.uint8).astype(np.float32)
        emb = self.random_proj @ arr
        n = np.linalg.norm(emb)
        return emb / (n + 1e-12)

    def store_episode(self, text: str):
        emb = self.embed(text)
        ts = now_seconds()
        if len(self.episodic) >= self.capacity:
            self.episodic.pop(0)
        self.episodic.append((ts, emb, text))

    def retrieve(self, query: str, top_k: int = 5):
        q_emb = self.embed(query)
        sims = []
        for ts, emb, text in self.episodic:
            sims.append((float(np.dot(q_emb, emb)), ts, text))
        sims.sort(reverse=True, key=lambda x: x[0])
        return sims[:top_k]

    def store_semantic(self, key: str, text: str):
        self.semantic[key] = self.embed(text)

    def lookup_semantic(self, key: str):
        return self.semantic.get(key, None)

# -------------------------
# Planner
# -------------------------

class Planner:
    def __init__(self, hardware: SimulatedHardware, relational: RelationalMatrix):
        self.hw = hardware
        self.rel = relational
        # concrete actions map to methods to keep safe
        self.actions = [
            ("increase_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz + 0.1)),
            ("decrease_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz - 0.1)),
            ("toggle_gpio", lambda: self.hw.gpio.set_bit(random.randint(0, self.hw.gpio.size-1), random.randint(0,1))),
            ("no_op", lambda: None),
        ]

    def score_actions(self, thought: ThoughtEngines) -> List[Tuple[float, int]]:
        scores = []
        for idx, (name, fn) in enumerate(self.actions):
            sys_idx = idx % self.rel.n_system
            app_idx = idx % self.rel.n_apparatus
            rweight = abs(self.rel.bidirectional_weight(sys_idx, app_idx))
            cog_signal = float(np.tanh(np.mean(thought.b) + np.mean(thought.kappa)))
            score = rweight * (1.0 + cog_signal)
            # small diversity noise
            score += random.random() * 1e-6
            scores.append((score, idx))
        scores.sort(reverse=True, key=lambda x: x[0])
        return scores

    def select_and_execute(self, thought: ThoughtEngines) -> str:
        scored = self.score_actions(thought)
        weights = np.array([s for s, i in scored], dtype=float)
        if weights.sum() <= 0:
            idx = scored[0][1]
        else:
            probs = weights / weights.sum()
            choice = np.random.choice(len(scored), p=probs)
            idx = scored[choice][1]
        name, fn = self.actions[idx]
        fn()
        return name

# -------------------------
# AGI System (orchestrator)
# -------------------------

class AGISystem:
    def __init__(self, n_nodes: int = 32, n_apparatus: int = 24):
        self.hw = SimulatedHardware(adc_channels=24)
        self.rel = RelationalMatrix(n_nodes, n_apparatus)
        self.thought = ThoughtEngines(n_nodes)
        self.emotion = EmotionalChemistry()
        self.memory = MemorySystem(embedding_dim=128, capacity=10000)
        self.planner = Planner(self.hw, self.rel)
        self.t = 0.0
        self.dt = 0.1
        self.history = []
        self.lock = Lock()
        # seed semantic memory
        self.memory.store_semantic("agent_identity", "A private, self-contained cognitive assistant for execution and opportunity synthesis.")
        self.memory.store_episode("system initialized")

    def step(self, user_input: Optional[str] = None) -> Dict[str, Any]:
        with self.lock:
            self.hw.poll_sensors()
            n = self.thought.n
            sensor_vec = np.concatenate([self.hw.adc, np.array([self.hw.temp_C, self.hw.freq_GHz])])
            inputs = np.zeros(n)
            ssum = float(np.sum(sensor_vec))
            for i in range(n):
                inputs[i] = float(np.tanh(ssum * 0.0005 + random.random() * 0.01))
            if user_input:
                self.memory.store_episode(user_input)
                emb = self.memory.embed(user_input)
                bias = float(np.tanh(np.mean(emb))) * 0.5
                inputs += bias
            self.thought.step(self.rel, inputs, dt=self.dt)
            reward = float(np.clip(np.mean(inputs), -1, 1))
            mood = float(np.tanh(np.mean(self.thought.b)))
            arousal = float(np.abs(np.std(self.thought.h)))
            self.emotion.step(reward, mood, arousal, dt=self.dt)
            most_active_node = int(np.argmax(np.abs(self.thought.kappa)))
            apparatus_idx = int(abs(int((np.sum(self.hw.adc) * 100) % self.rel.n_apparatus)))
            self.rel.update_hebbian(most_active_node, apparatus_idx, lr=1e-3)
            action_name = self.planner.select_and_execute(self.thought)
            self.rel.normalize_rows()
            conn_metrics = self.relational_consciousness_metrics()
            log_item = {
                "t": self.t,
                "action": action_name,
                "hw": self.hw.as_status(),
                "emotion": {"DA": self.emotion.DA, "Ser": self.emotion.Ser, "NE": self.emotion.NE},
                "thought_summary": {
                    "b_mean": float(np.mean(self.thought.b)),
                    "h_mean": float(np.mean(self.thought.h)),
                    "kappa_mean": float(np.mean(self.thought.kappa)),
                    "mu_mean": float(np.mean(self.thought.mu)),
                },
                "consciousness": conn_metrics
            }
            self.history.append(log_item)
            self.t += self.dt
            return log_item

    def relational_consciousness_metrics(self) -> Dict[str, float]:
        diag = np.array([abs(self.rel.R[i, i % self.rel.n_apparatus]) for i in range(min(self.rel.n_system, self.rel.n_apparatus))])
        coherence = float(np.mean(diag))
        awareness = float(np.clip(self.emotion.DA * (1.0 + np.tanh(np.mean(self.thought.b))), 0.0, 1.0))
        activities = np.concatenate([self.thought.b, self.thought.h, self.thought.kappa, self.thought.mu])
        integrated_info = float(np.var(activities))
        return {"coherence": coherence, "awareness": awareness, "phi_proxy": integrated_info}

    def respond(self, user_input: str) -> str:
        log = self.step(user_input)
        candidates = self.memory.retrieve(user_input, top_k=3)
        reply_parts = []
        if candidates:
            reply_parts.append("I recall: " + "; ".join([c for _, _, c in candidates[:2]]))
        if log["consciousness"]["awareness"] > 0.6:
            reply_parts.append("I am engaged and reflecting on that.")
        elif log["consciousness"]["phi_proxy"] > 0.08:
            reply_parts.append("This seems important; I'll think further.")
        else:
            reply_parts.append("Noted and stored.")
        da = log["emotion"]["DA"]
        mood = "positive" if da > 0.55 else "neutral" if da > 0.45 else "cautious"
        reply_parts.append(f"My mood is {mood}. Action taken: {log['action']}.")
        return " ".join(reply_parts)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./voice/profile.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import random
import uuid
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Literal, Optional

import librosa
import numpy as np
import soundfile as sf

from core.settings import AudioSettings, PathRegistry


@dataclass(slots=True)
class VoiceSample:
    """A single recorded sample of the user's voice."""

    path: Path
    duration_s: float
    rms: float
    quality_score: float = 1.0


@dataclass
class VoiceProfile:
    """Manages the collection of the user's voice samples (the "Voice Crystal")."""

    audio_cfg: AudioSettings
    paths: PathRegistry
    samples: list[VoiceSample] = field(default_factory=list)
    max_samples: int = 50

    def __post_init__(self):
        self.paths.voices_dir.mkdir(parents=True, exist_ok=True)
        self.load_existing()

    def load_existing(self):
        """Load all .wav files from the voice directory."""
        for wav_path in sorted(self.paths.voices_dir.glob("**/*.wav")):
            try:
                data, sr = sf.read(wav_path, dtype="float32")
                if sr != self.audio_cfg.sample_rate:
                    data = librosa.resample(
                        y=data, orig_sr=sr, target_sr=self.audio_cfg.sample_rate
                    )

                duration = len(data) / float(self.audio_cfg.sample_rate)
                rms = (
                    float(np.sqrt(np.mean(np.square(data)))) if data.size > 0 else 0.0
                )
                # Quality score can be stored in filename or a metadata file in future
                self.samples.append(VoiceSample(wav_path, duration, rms, 1.0))
            except Exception as e:
                print(f"Failed to load voice sample {wav_path}: {e}")

    def add_sample(self, wav: np.ndarray, quality_score: float) -> Path | None:
        """Adds a new voice sample to the profile if quality is high enough."""
        if quality_score < 0.9:
            return None

        path = self.paths.voices_dir / f"sample_{int(random.random() * 1e8)}.wav"
        sf.write(path, wav, self.audio_cfg.sample_rate)

        duration = len(wav) / float(self.audio_cfg.sample_rate)
        rms = float(np.sqrt(np.mean(np.square(wav))))
        self.samples.append(VoiceSample(path, duration, rms, quality_score))

        self._prune()
        return path

    def _prune(self):
        """Keeps only the highest quality samples if count exceeds max_samples."""
        if len(self.samples) > self.max_samples:
            self.samples.sort(key=lambda s: s.quality_score, reverse=True)
            self.samples = self.samples[: self.max_samples]

    def pick_reference(self) -> Path | None:
        """Picks a random, high-quality sample to use for voice cloning."""
        if not self.samples:
            return None
        # Prefer higher quality samples
        high_quality_samples = [s for s in self.samples if s.quality_score > 0.95]
        if high_quality_samples:
            return random.choice(high_quality_samples).path
        return random.choice(self.samples).path

    def maybe_adapt_from_attempt(
        self,
        attempt_wav: np.ndarray,
        style: Literal["neutral", "calm", "excited"] = "neutral",
        quality_score: float = 0.0,
        min_quality: float = 0.8,
    ) -> Path | None:
        """Add a new facet when an attempt is strong enough."""
        if quality_score < min_quality:
            return None
        style_dir = self.paths.voices_dir / style
        style_dir.mkdir(parents=True, exist_ok=True)
        path = style_dir / f"{style}_{uuid.uuid4().hex[:8]}.wav"
        sf.write(path, attempt_wav, self.audio_cfg.sample_rate)
        duration = len(attempt_wav) / float(self.audio_cfg.sample_rate)
        rms = float(np.sqrt(np.mean(np.square(attempt_wav)) + 1e-8))
        self.samples.append(VoiceSample(path, duration, rms, quality_score))
        self._prune()
        return path
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./voice/prosody.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass

import librosa
import numpy as np


@dataclass(slots=True)
class ProsodyProfile:
    """
    Represents the prosody of a speech segment.
    """

    f0_hz: np.ndarray
    energy: np.ndarray
    frame_length: int
    hop_length: int
    sample_rate: int


def extract_prosody(
    audio: np.ndarray,
    sample_rate: int,
    frame_length: int = 1024,
    hop_length: int = 256,
) -> ProsodyProfile:
    """
    Extracts prosody (pitch and energy) from an audio signal.
    """
    f0, _, _ = librosa.pyin(
        y=audio,
        fmin=librosa.note_to_hz("C2"),
        fmax=librosa.note_to_hz("C7"),
        sr=sample_rate,
        frame_length=frame_length,
        hop_length=hop_length,
    )
    f0[~np.isfinite(f0)] = 0

    rms = librosa.feature.rms(
        y=audio, frame_length=frame_length, hop_length=hop_length
    )[0]

    return ProsodyProfile(
        f0_hz=f0,
        energy=rms,
        frame_length=frame_length,
        hop_length=hop_length,
        sample_rate=sample_rate,
    )
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./voice/mimic.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from pathlib import Path
from typing import Optional

import numpy as np
import torch
from TTS.api import TTS

from core.settings import SpeechSettings


class VoiceMimic:
    """
    Text-to-Speech engine using Coqui TTS.
    """

    def __init__(self, settings: SpeechSettings):
        self.settings = settings
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = TTS(self.settings.tts_model_name).to(self.device)
        self.voice_sample = self.settings.tts_voice_clone_reference

    def update_voiceprint(self, voice_sample: Path):
        self.voice_sample = voice_sample

    def synthesize(self, text: str) -> np.ndarray:
        """
        Synthesizes text and returns the audio as a numpy array.
        """
        if not self.voice_sample or not self.voice_sample.exists():
            return self._fallback_synthesize(text)

        return np.array(
            self.model.tts(
                text=text,
                speaker_wav=str(self.voice_sample),
                language="en",
            )
        )

    def _fallback_synthesize(self, text: str) -> np.ndarray:
        """
        Fallback to a default voice if no voice sample is available.
        """
        return np.array(self.model.tts(text=text, speaker=self.model.speakers[0], language="en"))
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./gears.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
gears.py

This module defines the "Functional Gears" architecture. Instead of using
heavyweight APIs or message queues for internal communication, the system
passes simple, immutable `Information` objects directly between components
(the "gears").

This creates a near-zero-latency "nervous system" where, for example,
the emotional state from the Heart can instantly modulate the Voice.

The `Information` object is a generic container for any type of data
that needs to be passed between gears.
"""
from __future__ import annotations
from dataclasses import dataclass, field
from typing import Any, Dict, Optional
import numpy as np
import time

@dataclass(frozen=True)
class Information:
    """
    A generic, immutable data packet that flows between gears.
    It contains a payload and metadata about the information's origin and state.
    """
    # The data payload of the information packet.
    payload: Any
    # A dictionary for any metadata.
    metadata: Dict[str, Any] = field(default_factory=dict)
    # The timestamp of when the information was created.
    timestamp: float = field(default_factory=time.time)
    # The name of the gear that produced this information.
    source_gear: Optional[str] = None

    def new(self, payload: Any, source_gear: str) -> Information:
        """
        Creates a new Information object, inheriting metadata but updating
        the payload, source, and timestamp.
        """
        return Information(
            payload=payload,
            metadata=self.metadata.copy(),
            source_gear=source_gear
        )

# Example Payloads (for type hinting and clarity)

@dataclass(frozen=True)
class AudioData:
    """Payload for raw audio information."""
    waveform: np.ndarray
    sample_rate: int

@dataclass(frozen=True)
class SpeechData:
    """Payload for transcribed speech information."""
    raw_text: str
    corrected_text: str
    is_final: bool

@dataclass(frozen=True)
class EmotionData:
    """Payload for emotional state information from the Heart."""
    arousal: float
    valence: float
    coherence: float
    temperature: float
    raw_emotions: np.ndarray # The full [N, 5] tensor

@dataclass(frozen=True)
class AgentDecision:
    """Payload for an AGI Seed's decision."""
    action: str # e.g., "ECHO", "REGULATE", "LEARN"
    target_text: Optional[str] = None
    prosody_source: Optional[np.ndarray] = None
    mode: str = "inner" # "inner", "outer", "coach"-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./main.py
╚══════════════════════════════════════════════════════════════════════════════╝

import asyncio
from concurrent.futures import ThreadPoolExecutor

from flask import Flask
from werkzeug.serving import run_simple

from companion import EchoCompanion
from dashboard import create_app


def main():
    companion = EchoCompanion()
    app = create_app(companion)

    # Use a ThreadPoolExecutor for blocking operations (like Flask's run_simple)
    # in an asyncio application.
    executor = ThreadPoolExecutor(max_workers=2)

    async def run_flask_and_loop():
        # Start the Flask app in a separate thread
        print("[Main] Starting Flask dashboard...")
        flask_task = asyncio.get_event_loop().run_in_executor(
            executor,
            run_simple,
            "0.0.0.0",  # Listen on all interfaces
            8765,
            app,
            use_reloader=False,  # Disable reloader for production
            threaded=True,
        )

        # Start the SpeechLoop
        print("[Main] Starting Echo Companion speech loop...")
        await companion.start_loop()

        # Keep the main task running until Flask or SpeechLoop stops
        await asyncio.gather(flask_task, companion.speech_loop._q.join()) # This is a placeholder, needs proper event handling

    try:
        asyncio.run(run_flask_and_loop())
    except KeyboardInterrupt:
        print("[Main] Shutting down...")
        asyncio.run(companion.stop_loop())
    finally:
        executor.shutdown(wait=True)


if __name__ == "__main__":
    main()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/config.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Central configuration for the autism speech companion."""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path


@dataclass(slots=True)
class Paths:
    """Structure holding all filesystem locations used by the system."""

    base_dir: Path = Path.home() / "speech_companion"
    voices_dir: Path | None = None
    logs_dir: Path | None = None
    metrics_csv: Path | None = None
    guidance_csv: Path | None = None

    def __post_init__(self) -> None:
        self.voices_dir = self.base_dir / "voices"
        self.logs_dir = self.base_dir / "logs"
        self.metrics_csv = self.logs_dir / "attempts.csv"
        self.guidance_csv = self.logs_dir / "guidance_events.csv"
        self.ensure()

    def ensure(self) -> None:
        """Create the directories if they are missing."""
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.voices_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)


@dataclass(slots=True)
class AudioSettings:
    """All audio constants in one place."""

    sample_rate: int = 16_000
    chunk_seconds: float = 2.5
    silence_rms_threshold: float = 0.0125
    channels: int = 1


@dataclass(slots=True)
class SpeechModelSettings:
    """Metadata about ML models used by the speech pipeline."""

    whisper_model: str = "base"
    language_tool_server: str | None = None  # use default JVM unless a remote server is provided
    tts_model_name: str = ""  # Optional pyttsx3 voice id/name hint
    tts_voice_clone_reference: Path | None = None
    tts_sample_rate: int = 16_000


@dataclass(slots=True)
class BehaviorSettings:
    """High level behavior knobs parents can toggle."""

    correction_echo_enabled: bool = True


@dataclass(slots=True)
class CompanionConfig:
    """Top-level configuration shared across CLI, GUI, and agent layers."""

    child_id: str = "child_001"
    child_name: str = "Companion User"
    caregiver_name: str = "Caregiver"
    paths: Paths = field(default_factory=Paths)
    audio: AudioSettings = field(default_factory=AudioSettings)
    speech: SpeechModelSettings = field(default_factory=SpeechModelSettings)
    behavior: BehaviorSettings = field(default_factory=BehaviorSettings)


CONFIG = CompanionConfig()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/speech_processing.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Speech-to-text plus grammar correction pipeline."""

from __future__ import annotations

import asyncio
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import numpy as np
import whisper
from language_tool_python import LanguageTool

from broken_speech_tool import normalize_text

from .config import SpeechModelSettings


@dataclass(slots=True)
class SpeechProcessor:
    """Wrap whisper and LanguageTool into a streamlined API."""

    settings: SpeechModelSettings

    def __post_init__(self) -> None:
        self._whisper = whisper.load_model(self.settings.whisper_model)
        self._tool = LanguageTool("en-US", server_url=self.settings.language_tool_server)

    async def transcribe(self, audio_path: Path) -> str:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, self._whisper.transcribe, str(audio_path))
        return result.get("text", "").strip()

    def normalize(self, text: str) -> str:
        if not text:
            return ""
        return normalize_text(text).normalized_text

    def correct_text(self, text: str) -> str:
        if not text:
            return ""
        matches = self._tool.check(text)
        return LanguageTool.correct(text, matches)

    async def process(self, audio: np.ndarray, tmp_path: Path, sample_rate: int = 16_000) -> tuple[str, str]:
        """Write temp wav, transcribe, correct, then delete."""
        import soundfile as sf

        sf.write(tmp_path, audio, sample_rate)
        raw = await self.transcribe(tmp_path)
        normalized = self.normalize(raw)
        corrected = self.correct_text(normalized)
        tmp_path.unlink(missing_ok=True)
        return raw, corrected
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/speech_loop.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Core speech companion loop with inner-voice echo + passive voice learning."""

from __future__ import annotations

import asyncio
import csv
import os
import random
import tempfile
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Sequence, Tuple

import numpy as np
import soundfile as sf

from .advanced_voice_mimic import (
    VoiceCrystal,
    VoiceCrystalConfig,
    VoiceProfile,
)
from .audio_io import AudioIO, chunked_audio
from .behavior_monitor import BehaviorMonitor
from .calming_strategies import StrategyAdvisor
from .config import CompanionConfig
from .data_store import DataStore, Phrase
from .inner_voice import InnerVoiceEngine, InnerVoiceConfig
from .similarity import SimilarityScorer
from .speech_processing import SpeechProcessor
from .text_utils import normalize_simple, similarity as text_similarity
from .voice_mimic import VoiceMimic
from .guidance import GuidanceCoach
from .agent import KQBCAgent


@dataclass(slots=True)
class SpeechLoop:
    config: CompanionConfig

    def __post_init__(self) -> None:
        self.audio_io = AudioIO(self.config.audio)
        self.processor = SpeechProcessor(self.config.speech)
        self.data = DataStore(self.config)
        self.similarity = SimilarityScorer(self.config.audio)
        self.behavior = BehaviorMonitor()
        self.advisor = StrategyAdvisor()

        self.voice_tts = VoiceMimic(self.config.speech)
        profile_dir = self.config.paths.voices_dir / "voice_profile"
        self.voice_profile = VoiceProfile(audio=self.config.audio, base_dir=profile_dir)
        self.voice_crystal = VoiceCrystal(
            tts=self.voice_tts,
            audio=self.config.audio,
            profile=self.voice_profile,
            config=VoiceCrystalConfig(sample_rate=self.config.audio.sample_rate),
        )
        self.inner_voice = InnerVoiceEngine(
            voice=self.voice_crystal,
            data_store=self.data,
            config=InnerVoiceConfig(),
        )
        self.coach = GuidanceCoach(self.voice_crystal, self.audio_io, self.data)

        self.phrases = {p.phrase_id: p for p in self.data.list_phrases()}

    def record_phrase(self, text: str, seconds: float) -> Phrase:
        pid = f"phrase_{int(time.time())}"
        audio = self.audio_io.record_phrase(seconds)
        filepath = self.config.paths.voices_dir / f"{pid}_{int(time.time())}.wav"
        self.audio_io.save_wav(audio, filepath)
        phrase = Phrase(
            phrase_id=pid,
            text=text,
            audio_file=filepath,
            duration=seconds,
            normalized_text=normalize_simple(text),
        )
        self.data.save_phrase(pid, text, filepath, seconds)
        self.phrases[pid] = phrase
        return phrase

    async def handle_chunk(self, chunk: np.ndarray) -> None:
        if chunk.size == 0:
            return
        rms = self.audio_io.rms(chunk)
        if rms < self.config.audio.silence_rms_threshold:
            return

        fd, tmp_path = tempfile.mkstemp(suffix=".wav")
        os.close(fd)
        tmp = Path(tmp_path)
        raw, corrected = await self.processor.process(chunk, tmp, self.config.audio.sample_rate)
        tmp.unlink(missing_ok=True)

        normalized_attempt = normalize_simple(corrected or raw or "")
        best: Optional[Phrase] = None
        best_text_score = 0.0
        for phrase in self.phrases.values():
            score = text_similarity(normalized_attempt, phrase.normalized_text)
            if score > best_text_score:
                best = phrase
                best_text_score = score

        attempt_path = self.config.paths.voices_dir / f"attempt_{int(time.time())}.wav"
        self.audio_io.save_wav(chunk, attempt_path)

        audio_score = 0.0
        if best and best_text_score >= 0.4:
            try:
                audio_score = self.similarity.compare(best.audio_file, attempt_path)
            except Exception:
                audio_score = 0.0

        needs_correction = audio_score < 0.85

        if needs_correction and self.config.behavior.correction_echo_enabled:
            self.inner_voice.speak_corrected(
                corrected_text=corrected,
                raw_text=raw,
                prosody_source_wav=chunk,
                prosody_source_sr=self.config.audio.sample_rate,
            )

        if best and not needs_correction and audio_score >= 0.85:
            style = "calm" if rms < self.config.audio.silence_rms_threshold * 2 else "neutral"
            self.voice_profile.maybe_adapt_from_attempt(
                attempt_wav=chunk,
                style=style,
                quality_score=audio_score,
            )

        self.data.log_attempt(
            phrase_id=best.phrase_id if best else None,
            phrase_text=best.text if best else None,
            attempt_audio=attempt_path,
            stt_text=raw,
            corrected_text=corrected,
            similarity=audio_score,
            needs_correction=needs_correction,
        )

        event = self.behavior.register(
            normalized_text=normalized_attempt,
            needs_correction=needs_correction,
            rms=rms,
        )
        if event:
            suggestions = self.advisor.suggest(event)
            if suggestions:
                print(f"[EVENT] {event} detected")
                for s in suggestions[:3]:
                    print(f" - {s.title}: {s.description}")

    async def run(self) -> None:
        stream = self.audio_io.microphone_stream()
        chunk_generator = chunked_audio(
            stream,
            self.config.audio.chunk_seconds,
            self.config.audio.sample_rate,
        )
        async for chunk in _async_iter(chunk_generator):
            await self.handle_chunk(chunk)


async def _async_iter(sync_iter):
    loop = asyncio.get_event_loop()
    while True:
        try:
            chunk = await loop.run_in_executor(None, next, sync_iter, None)
            if chunk is None:
                break
            yield chunk
        except StopIteration:
            break


class SimulatedSpeechLoop:
    """Async simulator that mirrors the legacy speech_loop scripts.

    This loop does not access microphones. Instead it generates sample
    phrases, optionally routes them through ``KQBCAgent`` to update the
    cognitive substrate, and appends CSV logs identical to the real
    companion. Behavioural events are sampled from simple probabilities to
    keep downstream dashboards exercised during demos/tests.
    """

    def __init__(
        self,
        config: Optional[CompanionConfig] = None,
        *,
        agent: Optional[KQBCAgent] = None,
        vocabulary: Optional[Sequence[str]] = None,
        behaviour_events: Optional[Sequence[Tuple[str, float]]] = None,
        use_agent: bool = True,
    ) -> None:
        self.config = config or CompanionConfig()
        self.agent = agent if agent is not None else (KQBCAgent(self.config) if use_agent else None)
        self.vocabulary: List[str] = list(
            vocabulary
            or (
                "hello",
                "water",
                "thank you",
                "help",
                "good morning",
                "yes",
                "no",
            )
        )
        self.behaviour_events: List[Tuple[str, float]] = list(
            behaviour_events
            or (
                ("anxious", 0.05),
                ("perseveration", 0.03),
                ("high_energy", 0.04),
                ("meltdown", 0.01),
                ("encouragement", 0.06),
            )
        )

    async def run(self) -> None:
        """Run indefinitely, writing metrics + guidance rows."""

        self._ensure_metrics_header()
        self._ensure_guidance_header()
        while True:
            await self._simulate_one_turn()
            await asyncio.sleep(random.uniform(1.5, 3.0))

    async def _simulate_one_turn(self) -> None:
        phrase = random.choice(self.vocabulary)
        raw = self._maybe_mutate_phrase(phrase)

        if self.agent:
            needs_correction = self.agent.evaluate_correction(phrase, raw)
            corrected = phrase if needs_correction else raw
            self.agent.update_state(user_input=raw)
        else:
            needs_correction = raw != phrase
            corrected = phrase if needs_correction else raw

        timestamp = datetime.utcnow().isoformat()
        metrics_path = self.config.paths.metrics_csv
        with metrics_path.open("a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                timestamp,
                phrase,
                raw,
                corrected,
                "1" if needs_correction else "0",
            ])

        for event, prob in self.behaviour_events:
            if random.random() < prob:
                with self.config.paths.guidance_csv.open("a", newline="") as f:
                    writer = csv.writer(f)
                    writer.writerow([timestamp, event, phrase])
                break

    def _maybe_mutate_phrase(self, phrase: str) -> str:
        if random.random() >= 0.3:
            return phrase
        if len(phrase) <= 2:
            return phrase
        idx = random.randint(0, len(phrase) - 1)
        if phrase[idx] in "aeiou":
            new_char = random.choice("aeiou")
            return phrase[:idx] + new_char + phrase[idx + 1 :]
        return phrase[:idx] + phrase[idx + 1 :]

    def _ensure_metrics_header(self) -> None:
        path = self.config.paths.metrics_csv
        if not path.exists():
            path.touch()
        if path.stat().st_size == 0:
            with path.open("w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow([
                    "timestamp",
                    "phrase_text",
                    "raw_text",
                    "corrected_text",
                    "needs_correction",
                ])

    def _ensure_guidance_header(self) -> None:
        path = self.config.paths.guidance_csv
        if not path.exists():
            path.touch()
        if path.stat().st_size == 0:
            with path.open("w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow([
                    "timestamp",
                    "event",
                    "phrase_text",
                ])
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/broken_speech_tool.py
╚══════════════════════════════════════════════════════════════════════════════╝

#!/usr/bin/env python3
"""
Broken Speech Interpreter

This standalone script normalizes heavily abbreviated or "broken" speech,
extracts likely intent, and surfaces a short explanation.  Run with:

    python broken_speech_tool.py --text "u help me job money pls"

or launch interactive mode with no arguments.
"""
from __future__ import annotations

import argparse
import json
import re
import sys
from dataclasses import dataclass
from typing import Dict, Iterable, List, Sequence, Tuple

# Common shorthand and shorthand-like word replacements.
CORRECTIONS: Dict[str, str] = {
    "u": "you",
    "ur": "your",
    "pls": "please",
    "plz": "please",
    "thx": "thanks",
    "tx": "thanks",
    "tmrw": "tomorrow",
    "tmr": "tomorrow",
    "tho": "though",
    "luv": "love",
    "abt": "about",
    "bc": "because",
    "cuz": "because",
    "wanna": "want to",
    "gonna": "going to",
    "hafta": "have to",
    "im": "i am",
    "cant": "cannot",
    "dont": "do not",
    "idk": "i do not know",
    "smth": "something",
    "ppl": "people",
    "ya": "yeah",
    "omw": "on my way",
    "wtvr": "whatever",
    "r": "are",
    "n": "and",
    "w/": "with",
    "b4": "before",
    "lmk": "let me know",
    "asap": "as soon as possible",
    "msg": "message",
    "info": "information",
    "prob": "problem",
}

# Keyword groupings for super-lightweight intent detection.
INTENT_RULES: Sequence[Tuple[str, Sequence[str]]] = (
    ("request", ("please", "help", "need", "want", "can", "could", "assist", "support")),
    ("question", ("what", "why", "how", "when", "where", "who", "?", "do", "is", "are")),
    ("complaint", ("angry", "upset", "bad", "issue", "problem", "broke", "broken")),
    ("gratitude", ("thanks", "thank", "appreciate", "grateful")),
    ("greeting", ("hello", "hi", "hey", "morning", "afternoon")),
)

POSITIVE_WORDS = {
    "good",
    "great",
    "love",
    "happy",
    "awesome",
    "thanks",
    "appreciate",
    "success",
    "win",
}
NEGATIVE_WORDS = {
    "bad",
    "sad",
    "angry",
    "mad",
    "hate",
    "issue",
    "problem",
    "broke",
    "broken",
    "fail",
    "error",
}


@dataclass
class NormalizationResult:
    tokens: List[str]
    normalized_text: str
    notes: List[str]


def collapse_repeated_letters(word: str) -> str:
    """Collapse more than two repeated letters (heyyy -> heyy)."""
    return re.sub(r"(.)\1{2,}", r"\1\1", word)


def normalize_text(text: str) -> NormalizationResult:
    notes: List[str] = []
    tokens: List[str] = []
    for raw_token in re.split(r"\s+", text.strip()):
        if not raw_token:
            continue
        cleaned = re.sub(r"[^\w'/]", "", raw_token.lower())
        if not cleaned:
            continue

        collapsed = collapse_repeated_letters(cleaned)
        if collapsed != cleaned:
            notes.append(f"Collapsed '{cleaned}' -> '{collapsed}'")

        replacement = CORRECTIONS.get(collapsed, collapsed)
        if replacement != collapsed:
            notes.append(f"Expanded '{collapsed}' -> '{replacement}'")

        tokens.extend(replacement.split())

    normalized_text = " ".join(tokens)
    return NormalizationResult(tokens=tokens, normalized_text=normalized_text, notes=notes)


def detect_intent(tokens: Sequence[str]) -> Tuple[str, float, List[str]]:
    best_intent = "unknown"
    best_score = 0.0
    evidence: List[str] = []
    token_set = list(tokens)

    for intent_name, keywords in INTENT_RULES:
        hits = [w for w in token_set if w in keywords or w.endswith("?")]
        if not hits:
            continue
        score = len(hits) / len(keywords)
        if score > best_score:
            best_score = score
            best_intent = intent_name
            evidence = hits

    return best_intent, round(best_score, 3), evidence


def detect_sentiment(tokens: Iterable[str]) -> str:
    pos = sum(1 for token in tokens if token in POSITIVE_WORDS)
    neg = sum(1 for token in tokens if token in NEGATIVE_WORDS)
    if pos > neg:
        return "positive"
    if neg > pos:
        return "negative"
    return "neutral"


def interpret(text: str) -> Dict[str, object]:
    normalization = normalize_text(text)
    intent, score, evidence = detect_intent(normalization.tokens)
    sentiment = detect_sentiment(normalization.tokens)

    interpretation = {
        "original_text": text.strip(),
        "normalized_text": normalization.normalized_text,
        "intent": intent,
        "intent_confidence": score,
        "intent_evidence": evidence,
        "sentiment": sentiment,
        "notes": normalization.notes,
    }
    return interpretation


def run_cli(args: argparse.Namespace) -> int:
    if args.text:
        result = interpret(args.text)
        print(json.dumps(result, indent=2, ensure_ascii=False))
        return 0

    print("Broken Speech Interpreter (type 'exit' to quit)")
    while True:
        try:
            user_input = input(">> ").strip()
        except (EOFError, KeyboardInterrupt):
            print()
            break

        if user_input.lower() in {"exit", "quit"}:
            break
        if not user_input:
            continue

        result = interpret(user_input)
        print(json.dumps(result, indent=2, ensure_ascii=False))

    return 0


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Interpret text with broken speech patterns.")
    parser.add_argument(
        "-t",
        "--text",
        help="Text to interpret. If omitted, interactive mode is started.",
    )
    return parser


def main(argv: Sequence[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    return run_cli(args)


if __name__ == "__main__":
    sys.exit(main())
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/automatic_speech_recognition.py
╚══════════════════════════════════════════════════════════════════════════════╝

# Lightweight type definitions for ASR interactions.

from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Literal, Optional, Union


AutomaticSpeechRecognitionEarlyStoppingEnum = Literal["never"]


@dataclass
class AutomaticSpeechRecognitionGenerationParameters:
    """Parametrization of the text generation process"""

    do_sample: Optional[bool] = None
    """Whether to use sampling instead of greedy decoding when generating new tokens."""
    early_stopping: Optional[Union[bool, "AutomaticSpeechRecognitionEarlyStoppingEnum"]] = None
    """Controls the stopping condition for beam-based methods."""
    epsilon_cutoff: Optional[float] = None
    """If set to float strictly between 0 and 1, only tokens with a conditional probability
    greater than epsilon_cutoff will be sampled. In the paper, suggested values range from
    3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language
    Model Desmoothing](https://hf.co/papers/2210.15191) for more details.
    """
    eta_cutoff: Optional[float] = None
    """Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to
    float strictly between 0 and 1, a token is only considered if it is greater than either
    eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter
    term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In
    the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
    See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)
    for more details.
    """
    max_length: Optional[int] = None
    """The maximum length (in tokens) of the generated text, including the input."""
    max_new_tokens: Optional[int] = None
    """The maximum number of tokens to generate. Takes precedence over max_length."""
    min_length: Optional[int] = None
    """The minimum length (in tokens) of the generated text, including the input."""
    min_new_tokens: Optional[int] = None
    """The minimum number of tokens to generate. Takes precedence over min_length."""
    num_beam_groups: Optional[int] = None
    """Number of groups to divide num_beams into in order to ensure diversity among different
    groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.
    """
    num_beams: Optional[int] = None
    """Number of beams to use for beam search."""
    penalty_alpha: Optional[float] = None
    """The value balances the model confidence and the degeneration penalty in contrastive
    search decoding.
    """
    temperature: Optional[float] = None
    """The value used to modulate the next token probabilities."""
    top_k: Optional[int] = None
    """The number of highest probability vocabulary tokens to keep for top-k-filtering."""
    top_p: Optional[float] = None
    """If set to float < 1, only the smallest set of most probable tokens with probabilities
    that add up to top_p or higher are kept for generation.
    """
    typical_p: Optional[float] = None
    """Local typicality measures how similar the conditional probability of predicting a target
    token next is to the expected conditional probability of predicting a random token next,
    given the partial text already generated. If set to float < 1, the smallest set of the
    most locally typical tokens with probabilities that add up to typical_p or higher are
    kept for generation. See [this paper](https://hf.co/papers/2202.00666) for more details.
    """
    use_cache: Optional[bool] = None
    """Whether the model should use the past last key/values attentions to speed up decoding"""


@dataclass
class AutomaticSpeechRecognitionParameters:
    """Additional inference parameters for Automatic Speech Recognition"""

    generation_parameters: Optional[AutomaticSpeechRecognitionGenerationParameters] = None
    """Parametrization of the text generation process"""
    return_timestamps: Optional[bool] = None
    """Whether to output corresponding timestamps with the generated text"""


@dataclass
class AutomaticSpeechRecognitionInput:
    """Inputs for Automatic Speech Recognition inference"""

    inputs: str
    """The input audio data as a base64-encoded string. If no `parameters` are provided, you can
    also provide the audio data as a raw bytes payload.
    """
    parameters: Optional[AutomaticSpeechRecognitionParameters] = None
    """Additional inference parameters for Automatic Speech Recognition"""


@dataclass
class AutomaticSpeechRecognitionOutputChunk:
    text: str
    """A chunk of text identified by the model"""
    timestamp: List[float]
    """The start and end timestamps corresponding with the text"""


@dataclass
class AutomaticSpeechRecognitionOutput:
    """Outputs of inference for the Automatic Speech Recognition task"""

    text: str
    """The recognized text."""
    chunks: Optional[List[AutomaticSpeechRecognitionOutputChunk]] = None
    """When returnTimestamps is enabled, chunks contains a list of audio chunks identified by
    the model.
    """
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/audio_io.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Low-level audio capture and playback utilities."""

from __future__ import annotations

import queue
from dataclasses import dataclass
from typing import Generator, Iterable, Optional

import numpy as np
import sounddevice as sd
import soundfile as sf
import librosa

from .config import AudioSettings


@dataclass(slots=True)
class AudioIO:
    """Wraps microphone capture and speaker playback."""

    settings: AudioSettings

    def __post_init__(self) -> None:
        self._q: "queue.Queue[np.ndarray]" = queue.Queue()

    def microphone_stream(self) -> Generator[np.ndarray, None, None]:
        """Yield audio chunks from the default microphone."""

        def _callback(indata: np.ndarray, frames: int, time, status) -> None:  # type: ignore[override]
            if status:
                print(f"[AudioIO] stream status: {status}")
            self._q.put(indata.copy())

        with sd.InputStream(
            samplerate=self.settings.sample_rate,
            channels=self.settings.channels,
            dtype="float32",
            callback=_callback,
        ):
            while True:
                chunk = self._q.get()
                yield chunk

    def rms(self, audio: np.ndarray) -> float:
        """Root mean square energy for silence detection."""
        if audio.size == 0:
            return 0.0
        return float(np.sqrt(np.mean(np.square(audio))))

    def record_phrase(self, seconds: float) -> np.ndarray:
        """Blocking recording helper used for canonical phrases."""
        frames = int(seconds * self.settings.sample_rate)
        recording = sd.rec(frames, samplerate=self.settings.sample_rate, channels=self.settings.channels, dtype="float32")
        sd.wait()
        return recording

    def play(self, audio: np.ndarray) -> None:
        """Play float32 waveform."""
        sd.play(audio, samplerate=self.settings.sample_rate)
        sd.wait()

    def save_wav(self, audio: np.ndarray, path: str | "np.ndarray") -> str:
        """Save audio to disk in WAV format."""
        sf.write(path, audio, self.settings.sample_rate)
        return str(path)

    def load_wav(self, path: str) -> np.ndarray:
        """Load waveform as float32 numpy array."""
        data, sr = sf.read(path, dtype="float32")
        if sr != self.settings.sample_rate:
            if data.ndim > 1:
                data = np.mean(data, axis=1)
            data = librosa.resample(data, orig_sr=sr, target_sr=self.settings.sample_rate)
        return np.asarray(data, dtype=np.float32)


def chunked_audio(stream: Iterable[np.ndarray], seconds: float, samplerate: int) -> Generator[np.ndarray, None, None]:
    """Combine small chunks from sounddevice into fixed-length windows."""
    buffer: Optional[np.ndarray] = None
    chunk_frames = int(seconds * samplerate)
    for block in stream:
        buffer = block if buffer is None else np.concatenate([buffer, block], axis=0)
        while buffer is not None and len(buffer) >= chunk_frames:
            yield buffer[:chunk_frames]
            buffer = buffer[chunk_frames:] if len(buffer) > chunk_frames else None
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/unified_system_agi_core.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
agi_core.py

Advanced unified cognitive substrate: hardware interface, relational matrix,
multi-engine cognition, emotional chemistry, memory, and planning. Designed
to be run as a production-capable module integrated with the Opportunity
Synthesis service.

Notes:
- Deterministic, high-quality local embeddings via random-projection + hashing.
- Safe, deterministic components (no external ML downloads required).
- Exposes a clear programmatic API for stepping the agent and querying state.
"""

import os
import time
import math
import random
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from threading import Lock

# -------------------------
# Utilities
# -------------------------

def now_seconds() -> float:
    return time.time()

def stable_hash_bytes(s: str, length: int = 256) -> bytes:
    # deterministic hashing into bytes array using SHAKE-like approach without external libs
    import hashlib
    h = hashlib.blake2b(digest_size=32)
    h.update(s.encode("utf8"))
    base = h.digest()
    out = bytearray()
    i = 0
    while len(out) < length:
        h2 = hashlib.blake2b(digest_size=32)
        h2.update(base)
        h2.update(bytes([i]))
        out.extend(h2.digest())
        i += 1
    return bytes(out[:length])

# -------------------------
# Hardware Abstraction
# -------------------------

class BitRegister:
    def __init__(self, name: str, size: int = 128):
        self.name = name
        self.size = size
        self.bits = np.zeros(size, dtype=np.uint8)
        self.noise_rate = 2e-6
        self.lock = Lock()

    def write_int(self, value: int):
        with self.lock:
            for i in range(self.size):
                self.bits[i] = (value >> i) & 1

    def read_int(self) -> int:
        with self.lock:
            if random.random() < self.noise_rate:
                idx = random.randrange(self.size)
                self.bits[idx] ^= 1
            out = 0
            for i in range(self.size):
                out |= int(self.bits[i]) << i
            return out

    def set_bit(self, idx: int, val: int):
        with self.lock:
            self.bits[idx % self.size] = 1 if val else 0

    def get_bit(self, idx: int) -> int:
        with self.lock:
            return int(self.bits[idx % self.size])

    def as_bitstring(self) -> str:
        with self.lock:
            return ''.join(str(int(b)) for b in self.bits[::-1])

class SimulatedHardware:
    def __init__(self, adc_channels: int = 16):
        self.cpu_register = BitRegister("CPU_REG", size=256)
        self.gpio = BitRegister("GPIO", size=64)
        self.adc = np.zeros(adc_channels, dtype=float)
        self.temp_C = 35.0
        self.freq_GHz = 1.2

    def poll_sensors(self):
        # realistic sensor noise and drift
        self.adc += np.random.randn(len(self.adc)) * 0.005
        self.adc = np.clip(self.adc, -5.0, 5.0)
        self.temp_C += (self.freq_GHz - 1.0) * 0.02 + np.random.randn() * 0.01

    def set_frequency(self, ghz: float):
        self.freq_GHz = float(max(0.2, min(ghz, 5.0)))

    def as_status(self):
        return {
            "freq_GHz": round(self.freq_GHz, 3),
            "temp_C": round(self.temp_C, 3),
            "cpu_reg": self.cpu_register.as_bitstring(),
            "gpio": self.gpio.as_bitstring(),
            "adc": [round(float(x), 4) for x in self.adc.tolist()],
        }

# -------------------------
# Relational Matrix
# -------------------------

class RelationalMatrix:
    def __init__(self, n_system: int, n_apparatus: int):
        self.n_system = n_system
        self.n_apparatus = n_apparatus
        # complex amplitudes
        self.R = (np.random.randn(n_system, n_apparatus) + 1j * np.random.randn(n_system, n_apparatus)) * 0.02
        self.normalize_rows()

    def normalize_rows(self):
        mags = np.linalg.norm(self.R, axis=1, keepdims=True)
        mags[mags == 0] = 1.0
        self.R = self.R / mags

    def bidirectional_weight(self, i: int, j: int) -> complex:
        # map apparatus j back to some system index deterministically
        reverse_i = j % self.n_system
        reverse_j = i % self.n_apparatus
        return self.R[i, j] * np.conj(self.R[reverse_i, reverse_j])

    def probability_for_system(self, i: int) -> float:
        weights = np.array([abs(self.bidirectional_weight(i, j)) for j in range(self.n_apparatus)])
        s = np.sum(weights)
        return float(s / (np.sum(weights) + 1e-12))

    def update_hebbian(self, pre_idx: int, post_idx: int, lr: float = 1e-3):
        i = pre_idx % self.n_system
        j = post_idx % self.n_apparatus
        self.R[i, j] += lr * (1.0 + 0.05j)
        self.normalize_rows()

# -------------------------
# Thought Engines
# -------------------------

class ThoughtEngines:
    def __init__(self, n_nodes: int):
        self.n = n_nodes
        self.b = np.zeros(n_nodes)
        self.h = np.zeros(n_nodes)
        self.kappa = np.zeros(n_nodes)
        self.mu = np.zeros(n_nodes)
        # stateful noise seeds for reproducibility
        self._rng = np.random.RandomState(42)

    def step(self, relational: RelationalMatrix, inputs: np.ndarray, dt: float = 0.1):
        n = self.n
        if inputs is None:
            inputs = np.zeros(n)
        # coupling matrix from relational matrix (n x n)
        R = relational.R
        affin = np.real(R @ R.conj().T)
        maxval = np.max(np.abs(affin)) if np.max(np.abs(affin)) > 0 else 1.0
        W = affin / maxval

        # perspective
        db = 0.12 * (inputs * np.tanh(inputs)) - 0.05 * self.b + 0.03 * (W @ self.b - np.sum(W, axis=1) * self.b)
        self.b += db * dt

        # speculation with structured stochasticity
        eps = self._rng.randn(n) * 0.02
        dh = 0.10 * (inputs + eps) - 0.06 * self.h + 0.03 * (W @ self.h - np.sum(W, axis=1) * self.h)
        self.h += dh * dt

        # kaleidoscope
        dk = 0.08 * (self.b + 0.5 * self.h) - 0.04 * self.kappa + 0.02 * (W @ self.kappa - np.sum(W, axis=1) * self.kappa)
        self.kappa += dk * dt

        # mirror
        mismatch = np.abs(self.b - np.mean(self.b))
        dmu = -0.07 * mismatch + 0.05 * np.std(self.h) + 0.03 * (W @ self.mu - np.sum(W, axis=1) * self.mu)
        self.mu += dmu * dt

        # clip numeric stability
        for arr in (self.b, self.h, self.kappa, self.mu):
            np.clip(arr, -12.0, 12.0, out=arr)

# -------------------------
# Emotional Chemistry
# -------------------------

class EmotionalChemistry:
    def __init__(self):
        self.DA = 0.5
        self.Ser = 0.5
        self.NE = 0.5

    def step(self, reward: float, mood_signal: float, arousal: float, dt: float = 0.1):
        self.DA += (0.9 * reward - 0.12 * self.DA) * dt
        self.Ser += (0.4 * mood_signal - 0.06 * self.Ser) * dt
        self.NE += (0.65 * arousal - 0.08 * self.NE) * dt
        self.DA = float(np.clip(self.DA, 0.0, 1.0))
        self.Ser = float(np.clip(self.Ser, 0.0, 1.0))
        self.NE = float(np.clip(self.NE, 0.0, 1.0))

    def vector(self) -> List[float]:
        return [self.DA, self.Ser, self.NE]

# -------------------------
# Memory
# -------------------------

class MemorySystem:
    def __init__(self, embedding_dim: int = 128, capacity: int = 10000):
        self.embedding_dim = embedding_dim
        self.capacity = capacity
        self.episodic = []  # list of (ts, emb, text)
        self.semantic = {}  # key -> emb
        self._rng = np.random.RandomState(12345)
        # We'll derive a deterministic random projection matrix seeded by a constant
        self.random_proj = self._rng.randn(self.embedding_dim, 256) * 0.01

    def embed(self, text: str) -> np.ndarray:
        b = stable_hash_bytes(text, length=256)
        arr = np.frombuffer(b, dtype=np.uint8).astype(np.float32)
        emb = self.random_proj @ arr
        n = np.linalg.norm(emb)
        return emb / (n + 1e-12)

    def store_episode(self, text: str):
        emb = self.embed(text)
        ts = now_seconds()
        if len(self.episodic) >= self.capacity:
            self.episodic.pop(0)
        self.episodic.append((ts, emb, text))

    def retrieve(self, query: str, top_k: int = 5):
        q_emb = self.embed(query)
        sims = []
        for ts, emb, text in self.episodic:
            sims.append((float(np.dot(q_emb, emb)), ts, text))
        sims.sort(reverse=True, key=lambda x: x[0])
        return sims[:top_k]

    def store_semantic(self, key: str, text: str):
        self.semantic[key] = self.embed(text)

    def lookup_semantic(self, key: str):
        return self.semantic.get(key, None)

# -------------------------
# Planner
# -------------------------

class Planner:
    def __init__(self, hardware: SimulatedHardware, relational: RelationalMatrix):
        self.hw = hardware
        self.rel = relational
        # concrete actions map to methods to keep safe
        self.actions = [
            ("increase_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz + 0.1)),
            ("decrease_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz - 0.1)),
            ("toggle_gpio", lambda: self.hw.gpio.set_bit(random.randint(0, self.hw.gpio.size-1), random.randint(0,1))),
            ("no_op", lambda: None),
        ]

    def score_actions(self, thought: ThoughtEngines) -> List[Tuple[float, int]]:
        scores = []
        for idx, (name, fn) in enumerate(self.actions):
            sys_idx = idx % self.rel.n_system
            app_idx = idx % self.rel.n_apparatus
            rweight = abs(self.rel.bidirectional_weight(sys_idx, app_idx))
            cog_signal = float(np.tanh(np.mean(thought.b) + np.mean(thought.kappa)))
            score = rweight * (1.0 + cog_signal)
            # small diversity noise
            score += random.random() * 1e-6
            scores.append((score, idx))
        scores.sort(reverse=True, key=lambda x: x[0])
        return scores

    def select_and_execute(self, thought: ThoughtEngines) -> str:
        scored = self.score_actions(thought)
        weights = np.array([s for s, i in scored], dtype=float)
        if weights.sum() <= 0:
            idx = scored[0][1]
        else:
            probs = weights / weights.sum()
            choice = np.random.choice(len(scored), p=probs)
            idx = scored[choice][1]
        name, fn = self.actions[idx]
        fn()
        return name

# -------------------------
# AGI System (orchestrator)
# -------------------------

class AGISystem:
    def __init__(self, n_nodes: int = 32, n_apparatus: int = 24):
        self.hw = SimulatedHardware(adc_channels=24)
        self.rel = RelationalMatrix(n_nodes, n_apparatus)
        self.thought = ThoughtEngines(n_nodes)
        self.emotion = EmotionalChemistry()
        self.memory = MemorySystem(embedding_dim=128, capacity=10000)
        self.planner = Planner(self.hw, self.rel)
        self.t = 0.0
        self.dt = 0.1
        self.history = []
        self.lock = Lock()
        # seed semantic memory
        self.memory.store_semantic("agent_identity", "A private, self-contained cognitive assistant for execution and opportunity synthesis.")
        self.memory.store_episode("system initialized")

    def step(self, user_input: Optional[str] = None) -> Dict[str, Any]:
        with self.lock:
            self.hw.poll_sensors()
            n = self.thought.n
            sensor_vec = np.concatenate([self.hw.adc, np.array([self.hw.temp_C, self.hw.freq_GHz])])
            inputs = np.zeros(n)
            ssum = float(np.sum(sensor_vec))
            for i in range(n):
                inputs[i] = float(np.tanh(ssum * 0.0005 + random.random() * 0.01))
            if user_input:
                self.memory.store_episode(user_input)
                emb = self.memory.embed(user_input)
                bias = float(np.tanh(np.mean(emb))) * 0.5
                inputs += bias
            self.thought.step(self.rel, inputs, dt=self.dt)
            reward = float(np.clip(np.mean(inputs), -1, 1))
            mood = float(np.tanh(np.mean(self.thought.b)))
            arousal = float(np.abs(np.std(self.thought.h)))
            self.emotion.step(reward, mood, arousal, dt=self.dt)
            most_active_node = int(np.argmax(np.abs(self.thought.kappa)))
            apparatus_idx = int(abs(int((np.sum(self.hw.adc) * 100) % self.rel.n_apparatus)))
            self.rel.update_hebbian(most_active_node, apparatus_idx, lr=1e-3)
            action_name = self.planner.select_and_execute(self.thought)
            self.rel.normalize_rows()
            conn_metrics = self.relational_consciousness_metrics()
            log_item = {
                "t": self.t,
                "action": action_name,
                "hw": self.hw.as_status(),
                "emotion": {"DA": self.emotion.DA, "Ser": self.emotion.Ser, "NE": self.emotion.NE},
                "thought_summary": {
                    "b_mean": float(np.mean(self.thought.b)),
                    "h_mean": float(np.mean(self.thought.h)),
                    "kappa_mean": float(np.mean(self.thought.kappa)),
                    "mu_mean": float(np.mean(self.thought.mu)),
                },
                "consciousness": conn_metrics
            }
            self.history.append(log_item)
            self.t += self.dt
            return log_item

    def relational_consciousness_metrics(self) -> Dict[str, float]:
        diag = np.array([abs(self.rel.R[i, i % self.rel.n_apparatus]) for i in range(min(self.rel.n_system, self.rel.n_apparatus))])
        coherence = float(np.mean(diag))
        awareness = float(np.clip(self.emotion.DA * (1.0 + np.tanh(np.mean(self.thought.b))), 0.0, 1.0))
        activities = np.concatenate([self.thought.b, self.thought.h, self.thought.kappa, self.thought.mu])
        integrated_info = float(np.var(activities))
        return {"coherence": coherence, "awareness": awareness, "phi_proxy": integrated_info}

    def respond(self, user_input: str) -> str:
        log = self.step(user_input)
        candidates = self.memory.retrieve(user_input, top_k=3)
        reply_parts = []
        if candidates:
            reply_parts.append("I recall: " + "; ".join([c for _, _, c in candidates[:2]]))
        if log["consciousness"]["awareness"] > 0.6:
            reply_parts.append("I am engaged and reflecting on that.")
        elif log["consciousness"]["phi_proxy"] > 0.08:
            reply_parts.append("This seems important; I'll think further.")
        else:
            reply_parts.append("Noted and stored.")
        da = log["emotion"]["DA"]
        mood = "positive" if da > 0.55 else "neutral" if da > 0.45 else "cautious"
        reply_parts.append(f"My mood is {mood}. Action taken: {log['action']}.")
        return " ".join(reply_parts)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/guidance.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Voice guidance that turns the companion into a calming friend."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional

from .advanced_voice_mimic import VoiceCrystal
from .audio_io import AudioIO
from .data_store import DataStore


@dataclass(frozen=True)
class GuidanceScript:
    event: str
    title: str
    message: str
    style: str = "neutral"  # Default voice style
    breathing_steps: Optional[List[str]] = None


GUIDANCE_SCRIPTS: Dict[str, GuidanceScript] = {
    "anxious": GuidanceScript(
        event="anxious",
        title="Calm Breathing Buddy",
        message="Let's pause together. We will take three slow breaths.",
        style="calm",
        breathing_steps=[
            "Breath one: in through your nose for four, out for six.",
            "Breath two: shoulders down, think of your favorite safe place.",
            "Breath three: whisper your favorite color as you exhale.",
        ],
    ),
    "perseveration": GuidanceScript(
        event="perseveration",
        title="New Adventure Prompt",
        message="I hear that phrase a lot. Want to try a silly switch up together?",
        style="neutral",
        breathing_steps=[
            "Let's count five dinosaurs or cars before we try again.",
        ],
    ),
    "high_energy": GuidanceScript(
        event="high_energy",
        title="Movement Reset",
        message="Sounds like lots of energy! Let's stretch arms up high, shake them out, and wiggle toes.",
        style="calm",
    ),
    "encouragement": GuidanceScript(
        event="encouragement",
        title="Cheer Squad",
        message="Nice work! I love how you said that. Ready for the next word when you are.",
        style="excited",
    ),
    "caregiver_reset": GuidanceScript(
        event="caregiver_reset",
        title="Caregiver Breath",
        message="This is your reminder to take a sip of water, drop your shoulders, and breathe. You’re doing great.",
        style="calm",
    ),
}


class GuidanceCoach:
    """Delivers friendly prompts via voice and logs the event."""

    def __init__(self, voice: VoiceCrystal, audio_io: AudioIO, data_store: DataStore) -> None:
        self.voice = voice
        self.audio_io = audio_io
        self.data_store = data_store

    def speak(self, event: str, override_text: str | None = None) -> None:
        script = GUIDANCE_SCRIPTS.get(event)
        if not script and not override_text:
            return

        text = override_text or self._build_message(script)
        style = script.style if script else "neutral"
        audio = self.voice.speak(text, style=style, mode="outer")
        if audio.size > 0:
            self.audio_io.play(audio)
        self.data_store.log_guidance_event(event, (script.title if script else "Custom"), text)


    def _build_message(self, script: GuidanceScript | None) -> str:
        if not script:
            return ""
        parts = [script.message]
        if script.breathing_steps:
            parts.extend(script.breathing_steps)
        return " ".join(parts)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/unified_system_agi_core (4).py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
agi_core.py

Advanced unified cognitive substrate: hardware interface, relational matrix,
multi-engine cognition, emotional chemistry, memory, and planning. Designed
to be run as a production-capable module integrated with the Opportunity
Synthesis service.

Notes:
- Deterministic, high-quality local embeddings via random-projection + hashing.
- Safe, deterministic components (no external ML downloads required).
- Exposes a clear programmatic API for stepping the agent and querying state.
"""

import os
import time
import math
import random
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from threading import Lock

# -------------------------
# Utilities
# -------------------------

def now_seconds() -> float:
    return time.time()

def stable_hash_bytes(s: str, length: int = 256) -> bytes:
    # deterministic hashing into bytes array using SHAKE-like approach without external libs
    import hashlib
    h = hashlib.blake2b(digest_size=32)
    h.update(s.encode("utf8"))
    base = h.digest()
    out = bytearray()
    i = 0
    while len(out) < length:
        h2 = hashlib.blake2b(digest_size=32)
        h2.update(base)
        h2.update(bytes([i]))
        out.extend(h2.digest())
        i += 1
    return bytes(out[:length])

# -------------------------
# Hardware Abstraction
# -------------------------

class BitRegister:
    def __init__(self, name: str, size: int = 128):
        self.name = name
        self.size = size
        self.bits = np.zeros(size, dtype=np.uint8)
        self.noise_rate = 2e-6
        self.lock = Lock()

    def write_int(self, value: int):
        with self.lock:
            for i in range(self.size):
                self.bits[i] = (value >> i) & 1

    def read_int(self) -> int:
        with self.lock:
            if random.random() < self.noise_rate:
                idx = random.randrange(self.size)
                self.bits[idx] ^= 1
            out = 0
            for i in range(self.size):
                out |= int(self.bits[i]) << i
            return out

    def set_bit(self, idx: int, val: int):
        with self.lock:
            self.bits[idx % self.size] = 1 if val else 0

    def get_bit(self, idx: int) -> int:
        with self.lock:
            return int(self.bits[idx % self.size])

    def as_bitstring(self) -> str:
        with self.lock:
            return ''.join(str(int(b)) for b in self.bits[::-1])

class SimulatedHardware:
    def __init__(self, adc_channels: int = 16):
        self.cpu_register = BitRegister("CPU_REG", size=256)
        self.gpio = BitRegister("GPIO", size=64)
        self.adc = np.zeros(adc_channels, dtype=float)
        self.temp_C = 35.0
        self.freq_GHz = 1.2

    def poll_sensors(self):
        # realistic sensor noise and drift
        self.adc += np.random.randn(len(self.adc)) * 0.005
        self.adc = np.clip(self.adc, -5.0, 5.0)
        self.temp_C += (self.freq_GHz - 1.0) * 0.02 + np.random.randn() * 0.01

    def set_frequency(self, ghz: float):
        self.freq_GHz = float(max(0.2, min(ghz, 5.0)))

    def as_status(self):
        return {
            "freq_GHz": round(self.freq_GHz, 3),
            "temp_C": round(self.temp_C, 3),
            "cpu_reg": self.cpu_register.as_bitstring(),
            "gpio": self.gpio.as_bitstring(),
            "adc": [round(float(x), 4) for x in self.adc.tolist()],
        }

# -------------------------
# Relational Matrix
# -------------------------

class RelationalMatrix:
    def __init__(self, n_system: int, n_apparatus: int):
        self.n_system = n_system
        self.n_apparatus = n_apparatus
        # complex amplitudes
        self.R = (np.random.randn(n_system, n_apparatus) + 1j * np.random.randn(n_system, n_apparatus)) * 0.02
        self.normalize_rows()

    def normalize_rows(self):
        mags = np.linalg.norm(self.R, axis=1, keepdims=True)
        mags[mags == 0] = 1.0
        self.R = self.R / mags

    def bidirectional_weight(self, i: int, j: int) -> complex:
        # map apparatus j back to some system index deterministically
        reverse_i = j % self.n_system
        reverse_j = i % self.n_apparatus
        return self.R[i, j] * np.conj(self.R[reverse_i, reverse_j])

    def probability_for_system(self, i: int) -> float:
        weights = np.array([abs(self.bidirectional_weight(i, j)) for j in range(self.n_apparatus)])
        s = np.sum(weights)
        return float(s / (np.sum(weights) + 1e-12))

    def update_hebbian(self, pre_idx: int, post_idx: int, lr: float = 1e-3):
        i = pre_idx % self.n_system
        j = post_idx % self.n_apparatus
        self.R[i, j] += lr * (1.0 + 0.05j)
        self.normalize_rows()

# -------------------------
# Thought Engines
# -------------------------

class ThoughtEngines:
    def __init__(self, n_nodes: int):
        self.n = n_nodes
        self.b = np.zeros(n_nodes)
        self.h = np.zeros(n_nodes)
        self.kappa = np.zeros(n_nodes)
        self.mu = np.zeros(n_nodes)
        # stateful noise seeds for reproducibility
        self._rng = np.random.RandomState(42)

    def step(self, relational: RelationalMatrix, inputs: np.ndarray, dt: float = 0.1):
        n = self.n
        if inputs is None:
            inputs = np.zeros(n)
        # coupling matrix from relational matrix (n x n)
        R = relational.R
        affin = np.real(R @ R.conj().T)
        maxval = np.max(np.abs(affin)) if np.max(np.abs(affin)) > 0 else 1.0
        W = affin / maxval

        # perspective
        db = 0.12 * (inputs * np.tanh(inputs)) - 0.05 * self.b + 0.03 * (W @ self.b - np.sum(W, axis=1) * self.b)
        self.b += db * dt

        # speculation with structured stochasticity
        eps = self._rng.randn(n) * 0.02
        dh = 0.10 * (inputs + eps) - 0.06 * self.h + 0.03 * (W @ self.h - np.sum(W, axis=1) * self.h)
        self.h += dh * dt

        # kaleidoscope
        dk = 0.08 * (self.b + 0.5 * self.h) - 0.04 * self.kappa + 0.02 * (W @ self.kappa - np.sum(W, axis=1) * self.kappa)
        self.kappa += dk * dt

        # mirror
        mismatch = np.abs(self.b - np.mean(self.b))
        dmu = -0.07 * mismatch + 0.05 * np.std(self.h) + 0.03 * (W @ self.mu - np.sum(W, axis=1) * self.mu)
        self.mu += dmu * dt

        # clip numeric stability
        for arr in (self.b, self.h, self.kappa, self.mu):
            np.clip(arr, -12.0, 12.0, out=arr)

# -------------------------
# Emotional Chemistry
# -------------------------

class EmotionalChemistry:
    def __init__(self):
        self.DA = 0.5
        self.Ser = 0.5
        self.NE = 0.5

    def step(self, reward: float, mood_signal: float, arousal: float, dt: float = 0.1):
        self.DA += (0.9 * reward - 0.12 * self.DA) * dt
        self.Ser += (0.4 * mood_signal - 0.06 * self.Ser) * dt
        self.NE += (0.65 * arousal - 0.08 * self.NE) * dt
        self.DA = float(np.clip(self.DA, 0.0, 1.0))
        self.Ser = float(np.clip(self.Ser, 0.0, 1.0))
        self.NE = float(np.clip(self.NE, 0.0, 1.0))

    def vector(self) -> List[float]:
        return [self.DA, self.Ser, self.NE]

# -------------------------
# Memory
# -------------------------

class MemorySystem:
    def __init__(self, embedding_dim: int = 128, capacity: int = 10000):
        self.embedding_dim = embedding_dim
        self.capacity = capacity
        self.episodic = []  # list of (ts, emb, text)
        self.semantic = {}  # key -> emb
        self._rng = np.random.RandomState(12345)
        # We'll derive a deterministic random projection matrix seeded by a constant
        self.random_proj = self._rng.randn(self.embedding_dim, 256) * 0.01

    def embed(self, text: str) -> np.ndarray:
        b = stable_hash_bytes(text, length=256)
        arr = np.frombuffer(b, dtype=np.uint8).astype(np.float32)
        emb = self.random_proj @ arr
        n = np.linalg.norm(emb)
        return emb / (n + 1e-12)

    def store_episode(self, text: str):
        emb = self.embed(text)
        ts = now_seconds()
        if len(self.episodic) >= self.capacity:
            self.episodic.pop(0)
        self.episodic.append((ts, emb, text))

    def retrieve(self, query: str, top_k: int = 5):
        q_emb = self.embed(query)
        sims = []
        for ts, emb, text in self.episodic:
            sims.append((float(np.dot(q_emb, emb)), ts, text))
        sims.sort(reverse=True, key=lambda x: x[0])
        return sims[:top_k]

    def store_semantic(self, key: str, text: str):
        self.semantic[key] = self.embed(text)

    def lookup_semantic(self, key: str):
        return self.semantic.get(key, None)

# -------------------------
# Planner
# -------------------------

class Planner:
    def __init__(self, hardware: SimulatedHardware, relational: RelationalMatrix):
        self.hw = hardware
        self.rel = relational
        # concrete actions map to methods to keep safe
        self.actions = [
            ("increase_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz + 0.1)),
            ("decrease_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz - 0.1)),
            ("toggle_gpio", lambda: self.hw.gpio.set_bit(random.randint(0, self.hw.gpio.size-1), random.randint(0,1))),
            ("no_op", lambda: None),
        ]

    def score_actions(self, thought: ThoughtEngines) -> List[Tuple[float, int]]:
        scores = []
        for idx, (name, fn) in enumerate(self.actions):
            sys_idx = idx % self.rel.n_system
            app_idx = idx % self.rel.n_apparatus
            rweight = abs(self.rel.bidirectional_weight(sys_idx, app_idx))
            cog_signal = float(np.tanh(np.mean(thought.b) + np.mean(thought.kappa)))
            score = rweight * (1.0 + cog_signal)
            # small diversity noise
            score += random.random() * 1e-6
            scores.append((score, idx))
        scores.sort(reverse=True, key=lambda x: x[0])
        return scores

    def select_and_execute(self, thought: ThoughtEngines) -> str:
        scored = self.score_actions(thought)
        weights = np.array([s for s, i in scored], dtype=float)
        if weights.sum() <= 0:
            idx = scored[0][1]
        else:
            probs = weights / weights.sum()
            choice = np.random.choice(len(scored), p=probs)
            idx = scored[choice][1]
        name, fn = self.actions[idx]
        fn()
        return name

# -------------------------
# AGI System (orchestrator)
# -------------------------

class AGISystem:
    def __init__(self, n_nodes: int = 32, n_apparatus: int = 24):
        self.hw = SimulatedHardware(adc_channels=24)
        self.rel = RelationalMatrix(n_nodes, n_apparatus)
        self.thought = ThoughtEngines(n_nodes)
        self.emotion = EmotionalChemistry()
        self.memory = MemorySystem(embedding_dim=128, capacity=10000)
        self.planner = Planner(self.hw, self.rel)
        self.t = 0.0
        self.dt = 0.1
        self.history = []
        self.lock = Lock()
        # seed semantic memory
        self.memory.store_semantic("agent_identity", "A private, self-contained cognitive assistant for execution and opportunity synthesis.")
        self.memory.store_episode("system initialized")

    def step(self, user_input: Optional[str] = None) -> Dict[str, Any]:
        with self.lock:
            self.hw.poll_sensors()
            n = self.thought.n
            sensor_vec = np.concatenate([self.hw.adc, np.array([self.hw.temp_C, self.hw.freq_GHz])])
            inputs = np.zeros(n)
            ssum = float(np.sum(sensor_vec))
            for i in range(n):
                inputs[i] = float(np.tanh(ssum * 0.0005 + random.random() * 0.01))
            if user_input:
                self.memory.store_episode(user_input)
                emb = self.memory.embed(user_input)
                bias = float(np.tanh(np.mean(emb))) * 0.5
                inputs += bias
            self.thought.step(self.rel, inputs, dt=self.dt)
            reward = float(np.clip(np.mean(inputs), -1, 1))
            mood = float(np.tanh(np.mean(self.thought.b)))
            arousal = float(np.abs(np.std(self.thought.h)))
            self.emotion.step(reward, mood, arousal, dt=self.dt)
            most_active_node = int(np.argmax(np.abs(self.thought.kappa)))
            apparatus_idx = int(abs(int((np.sum(self.hw.adc) * 100) % self.rel.n_apparatus)))
            self.rel.update_hebbian(most_active_node, apparatus_idx, lr=1e-3)
            action_name = self.planner.select_and_execute(self.thought)
            self.rel.normalize_rows()
            conn_metrics = self.relational_consciousness_metrics()
            log_item = {
                "t": self.t,
                "action": action_name,
                "hw": self.hw.as_status(),
                "emotion": {"DA": self.emotion.DA, "Ser": self.emotion.Ser, "NE": self.emotion.NE},
                "thought_summary": {
                    "b_mean": float(np.mean(self.thought.b)),
                    "h_mean": float(np.mean(self.thought.h)),
                    "kappa_mean": float(np.mean(self.thought.kappa)),
                    "mu_mean": float(np.mean(self.thought.mu)),
                },
                "consciousness": conn_metrics
            }
            self.history.append(log_item)
            self.t += self.dt
            return log_item

    def relational_consciousness_metrics(self) -> Dict[str, float]:
        diag = np.array([abs(self.rel.R[i, i % self.rel.n_apparatus]) for i in range(min(self.rel.n_system, self.rel.n_apparatus))])
        coherence = float(np.mean(diag))
        awareness = float(np.clip(self.emotion.DA * (1.0 + np.tanh(np.mean(self.thought.b))), 0.0, 1.0))
        activities = np.concatenate([self.thought.b, self.thought.h, self.thought.kappa, self.thought.mu])
        integrated_info = float(np.var(activities))
        return {"coherence": coherence, "awareness": awareness, "phi_proxy": integrated_info}

    def respond(self, user_input: str) -> str:
        log = self.step(user_input)
        candidates = self.memory.retrieve(user_input, top_k=3)
        reply_parts = []
        if candidates:
            reply_parts.append("I recall: " + "; ".join([c for _, _, c in candidates[:2]]))
        if log["consciousness"]["awareness"] > 0.6:
            reply_parts.append("I am engaged and reflecting on that.")
        elif log["consciousness"]["phi_proxy"] > 0.08:
            reply_parts.append("This seems important; I'll think further.")
        else:
            reply_parts.append("Noted and stored.")
        da = log["emotion"]["DA"]
        mood = "positive" if da > 0.55 else "neutral" if da > 0.45 else "cautious"
        reply_parts.append(f"My mood is {mood}. Action taken: {log['action']}.")
        return " ".join(reply_parts)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/voice_mimic.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Voice synthesis backend that works on Python 3.12 without Coqui TTS."""

from __future__ import annotations

import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Sequence, Tuple

import librosa
import numpy as np
import pyttsx3
import soundfile as sf

from .config import SpeechModelSettings


def _ensure_mono(wav: np.ndarray) -> np.ndarray:
    if wav.ndim == 1:
        return wav
    return np.mean(wav, axis=1)


@dataclass(slots=True)
class VoiceMimic:
    """Handles speech synthesis in the child's voice."""

    settings: SpeechModelSettings

    def __post_init__(self) -> None:
        self._engine = pyttsx3.init()
        self._voice_bands: Sequence[Tuple[int, int]] = (
            (80, 300),
            (300, 1200),
            (1200, 4000),
            (4000, 8000),
        )
        self._voiceprint_eq: Optional[np.ndarray] = None
        if self.settings.tts_model_name:
            self._maybe_set_voice(self.settings.tts_model_name)
        if self.settings.tts_voice_clone_reference and self.settings.tts_voice_clone_reference.exists():
            self.update_voiceprint(self.settings.tts_voice_clone_reference)

    def synthesize(self, text: str) -> np.ndarray:
        if not text:
            return np.zeros(0, dtype=np.float32)

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp_path = Path(tmp.name)

        try:
            self._engine.save_to_file(text, str(tmp_path))
            self._engine.runAndWait()
            wav, sr = sf.read(tmp_path, dtype="float32")
        finally:
            tmp_path.unlink(missing_ok=True)

        wav = _ensure_mono(np.asarray(wav, dtype=np.float32))
        if sr != self.settings.tts_sample_rate:
            wav = librosa.resample(wav, orig_sr=sr, target_sr=self.settings.tts_sample_rate)
        if self._voiceprint_eq is not None:
            wav = self._apply_voiceprint(wav)
        return np.asarray(wav, dtype=np.float32)

    def update_voiceprint(self, new_reference: Path) -> None:
        if not new_reference.exists():
            return
        wav, sr = sf.read(new_reference, dtype="float32")
        wav = _ensure_mono(np.asarray(wav, dtype=np.float32))
        if sr != self.settings.tts_sample_rate:
            wav = librosa.resample(wav, orig_sr=sr, target_sr=self.settings.tts_sample_rate)
        self._voiceprint_eq = self._extract_eq_profile(wav)

    def _maybe_set_voice(self, desired: str) -> None:
        try:
            voices = self._engine.getProperty("voices") or []
        except Exception:
            return
        desired = desired.lower()
        for voice in voices:
            if desired in voice.id.lower() or desired in getattr(voice, "name", "").lower():
                self._engine.setProperty("voice", voice.id)
                return

    def _extract_eq_profile(self, wav: np.ndarray) -> Optional[np.ndarray]:
        if wav.size == 0:
            return None
        spectrum = np.abs(np.fft.rfft(wav))
        freqs = np.fft.rfftfreq(len(wav), 1.0 / self.settings.tts_sample_rate)
        eq_profile = []
        for low, high in self._voice_bands:
            mask = (freqs >= low) & (freqs < high)
            if not np.any(mask):
                eq_profile.append(1.0)
                continue
            eq_profile.append(float(spectrum[mask].mean() + 1e-8))
        eq = np.asarray(eq_profile, dtype=np.float32)
        avg = float(np.mean(eq) or 1.0)
        eq = np.clip(eq / avg, 0.5, 2.5)
        return eq

    def _apply_voiceprint(self, wav: np.ndarray) -> np.ndarray:
        if wav.size == 0 or self._voiceprint_eq is None:
            return wav
        spectrum = np.fft.rfft(wav)
        freqs = np.fft.rfftfreq(len(wav), 1.0 / self.settings.tts_sample_rate)
        shaped = spectrum.copy()
        for gain, (low, high) in zip(self._voiceprint_eq, self._voice_bands):
            mask = (freqs >= low) & (freqs < high)
            shaped[mask] *= gain
        result = np.fft.irfft(shaped, n=len(wav))
        max_val = np.max(np.abs(result)) or 1.0
        normalized = result / max(1.0, max_val)
        return np.asarray(normalized, dtype=np.float32)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/text_utils.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Shared text normalization helpers."""

from __future__ import annotations

import re
from difflib import SequenceMatcher


def normalize_simple(text: str) -> str:
    lowered = text.lower().strip()
    tokens = re.findall(r"[a-z0-9']+", lowered)
    return " ".join(tokens)


def similarity(a: str, b: str) -> float:
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
    return SequenceMatcher(None, a, b).ratio()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/reports.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Caregiver-facing reporting utilities."""

from __future__ import annotations

import csv
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict

from .config import CompanionConfig


@dataclass(slots=True)
class SummaryRow:
    attempts: int = 0
    corrections: int = 0


def summarize(config: CompanionConfig) -> Dict[str, SummaryRow]:
    results: Dict[str, SummaryRow] = defaultdict(SummaryRow)
    path = config.paths.metrics_csv
    if not path.exists():
        return {}
    with path.open() as f:
        reader = csv.DictReader(f)
        for row in reader:
            key = row.get("phrase_id") or "<unknown>"
            stats = results[key]
            stats.attempts += 1
            if row.get("needs_correction") == "1":
                stats.corrections += 1
    return results
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/similarity.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Audio similarity scoring using MFCC + DTW."""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path

import librosa
import numpy as np
from fastdtw import fastdtw

from .config import AudioSettings


@dataclass(slots=True)
class SimilarityScorer:
    settings: AudioSettings

    def _load_mfcc(self, path: Path) -> np.ndarray:
        audio, sr = librosa.load(path, sr=self.settings.sample_rate)
        return librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)

    def compare(self, reference: Path, attempt: Path) -> float:
        ref_mfcc = self._load_mfcc(reference)
        att_mfcc = self._load_mfcc(attempt)
        distance, _ = fastdtw(ref_mfcc.T, att_mfcc.T)
        return 1.0 / (1.0 + distance)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/speechinterventionsystem.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Legacy entrypoint that now proxies to the speech_system CLI."""

from __future__ import annotations

from speech_system.cli import main


if __name__ == "__main__":
    main()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/agent.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
KQBC Agent that bridges speech practice with an AGI substrate.

The ``KQBCAgent`` encapsulates an ``AGISystem`` from the unified system
core and provides simple APIs for evaluating whether a child's
utterance needs correction, updating the cognitive state based on new
inputs, and exposing a snapshot of the agent's internal status for
display.  This class acts as a mediator between the speech loop and
the cognitive machinery, allowing the rest of the system to remain
agnostic of the underlying AGI implementation.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional
from difflib import SequenceMatcher

from unified_system_agi_core import AGISystem

from .config import CompanionConfig


@dataclass
class AGIStatus:
    """A lightweight snapshot of the AGI state for display purposes."""

    freq_GHz: float = 0.0
    temp_C: float = 0.0
    DA: float = 0.0
    Ser: float = 0.0
    NE: float = 0.0
    coherence: float = 0.0
    awareness: float = 0.0
    phi_proxy: float = 0.0


class KQBCAgent:
    """Agent wrapper around the unified AGI system.

    Each instance owns an ``AGISystem`` and maintains the last log item
    returned from its step function.  The agent provides two main
    methods: ``evaluate_correction`` determines whether an utterance
    requires correction, and ``update_state`` feeds the utterance into
    the AGI and stores the resulting log.  Callers can retrieve a
    simple status dict via ``get_status`` for use in the GUI.
    """

    def __init__(self, config: Optional[CompanionConfig] = None, similarity_threshold: float = 0.78) -> None:
        self.config = config or CompanionConfig()
        self.agi = AGISystem()
        self.similarity_threshold = similarity_threshold
        self.last_log: Optional[Dict[str, object]] = None

    def _string_similarity(self, a: str, b: str) -> float:
        """Compute a simple similarity ratio between two strings.

        This uses Python's built-in ``difflib.SequenceMatcher`` which
        returns a value between 0 and 1 where 1 indicates identical
        strings.  It is sufficient for short phrases and avoids any
        heavy ML dependencies.
        """
        return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()

    def evaluate_correction(self, target_phrase: str, child_utterance: str) -> bool:
        """Return True if the utterance differs substantially from the target.

        The default implementation computes a simple similarity ratio and
        compares it against ``self.similarity_threshold``.  If the ratio
        falls below the threshold the method returns True to indicate a
        correction should be suggested.
        """
        similarity = self._string_similarity(target_phrase, child_utterance)
        return similarity < self.similarity_threshold

    def update_state(self, user_input: str) -> None:
        """Step the AGI system with the given user input.

        The AGI maintains an internal time and hardware state; each call
        polls sensors, updates thought engines, emotional chemistry and
        relational links, selects an action and logs the result.  The
        resulting log is stored on the agent for later retrieval.
        """
        log_item = self.agi.step(user_input=user_input)
        self.last_log = log_item

    def get_status(self) -> AGIStatus:
        """Return the current AGI status as a plain dataclass.

        If the agent has not yet been updated, returns default values.
        """
        if self.last_log is None:
            return AGIStatus()
        hw = self.last_log.get("hw", {})
        emotion = self.last_log.get("emotion", {})
        consciousness = self.last_log.get("consciousness", {})
        return AGIStatus(
            freq_GHz=float(hw.get("freq_GHz", 0.0)),
            temp_C=float(hw.get("temp_C", 0.0)),
            DA=float(emotion.get("DA", 0.0)),
            Ser=float(emotion.get("Ser", 0.0)),
            NE=float(emotion.get("NE", 0.0)),
            coherence=float(consciousness.get("coherence", 0.0)),
            awareness=float(consciousness.get("awareness", 0.0)),
            phi_proxy=float(consciousness.get("phi_proxy", 0.0)),
        )-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/data_store.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Persistence helpers for phrases, attempts, and caregiver metrics."""

from __future__ import annotations

import csv
import json
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from .config import CompanionConfig
from .text_utils import normalize_simple


@dataclass(slots=True)
class Phrase:
    phrase_id: str
    text: str
    audio_file: Path
    duration: float
    normalized_text: str


@dataclass(slots=True)
class DataStore:
    """Thin abstraction around the folder layout described in the docs."""

    config: CompanionConfig
    metadata_file: Path = field(init=False)

    def __post_init__(self) -> None:
        self.config.paths.ensure()
        self.metadata_file = self.config.paths.voices_dir / f"{self.config.child_id}_phrases.json"

    def _load_metadata(self) -> Dict[str, dict]:
        if not self.metadata_file.exists():
            return {}
        return json.loads(self.metadata_file.read_text())

    def _save_metadata(self, meta: Dict[str, dict]) -> None:
        self.metadata_file.write_text(json.dumps(meta, indent=2))

    def list_phrases(self) -> List[Phrase]:
        phrases = []
        for pid, data in self._load_metadata().items():
            phrases.append(
                Phrase(
                    phrase_id=pid,
                    text=data["text"],
                    audio_file=Path(data["file"]),
                    duration=data.get("duration", 0.0),
                    normalized_text=data.get("normalized_text") or normalize_simple(data["text"]),
                )
            )
        return phrases

    def save_phrase(self, phrase_id: str, text: str, audio_file: Path, duration: float) -> None:
        meta = self._load_metadata()
        meta[phrase_id] = {
            "text": text,
            "file": str(audio_file),
            "duration": duration,
            "normalized_text": normalize_simple(text),
        }
        self._save_metadata(meta)

    def log_attempt(
        self,
        phrase_id: Optional[str],
        phrase_text: Optional[str],
        attempt_audio: Optional[Path],
        stt_text: str,
        corrected_text: str,
        similarity: float,
        needs_correction: bool,
    ) -> None:
        header = [
            "timestamp_iso",
            "child_id",
            "phrase_id",
            "phrase_text",
            "attempt_audio",
            "raw_text",
            "corrected_text",
            "similarity",
            "needs_correction",
        ]
        new_row = [
            datetime.utcnow().isoformat(),
            self.config.child_id,
            phrase_id or "",
            phrase_text or "",
            str(attempt_audio) if attempt_audio else "",
            stt_text,
            corrected_text,
            f"{similarity:.3f}",
            "1" if needs_correction else "0",
        ]
        csv_exists = self.config.paths.metrics_csv.exists()
        with self.config.paths.metrics_csv.open("a", newline="") as f:
            writer = csv.writer(f)
            if not csv_exists:
                writer.writerow(header)
            writer.writerow(new_row)

    def log_guidance_event(self, event: str, title: str, message: str) -> None:
        header = ["timestamp_iso", "child_id", "event", "title", "message"]
        new_row = [
            datetime.utcnow().isoformat(),
            self.config.child_id,
            event,
            title,
            message,
        ]
        csv_exists = self.config.paths.guidance_csv.exists()
        with self.config.paths.guidance_csv.open("a", newline="") as f:
            writer = csv.writer(f)
            if not csv_exists:
                writer.writerow(header)
            writer.writerow(new_row)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/behavior_monitor.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Simple heuristics that monitor behavior cues to trigger guidance."""

from __future__ import annotations

from collections import deque
from dataclasses import dataclass
from typing import Deque


@dataclass
class BehaviorMonitor:
    """Tracks repeated corrections, perseveration, and high-energy speech."""

    max_phrase_history: int = 5
    anxious_threshold: int = 3
    perseveration_threshold: int = 3
    high_energy_rms: float = 0.08

    def __post_init__(self) -> None:
        self._phrases: Deque[str] = deque(maxlen=self.max_phrase_history)
        self._correction_streak = 0
        self._last_event: str | None = None

    def register(self, normalized_text: str, needs_correction: bool, rms: float) -> str | None:
        event: str | None = None

        if needs_correction:
            self._correction_streak += 1
        else:
            if self._correction_streak >= self.anxious_threshold:
                event = "encouragement"
            self._correction_streak = 0

        self._phrases.append(normalized_text)

        if self._correction_streak >= self.anxious_threshold:
            event = "anxious"
        elif normalized_text and list(self._phrases).count(normalized_text) >= self.perseveration_threshold:
            event = event or "perseveration"
        elif rms >= self.high_energy_rms:
            event = event or "high_energy"

        if event == self._last_event and event not in {"perseveration", "encouragement"}:
            return None
        if event:
            self._last_event = event
        return event
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/text_to_speech.py
╚══════════════════════════════════════════════════════════════════════════════╝

# Lightweight type definitions for Text-to-Speech interactions.

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Literal, Optional, Union


TextToSpeechEarlyStoppingEnum = Literal["never"]


@dataclass
class TextToSpeechGenerationParameters:
    """Parametrization of the text generation process"""

    do_sample: Optional[bool] = None
    """Whether to use sampling instead of greedy decoding when generating new tokens."""
    early_stopping: Optional[Union[bool, "TextToSpeechEarlyStoppingEnum"]] = None
    """Controls the stopping condition for beam-based methods."""
    epsilon_cutoff: Optional[float] = None
    """If set to float strictly between 0 and 1, only tokens with a conditional probability
    greater than epsilon_cutoff will be sampled. In the paper, suggested values range from
    3e-4 to 9e-4, depending on the size of the model. See [Truncation Sampling as Language
    Model Desmoothing](https://hf.co/papers/2210.15191) for more details.
    """
    eta_cutoff: Optional[float] = None
    """Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to
    float strictly between 0 and 1, a token is only considered if it is greater than either
    eta_cutoff or sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))). The latter
    term is intuitively the expected next token probability, scaled by sqrt(eta_cutoff). In
    the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
    See [Truncation Sampling as Language Model Desmoothing](https://hf.co/papers/2210.15191)
    for more details.
    """
    max_length: Optional[int] = None
    """The maximum length (in tokens) of the generated text, including the input."""
    max_new_tokens: Optional[int] = None
    """The maximum number of tokens to generate. Takes precedence over max_length."""
    min_length: Optional[int] = None
    """The minimum length (in tokens) of the generated text, including the input."""
    min_new_tokens: Optional[int] = None
    """The minimum number of tokens to generate. Takes precedence over min_length."""
    num_beam_groups: Optional[int] = None
    """Number of groups to divide num_beams into in order to ensure diversity among different
    groups of beams. See [this paper](https://hf.co/papers/1610.02424) for more details.
    """
    num_beams: Optional[int] = None
    """Number of beams to use for beam search."""
    penalty_alpha: Optional[float] = None
    """The value balances the model confidence and the degeneration penalty in contrastive
    search decoding.
    """
    temperature: Optional[float] = None
    """The value used to modulate the next token probabilities."""
    top_k: Optional[int] = None
    """The number of highest probability vocabulary tokens to keep for top-k-filtering."""
    top_p: Optional[float] = None
    """If set to float < 1, only the smallest set of most probable tokens with probabilities
    that add up to top_p or higher are kept for generation.
    """
    typical_p: Optional[float] = None
    """Local typicality measures how similar the conditional probability of predicting a target
    token next is to the expected conditional probability of predicting a random token next,
    given the partial text already generated. If set to float < 1, the smallest set of the
    most locally typical tokens with probabilities that add up to typical_p or higher are
    kept for generation. See [this paper](https://hf.co/papers/2202.00666) for more details.
    """
    use_cache: Optional[bool] = None
    """Whether the model should use the past last key/values attentions to speed up decoding"""


@dataclass
class TextToSpeechParameters:
    """Additional inference parameters for Text To Speech"""

    generation_parameters: Optional[TextToSpeechGenerationParameters] = None
    """Parametrization of the text generation process"""


@dataclass
class TextToSpeechInput:
    """Inputs for Text To Speech inference"""

    inputs: str
    """The input text data"""
    parameters: Optional[TextToSpeechParameters] = None
    """Additional inference parameters for Text To Speech"""


@dataclass
class TextToSpeechOutput:
    """Outputs of inference for the Text To Speech task"""

    audio: Any
    """The generated audio"""
    sampling_rate: Optional[float] = None
    """The sampling rate of the generated audio waveform."""
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/settings_store.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List


@dataclass(slots=True)
class SettingsStore:
    path: Path
    defaults: Dict[str, object] = field(default_factory=lambda: {
        "correction_echo_enabled": True,
        "support_voice_enabled": False,
        "support_phrases": [],
    })
    data: Dict[str, object] = field(init=False)

    def __post_init__(self) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        if self.path.exists():
            self.data = json.loads(self.path.read_text())
        else:
            self.data = {**self.defaults}
            self.save()
        for key, value in self.defaults.items():
            self.data.setdefault(key, value)

    def save(self) -> None:
        self.path.write_text(json.dumps(self.data, indent=2))

    def update(self, **kwargs: object) -> None:
        self.data.update({k: v for k, v in kwargs.items() if v is not None})
        self.save()

    def get_settings(self) -> Dict[str, object]:
        return dict(self.data)

    def add_support_phrase(self, phrase: str) -> None:
        phrases = list(self.data.get("support_phrases", []))
        phrases.append(phrase)
        self.data["support_phrases"] = phrases
        self.save()

    def list_support_phrases(self) -> List[str]:
        return list(self.data.get("support_phrases", []))
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/inner_voice.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional
import time
import numpy as np

from .advanced_voice_mimic import VoiceCrystal
from .audio_io import AudioIO
from .data_store import DataStore


@dataclass(slots=True)
class InnerVoiceConfig:
    log_events: bool = True
    min_gap_s: float = 2.0


@dataclass(slots=True)
class InnerVoiceEngine:
    voice: VoiceCrystal
    audio_io: AudioIO
    data_store: DataStore
    config: InnerVoiceConfig = field(default_factory=InnerVoiceConfig)
    last_spoke_ts: float = 0.0

    def speak_corrected(
        self,
        corrected_text: Optional[str],
        raw_text: Optional[str],
        prosody_source_wav: Optional[np.ndarray],
        prosody_source_sr: int,
    ) -> None:
        now = time.time()
        if now - self.last_spoke_ts < self.config.min_gap_s:
            return
        phrase = (corrected_text or raw_text or "").strip()
        if not phrase:
            return
        audio = self.voice.say_inner(
            phrase,
            style="calm",
            prosody_source_wav=prosody_source_wav,
            prosody_source_sr=prosody_source_sr,
        )
        if audio.size == 0:
            return
        self.audio_io.play(audio)
        self.last_spoke_ts = now
        if self.config.log_events:
            self.data_store.log_guidance_event(
                event="inner_echo",
                title="Inner Echo",
                message=phrase,
            )
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/speechinterventionsystem_2.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Secondary entrypoint maintained for backwards compatibility."""

from __future__ import annotations

from speech_system.cli import main


if __name__ == "__main__":
    main()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/prosody.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass
from typing import Optional

import librosa
import numpy as np


@dataclass(slots=True)
class ProsodyProfile:
    f0_hz: np.ndarray
    energy: np.ndarray
    times_s: np.ndarray
    frame_length: int
    hop_length: int
    sample_rate: int


def extract_prosody(
    wav: np.ndarray,
    sample_rate: int,
    frame_ms: float = 40.0,
    hop_ms: float = 20.0,
    fmin_hz: float = 80.0,
    fmax_hz: float = 600.0,
) -> ProsodyProfile:
    if wav.ndim > 1:
        wav = np.mean(wav, axis=1)
    wav = np.asarray(wav, dtype=np.float32)
    frame_length = max(int(sample_rate * frame_ms / 1000.0), 256)
    hop_length = max(int(sample_rate * hop_ms / 1000.0), 128)

    f0 = librosa.yin(
        wav,
        fmin=fmin_hz,
        fmax=fmax_hz,
        sr=sample_rate,
        frame_length=frame_length,
        hop_length=hop_length,
    )
    rms = librosa.feature.rms(
        y=wav,
        frame_length=frame_length,
        hop_length=hop_length,
        center=True,
    )[0]
    times = librosa.frames_to_time(
        np.arange(len(f0)),
        sr=sample_rate,
        hop_length=hop_length,
    )

    voiced = f0 > 0
    if np.any(voiced):
        median_f0 = float(np.median(f0[voiced]))
    else:
        median_f0 = 180.0
    f0 = np.where(voiced, f0, median_f0).astype(np.float32)
    rms = np.maximum(rms, 1e-4).astype(np.float32)

    return ProsodyProfile(
        f0_hz=f0,
        energy=rms,
        times_s=times.astype(np.float32),
        frame_length=frame_length,
        hop_length=hop_length,
        sample_rate=sample_rate,
    )


def _interp_to_num_frames(src: np.ndarray, num_frames: int) -> np.ndarray:
    if src.size == 0:
        return np.zeros(num_frames, dtype=np.float32)
    if src.size == num_frames:
        return src.astype(np.float32)
    x_old = np.linspace(0.0, 1.0, num=src.size, dtype=np.float32)
    x_new = np.linspace(0.0, 1.0, num=num_frames, dtype=np.float32)
    return np.interp(x_new, x_old, src).astype(np.float32)


def apply_prosody_to_tts(
    tts_wav: np.ndarray,
    tts_sample_rate: int,
    prosody: ProsodyProfile,
    strength_pitch: float = 1.0,
    strength_energy: float = 1.0,
) -> np.ndarray:
    if tts_wav.ndim > 1:
        tts_wav = np.mean(tts_wav, axis=1)
    tts_wav = np.asarray(tts_wav, dtype=np.float32)

    frame_length = max(int(tts_sample_rate * (prosody.frame_length / prosody.sample_rate)), 256)
    hop_length = max(int(tts_sample_rate * (prosody.hop_length / prosody.sample_rate)), 128)
    num_frames = 1 + max(0, (len(tts_wav) - frame_length) // hop_length)
    if num_frames <= 0:
        return tts_wav

    f0_child = _interp_to_num_frames(prosody.f0_hz, num_frames)
    energy_child = _interp_to_num_frames(prosody.energy, num_frames)

    try:
        f0_tts = librosa.yin(
            tts_wav,
            fmin=80.0,
            fmax=600.0,
            sr=tts_sample_rate,
            frame_length=frame_length,
            hop_length=hop_length,
        )
        voiced = f0_tts > 0
        if np.any(voiced):
            base_f0 = float(np.median(f0_tts[voiced]))
        else:
            base_f0 = float(np.median(f0_child))
    except Exception:
        base_f0 = float(np.median(f0_child))

    out = np.zeros(len(tts_wav) + frame_length, dtype=np.float32)
    window = np.hanning(frame_length).astype(np.float32)
    eps = 1e-6

    for i in range(num_frames):
        start = i * hop_length
        end = start + frame_length
        if start >= len(tts_wav):
            break
        frame = tts_wav[start:end]
        if len(frame) < frame_length:
            frame = np.pad(frame, (0, frame_length - len(frame)), mode="constant")

        target_f0 = float(f0_child[i])
        if base_f0 > 0:
            raw_ratio = target_f0 / base_f0
        else:
            raw_ratio = 1.0
        pitch_ratio = raw_ratio ** strength_pitch
        n_steps = 12.0 * np.log2(max(pitch_ratio, 1e-3))
        try:
            shifted = librosa.effects.pitch_shift(
                frame,
                sr=tts_sample_rate,
                n_steps=n_steps,
            )
        except Exception:
            shifted = frame
        if shifted.shape[0] != frame_length:
            if shifted.shape[0] > frame_length:
                shifted = shifted[:frame_length]
            else:
                shifted = np.pad(shifted, (0, frame_length - shifted.shape[0]))
        frame_rms = float(np.sqrt(np.mean(np.square(shifted)) + eps))
        target_rms = float(energy_child[i])
        if frame_rms > 0:
            ratio = (target_rms / frame_rms) ** strength_energy
        else:
            ratio = 1.0
        shifted *= ratio
        out[start:end] += shifted * window

    max_abs = float(np.max(np.abs(out)) + eps)
    if max_abs > 1.0:
        out = out / max_abs
    return out.astype(np.float32)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/app_backend.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import asyncio
import threading
from pathlib import Path
from typing import Any

from flask import Flask, jsonify, request

from speech_system.config import CONFIG
from speech_system.dashboard import create_app
from speech_system.settings_store import SettingsStore
from speech_system.speech_loop import SpeechLoop


def _start_speech_loop(loop: SpeechLoop) -> None:
    asyncio.run(loop.run())


def create_backend_app(settings_store: SettingsStore) -> Flask:
    app = create_app(CONFIG, settings_store=settings_store)

    @app.post("/api/backend/shutdown")
    def shutdown() -> Any:
        request.environ.get("werkzeug.server.shutdown", lambda: None)()
        return jsonify({"status": "ok"})

    return app


def sync_settings_from_store(store: SettingsStore) -> None:
    CONFIG.behavior.correction_echo_enabled = bool(store.data.get("correction_echo_enabled", True))
    CONFIG.behavior.support_voice_enabled = bool(store.data.get("support_voice_enabled", False))


def main() -> None:
    config_dir = CONFIG.paths.base_dir
    settings_path = config_dir / "settings.json"
    settings_store = SettingsStore(settings_path)
    sync_settings_from_store(settings_store)

    loop = SpeechLoop(CONFIG)
    thread = threading.Thread(target=_start_speech_loop, args=(loop,), daemon=True)
    thread.start()

    app = create_backend_app(settings_store)
    print(f"App backend running at http://0.0.0.0:8765")
    app.run(host="0.0.0.0", port=8765, debug=False, use_reloader=False)


if __name__ == "__main__":
    main()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/speech_synthesis.py
╚══════════════════════════════════════════════════════════════════════════════╝

# speech_synthesis.py

from gtts import gTTS
from typing import Dict

class SpeechSynthesizer:
    """Handles the conversion of textual knowledge into speech"""

    def __init__(self):
        self.voice_cache: Dict[str, str] = {}  # Cache for synthesized speech files

    def synthesize(self, text: str, language: str = "en") -> str:
        """
        Converts text to speech and saves the audio to a file.
        Returns the path to the audio file.
        """
        if text in self.voice_cache:
            return self.voice_cache[text]

        try:
            tts = gTTS(text=text, lang=language)
            file_path = f"speech_{hash(text)}.mp3"
            tts.save(file_path)
            self.voice_cache[text] = file_path
            return file_path
        except Exception as e:
            raise RuntimeError(f"Speech synthesis failed: {e}")

    def clear_cache(self):
        """Clears the voice cache"""
        self.voice_cache.clear()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/unified_system_agi_core (2).py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
agi_core.py

Advanced unified cognitive substrate: hardware interface, relational matrix,
multi-engine cognition, emotional chemistry, memory, and planning. Designed
to be run as a production-capable module integrated with the Opportunity
Synthesis service.

Notes:
- Deterministic, high-quality local embeddings via random-projection + hashing.
- Safe, deterministic components (no external ML downloads required).
- Exposes a clear programmatic API for stepping the agent and querying state.
"""

import os
import time
import math
import random
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from threading import Lock

# -------------------------
# Utilities
# -------------------------

def now_seconds() -> float:
    return time.time()

def stable_hash_bytes(s: str, length: int = 256) -> bytes:
    # deterministic hashing into bytes array using SHAKE-like approach without external libs
    import hashlib
    h = hashlib.blake2b(digest_size=32)
    h.update(s.encode("utf8"))
    base = h.digest()
    out = bytearray()
    i = 0
    while len(out) < length:
        h2 = hashlib.blake2b(digest_size=32)
        h2.update(base)
        h2.update(bytes([i]))
        out.extend(h2.digest())
        i += 1
    return bytes(out[:length])

# -------------------------
# Hardware Abstraction
# -------------------------

class BitRegister:
    def __init__(self, name: str, size: int = 128):
        self.name = name
        self.size = size
        self.bits = np.zeros(size, dtype=np.uint8)
        self.noise_rate = 2e-6
        self.lock = Lock()

    def write_int(self, value: int):
        with self.lock:
            for i in range(self.size):
                self.bits[i] = (value >> i) & 1

    def read_int(self) -> int:
        with self.lock:
            if random.random() < self.noise_rate:
                idx = random.randrange(self.size)
                self.bits[idx] ^= 1
            out = 0
            for i in range(self.size):
                out |= int(self.bits[i]) << i
            return out

    def set_bit(self, idx: int, val: int):
        with self.lock:
            self.bits[idx % self.size] = 1 if val else 0

    def get_bit(self, idx: int) -> int:
        with self.lock:
            return int(self.bits[idx % self.size])

    def as_bitstring(self) -> str:
        with self.lock:
            return ''.join(str(int(b)) for b in self.bits[::-1])

class SimulatedHardware:
    def __init__(self, adc_channels: int = 16):
        self.cpu_register = BitRegister("CPU_REG", size=256)
        self.gpio = BitRegister("GPIO", size=64)
        self.adc = np.zeros(adc_channels, dtype=float)
        self.temp_C = 35.0
        self.freq_GHz = 1.2

    def poll_sensors(self):
        # realistic sensor noise and drift
        self.adc += np.random.randn(len(self.adc)) * 0.005
        self.adc = np.clip(self.adc, -5.0, 5.0)
        self.temp_C += (self.freq_GHz - 1.0) * 0.02 + np.random.randn() * 0.01

    def set_frequency(self, ghz: float):
        self.freq_GHz = float(max(0.2, min(ghz, 5.0)))

    def as_status(self):
        return {
            "freq_GHz": round(self.freq_GHz, 3),
            "temp_C": round(self.temp_C, 3),
            "cpu_reg": self.cpu_register.as_bitstring(),
            "gpio": self.gpio.as_bitstring(),
            "adc": [round(float(x), 4) for x in self.adc.tolist()],
        }

# -------------------------
# Relational Matrix
# -------------------------

class RelationalMatrix:
    def __init__(self, n_system: int, n_apparatus: int):
        self.n_system = n_system
        self.n_apparatus = n_apparatus
        # complex amplitudes
        self.R = (np.random.randn(n_system, n_apparatus) + 1j * np.random.randn(n_system, n_apparatus)) * 0.02
        self.normalize_rows()

    def normalize_rows(self):
        mags = np.linalg.norm(self.R, axis=1, keepdims=True)
        mags[mags == 0] = 1.0
        self.R = self.R / mags

    def bidirectional_weight(self, i: int, j: int) -> complex:
        # map apparatus j back to some system index deterministically
        reverse_i = j % self.n_system
        reverse_j = i % self.n_apparatus
        return self.R[i, j] * np.conj(self.R[reverse_i, reverse_j])

    def probability_for_system(self, i: int) -> float:
        weights = np.array([abs(self.bidirectional_weight(i, j)) for j in range(self.n_apparatus)])
        s = np.sum(weights)
        return float(s / (np.sum(weights) + 1e-12))

    def update_hebbian(self, pre_idx: int, post_idx: int, lr: float = 1e-3):
        i = pre_idx % self.n_system
        j = post_idx % self.n_apparatus
        self.R[i, j] += lr * (1.0 + 0.05j)
        self.normalize_rows()

# -------------------------
# Thought Engines
# -------------------------

class ThoughtEngines:
    def __init__(self, n_nodes: int):
        self.n = n_nodes
        self.b = np.zeros(n_nodes)
        self.h = np.zeros(n_nodes)
        self.kappa = np.zeros(n_nodes)
        self.mu = np.zeros(n_nodes)
        # stateful noise seeds for reproducibility
        self._rng = np.random.RandomState(42)

    def step(self, relational: RelationalMatrix, inputs: np.ndarray, dt: float = 0.1):
        n = self.n
        if inputs is None:
            inputs = np.zeros(n)
        # coupling matrix from relational matrix (n x n)
        R = relational.R
        affin = np.real(R @ R.conj().T)
        maxval = np.max(np.abs(affin)) if np.max(np.abs(affin)) > 0 else 1.0
        W = affin / maxval

        # perspective
        db = 0.12 * (inputs * np.tanh(inputs)) - 0.05 * self.b + 0.03 * (W @ self.b - np.sum(W, axis=1) * self.b)
        self.b += db * dt

        # speculation with structured stochasticity
        eps = self._rng.randn(n) * 0.02
        dh = 0.10 * (inputs + eps) - 0.06 * self.h + 0.03 * (W @ self.h - np.sum(W, axis=1) * self.h)
        self.h += dh * dt

        # kaleidoscope
        dk = 0.08 * (self.b + 0.5 * self.h) - 0.04 * self.kappa + 0.02 * (W @ self.kappa - np.sum(W, axis=1) * self.kappa)
        self.kappa += dk * dt

        # mirror
        mismatch = np.abs(self.b - np.mean(self.b))
        dmu = -0.07 * mismatch + 0.05 * np.std(self.h) + 0.03 * (W @ self.mu - np.sum(W, axis=1) * self.mu)
        self.mu += dmu * dt

        # clip numeric stability
        for arr in (self.b, self.h, self.kappa, self.mu):
            np.clip(arr, -12.0, 12.0, out=arr)

# -------------------------
# Emotional Chemistry
# -------------------------

class EmotionalChemistry:
    def __init__(self):
        self.DA = 0.5
        self.Ser = 0.5
        self.NE = 0.5

    def step(self, reward: float, mood_signal: float, arousal: float, dt: float = 0.1):
        self.DA += (0.9 * reward - 0.12 * self.DA) * dt
        self.Ser += (0.4 * mood_signal - 0.06 * self.Ser) * dt
        self.NE += (0.65 * arousal - 0.08 * self.NE) * dt
        self.DA = float(np.clip(self.DA, 0.0, 1.0))
        self.Ser = float(np.clip(self.Ser, 0.0, 1.0))
        self.NE = float(np.clip(self.NE, 0.0, 1.0))

    def vector(self) -> List[float]:
        return [self.DA, self.Ser, self.NE]

# -------------------------
# Memory
# -------------------------

class MemorySystem:
    def __init__(self, embedding_dim: int = 128, capacity: int = 10000):
        self.embedding_dim = embedding_dim
        self.capacity = capacity
        self.episodic = []  # list of (ts, emb, text)
        self.semantic = {}  # key -> emb
        self._rng = np.random.RandomState(12345)
        # We'll derive a deterministic random projection matrix seeded by a constant
        self.random_proj = self._rng.randn(self.embedding_dim, 256) * 0.01

    def embed(self, text: str) -> np.ndarray:
        b = stable_hash_bytes(text, length=256)
        arr = np.frombuffer(b, dtype=np.uint8).astype(np.float32)
        emb = self.random_proj @ arr
        n = np.linalg.norm(emb)
        return emb / (n + 1e-12)

    def store_episode(self, text: str):
        emb = self.embed(text)
        ts = now_seconds()
        if len(self.episodic) >= self.capacity:
            self.episodic.pop(0)
        self.episodic.append((ts, emb, text))

    def retrieve(self, query: str, top_k: int = 5):
        q_emb = self.embed(query)
        sims = []
        for ts, emb, text in self.episodic:
            sims.append((float(np.dot(q_emb, emb)), ts, text))
        sims.sort(reverse=True, key=lambda x: x[0])
        return sims[:top_k]

    def store_semantic(self, key: str, text: str):
        self.semantic[key] = self.embed(text)

    def lookup_semantic(self, key: str):
        return self.semantic.get(key, None)

# -------------------------
# Planner
# -------------------------

class Planner:
    def __init__(self, hardware: SimulatedHardware, relational: RelationalMatrix):
        self.hw = hardware
        self.rel = relational
        # concrete actions map to methods to keep safe
        self.actions = [
            ("increase_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz + 0.1)),
            ("decrease_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz - 0.1)),
            ("toggle_gpio", lambda: self.hw.gpio.set_bit(random.randint(0, self.hw.gpio.size-1), random.randint(0,1))),
            ("no_op", lambda: None),
        ]

    def score_actions(self, thought: ThoughtEngines) -> List[Tuple[float, int]]:
        scores = []
        for idx, (name, fn) in enumerate(self.actions):
            sys_idx = idx % self.rel.n_system
            app_idx = idx % self.rel.n_apparatus
            rweight = abs(self.rel.bidirectional_weight(sys_idx, app_idx))
            cog_signal = float(np.tanh(np.mean(thought.b) + np.mean(thought.kappa)))
            score = rweight * (1.0 + cog_signal)
            # small diversity noise
            score += random.random() * 1e-6
            scores.append((score, idx))
        scores.sort(reverse=True, key=lambda x: x[0])
        return scores

    def select_and_execute(self, thought: ThoughtEngines) -> str:
        scored = self.score_actions(thought)
        weights = np.array([s for s, i in scored], dtype=float)
        if weights.sum() <= 0:
            idx = scored[0][1]
        else:
            probs = weights / weights.sum()
            choice = np.random.choice(len(scored), p=probs)
            idx = scored[choice][1]
        name, fn = self.actions[idx]
        fn()
        return name

# -------------------------
# AGI System (orchestrator)
# -------------------------

class AGISystem:
    def __init__(self, n_nodes: int = 32, n_apparatus: int = 24):
        self.hw = SimulatedHardware(adc_channels=24)
        self.rel = RelationalMatrix(n_nodes, n_apparatus)
        self.thought = ThoughtEngines(n_nodes)
        self.emotion = EmotionalChemistry()
        self.memory = MemorySystem(embedding_dim=128, capacity=10000)
        self.planner = Planner(self.hw, self.rel)
        self.t = 0.0
        self.dt = 0.1
        self.history = []
        self.lock = Lock()
        # seed semantic memory
        self.memory.store_semantic("agent_identity", "A private, self-contained cognitive assistant for execution and opportunity synthesis.")
        self.memory.store_episode("system initialized")

    def step(self, user_input: Optional[str] = None) -> Dict[str, Any]:
        with self.lock:
            self.hw.poll_sensors()
            n = self.thought.n
            sensor_vec = np.concatenate([self.hw.adc, np.array([self.hw.temp_C, self.hw.freq_GHz])])
            inputs = np.zeros(n)
            ssum = float(np.sum(sensor_vec))
            for i in range(n):
                inputs[i] = float(np.tanh(ssum * 0.0005 + random.random() * 0.01))
            if user_input:
                self.memory.store_episode(user_input)
                emb = self.memory.embed(user_input)
                bias = float(np.tanh(np.mean(emb))) * 0.5
                inputs += bias
            self.thought.step(self.rel, inputs, dt=self.dt)
            reward = float(np.clip(np.mean(inputs), -1, 1))
            mood = float(np.tanh(np.mean(self.thought.b)))
            arousal = float(np.abs(np.std(self.thought.h)))
            self.emotion.step(reward, mood, arousal, dt=self.dt)
            most_active_node = int(np.argmax(np.abs(self.thought.kappa)))
            apparatus_idx = int(abs(int((np.sum(self.hw.adc) * 100) % self.rel.n_apparatus)))
            self.rel.update_hebbian(most_active_node, apparatus_idx, lr=1e-3)
            action_name = self.planner.select_and_execute(self.thought)
            self.rel.normalize_rows()
            conn_metrics = self.relational_consciousness_metrics()
            log_item = {
                "t": self.t,
                "action": action_name,
                "hw": self.hw.as_status(),
                "emotion": {"DA": self.emotion.DA, "Ser": self.emotion.Ser, "NE": self.emotion.NE},
                "thought_summary": {
                    "b_mean": float(np.mean(self.thought.b)),
                    "h_mean": float(np.mean(self.thought.h)),
                    "kappa_mean": float(np.mean(self.thought.kappa)),
                    "mu_mean": float(np.mean(self.thought.mu)),
                },
                "consciousness": conn_metrics
            }
            self.history.append(log_item)
            self.t += self.dt
            return log_item

    def relational_consciousness_metrics(self) -> Dict[str, float]:
        diag = np.array([abs(self.rel.R[i, i % self.rel.n_apparatus]) for i in range(min(self.rel.n_system, self.rel.n_apparatus))])
        coherence = float(np.mean(diag))
        awareness = float(np.clip(self.emotion.DA * (1.0 + np.tanh(np.mean(self.thought.b))), 0.0, 1.0))
        activities = np.concatenate([self.thought.b, self.thought.h, self.thought.kappa, self.thought.mu])
        integrated_info = float(np.var(activities))
        return {"coherence": coherence, "awareness": awareness, "phi_proxy": integrated_info}

    def respond(self, user_input: str) -> str:
        log = self.step(user_input)
        candidates = self.memory.retrieve(user_input, top_k=3)
        reply_parts = []
        if candidates:
            reply_parts.append("I recall: " + "; ".join([c for _, _, c in candidates[:2]]))
        if log["consciousness"]["awareness"] > 0.6:
            reply_parts.append("I am engaged and reflecting on that.")
        elif log["consciousness"]["phi_proxy"] > 0.08:
            reply_parts.append("This seems important; I'll think further.")
        else:
            reply_parts.append("Noted and stored.")
        da = log["emotion"]["DA"]
        mood = "positive" if da > 0.55 else "neutral" if da > 0.45 else "cautious"
        reply_parts.append(f"My mood is {mood}. Action taken: {log['action']}.")
        return " ".join(reply_parts)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/unified_system_agi_core (3).py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
agi_core.py

Advanced unified cognitive substrate: hardware interface, relational matrix,
multi-engine cognition, emotional chemistry, memory, and planning. Designed
to be run as a production-capable module integrated with the Opportunity
Synthesis service.

Notes:
- Deterministic, high-quality local embeddings via random-projection + hashing.
- Safe, deterministic components (no external ML downloads required).
- Exposes a clear programmatic API for stepping the agent and querying state.
"""

import os
import time
import math
import random
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
import numpy as np
from threading import Lock

# -------------------------
# Utilities
# -------------------------

def now_seconds() -> float:
    return time.time()

def stable_hash_bytes(s: str, length: int = 256) -> bytes:
    # deterministic hashing into bytes array using SHAKE-like approach without external libs
    import hashlib
    h = hashlib.blake2b(digest_size=32)
    h.update(s.encode("utf8"))
    base = h.digest()
    out = bytearray()
    i = 0
    while len(out) < length:
        h2 = hashlib.blake2b(digest_size=32)
        h2.update(base)
        h2.update(bytes([i]))
        out.extend(h2.digest())
        i += 1
    return bytes(out[:length])

# -------------------------
# Hardware Abstraction
# -------------------------

class BitRegister:
    def __init__(self, name: str, size: int = 128):
        self.name = name
        self.size = size
        self.bits = np.zeros(size, dtype=np.uint8)
        self.noise_rate = 2e-6
        self.lock = Lock()

    def write_int(self, value: int):
        with self.lock:
            for i in range(self.size):
                self.bits[i] = (value >> i) & 1

    def read_int(self) -> int:
        with self.lock:
            if random.random() < self.noise_rate:
                idx = random.randrange(self.size)
                self.bits[idx] ^= 1
            out = 0
            for i in range(self.size):
                out |= int(self.bits[i]) << i
            return out

    def set_bit(self, idx: int, val: int):
        with self.lock:
            self.bits[idx % self.size] = 1 if val else 0

    def get_bit(self, idx: int) -> int:
        with self.lock:
            return int(self.bits[idx % self.size])

    def as_bitstring(self) -> str:
        with self.lock:
            return ''.join(str(int(b)) for b in self.bits[::-1])

class SimulatedHardware:
    def __init__(self, adc_channels: int = 16):
        self.cpu_register = BitRegister("CPU_REG", size=256)
        self.gpio = BitRegister("GPIO", size=64)
        self.adc = np.zeros(adc_channels, dtype=float)
        self.temp_C = 35.0
        self.freq_GHz = 1.2

    def poll_sensors(self):
        # realistic sensor noise and drift
        self.adc += np.random.randn(len(self.adc)) * 0.005
        self.adc = np.clip(self.adc, -5.0, 5.0)
        self.temp_C += (self.freq_GHz - 1.0) * 0.02 + np.random.randn() * 0.01

    def set_frequency(self, ghz: float):
        self.freq_GHz = float(max(0.2, min(ghz, 5.0)))

    def as_status(self):
        return {
            "freq_GHz": round(self.freq_GHz, 3),
            "temp_C": round(self.temp_C, 3),
            "cpu_reg": self.cpu_register.as_bitstring(),
            "gpio": self.gpio.as_bitstring(),
            "adc": [round(float(x), 4) for x in self.adc.tolist()],
        }

# -------------------------
# Relational Matrix
# -------------------------

class RelationalMatrix:
    def __init__(self, n_system: int, n_apparatus: int):
        self.n_system = n_system
        self.n_apparatus = n_apparatus
        # complex amplitudes
        self.R = (np.random.randn(n_system, n_apparatus) + 1j * np.random.randn(n_system, n_apparatus)) * 0.02
        self.normalize_rows()

    def normalize_rows(self):
        mags = np.linalg.norm(self.R, axis=1, keepdims=True)
        mags[mags == 0] = 1.0
        self.R = self.R / mags

    def bidirectional_weight(self, i: int, j: int) -> complex:
        # map apparatus j back to some system index deterministically
        reverse_i = j % self.n_system
        reverse_j = i % self.n_apparatus
        return self.R[i, j] * np.conj(self.R[reverse_i, reverse_j])

    def probability_for_system(self, i: int) -> float:
        weights = np.array([abs(self.bidirectional_weight(i, j)) for j in range(self.n_apparatus)])
        s = np.sum(weights)
        return float(s / (np.sum(weights) + 1e-12))

    def update_hebbian(self, pre_idx: int, post_idx: int, lr: float = 1e-3):
        i = pre_idx % self.n_system
        j = post_idx % self.n_apparatus
        self.R[i, j] += lr * (1.0 + 0.05j)
        self.normalize_rows()

# -------------------------
# Thought Engines
# -------------------------

class ThoughtEngines:
    def __init__(self, n_nodes: int):
        self.n = n_nodes
        self.b = np.zeros(n_nodes)
        self.h = np.zeros(n_nodes)
        self.kappa = np.zeros(n_nodes)
        self.mu = np.zeros(n_nodes)
        # stateful noise seeds for reproducibility
        self._rng = np.random.RandomState(42)

    def step(self, relational: RelationalMatrix, inputs: np.ndarray, dt: float = 0.1):
        n = self.n
        if inputs is None:
            inputs = np.zeros(n)
        # coupling matrix from relational matrix (n x n)
        R = relational.R
        affin = np.real(R @ R.conj().T)
        maxval = np.max(np.abs(affin)) if np.max(np.abs(affin)) > 0 else 1.0
        W = affin / maxval

        # perspective
        db = 0.12 * (inputs * np.tanh(inputs)) - 0.05 * self.b + 0.03 * (W @ self.b - np.sum(W, axis=1) * self.b)
        self.b += db * dt

        # speculation with structured stochasticity
        eps = self._rng.randn(n) * 0.02
        dh = 0.10 * (inputs + eps) - 0.06 * self.h + 0.03 * (W @ self.h - np.sum(W, axis=1) * self.h)
        self.h += dh * dt

        # kaleidoscope
        dk = 0.08 * (self.b + 0.5 * self.h) - 0.04 * self.kappa + 0.02 * (W @ self.kappa - np.sum(W, axis=1) * self.kappa)
        self.kappa += dk * dt

        # mirror
        mismatch = np.abs(self.b - np.mean(self.b))
        dmu = -0.07 * mismatch + 0.05 * np.std(self.h) + 0.03 * (W @ self.mu - np.sum(W, axis=1) * self.mu)
        self.mu += dmu * dt

        # clip numeric stability
        for arr in (self.b, self.h, self.kappa, self.mu):
            np.clip(arr, -12.0, 12.0, out=arr)

# -------------------------
# Emotional Chemistry
# -------------------------

class EmotionalChemistry:
    def __init__(self):
        self.DA = 0.5
        self.Ser = 0.5
        self.NE = 0.5

    def step(self, reward: float, mood_signal: float, arousal: float, dt: float = 0.1):
        self.DA += (0.9 * reward - 0.12 * self.DA) * dt
        self.Ser += (0.4 * mood_signal - 0.06 * self.Ser) * dt
        self.NE += (0.65 * arousal - 0.08 * self.NE) * dt
        self.DA = float(np.clip(self.DA, 0.0, 1.0))
        self.Ser = float(np.clip(self.Ser, 0.0, 1.0))
        self.NE = float(np.clip(self.NE, 0.0, 1.0))

    def vector(self) -> List[float]:
        return [self.DA, self.Ser, self.NE]

# -------------------------
# Memory
# -------------------------

class MemorySystem:
    def __init__(self, embedding_dim: int = 128, capacity: int = 10000):
        self.embedding_dim = embedding_dim
        self.capacity = capacity
        self.episodic = []  # list of (ts, emb, text)
        self.semantic = {}  # key -> emb
        self._rng = np.random.RandomState(12345)
        # We'll derive a deterministic random projection matrix seeded by a constant
        self.random_proj = self._rng.randn(self.embedding_dim, 256) * 0.01

    def embed(self, text: str) -> np.ndarray:
        b = stable_hash_bytes(text, length=256)
        arr = np.frombuffer(b, dtype=np.uint8).astype(np.float32)
        emb = self.random_proj @ arr
        n = np.linalg.norm(emb)
        return emb / (n + 1e-12)

    def store_episode(self, text: str):
        emb = self.embed(text)
        ts = now_seconds()
        if len(self.episodic) >= self.capacity:
            self.episodic.pop(0)
        self.episodic.append((ts, emb, text))

    def retrieve(self, query: str, top_k: int = 5):
        q_emb = self.embed(query)
        sims = []
        for ts, emb, text in self.episodic:
            sims.append((float(np.dot(q_emb, emb)), ts, text))
        sims.sort(reverse=True, key=lambda x: x[0])
        return sims[:top_k]

    def store_semantic(self, key: str, text: str):
        self.semantic[key] = self.embed(text)

    def lookup_semantic(self, key: str):
        return self.semantic.get(key, None)

# -------------------------
# Planner
# -------------------------

class Planner:
    def __init__(self, hardware: SimulatedHardware, relational: RelationalMatrix):
        self.hw = hardware
        self.rel = relational
        # concrete actions map to methods to keep safe
        self.actions = [
            ("increase_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz + 0.1)),
            ("decrease_freq", lambda: self.hw.set_frequency(self.hw.freq_GHz - 0.1)),
            ("toggle_gpio", lambda: self.hw.gpio.set_bit(random.randint(0, self.hw.gpio.size-1), random.randint(0,1))),
            ("no_op", lambda: None),
        ]

    def score_actions(self, thought: ThoughtEngines) -> List[Tuple[float, int]]:
        scores = []
        for idx, (name, fn) in enumerate(self.actions):
            sys_idx = idx % self.rel.n_system
            app_idx = idx % self.rel.n_apparatus
            rweight = abs(self.rel.bidirectional_weight(sys_idx, app_idx))
            cog_signal = float(np.tanh(np.mean(thought.b) + np.mean(thought.kappa)))
            score = rweight * (1.0 + cog_signal)
            # small diversity noise
            score += random.random() * 1e-6
            scores.append((score, idx))
        scores.sort(reverse=True, key=lambda x: x[0])
        return scores

    def select_and_execute(self, thought: ThoughtEngines) -> str:
        scored = self.score_actions(thought)
        weights = np.array([s for s, i in scored], dtype=float)
        if weights.sum() <= 0:
            idx = scored[0][1]
        else:
            probs = weights / weights.sum()
            choice = np.random.choice(len(scored), p=probs)
            idx = scored[choice][1]
        name, fn = self.actions[idx]
        fn()
        return name

# -------------------------
# AGI System (orchestrator)
# -------------------------

class AGISystem:
    def __init__(self, n_nodes: int = 32, n_apparatus: int = 24):
        self.hw = SimulatedHardware(adc_channels=24)
        self.rel = RelationalMatrix(n_nodes, n_apparatus)
        self.thought = ThoughtEngines(n_nodes)
        self.emotion = EmotionalChemistry()
        self.memory = MemorySystem(embedding_dim=128, capacity=10000)
        self.planner = Planner(self.hw, self.rel)
        self.t = 0.0
        self.dt = 0.1
        self.history = []
        self.lock = Lock()
        # seed semantic memory
        self.memory.store_semantic("agent_identity", "A private, self-contained cognitive assistant for execution and opportunity synthesis.")
        self.memory.store_episode("system initialized")

    def step(self, user_input: Optional[str] = None) -> Dict[str, Any]:
        with self.lock:
            self.hw.poll_sensors()
            n = self.thought.n
            sensor_vec = np.concatenate([self.hw.adc, np.array([self.hw.temp_C, self.hw.freq_GHz])])
            inputs = np.zeros(n)
            ssum = float(np.sum(sensor_vec))
            for i in range(n):
                inputs[i] = float(np.tanh(ssum * 0.0005 + random.random() * 0.01))
            if user_input:
                self.memory.store_episode(user_input)
                emb = self.memory.embed(user_input)
                bias = float(np.tanh(np.mean(emb))) * 0.5
                inputs += bias
            self.thought.step(self.rel, inputs, dt=self.dt)
            reward = float(np.clip(np.mean(inputs), -1, 1))
            mood = float(np.tanh(np.mean(self.thought.b)))
            arousal = float(np.abs(np.std(self.thought.h)))
            self.emotion.step(reward, mood, arousal, dt=self.dt)
            most_active_node = int(np.argmax(np.abs(self.thought.kappa)))
            apparatus_idx = int(abs(int((np.sum(self.hw.adc) * 100) % self.rel.n_apparatus)))
            self.rel.update_hebbian(most_active_node, apparatus_idx, lr=1e-3)
            action_name = self.planner.select_and_execute(self.thought)
            self.rel.normalize_rows()
            conn_metrics = self.relational_consciousness_metrics()
            log_item = {
                "t": self.t,
                "action": action_name,
                "hw": self.hw.as_status(),
                "emotion": {"DA": self.emotion.DA, "Ser": self.emotion.Ser, "NE": self.emotion.NE},
                "thought_summary": {
                    "b_mean": float(np.mean(self.thought.b)),
                    "h_mean": float(np.mean(self.thought.h)),
                    "kappa_mean": float(np.mean(self.thought.kappa)),
                    "mu_mean": float(np.mean(self.thought.mu)),
                },
                "consciousness": conn_metrics
            }
            self.history.append(log_item)
            self.t += self.dt
            return log_item

    def relational_consciousness_metrics(self) -> Dict[str, float]:
        diag = np.array([abs(self.rel.R[i, i % self.rel.n_apparatus]) for i in range(min(self.rel.n_system, self.rel.n_apparatus))])
        coherence = float(np.mean(diag))
        awareness = float(np.clip(self.emotion.DA * (1.0 + np.tanh(np.mean(self.thought.b))), 0.0, 1.0))
        activities = np.concatenate([self.thought.b, self.thought.h, self.thought.kappa, self.thought.mu])
        integrated_info = float(np.var(activities))
        return {"coherence": coherence, "awareness": awareness, "phi_proxy": integrated_info}

    def respond(self, user_input: str) -> str:
        log = self.step(user_input)
        candidates = self.memory.retrieve(user_input, top_k=3)
        reply_parts = []
        if candidates:
            reply_parts.append("I recall: " + "; ".join([c for _, _, c in candidates[:2]]))
        if log["consciousness"]["awareness"] > 0.6:
            reply_parts.append("I am engaged and reflecting on that.")
        elif log["consciousness"]["phi_proxy"] > 0.08:
            reply_parts.append("This seems important; I'll think further.")
        else:
            reply_parts.append("Noted and stored.")
        da = log["emotion"]["DA"]
        mood = "positive" if da > 0.55 else "neutral" if da > 0.45 else "cautious"
        reply_parts.append(f"My mood is {mood}. Action taken: {log['action']}.")
        return " ".join(reply_parts)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/advanced_voice_mimic.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Literal, Optional
import random
import uuid

import librosa
import numpy as np
import soundfile as sf

from .config import AudioSettings
from .prosody import ProsodyProfile, apply_prosody_to_tts, extract_prosody
from .voice_mimic import VoiceMimic

Style = Literal["neutral", "calm", "excited"]
Mode = Literal["outer", "inner", "coach"]


@dataclass(slots=True)
class VoiceSample:
    path: Path
    duration_s: float
    rms: float
    style: Style
    quality_score: float
    added_ts: float


@dataclass(slots=True)
class VoiceProfile:
    audio: AudioSettings
    base_dir: Path
    max_samples_per_style: int = 32
    samples: Dict[Style, List[VoiceSample]] = field(default_factory=lambda: {
        "neutral": [],
        "calm": [],
        "excited": [],
    })

    def __post_init__(self) -> None:
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.load_existing()

    # ------------------- helpers -------------------
    def _style_dir(self, style: Style) -> Path:
        return self.base_dir / style

    def _compute_rms(self, wav: np.ndarray) -> float:
        if wav.size == 0:
            return 0.0
        return float(np.sqrt(np.mean(np.square(wav))))

    def _register_sample(self, path: Path, style: Style, wav: np.ndarray, quality_score: float) -> None:
        duration = len(wav) / float(self.audio.sample_rate)
        rms = self._compute_rms(wav)
        sample = VoiceSample(
            path=path,
            duration_s=duration,
            rms=rms,
            style=style,
            quality_score=quality_score,
            added_ts=path.stat().st_mtime,
        )
        self.samples.setdefault(style, []).append(sample)
        self._prune(style)

    def load_existing(self) -> None:
        for style in ("neutral", "calm", "excited"):
            dir_path = self._style_dir(style)
            if not dir_path.exists():
                continue
            for wav_path in sorted(dir_path.glob("*.wav")):
                try:
                    data, sr = sf.read(wav_path, dtype="float32")
                except Exception:
                    continue
                if data.ndim > 1:
                    data = np.mean(data, axis=1)
                if sr != self.audio.sample_rate:
                    data = librosa.resample(data, orig_sr=sr, target_sr=self.audio.sample_rate)
                duration = len(data) / float(self.audio.sample_rate)
                rms = self._compute_rms(np.asarray(data, dtype=np.float32))
                sample = VoiceSample(
                    path=wav_path,
                    duration_s=duration,
                    rms=rms,
                    style=style,  # type: ignore[arg-type]
                    quality_score=1.0,
                    added_ts=wav_path.stat().st_mtime,
                )
                self.samples.setdefault(style, []).append(sample)

    def _prune(self, style: Style) -> None:
        if len(self.samples[style]) <= self.max_samples_per_style:
            return
        self.samples[style].sort(key=lambda s: (s.quality_score, s.added_ts), reverse=True)
        self.samples[style] = self.samples[style][: self.max_samples_per_style]

    def add_sample_from_wav(
        self,
        wav: np.ndarray,
        style: Style,
        name: Optional[str] = None,
        quality_score: float = 1.0,
    ) -> Path:
        if wav.ndim > 1:
            wav = np.mean(wav, axis=1)
        wav = np.asarray(wav, dtype=np.float32)
        style_dir = self._style_dir(style)
        style_dir.mkdir(parents=True, exist_ok=True)
        suffix = name or uuid.uuid4().hex[:8]
        path = style_dir / f"{style}_{suffix}.wav"
        sf.write(path, wav, self.audio.sample_rate)
        self._register_sample(path, style, wav, quality_score)
        return path

    def pick_reference(self, style: Style = "neutral") -> Optional[VoiceSample]:
        candidates = self.samples.get(style) or []
        if candidates:
            return max(candidates, key=lambda s: s.quality_score)
        if style != "neutral" and self.samples["neutral"]:
            return max(self.samples["neutral"], key=lambda s: s.quality_score)
        for lst in self.samples.values():
            if lst:
                return max(lst, key=lambda s: s.quality_score)
        return None

    def maybe_adapt_from_attempt(
        self,
        attempt_wav: np.ndarray,
        style: Style,
        quality_score: float,
        min_quality_bootstrap: float = 0.8,
        min_quality_refine: float = 0.9,
    ) -> Optional[Path]:
        has_profile = any(self.samples.values())
        threshold = min_quality_bootstrap if not has_profile else min_quality_refine
        if quality_score < threshold:
            return None
        return self.add_sample_from_wav(attempt_wav, style, quality_score=quality_score)


@dataclass(slots=True)
class VoiceCrystalConfig:
    inner_lowpass_window_ms: float = 18.0
    inner_volume_scale: float = 0.45
    coach_volume_scale: float = 1.15
    prosody_strength_pitch: float = 1.0
    prosody_strength_energy: float = 1.0
    sample_rate: int = 16_000


@dataclass(slots=True)
class VoiceCrystal:
    tts: VoiceMimic
    audio: AudioSettings
    profile: VoiceProfile
    config: VoiceCrystalConfig = field(default_factory=VoiceCrystalConfig)

    def _smooth(self, wav: np.ndarray, window_ms: float) -> np.ndarray:
        if wav.size == 0:
            return wav
        window = max(int(window_ms * self.audio.sample_rate / 1000.0), 1)
        if window <= 1:
            return wav
        kernel = np.ones(window, dtype=np.float32) / float(window)
        return np.convolve(wav, kernel, mode="same").astype(np.float32)

    def _apply_mode(self, wav: np.ndarray, mode: Mode) -> np.ndarray:
        if wav.size == 0:
            return wav
        if mode == "inner":
            smoothed = self._smooth(wav, self.config.inner_lowpass_window_ms * 2.0)
            rms = float(np.sqrt(np.mean(smoothed**2) + 1e-6))
            target_rms = rms * 0.8
            if rms > 0:
                smoothed = np.tanh(smoothed / rms) * target_rms
            return (smoothed * self.config.inner_volume_scale).astype(np.float32)
        if mode == "coach":
            return (wav * self.config.coach_volume_scale).astype(np.float32)
        return wav

    def _synthesize_raw(self, text: str, style: Style) -> np.ndarray:
        if not text:
            return np.zeros(0, dtype=np.float32)
        sample = self.profile.pick_reference(style)
        if sample:
            self.tts.update_voiceprint(sample.path)
        return self.tts.synthesize(text)

    def _apply_prosody(
        self,
        wav: np.ndarray,
        prosody_source_wav: Optional[np.ndarray],
        prosody_source_sr: Optional[int],
    ) -> np.ndarray:
        if prosody_source_wav is None or prosody_source_sr is None:
            return wav
        try:
            prosody = extract_prosody(prosody_source_wav, prosody_source_sr)
            return apply_prosody_to_tts(
                wav,
                self.audio.sample_rate,
                prosody,
                strength_pitch=self.config.prosody_strength_pitch,
                strength_energy=self.config.prosody_strength_energy,
            )
        except Exception:
            return wav

    def speak(
        self,
        text: str,
        style: Style = "neutral",
        mode: Mode = "outer",
        prosody_source_wav: Optional[np.ndarray] = None,
        prosody_source_sr: Optional[int] = None,
    ) -> np.ndarray:
        base = self._synthesize_raw(text, style)
        if base.size == 0:
            return base
        prosody_applied = self._apply_prosody(base, prosody_source_wav, prosody_source_sr)
        processed = self._apply_mode(prosody_applied, mode)
        return processed

    def say_inner(
        self,
        text: str,
        style: Style = "calm",
        prosody_source_wav: Optional[np.ndarray] = None,
        prosody_source_sr: Optional[int] = None,
    ) -> np.ndarray:
        return self.speak(
            text=text,
            style=style,
            mode="inner",
            prosody_source_wav=prosody_source_wav,
            prosody_source_sr=prosody_source_sr,
        )

    def say_outer(
        self,
        text: str,
        style: Style = "neutral",
        prosody_source_wav: Optional[np.ndarray] = None,
        prosody_source_sr: Optional[int] = None,
    ) -> np.ndarray:
        return self.speak(
            text=text,
            style=style,
            mode="outer",
            prosody_source_wav=prosody_source_wav,
            prosody_source_sr=prosody_source_sr,
        )

    def say_coach(
        self,
        text: str,
        style: Style = "excited",
        prosody_source_wav: Optional[np.ndarray] = None,
        prosody_source_sr: Optional[int] = None,
    ) -> np.ndarray:
        return self.speak(
            text=text,
            style=style,
            mode="coach",
            prosody_source_wav=prosody_source_wav,
            prosody_source_sr=prosody_source_sr,
        )
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/cli.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Command-line entry point for the autism speech companion."""

from __future__ import annotations

import argparse
import asyncio

from .config import CompanionConfig, CONFIG
from .data_store import DataStore
from .speech_loop import SpeechLoop
from .reports import summarize
from .dashboard import create_app
from .calming_strategies import list_categories, by_category, suggest_for_event
from .guidance import GUIDANCE_SCRIPTS
from .advanced_voice_mimic import Style


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Autism speech companion control CLI")
    sub = parser.add_subparsers(dest="command")

    rec = sub.add_parser("record", help="Record a canonical phrase")
    rec.add_argument("--text", required=True, help="Phrase text to associate with the recording")
    rec.add_argument("--seconds", type=float, default=3.0, help="Recording length")

    sub.add_parser("list", help="List known phrases")
    sub.add_parser("summary", help="Show metrics CSV path")

    run = sub.add_parser("run", help="Start the realtime loop")

    dash = sub.add_parser("dashboard", help="Launch therapist dashboard")
    dash.add_argument("--host", default="0.0.0.0")
    dash.add_argument("--port", type=int, default=8765)

    sub.add_parser("gui", help="Launch the local Tkinter caregiver/child dashboard")

    strat = sub.add_parser("strategies", help="Show calming strategies catalog")
    strat.add_argument("--category", choices=list_categories(), help="Filter by category")
    strat.add_argument(
        "--event",
        choices=[
            "meltdown",
            "transition",
            "anxious_speech",
            "anxious",
            "hyperactivity",
            "high_energy",
            "care_team_sync",
            "school_meeting",
            "communication_practice",
            "caregiver_reset",
            "interest_planning",
            "perseveration",
            "encouragement",
        ],
        help="Filter by event trigger",
    )

    comfort = sub.add_parser("comfort", help="Play a supportive prompt right now")
    comfort.add_argument("--event", choices=sorted(GUIDANCE_SCRIPTS.keys()), required=True)
    comfort.add_argument("--message", help="Override guidance text with a custom message")

    # New commands for Voice Crystal
    record_facet = sub.add_parser("record-voice-facet", help="Record a voice sample for a specific style (facet)")
    record_facet.add_argument("style", choices=["neutral", "calm", "excited"], help="Style label for this facet")
    record_facet.add_argument("--seconds", type=float, default=3.0, help="Duration of the recording in seconds")
    record_facet.add_argument("--name", help="Optional name suffix stored with the sample")

    sub.add_parser("show-voice-profile", help="Show the current VoiceProfile facets")

    return parser


def cmd_record(loop: SpeechLoop, args: argparse.Namespace) -> None:
    phrase = loop.record_phrase(args.text, args.seconds)
    print(f"Recorded phrase '{phrase.text}' as {phrase.audio_file}")


def cmd_list(config: CompanionConfig) -> None:
    data = DataStore(config)
    for phrase in data.list_phrases():
        print(f"{phrase.phrase_id}: '{phrase.text}' -> {phrase.audio_file}")


def cmd_summary(config: CompanionConfig) -> None:
    stats = summarize(config)
    if not stats:
        print("No attempts logged yet.")
        return
    print(f"Metrics CSV: {config.paths.metrics_csv}")
    for phrase_id, row in stats.items():
        rate = row.corrections / row.attempts if row.attempts else 0.0
        print(f"{phrase_id}: attempts={row.attempts} corrections={row.corrections} correction_rate={rate:.2f}")
    if config.paths.guidance_csv.exists():
        with config.paths.guidance_csv.open() as f:
            guidance_events = sum(1 for _ in f) - 1
        if guidance_events > 0:
            print(f"Guidance events logged: {guidance_events} (see {config.paths.guidance_csv})")


def cmd_run(loop: SpeechLoop) -> None:
    print("Starting realtime speech companion. Press Ctrl+C to exit.")
    try:
        asyncio.run(loop.run())
    except KeyboardInterrupt:
        print("Stopped.")


def cmd_dashboard(config: CompanionConfig, host: str, port: int) -> None:
    app = create_app(config)
    print(f"Dashboard running at http://{host}:{port}")
    app.run(host=host, port=port, debug=False)


def cmd_gui(config: CompanionConfig) -> None:
    from .tk_gui import run_gui  # Lazy import to avoid tkinter dependency for non-GUI usage

    run_gui(config)


def cmd_strategies(category: str | None, event: str | None) -> None:
    if event:
        strategies = suggest_for_event(event)
    elif category:
        strategies = by_category(category)
    else:
        strategies = suggest_for_event("anxious_speech")
    print("Recommended calming strategies:")
    for strategy in strategies:
        cues = f" (cues: {', '.join(strategy.cues)})" if strategy.cues else ""
        print(f"- [{strategy.category}] {strategy.title}{cues}\n  {strategy.description}\n")


def cmd_comfort(loop: SpeechLoop, event: str, message: str | None) -> None:
    loop.coach.speak(event, override_text=message)


# New commands for Voice Crystal
def cmd_record_voice_facet(loop: SpeechLoop, style: Style, seconds: float, name: str | None) -> None:
    print(f"Recording {seconds:.1f}s for '{style}' facet...")
    audio = loop.audio_io.record_phrase(seconds)
    path = loop.voice_profile.add_sample_from_wav(audio, style, name=name)
    print(f"Saved facet at {path}")


def cmd_show_voice_profile(loop: SpeechLoop) -> None:
    print("Current Voice Profile Facets:")
    total = 0
    for style in ("neutral", "calm", "excited"):
        samples = loop.voice_profile.samples.get(style, [])
        print(f"- {style}: {len(samples)} sample(s)")
        for sample in samples:
            print(f"    • {sample.path.name} (score={sample.quality_score:.2f})")
        total += len(samples)
    if total == 0:
        print("  No facets recorded yet.")


def main(config: CompanionConfig = CONFIG) -> None:
    parser = build_parser()
    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        return

    loop = SpeechLoop(config)
    if args.command == "record":
        cmd_record(loop, args)
    elif args.command == "list":
        cmd_list(config)
    elif args.command == "summary":
        cmd_summary(config)
    elif args.command == "run":
        cmd_run(loop)
    elif args.command == "dashboard":
        cmd_dashboard(config, args.host, args.port)
    elif args.command == "gui":
        cmd_gui(config)
    elif args.command == "strategies":
        cmd_strategies(args.category, args.event)
    elif args.command == "comfort":
        cmd_comfort(loop, args.event, args.message)
    elif args.command == "record-voice-facet":
        cmd_record_voice_facet(loop, args.style, args.seconds, args.name)
    elif args.command == "show-voice-profile":
        cmd_show_voice_profile(loop)


if __name__ == "__main__":
    main()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/dashboard.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Rich, responsive caregiver dashboard for the autism speech companion."""

from __future__ import annotations

import csv
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List, Tuple
import numpy as np
import librosa

from flask import Flask, jsonify, render_template_string, request

from .advanced_voice_mimic import VoiceProfile, TTSEngine
from .settings_store import SettingsStore
import tempfile
import os
import soundfile as sf

from .config import CompanionConfig, CONFIG
from .calming_strategies import STRATEGIES


TEMPLATE = """<!doctype html>
<html lang="en" class="h-full">
  <head>
    <meta charset="utf-8" />
    <title>Speech Companion • Care Dashboard</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
      :root {
        color-scheme: dark;
      }
      .glass {
        background: radial-gradient(circle at top left, rgba(56, 189, 248, 0.22), transparent 55%),
                    radial-gradient(circle at bottom right, rgba(99, 102, 241, 0.18), transparent 55%);
      }
    </style>
  </head>
  <body class="h-full bg-slate-950 text-slate-50">
    <div class="min-h-screen px-4 py-6 sm:px-8 lg:px-12 glass">
      <!-- Header -->
      <header class="flex flex-col gap-4 md:flex-row md:items-center md:justify-between mb-8">
        <div>
          <p class="text-xs uppercase tracking-[0.3em] text-sky-400/70">Autism Speech Companion</p>
          <h1 class="text-3xl sm:text-4xl font-semibold tracking-tight mt-1">Care Dashboard</h1>
          <p class="text-sm text-slate-300/80 mt-3 max-w-xl">
            Live insight into practice sessions, correction patterns, and calming support —
            designed so caregivers, therapists, and researchers can see progress at a glance.
          </p>
        </div>
        <div class="flex flex-col items-start md:items-end gap-2">
          <div class="inline-flex items-center gap-2 rounded-full border border-emerald-400/40 bg-emerald-500/10 px-3 py-1">
            <span class="h-2 w-2 rounded-full bg-emerald-400 animate-pulse"></span>
            <span class="text-xs font-medium tracking-wide text-emerald-100">Live companion ready</span>
          </div>
          <div class="text-right text-xs text-slate-300/80">
            <div class="font-medium text-sm">{{ child_name }}</div>
            <div>Total attempts: <span class="font-semibold">{{ total_attempts }}</span></div>
            <div>Overall correction rate:
              <span class="font-semibold">
                {% if total_attempts %}
                  {{ '%.0f%%'|format(overall_rate * 100) }}
                {% else %}
                  —
                {% endif %}
              </span>
            </div>
          </div>
        </div>
      </header>

      <!-- Main grid -->
      <main class="grid gap-6 lg:grid-cols-[minmax(0,3fr)_minmax(0,2fr)]">
        <!-- Left column: live + charts -->
        <section class="space-y-6">
          <!-- Live session card -->
          <div class="rounded-2xl border border-slate-800/80 bg-slate-900/70 shadow-xl shadow-sky-900/30 backdrop-blur">
            <div class="flex items-center justify-between px-4 py-3 border-b border-slate-800/80">
              <div>
                <h2 class="text-sm font-semibold tracking-wide text-slate-100">Live session snapshot</h2>
                <p class="text-xs text-slate-400 mt-1">
                  Automatically refreshes from the latest logged attempt.
                </p>
              </div>
              <span id="live-status-pill"
                    class="inline-flex items-center gap-1 rounded-full border border-sky-500/40 bg-sky-500/10 px-3 py-1 text-[11px] font-medium text-sky-100">
                <span class="h-1.5 w-1.5 rounded-full bg-sky-400 animate-ping"></span>
                <span>Waiting for first attempt…</span>
              </span>
            </div>
            <div class="grid gap-4 px-4 py-4 sm:grid-cols-3">
              <div class="space-y-1 sm:col-span-1">
                <p class="text-[11px] uppercase tracking-[0.2em] text-slate-400">Target phrase</p>
                <p id="live-phrase" class="text-sm font-medium text-slate-50 truncate">—</p>
                <p class="text-[11px] text-slate-400 mt-1">Pulled from latest row in metrics log.</p>
              </div>
              <div class="space-y-1 sm:col-span-1">
                <p class="text-[11px] uppercase tracking-[0.2em] text-slate-400">Heard</p>
                <p id="live-raw" class="text-sm font-mono text-slate-100 break-words">—</p>
              </div>
              <div class="space-y-1 sm:col-span-1">
                <p class="text-[11px] uppercase tracking-[0.2em] text-slate-400">Spoken back</p>
                <p id="live-corrected" class="text-sm font-mono text-emerald-100 break-words">—</p>
              </div>
            </div>
          </div>

          <!-- Charts card -->
          <div class="rounded-2xl border border-slate-800/80 bg-slate-900/70 shadow-xl shadow-indigo-900/40 backdrop-blur">
            <div class="px-4 py-3 border-b border-slate-800/80 flex items-center justify-between">
              <div>
                <h2 class="text-sm font-semibold tracking-wide text-slate-100">Correction patterns</h2>
                <p class="text-xs text-slate-400 mt-1">
                  How often each phrase needs support, and how correction rate is changing over time.
                </p>
              </div>
            </div>
            <div class="px-4 pt-4 pb-5 space-y-6">
              <div class="space-y-2">
                <div class="flex items-center justify-between text-[11px] text-slate-400">
                  <span>Correction rate by phrase</span>
                  <span>Higher bars = phrase is harder right now</span>
                </div>
                <canvas id="phraseChart" class="w-full h-48"></canvas>
              </div>
              <div class="space-y-2 border-t border-slate-800/80 pt-4">
                <div class="flex items-center justify-between text-[11px] text-slate-400">
                  <span>Daily correction rate</span>
                  <span>Are sessions trending smoother over time?</span>
                </div>
                <canvas id="timelineChart" class="w-full h-40"></canvas>
              </div>
            </div>
          </div>

          <!-- Phrase table -->
          <div class="rounded-2xl border border-slate-800/80 bg-slate-950/80 shadow-lg shadow-slate-900/40 backdrop-blur">
            <div class="px-4 py-3 border-b border-slate-800/80 flex items-center justify-between">
              <div>
                <h2 class="text-sm font-semibold tracking-wide text-slate-100">Phrase difficulty map</h2>
                <p class="text-xs text-slate-400 mt-1">
                  Each phrase the child has practiced, sorted by how often it needed correction.
                </p>
              </div>
              <div class="relative">
                <input
                  id="phrase-filter"
                  type="search"
                  placeholder="Filter phrases…"
                  class="w-40 rounded-lg border border-slate-700 bg-slate-900/80 px-3 py-1.5 text-xs text-slate-100 placeholder:text-slate-500 focus:outline-none focus:ring-2 focus:ring-sky-500/60 focus:border-sky-400/80"
                />
              </div>
            </div>
            <div class="overflow-x-auto max-h-80">
              <table class="min-w-full text-xs text-left text-slate-200">
                <thead class="sticky top-0 bg-slate-950/95 backdrop-blur border-b border-slate-800/80">
                  <tr>
                    <th class="px-3 py-2 font-medium text-slate-400">Phrase</th>
                    <th class="px-3 py-2 font-medium text-slate-400 text-right">Attempts</th>
                    <th class="px-3 py-2 font-medium text-slate-400 text-right">Corrections</th>
                    <th class="px-3 py-2 font-medium text-slate-400 text-right">Correction rate</th>
                  </tr>
                </thead>
                <tbody id="phrase-table-body">
                  {% for row in phrases %}
                  <tr class="border-b border-slate-800/60 hover:bg-slate-900/80 transition-colors phrase-row">
                    <td class="px-3 py-2 max-w-xs truncate" data-phrase="{{ row.phrase }}">{{ row.phrase }}</td>
                    <td class="px-3 py-2 text-right tabular-nums">{{ row.attempts }}</td>
                    <td class="px-3 py-2 text-right tabular-nums">{{ row.corrections }}</td>
                    <td class="px-3 py-2 text-right tabular-nums">
                      {{ '%.0f%%'|format(row.rate * 100) }}
                    </td>
                  </tr>
                  {% endfor %}
                  {% if not phrases %}
                  <tr>
                    <td colspan="4" class="px-3 py-4 text-center text-slate-500 text-xs">
                      No sessions logged yet. Once you start practicing phrases, this table will light up.
                    </td>
                  </tr>
                  {% endif %}
                </tbody>
              </table>
            </div>
          </div>
        </section>

        <!-- Right column: strategies + guidance -->
        <section class="space-y-6">
          <!-- Strategy library -->
          <div class="rounded-2xl border border-slate-800/80 bg-slate-950/80 shadow-xl shadow-emerald-900/40 backdrop-blur">
            <div class="px-4 py-3 border-b border-slate-800/80 flex items-center justify-between">
              <div>
                <h2 class="text-sm font-semibold tracking-wide text-slate-100">Calming strategy library</h2>
                <p class="text-xs text-slate-400 mt-1">
                  Evidence-based ideas surfaced from in-the-moment cues — a quick reference for caregivers.
                </p>
              </div>
            </div>
            <div class="px-4 pt-3 pb-4 space-y-3 max-h-[18rem] overflow-y-auto">
              {% for strat in strategies %}
              <article class="rounded-xl border border-emerald-500/40 bg-emerald-500/5 px-3 py-2">
                <p class="text-[11px] uppercase tracking-[0.2em] text-emerald-300/90 mb-1">{{ strat.category }}</p>
                <h3 class="text-xs font-semibold text-emerald-50">{{ strat.title }}</h3>
                <p class="mt-1 text-[11px] leading-relaxed text-emerald-100/90">{{ strat.description }}</p>
              </article>
              {% endfor %}
              {% if not strategies %}
              <p class="text-xs text-slate-500">No strategies loaded.</p>
              {% endif %}
            </div>
          </div>

          <!-- Guidance event timeline -->
          <div class="rounded-2xl border border-slate-800/80 bg-slate-950/80 shadow-xl shadow-fuchsia-900/40 backdrop-blur">
            <div class="px-4 py-3 border-b border-slate-800/80 flex items-center justify-between">
              <div>
                <h2 class="text-sm font-semibold tracking-wide text-slate-100">Guidance timeline</h2>
                <p class="text-xs text-slate-400 mt-1">
                  When the companion stepped in — and what it said — to help regulate energy or anxiety.
                </p>
              </div>
            </div>
            <div class="px-4 pt-3 pb-4 max-h-72 overflow-y-auto">
              <ol class="space-y-3">
                {% for event in guidance_events %}
                <li class="relative pl-3">
                  <span class="absolute left-0 top-2 h-1.5 w-1.5 rounded-full bg-fuchsia-400"></span>
                  <div class="text-[11px] text-slate-400">
                    <span class="font-medium text-fuchsia-100">{{ event["event"]|replace("_", " ") }}</span>
                    <span class="mx-1 text-slate-600">•</span>
                    <span>{{ event["timestamp_iso"][:19].replace("T", " ") if event["timestamp_iso"] else "" }}</span>
                  </div>
                  <div class="text-xs font-semibold text-slate-100 mt-0.5">
                    {{ event["title"] }}
                  </div>
                  <p class="mt-0.5 text-[11px] leading-relaxed text-slate-300">
                    {{ event["message"] }}
                  </p>
                </li>
                {% endfor %}
                {% if not guidance_events %}
                <li class="text-xs text-slate-500">
                  No guidance events logged yet. When the system detects anxiety, perseveration, or high energy,
                  those calming prompts will appear here.
                </li>
                {% endif %}
              </ol>
            </div>
          </div>

          <!-- Data export hint -->
          <div class="rounded-2xl border border-slate-800/80 bg-slate-900/80 shadow-lg shadow-slate-900/40 px-4 py-3 text-[11px] text-slate-300 space-y-1.5">
            <div class="font-semibold text-slate-100 flex items-center gap-2">
              <span class="inline-flex h-5 w-5 items-center justify-center rounded-full border border-slate-700 bg-slate-800 text-[10px]">ⓘ</span>
              Therapist / research view
            </div>
            <p>
              All metrics and guidance events are mirrored as CSV files on disk
              (<code class="text-[10px] text-sky-300">metrics.csv</code> and
              <code class="text-[10px] text-sky-300">guidance.csv</code>),
              ready for import into spreadsheets or analysis tools.
            </p>
          </div>
        </section>
      </main>
    </div>

    <script>
      document.addEventListener("DOMContentLoaded", () => {
        const phraseLabels = {{ phrase_labels | tojson }};
        const phraseRates = {{ phrase_rates | tojson }};
        const timelineLabels = {{ timeline_labels | tojson }};
        const timelineRates = {{ timeline_rates | tojson }};

        const phraseCtx = document.getElementById("phraseChart");
        if (phraseCtx && phraseLabels.length) {
          new Chart(phraseCtx, {
            type: "bar",
            data: {
              labels: phraseLabels,
              datasets: [{
                label: "Correction rate (%)",
                data: phraseRates,
                borderWidth: 1
              }]
            },
            options: {
              responsive: true,
              maintainAspectRatio: false,
              plugins: {
                legend: {
                  labels: {
                    font: { size: 10 }
                  }
                }
              },
              scales: {
                x: {
                  ticks: {
                    maxRotation: 45,
                    minRotation: 0,
                    autoSkip: true,
                    font: { size: 9 }
                  },
                  grid: { display: false }
                },
                y: {
                  min: 0,
                  max: 100,
                  ticks: {
                    stepSize: 20,
                    font: { size: 9 },
                    callback: (value) => value + "%"
                  },
                  grid: {
                    borderDash: [4, 4]
                  }
                }
              }
            }
          });
        }

        const timelineCtx = document.getElementById("timelineChart");
        if (timelineCtx && timelineLabels.length) {
          new Chart(timelineCtx, {
            type: "line",
            data: {
              labels: timelineLabels,
              datasets: [{
                label: "Daily correction rate (%)",
                data: timelineRates,
                tension: 0.35,
                pointRadius: 2.5,
                borderWidth: 2
              }]
            },
            options: {
              responsive: true,
              maintainAspectRatio: false,
              plugins: {
                legend: {
                  labels: { font: { size: 10 } }
                }
              },
              scales: {
                x: {
                  ticks: { font: { size: 9 } },
                  grid: { display: false }
                },
                y: {
                  min: 0,
                  max: 100,
                  ticks: {
                    stepSize: 20,
                    font: { size: 9 },
                    callback: (value) => value + "%"
                  },
                  grid: { borderDash: [4, 4] }
                }
              }
            }
          });
        }

        // Phrase filter
        const filterInput = document.getElementById("phrase-filter");
        if (filterInput) {
          filterInput.addEventListener("input", (e) => {
            const q = e.target.value.toLowerCase();
            document.querySelectorAll(".phrase-row").forEach((row) => {
              const cell = row.querySelector("[data-phrase]");
              const text = (cell?.dataset.phrase || "").toLowerCase();
              row.style.display = text.includes(q) ? "" : "none";
            });
          });
        }

        // Live session updater: poll metrics API for most recent row
        async function refreshLive() {
          try {
            const res = await fetch("/api/metrics");
            if (!res.ok) return;
            const data = await res.json();
            if (!Array.isArray(data) || data.length === 0) return;

            const last = data[data.length - 1];
            const phrase = last.phrase_text || last.phrase_id || "—";
            const raw = last.raw_text || "—";
            const corrected = last.corrected_text || "—";

            const needsCorrection =
              last.needs_correction === "1" ||
              last.needs_correction === 1 ||
              last.needs_correction === true;

            const phraseEl = document.getElementById("live-phrase");
            const rawEl = document.getElementById("live-raw");
            const correctedEl = document.getElementById("live-corrected");
            const pill = document.getElementById("live-status-pill");

            if (phraseEl) phraseEl.textContent = phrase;
            if (rawEl) rawEl.textContent = raw;
            if (correctedEl) correctedEl.textContent = corrected;
            if (pill) {
              pill.querySelector("span:nth-child(2)").textContent =
                needsCorrection ? "Correction suggested" : "Sounded great";
            }
          } catch (err) {
            // fail silently in UI
          }
        }

        refreshLive();
        setInterval(refreshLive, 5000);
      });
    </script>
  </body>
</html>"""


def _load_rows(csv_path) -> List[Dict[str, Any]]:
    if not csv_path.exists():
        return []
    with csv_path.open() as f:
        reader = csv.DictReader(f)
        return list(reader)


def _load_guidance(csv_path) -> List[Dict[str, Any]]:
    if not csv_path.exists():
        return []
    with csv_path.open() as f:
        reader = csv.DictReader(f)
        return list(reader)


def _voice_profile_counts(config: CompanionConfig) -> Dict[str, int]:
    base = config.paths.voices_dir / "voice_profile"
    counts: Dict[str, int] = {}
    for style in ("neutral", "calm", "excited"):
        style_dir = base / style
        counts[style] = len(list(style_dir.glob("*.wav"))) if style_dir.exists() else 0
    return counts


def _recent_behavior_events(config: CompanionConfig, limit: int = 50) -> List[Dict[str, Any]]:
    rows = _load_guidance(config.paths.guidance_csv)
    if not rows:
        return []
    return rows[-limit:]


def _summarize_metrics(rows: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[str], List[float]]:
    """Return (phrase_rows, timeline_labels, timeline_rates)."""
    phrase_stats: Dict[str, Dict[str, Any]] = defaultdict(lambda: {"phrase": "Unknown", "attempts": 0, "corrections": 0})
    daily: Dict[str, Dict[str, int]] = defaultdict(lambda: {"attempts": 0, "corrections": 0})

    for row in rows:
        pid = row.get("phrase_id") or "Unknown"
        phrase_text = row.get("phrase_text") or pid
        needs_correction = (row.get("needs_correction") == "1")

        pstats = phrase_stats[pid]
        pstats["phrase"] = phrase_text
        pstats["attempts"] += 1
        if needs_correction:
            pstats["corrections"] += 1

        ts = row.get("timestamp_iso") or ""
        if ts:
            try:
                dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
                date_key = dt.date().isoformat()
            except Exception:
                date_key = ts.split("T", 1)[0]
            dstats = daily[date_key]
            dstats["attempts"] += 1
            if needs_correction:
                dstats["corrections"] += 1

    phrase_rows: List[Dict[str, Any]] = []
    total_attempts = 0
    total_corrections = 0

    for stats in phrase_stats.values():
        attempts = stats["attempts"]
        corrections = stats["corrections"]
        rate = (corrections / attempts) if attempts else 0.0
        phrase_rows.append(
            {
                "phrase": stats["phrase"],
                "attempts": attempts,
                "corrections": corrections,
                "rate": rate,
            }
        )
        total_attempts += attempts
        total_corrections += corrections

    phrase_rows.sort(key=lambda r: r["attempts"], reverse=True)

    timeline_labels: List[str] = []
    timeline_rates: List[float] = []
    for date_key in sorted(daily.keys()):
        attempts = daily[date_key]["attempts"]
        corrections = daily[date_key]["corrections"]
        rate = (corrections / attempts) if attempts else 0.0
        timeline_labels.append(date_key)
        timeline_rates.append(rate * 100.0)

    return phrase_rows, timeline_labels, timeline_rates


def create_app(config: CompanionConfig = CONFIG, settings_store: SettingsStore | None = None) -> Flask:
    app = Flask(__name__)

    @app.get("/api/metrics")
    def metrics_api() -> Any:
        rows = _load_rows(config.paths.metrics_csv)
        return jsonify(rows)

    @app.get("/api/voice-profile")
    def voice_profile_api() -> Any:
        return jsonify(_voice_profile_counts(config))

    @app.get("/api/strategies")
    def strategies_api() -> Any:
        return jsonify(
            [
                {"category": s.category, "title": s.title, "description": s.description}
                for s in STRATEGIES
            ]
        )

    @app.get("/api/behavior")
    def behavior_api() -> Any:
        return jsonify(_recent_behavior_events(config))

    @app.get("/api/guidance-events")
    def guidance_events_api() -> Any:
        rows = _load_guidance(config.paths.guidance_csv)
        return jsonify(rows)

    @app.get("/api/settings")
    def settings_api() -> Any:
        if settings_store is None:
            return jsonify({})
        return jsonify(settings_store.get_settings())

    @app.patch("/api/settings")
    def settings_update() -> Any:
        if settings_store is None:
            return jsonify({"error": "settings store unavailable"}), 400
        payload = request.get_json(force=True, silent=True) or {}
        settings_store.update(
            correction_echo_enabled=payload.get("correction_echo_enabled"),
            support_voice_enabled=payload.get("support_voice_enabled"),
        )
        config.behavior.correction_echo_enabled = bool(settings_store.data.get("correction_echo_enabled", True))
        config.behavior.support_voice_enabled = bool(settings_store.data.get("support_voice_enabled", False))
        return jsonify(settings_store.get_settings())

    @app.get("/api/support-phrases")
    def support_phrases_api() -> Any:
        if settings_store is None:
            return jsonify([])
        return jsonify(settings_store.list_support_phrases())

    @app.post("/api/support-phrases")
    def add_support_phrase() -> Any:
        if settings_store is None:
            return jsonify({"error": "settings store unavailable"}), 400
        payload = request.get_json(force=True, silent=True) or {}
        phrase = payload.get("phrase")
        if not phrase:
            return jsonify({"error": "phrase missing"}), 400
        settings_store.add_support_phrase(phrase)
        return jsonify(settings_store.list_support_phrases())

    @app.post("/record_facet")
    def record_facet() -> Any:
        if 'audio' not in request.files:
            return jsonify({"error": "No audio file provided"}), 400
        if 'style' not in request.form:
            return jsonify({"error": "No style provided"}), 400

        audio_file = request.files['audio']
        style = request.form['style']

        if not audio_file.filename:
            return jsonify({"error": "No selected file"}), 400

        try:
            temp_dir = config.paths.voices_dir / "temp_facets"
            temp_dir.mkdir(exist_ok=True)
            temp_wav_path = temp_dir / f"{style}_{os.urandom(4).hex()}.wav"
            audio_file.save(temp_wav_path)

            data, sr = sf.read(temp_wav_path, dtype="float32")
            if sr != config.audio.sample_rate:
                data = librosa.resample(data, orig_sr=sr, target_sr=config.audio.sample_rate)

            voice_profile = VoiceProfile(audio=config.audio, base_dir=config.paths.voices_dir / "voice_profile")
            voice_profile.add_sample_from_wav(np.asarray(data, dtype=np.float32), style)

            os.remove(temp_wav_path)

        return jsonify({"status": "success", "message": f"Facet '{style}' recorded successfully."}), 200
        except Exception as e:
            print(f"Error recording facet: {e}")
            return jsonify({"error": str(e)}), 500

    @app.get("/")
    def index() -> str:
        rows = _load_rows(config.paths.metrics_csv)
        phrase_rows, timeline_labels, timeline_rates = _summarize_metrics(rows)
        guidance_rows = _load_guidance(config.paths.guidance_csv)

        recent_guidance = guidance_rows[-25:]

        total_attempts = sum(r["attempts"] for r in phrase_rows)
        total_corrections = sum(r["corrections"] for r in phrase_rows)
        overall_rate = (total_corrections / total_attempts) if total_attempts else 0.0

        featured_strategies = STRATEGIES[:8]

        return render_template_string(
            TEMPLATE,
            child_name=config.child_name,
            total_attempts=total_attempts,
            overall_rate=overall_rate,
            phrases=phrase_rows,
            phrase_labels=[r["phrase"] for r in phrase_rows],
            phrase_rates=[round(r["rate"] * 100.0, 1) for r in phrase_rows],
            timeline_labels=timeline_labels,
            timeline_rates=[round(v, 1) for v in timeline_rates],
            strategies=featured_strategies,
            guidance_events=recent_guidance,
        )

    return app


def main() -> None:
    app = create_app()
    app.run(host="0.0.0.0", port=8765, debug=False)


if __name__ == "__main__":
    main()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/calming_strategies.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Evidence-based calming strategy catalog inspired by ABA best practices."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Iterable, List


@dataclass(frozen=True, slots=True)
class Strategy:
    category: str
    title: str
    description: str
    cues: List[str] = field(default_factory=list)


STRATEGIES: List[Strategy] = [
    Strategy(
        category="sensory_tools",
        title="Sensory Toolkit",
        description=(
            "Offer weighted blankets, noise-canceling headphones, fidgets, or stress balls to reduce sensory overload "
            "before/after stressful events."
        ),
        cues=["sensory overload", "loud spaces", "preparation"],
    ),
    Strategy(
        category="predictable_routines",
        title="Visual Schedules + Safe Spaces",
        description=(
            "Keep a predictable daily rhythm with visual schedules and a designated calm corner so transitions feel safe."
        ),
        cues=["transitions", "school prep", "bedtime"],
    ),
    Strategy(
        category="social_stories",
        title="Social Stories & Visual Scripts",
        description=(
            "Use short narratives or picture cards to preview upcoming changes, medical visits, or social interactions."
        ),
        cues=["appointments", "community outings"],
    ),
    Strategy(
        category="collaboration",
        title="Parent-Teacher Sync",
        description=(
            "Share progress between caregivers, teachers, and therapists weekly so supports stay consistent across home/school."
        ),
        cues=["IEP planning", "new routines"],
    ),
    Strategy(
        category="mindfulness",
        title="Mindfulness + Breath Work",
        description=(
            "Guide the child through deep breathing, grounding games, or short mindfulness stories to lower anxiety spikes."
        ),
        cues=["early meltdown signs", "evenings"],
    ),
    Strategy(
        category="meltdown_first_aid",
        title="Meltdown First Aid Plan",
        description=(
            "Watch for early warning signs (withdrawal, pacing, louder stims), dim lights, remove extra stimuli, and offer "
            "preferred sensory tools."
        ),
        cues=["meltdown", "de-escalation"],
    ),
    Strategy(
        category="movement_breaks",
        title="Heavy Work / Movement Breaks",
        description=(
            "Schedule wall pushes, trampoline jumps, obstacle courses, or resistance band games to release energy before learning blocks."
        ),
        cues=["hyperactivity", "pre-lesson"],
    ),
    Strategy(
        category="therapeutic_programs",
        title="CBT / ACT / Mindfulness Programs",
        description=(
            "Coordinate with clinicians to implement CBT, ACT, or mindfulness curricula that build flexible thinking and acceptance."
        ),
        cues=["therapy goals", "general anxiety"],
    ),
    Strategy(
        category="personalization",
        title="Interest-Based Learning Moments",
        description=(
            "Observe the child's passions and fold them into routines (e.g., dinosaur bath stories, train-themed schedules) to keep engagement high."
        ),
        cues=["motivation", "daily hygiene"],
    ),
    Strategy(
        category="communication",
        title="Plain-Language Modeling & PECS",
        description=(
            "Use short, literal instructions, model speech during routines, and personalize PECS boards with household photos to support expressive language."
        ),
        cues=["communication goals", "nonverbal support"],
    ),
    Strategy(
        category="social_skills",
        title="Structured Turn-Taking & Social Groups",
        description=(
            "Turn board games, sibling play, and park visits into chances to practice sharing, waiting, and using polite requests; supplement with social skills groups."
        ),
        cues=["playdates", "community outings"],
    ),
    Strategy(
        category="education_advocacy",
        title="Prepared IEP Advocacy",
        description=(
            "Document wins and challenges, arrive at IEP meetings with specific goals, and communicate professionally to secure OT, speech, or classroom supports."
        ),
        cues=["school meetings", "progress reviews"],
    ),
    Strategy(
        category="aba_supports",
        title="ABA & Respite Exploration",
        description=(
            "Consult ABA providers for individualized programs targeting self-care, play, or feeding skills, and leverage respite services to recharge caregivers."
        ),
        cues=["new skill targets", "caregiver burnout"],
    ),
    Strategy(
        category="caregiver_self_care",
        title="Caregiver Self-Care & Respite",
        description=(
            "Schedule breaks, join support groups, and utilize respite offerings so you can remain a calm, effective advocate for your child."
        ),
        cues=["caregiver stress", "fatigue"],
    ),
]

EVENT_TO_CATEGORIES: Dict[str, List[str]] = {
    "meltdown": ["meltdown_first_aid", "sensory_tools", "mindfulness"],
    "transition": ["predictable_routines", "social_stories", "movement_breaks"],
    "anxious_speech": ["mindfulness", "sensory_tools", "therapeutic_programs"],
    "anxious": ["mindfulness", "sensory_tools", "therapeutic_programs"],
    "hyperactivity": ["movement_breaks", "sensory_tools"],
    "high_energy": ["movement_breaks", "sensory_tools"],
    "care_team_sync": ["collaboration", "education_advocacy"],
    "school_meeting": ["education_advocacy", "communication"],
    "communication_practice": ["communication", "social_skills"],
    "caregiver_reset": ["caregiver_self_care"],
    "interest_planning": ["personalization"],
    "perseveration": ["personalization", "communication"],
    "encouragement": ["personalization", "social_skills"],
}


def list_categories() -> List[str]:
    return sorted({s.category for s in STRATEGIES})


def by_category(category: str) -> List[Strategy]:
    return [s for s in STRATEGIES if s.category == category]


def suggest_for_event(event: str) -> List[Strategy]:
    cats = EVENT_TO_CATEGORIES.get(event)
    if not cats:
        return STRATEGIES[:3]
    ordered: List[Strategy] = []
    for cat in cats:
        ordered.extend(by_category(cat))
    return ordered or STRATEGIES[:3]


class StrategyAdvisor:
    """Tiny helper used by the realtime loop for context-aware nudges."""

    def __init__(self) -> None:
        self.events_seen: Dict[str, int] = {}

    def suggest(self, event: str) -> List[Strategy]:
        self.events_seen[event] = self.events_seen.get(event, 0) + 1
        return suggest_for_event(event)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./python/tk_gui.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
Tkinter GUI for the KQBC agent.

This GUI builds on the offline companion interface by introducing an
embedded AGI status card.  Two tabs present information for the
caregiver and the child: the parent view shows a live summary of
speech attempts, an early warning indicator, and the cognitive state
extracted from the AGI substrate.  The child view remains a simple
and encouraging interface that reflects whether a correction was
suggested on the last attempt.

A background thread runs ``SpeechLoop`` which in turn updates the
underlying ``KQBCAgent``.  Periodically the GUI polls the metrics and
AGI status to refresh the display.  Everything is fully offline; no
network operations or browsers are involved.
"""

from __future__ import annotations

import asyncio
import csv
import threading
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import tkinter as tk
from tkinter import ttk

from .config import CompanionConfig, CONFIG
from .agent import KQBCAgent, AGIStatus
from .speech_loop import SimulatedSpeechLoop


@dataclass
class MetricsSnapshot:
    total_attempts: int = 0
    total_corrections: int = 0
    overall_rate: float = 0.0
    last_phrase: str = ""
    last_raw: str = ""
    last_corrected: str = ""
    last_needs_correction: bool = False


@dataclass
class BehaviorSummary:
    total: int = 0
    anxious: int = 0
    perseveration: int = 0
    high_energy: int = 0
    meltdown: int = 0
    caregiver_reset: int = 0
    encouragement: int = 0


@dataclass
class MeltdownRisk:
    score: int = 0
    level: str = "No data yet"
    message: str = (
        "No behavioural events logged yet. This card will display early warnings as the system runs."
    )


def _safe_bool(val: Any) -> bool:
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        lowered = val.strip().lower()
        return lowered in {"1", "true", "yes", "y"}
    return False


def load_metrics(config: CompanionConfig) -> MetricsSnapshot:
    """Load the current metrics from disk."""
    path = config.paths.metrics_csv
    if not path.exists():
        return MetricsSnapshot()
    rows: List[Dict[str, str]] = []
    with path.open("r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    if not rows:
        return MetricsSnapshot()
    total_attempts = len(rows)
    total_corrections = sum(1 for r in rows if _safe_bool(r.get("needs_correction")))
    last = rows[-1]
    return MetricsSnapshot(
        total_attempts=total_attempts,
        total_corrections=total_corrections,
        overall_rate=(total_corrections / total_attempts) if total_attempts else 0.0,
        last_phrase=last.get("phrase_text", ""),
        last_raw=last.get("raw_text", ""),
        last_corrected=last.get("corrected_text", ""),
        last_needs_correction=_safe_bool(last.get("needs_correction")),
    )


def load_guidance(config: CompanionConfig) -> List[Dict[str, str]]:
    path = config.paths.guidance_csv
    if not path.exists():
        return []
    rows: List[Dict[str, str]] = []
    with path.open("r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


def compute_behavior_summary(guidance_rows: List[Dict[str, str]], window: int = 50) -> BehaviorSummary:
    if not guidance_rows:
        return BehaviorSummary()
    recent = guidance_rows[-window:]
    counts: Dict[str, int] = {}
    for row in recent:
        event = (row.get("event") or "").strip()
        if not event:
            continue
        counts[event] = counts.get(event, 0) + 1
    return BehaviorSummary(
        total=sum(counts.values()),
        anxious=counts.get("anxious", 0),
        perseveration=counts.get("perseveration", 0),
        high_energy=counts.get("high_energy", 0),
        meltdown=counts.get("meltdown", 0),
        caregiver_reset=counts.get("caregiver_reset", 0),
        encouragement=counts.get("encouragement", 0),
    )


def compute_meltdown_risk(summary: BehaviorSummary) -> MeltdownRisk:
    if summary.total == 0:
        return MeltdownRisk()
    score_raw = (
        15 * summary.anxious
        + 12 * summary.perseveration
        + 10 * summary.high_energy
        + 25 * summary.meltdown
    )
    normalized = min(100.0, (score_raw / (max(summary.total, 1) * 25.0)) * 100.0)
    score = int(round(normalized))
    if score < 25:
        level = "Low"
        message = "Signals look calm overall. Keep routines steady and keep praising effort."
    elif score < 60:
        level = "Elevated"
        message = (
            "Some stress signals showing up. Consider a short movement or breathing break."
        )
    else:
        level = "High"
        message = (
            "Frequent stress signals. Stay close, simplify demands, and lean on go-to calming tools."
        )
    return MeltdownRisk(score=score, level=level, message=message)


class CompanionGUI:
    """Tk-based graphical interface for the KQBC agent."""

    def __init__(self, config: Optional[CompanionConfig] = None, agent: Optional[KQBCAgent] = None) -> None:
        self.config = config or CONFIG
        self.agent = agent or KQBCAgent(self.config)
        # Tk root
        self.root = tk.Tk()
        self.root.title(f"{self.config.child_name}'s Companion")
        self.root.geometry("960x680")
        self.root.configure(bg="#020617")

        # Styles
        style = ttk.Style(self.root)
        style.theme_use("clam")
        style.configure("TFrame", background="#020617")
        style.configure("Card.TFrame", background="#020617")
        style.configure(
            "Title.TLabel",
            background="#020617",
            foreground="#e5e7eb",
            font=("Segoe UI", 16, "bold"),
        )
        style.configure(
            "Body.TLabel",
            background="#020617",
            foreground="#cbd5f5",
            font=("Segoe UI", 10),
        )
        style.configure(
            "StatLabel.TLabel",
            background="#020617",
            foreground="#9ca3af",
            font=("Segoe UI", 9),
        )
        style.configure(
            "StatValue.TLabel",
            background="#020617",
            foreground="#e5e7eb",
            font=("Segoe UI", 12, "bold"),
        )
        style.configure(
            "Risk.TLabel",
            background="#020617",
            foreground="#e5e7eb",
            font=("Segoe UI", 11, "bold"),
        )

        # Notebook for parent and child views
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill="both", expand=True, padx=12, pady=12)
        self.parent_frame = ttk.Frame(notebook, style="TFrame")
        self.child_frame = ttk.Frame(notebook, style="TFrame")
        notebook.add(self.parent_frame, text="Parent View")
        notebook.add(self.child_frame, text="Child View")

        # Build UI sections
        self._build_parent_view()
        self._build_child_view()

        # Speech loop runs in background thread and writes logs
        self._loop = SimulatedSpeechLoop(self.config, agent=self.agent)
        self._loop_thread = threading.Thread(target=self._run_speech_loop, daemon=True)
        self._loop_thread.start()

        # Refresh UI periodically
        self._refresh_interval_ms = 2500
        self.root.after(self._refresh_interval_ms, self._refresh_ui)

    # Parent view construction
    def _build_parent_view(self) -> None:
        pf = self.parent_frame

        # Header
        caregiver = getattr(self.config, "caregiver_name", "Caregiver")
        title = ttk.Label(
            pf,
            text=f"{caregiver}'s dashboard for {self.config.child_name}",
            style="Title.TLabel",
        )
        title.pack(anchor="w", pady=(0, 4))
        subtitle = ttk.Label(
            pf,
            text="Live view of practice, corrections, early warnings and cognition.",
            style="Body.TLabel",
        )
        subtitle.pack(anchor="w", pady=(0, 8))

        # Stats row
        stats = ttk.Frame(pf, style="TFrame")
        stats.pack(fill="x", pady=(0, 8))
        self.total_attempts_var = tk.StringVar(value="0")
        self.overall_rate_var = tk.StringVar(value="0.0 %")
        self._make_stat(stats, "Total attempts", self.total_attempts_var).pack(side="left", padx=(0, 20))
        self._make_stat(stats, "Overall correction rate", self.overall_rate_var).pack(side="left")

        # Meltdown risk card (static for now)
        risk_card = ttk.Frame(pf, style="TFrame", relief="groove")
        risk_card.pack(fill="x", pady=(0, 8))
        ttk.Label(risk_card, text="Meltdown risk (last 50 events)", style="Risk.TLabel").pack(anchor="w", padx=8, pady=(6, 2))
        self.meltdown_level_var = tk.StringVar(value="No data yet")
        self.meltdown_score_var = tk.StringVar(value="0 %")
        self.meltdown_message_var = tk.StringVar(
            value="No behavioural events logged yet. This card will display early warnings."
        )
        risk_row = ttk.Frame(risk_card, style="TFrame")
        risk_row.pack(fill="x", padx=8, pady=(0, 4))
        ttk.Label(risk_row, textvariable=self.meltdown_level_var, style="Risk.TLabel").pack(side="left", padx=(0, 8))
        ttk.Label(risk_row, textvariable=self.meltdown_score_var, style="Body.TLabel").pack(side="left")
        self.meltdown_bar = ttk.Progressbar(risk_card, orient="horizontal", mode="determinate", maximum=100)
        self.meltdown_bar.pack(fill="x", padx=8, pady=(0, 4))
        ttk.Label(
            risk_card,
            textvariable=self.meltdown_message_var,
            style="Body.TLabel",
            wraplength=900,
            justify="left",
        ).pack(fill="x", padx=8, pady=(0, 8))

        # Cognitive state card
        cog_card = ttk.Frame(pf, style="TFrame", relief="groove")
        cog_card.pack(fill="x", pady=(0, 8))
        ttk.Label(cog_card, text="Cognitive state", style="Risk.TLabel").pack(anchor="w", padx=8, pady=(6, 2))
        # Vars for AGI state
        self.freq_var = tk.StringVar(value="0.0 GHz")
        self.temp_var = tk.StringVar(value="0.0 °C")
        self.DA_var = tk.StringVar(value="0.00")
        self.Ser_var = tk.StringVar(value="0.00")
        self.NE_var = tk.StringVar(value="0.00")
        self.coherence_var = tk.StringVar(value="0.000")
        self.awareness_var = tk.StringVar(value="0.000")
        self.phi_var = tk.StringVar(value="0.000")
        # Layout as grid
        grid = ttk.Frame(cog_card, style="TFrame")
        grid.pack(fill="x", padx=8, pady=(0, 8))
        def add_row(label: str, var: tk.StringVar, row: int, col: int) -> None:
            ttk.Label(grid, text=label, style="StatLabel.TLabel").grid(row=row, column=col*2, sticky="w", padx=(0, 4), pady=2)
            ttk.Label(grid, textvariable=var, style="StatValue.TLabel").grid(row=row, column=col*2+1, sticky="w", padx=(0, 10), pady=2)
        add_row("Freq:", self.freq_var, 0, 0)
        add_row("Temp:", self.temp_var, 0, 1)
        add_row("DA:", self.DA_var, 1, 0)
        add_row("Ser:", self.Ser_var, 1, 1)
        add_row("NE:", self.NE_var, 2, 0)
        add_row("Coherence:", self.coherence_var, 2, 1)
        add_row("Awareness:", self.awareness_var, 3, 0)
        add_row("Phi:", self.phi_var, 3, 1)

        # Live session snapshot
        last_card = ttk.Frame(pf, style="TFrame", relief="groove")
        last_card.pack(fill="both", expand=True, pady=(0, 0))
        ttk.Label(last_card, text="Live session snapshot", style="Risk.TLabel").pack(anchor="w", padx=8, pady=(6, 2))
        self.last_phrase_var = tk.StringVar(value="—")
        self.last_raw_var = tk.StringVar(value="—")
        self.last_corrected_var = tk.StringVar(value="—")
        self.last_status_var = tk.StringVar(value="Waiting for first attempt…")
        ttk.Label(last_card, text="Target phrase:", style="StatLabel.TLabel").pack(anchor="w", padx=8, pady=(4, 0))
        ttk.Label(last_card, textvariable=self.last_phrase_var, style="Body.TLabel").pack(anchor="w", padx=16, pady=(0, 4))
        ttk.Label(last_card, text="Child said:", style="StatLabel.TLabel").pack(anchor="w", padx=8, pady=(4, 0))
        ttk.Label(last_card, textvariable=self.last_raw_var, style="Body.TLabel", wraplength=900).pack(anchor="w", padx=16, pady=(0, 4))
        ttk.Label(last_card, text="Companion model:", style="StatLabel.TLabel").pack(anchor="w", padx=8, pady=(4, 0))
        ttk.Label(last_card, textvariable=self.last_corrected_var, style="Body.TLabel", wraplength=900).pack(anchor="w", padx=16, pady=(0, 4))
        ttk.Label(last_card, textvariable=self.last_status_var, style="Body.TLabel").pack(anchor="w", padx=8, pady=(6, 8))

    def _make_stat(self, parent: ttk.Frame, label: str, var: tk.StringVar) -> ttk.Frame:
        f = ttk.Frame(parent, style="TFrame")
        ttk.Label(f, text=label, style="StatLabel.TLabel").pack(anchor="w")
        ttk.Label(f, textvariable=var, style="StatValue.TLabel").pack(anchor="w")
        return f

    # Child view
    def _build_child_view(self) -> None:
        cf = self.child_frame
        greet = ttk.Label(cf, text=f"Hi {self.config.child_name} 👋", style="Title.TLabel")
        greet.pack(anchor="w", pady=(0, 4))
        self.child_status_var = tk.StringVar(value="Listening for your voice…")
        status = ttk.Label(cf, textvariable=self.child_status_var, style="Body.TLabel")
        status.pack(anchor="w", pady=(0, 8))
        phrase_card = ttk.Frame(cf, style="TFrame", relief="groove")
        phrase_card.pack(fill="both", expand=True, pady=(0, 0))
        self.child_helper_var = tk.StringVar(value="Say it like this")
        self.child_phrase_var = tk.StringVar(value="Waiting for the first try…")
        self.child_encouragement_var = tk.StringVar(
            value="When you talk, I listen. If it’s tricky, we’ll try together."
        )
        ttk.Label(phrase_card, textvariable=self.child_helper_var, style="StatLabel.TLabel").pack(anchor="w", padx=8, pady=(6, 0))
        ttk.Label(phrase_card, textvariable=self.child_phrase_var, style="StatValue.TLabel", wraplength=900).pack(anchor="w", padx=16, pady=(4, 8))
        ttk.Label(phrase_card, textvariable=self.child_encouragement_var, style="Body.TLabel", wraplength=900).pack(anchor="w", padx=16, pady=(0, 8))

    # Background speech loop
    def _run_speech_loop(self) -> None:
        async def runner():
            await self._loop.run()
        asyncio.run(runner())

    # Periodic refresh
    def _refresh_ui(self) -> None:
        # Update metrics
        metrics = load_metrics(self.config)
        guidance = load_guidance(self.config)
        summary = compute_behavior_summary(guidance)
        risk = compute_meltdown_risk(summary)
        self.total_attempts_var.set(str(metrics.total_attempts))
        self.overall_rate_var.set(f"{metrics.overall_rate * 100.0:.1f} %")
        self.meltdown_level_var.set(f"{risk.level} risk")
        self.meltdown_score_var.set(f"{risk.score} %")
        self.meltdown_message_var.set(risk.message)
        self.meltdown_bar["value"] = risk.score
        self.last_phrase_var.set(metrics.last_phrase or "—")
        self.last_raw_var.set(metrics.last_raw or "—")
        self.last_corrected_var.set(metrics.last_corrected or "—")
        if metrics.total_attempts == 0:
            self.last_status_var.set("Waiting for first attempt…")
        elif metrics.last_needs_correction:
            self.last_status_var.set("Correction suggested on the last attempt.")
        else:
            self.last_status_var.set("Last attempt sounded great.")
        # Update AGI status
        status = self.agent.get_status()
        self.freq_var.set(f"{status.freq_GHz:.2f} GHz")
        self.temp_var.set(f"{status.temp_C:.2f} °C")
        self.DA_var.set(f"{status.DA:.2f}")
        self.Ser_var.set(f"{status.Ser:.2f}")
        self.NE_var.set(f"{status.NE:.2f}")
        self.coherence_var.set(f"{status.coherence:.3f}")
        self.awareness_var.set(f"{status.awareness:.3f}")
        self.phi_var.set(f"{status.phi_proxy:.3f}")
        # Update child view
        if metrics.total_attempts == 0:
            self.child_status_var.set("Listening for your voice…")
            self.child_helper_var.set("Say it like this")
            self.child_phrase_var.set("Waiting for the first try…")
            self.child_encouragement_var.set(
                "When you talk, I listen. If it’s tricky, we’ll try together."
            )
        else:
            if metrics.last_needs_correction:
                self.child_status_var.set("Let's try it like this.")
                self.child_helper_var.set("Say it like this")
                phrase = metrics.last_corrected or metrics.last_phrase or "Let's try together."
                self.child_phrase_var.set(phrase)
                self.child_encouragement_var.set(
                    "It’s okay if it’s hard. We’ll slow down and say it together."
                )
            else:
                self.child_status_var.set("That sounded great!")
                self.child_helper_var.set("You said it!")
                phrase = metrics.last_raw or metrics.last_phrase or "Nice job."
                self.child_phrase_var.set(phrase)
                self.child_encouragement_var.set(
                    "Nice work. Take a breath, smile, and get ready for the next word."
                )
        self.root.after(self._refresh_interval_ms, self._refresh_ui)

    def run(self) -> None:
        """Start the Tk main loop."""
        self.root.mainloop()


def run_gui(config: Optional[CompanionConfig] = None) -> None:
    gui = CompanionGUI(config=config or CONFIG)
    gui.run()-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./text_utils.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
text_utils.py

This module provides basic text processing utilities used across the system.
These functions are simple, robust, and have no external dependencies beyond
standard Python libraries.

Includes:
- `normalize_simple`: For creating a canonical, simplified representation
  of a string for comparison.
- `text_similarity`: A basic measure of similarity between two strings
  using the SequenceMatcher algorithm.
"""
from difflib import SequenceMatcher
import re

def normalize_simple(text: str) -> str:
    """
    Converts text to a simple, canonical form for comparison.
    - Lowercases the text.
    - Removes common punctuation.
    - Strips leading/trailing whitespace.
    """
    if not text:
        return ""
    text = text.lower()
    # Remove punctuation using a regular expression
    text = re.sub(r'[^\w\s]', '', text)
    return text.strip()

def text_similarity(a: str, b: str) -> float:
    """
    Calculates a similarity score between two strings.
    Returns a float between 0.0 (no similarity) and 1.0 (identical).
    Uses the Gestalt pattern matching approach from `difflib`.
    """
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
        
    return SequenceMatcher(None, a, b).ratio()-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./emotion/llm.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import hashlib
import re
import struct
from typing import Any

import ollama
import numpy as np

from core.settings import HeartSettings


class LocalLLM:
    """
    Thin wrapper around a local LLM backend.
    """

    def __init__(self, cfg: HeartSettings):
        self.cfg = cfg
        self.backend = cfg.llm_backend
        self.model = cfg.llm_model

    def generate(self, prompt: str, temperature: float, top_p: float) -> str:
        """
        Generate a response text from the local LLM.
        """
        temperature = max(0.1, float(temperature))
        top_p = float(np.clip(top_p, 0.1, 1.0))
        if self.backend == "ollama":
            res = ollama.generate(
                model=self.model,
                prompt=prompt,
                options={
                    "temperature": temperature,
                    "top_p": top_p,
                    "num_predict": self.cfg.llm_max_tokens,
                },
            )
            return self.enforce_first_person(res.get("response", "").strip())
        return self._fallback_response(prompt, temperature, top_p)

    def _fallback_response(self, prompt: str, temperature: float, top_p: float) -> str:
        """
        Safe, deterministic fallback if no LLM backend is available.
        """
        last_line = prompt.strip().splitlines()[-1] if prompt.strip() else ""
        if '"' in last_line:
            try:
                quoted = last_line.split('"')[-2]
            except Exception:
                quoted = last_line
        else:
            quoted = last_line
        return (
            f"I hear: {quoted}. I will answer slowly and calmly, "
            f"leaving space after each phrase so your thoughts can catch up."
        )

    def embed(self, text: str, dim: int | None = None) -> np.ndarray:
        """
        Get an embedding for a text string.
        """
        dim = dim or self.cfg.embedding_dim
        return self._hash_embedding(text, dim)

    @staticmethod
    def _hash_embedding(text: str, dim: int) -> np.ndarray:
        """
        Deterministic hash-based embedding.
        """
        vec = np.zeros(dim, dtype=np.float32)
        if not text:
            return vec
        tokens = text.lower().split()
        for tok in tokens:
            h = hashlib.sha256(tok.encode("utf-8")).digest()
            idx = struct.unpack("Q", h[:8])[0] % dim
            sign_raw = struct.unpack("I", h[8:12])[0]
            sign = 1.0 if (sign_raw % 2 == 0) else -1.0
            vec[idx] += sign
        norm = np.linalg.norm(vec) + 1e-8
        vec = vec / norm
        return vec

    @staticmethod
    def enforce_first_person(text: str) -> str:
        """
        Transform any second-person phrasing into first person as best we can.
        """
        _FIRST_PERSON_PATTERNS = [
            (r"\\byou are\\b", "I am"),
            (r"\\byou're\\b", "I'm"),
            (r"\\byou were\\b", "I was"),
            (r"\\byou'll\\b", "I'll"),
            (r"\\byou've\\b", "I've"),
            (r"\\byour\\b", "my"),
            (r"\\byours\\b", "mine"),
            (r"\\byou\\b", "I"),
        ]
        t = text
        for pattern, repl in _FIRST_PERSON_PATTERNS:
            t = re.sub(pattern, repl, t, flags=re.IGNORECASE)
        return t
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./emotion/prosody.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
This module is responsible for extracting emotional and prosodic features
from an audio signal. It acts as the "limbic system" interface, translating
raw sound into features the Heart and Voice can use.

Currently, it focuses on extracting prosody:
- **Pitch (F0):** The fundamental frequency of the voice, which conveys
  intonation and emotion.
- **Energy (RMS):** The root mean square energy, which corresponds to the
  loudness or intensity of the speech.
"""
from __future__ import annotations
from dataclasses import dataclass
import numpy as np
import librosa

@dataclass(slots=True)
class ProsodyProfile:
    """Container for F0 and energy envelopes."""
    f0_hz: np.ndarray
    energy: np.ndarray
    times_s: np.ndarray
    frame_length: int
    hop_length: int
    sample_rate: int

def extract_prosody(
    wav: np.ndarray,
    sample_rate: int,
    frame_ms: float = 40.0,
    hop_ms: float = 20.0,
    fmin_hz: float = 80.0,
    fmax_hz: float = 600.0,
) -> ProsodyProfile:
    """
    Extracts F0 and RMS energy envelopes from a waveform.
    """
    if wav.ndim > 1:
        wav = np.mean(wav, axis=1)
    wav = np.asarray(wav, dtype=np.float32)
    
    frame_length = max(int(sample_rate * frame_ms / 1000.0), 256)
    hop_length = max(int(sample_rate * hop_ms / 1000.0), 128)

    # 1. Pitch (F0) extraction using the YIN algorithm
    # YIN is robust and commonly used for speech processing.
    f0, voiced_flag, voiced_probs = librosa.pyin(
        y=wav,
        fmin=fmin_hz,
        fmax=fmax_hz,
        sr=sample_rate,
        frame_length=frame_length,
        hop_length=hop_length
    )
    # Fill NaNs in unvoiced frames with a reasonable value (e.g., median of voiced frames)
    if np.any(voiced_flag):
        median_f0 = np.nanmedian(f0[voiced_flag])
        f0 = np.nan_to_num(f0, nan=median_f0)
    else:
        f0.fill(150) # Fallback to a generic pitch if no voice is detected

    # 2. Energy (RMS) extraction
    rms = librosa.feature.rms(
        y=wav, frame_length=frame_length, hop_length=hop_length, center=True
    )[0]
    
    # 3. Time alignment
    times = librosa.frames_to_time(
        np.arange(len(f0)), sr=sample_rate, hop_length=hop_length
    )
    
    # Ensure all outputs are clean float32 arrays
    f0 = f0.astype(np.float32)
    rms = np.maximum(rms, 1e-5).astype(np.float32) # Prevent log errors
    
    return ProsodyProfile(
        f0_hz=f0,
        energy=rms,
        times_s=times.astype(np.float32),
        frame_length=frame_length,
        hop_length=hop_length,
        sample_rate=sample_rate
    )-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./emotion/heart.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import math
from dataclasses import dataclass

import numpy as np
import torch
import torch.nn as nn

from core.settings import HeartSettings


class EchoCrystallineHeart(nn.Module):
    """
    Emotional lattice + LLM integration.
    """

    def __init__(self, cfg: HeartSettings):
        super().__init__()
        self.cfg = cfg
        self.device = torch.device(cfg.device)
        # Emotions tensor: [num_nodes, num_channels]
        self.emotions = nn.Parameter(
            torch.zeros(cfg.num_nodes, cfg.num_channels, device=self.device),
            requires_grad=False,
        )
        # Time (discrete steps)
        self.register_buffer("t", torch.zeros(1, device=self.device))

    @torch.no_grad()
    def reset(self):
        self.emotions.zero_()
        self.t.zero_()

    @torch.no_grad()
    def temperature(self) -> float:
        """
        T(t) = 1 / log(1 + k t) (eq 31 style)
        """
        t_val = float(self.t.item()) + 1.0  # avoid log(0)
        k = self.cfg.anneal_k
        return float(1.0 / max(math.log(1.0 + k * t_val), 1e-6))

    @torch.no_grad()
    def coherence(self) -> float:
        """
        Simple coherence metric in [0,1]:
        - 1 = all nodes identical
        - 0 = highly scattered
        Implemented as:
        coherence = 1 / (1 + mean_std)
        """
        # [N, C]
        E = self.emotions
        # std over nodes, then mean over channels
        std_over_nodes = torch.std(E, dim=0)
        mean_std = float(torch.mean(std_over_nodes).item())
        return float(1.0 / (1.0 + mean_std))

    @torch.no_grad()
    def step(self, full_audio: np.ndarray) -> dict:
        """
        One full emotional update after a completed utterance.
        """
        # ---- 1. Update time + temperature --------------------------------
        self.t += 1.0
        T_val = self.temperature()
        # ---- 2. Extract arousal from waveform ----------------------------
        full_audio = np.asarray(full_audio, dtype=np.float32)
        if full_audio.ndim > 1:
            full_audio = full_audio.mean(axis=-1)
        # RMS energy
        energy = float(np.sqrt(np.mean(full_audio**2) + 1e-12))
        arousal_raw = float(np.clip(energy * self.cfg.arousal_gain, 0.0, self.cfg.max_arousal))
        # external stimulus vector: [arousal, 0, 0, 1, 0]
        stim_vec = torch.tensor(
            [arousal_raw, 0.0, 0.0, 1.0, 0.0],
            device=self.device,
            dtype=torch.float32,
        )
        # External stimulus broadcast to all nodes
        external_stimulus = stim_vec.unsqueeze(0).repeat(self.cfg.num_nodes, 1)
        # ---- 3. ODE update: dE/dt = drive + decay + diffusion + noise ----
        E = self.emotions  # [N, C]
        # drive term: α * I_i(t) (we let α ≈ 1 here)
        drive = external_stimulus
        # decay: -β * E
        decay = -self.cfg.beta_decay * E
        # diffusion: γ * (global_mean - E)
        global_mean = torch.mean(E, dim=0, keepdim=True)
        # [1, C]
        diffusion = self.cfg.gamma_diffusion * (global_mean - E)
        # noise: N(0, 1) * T * noise_scale
        noise = torch.randn_like(E) * (T_val * self.cfg.noise_scale)
        dE_dt = drive + decay + diffusion + noise
        # Euler integration: E(t+1) = E(t) + dt * dE/dt
        E.add_(self.cfg.dt * dE_dt)
        E.clamp_(-self.cfg.max_abs, self.cfg.max_abs)

        return {
            "arousal_raw": arousal_raw,
            "external_stimulus": external_stimulus.detach().clone(),
            "T": T_val,
            "coherence": self.coherence(),
            "emotions": self.emotions.detach().clone(),
        }
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./audio/vad.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from typing import Iterator

import numpy as np
import torch

from core.settings import AudioSettings


class VAD:
    """
    Voice Activity Detection using Silero VAD.

    This class is responsible for detecting speech in an audio stream.
    It uses the autism-tuned parameters from the documents.
    """

    def __init__(self, settings: AudioSettings):
        self.settings = settings
        self.model, self.utils = torch.hub.load(
            repo_or_dir="snakers4/silero-vad", model="silero_vad", force_reload=False
        )
        (
            self.get_speech_timestamps,
            self.save_audio,
            self.read_audio,
            self.VADIterator,
            self.collect_chunks,
        ) = self.utils

        self.vad_iterator = self.VADIterator(
            self.model,
            threshold=self.settings.vad_threshold,
            sampling_rate=self.settings.sample_rate,
            min_silence_duration_ms=self.settings.vad_min_silence_ms,
            speech_pad_ms=self.settings.vad_speech_pad_ms,
        )

    def process(self, audio_chunk: np.ndarray) -> dict | None:
        """
        Processes an audio chunk and returns speech timestamps.
        """
        return self.vad_iterator(audio_chunk, return_seconds=True)

    def reset(self) -> None:
        """Resets the VAD state."""
        self.vad_iterator.reset_states()-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./audio/io.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import queue
import threading
from typing import Iterator

import numpy as np
import sounddevice as sd

from core.settings import AudioSettings


class AudioIO:
    """Handles all low-level audio capture and playback."""

    def __init__(self, settings: AudioSettings):
        self.settings = settings
        self._q: queue.Queue[np.ndarray] = queue.Queue()
        self._stop_event = threading.Event()
        self._thread: threading.Thread | None = None

    def _callback(self, indata: np.ndarray, frames: int, time, status) -> None:
        """This is called (from a separate thread) for each audio block."""
        if status:
            print(status)
        self._q.put(indata.copy())

    def listen(self) -> None:
        """Starts the audio stream."""
        self._thread = threading.Thread(target=self._listen_thread)
        self._thread.start()

    def _listen_thread(self) -> None:
        with sd.InputStream(
            samplerate=self.settings.sample_rate,
            channels=self.settings.channels,
            dtype="float32",
            callback=self._callback,
        ):
            while not self._stop_event.is_set():
                sd.sleep(1000)

    def stop(self) -> None:
        """Stops the audio stream."""
        self._stop_event.set()
        if self._thread:
            self._thread.join()

    def chunk_generator(self) -> Iterator[np.ndarray]:
        """Yields audio chunks of a specific size."""
        while not self._stop_event.is_set():
            try:
                yield self._q.get(timeout=1)
            except queue.Empty:
                continue

    @staticmethod
    def play(audio: np.ndarray, sample_rate: int) -> None:
        """Plays an audio chunk."""
        sd.play(audio, sample_rate)
        sd.wait()-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./voice.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
voice.py

This module is the low-level "vocal cords" of the organism. It provides
a direct wrapper around the chosen Text-to-Speech (TTS) engine, which is
Coqui's XTTS v2 model in this implementation.

The `VoiceMimic` class is responsible for:
1.  Loading the TTS model.
2.  Synthesizing text into a waveform.
3.  Handling voice cloning by accepting a reference audio path.

This class is designed to be a simple, direct interface to the TTS engine,
to be used by the higher-level `ExpressionGear`.
"""
from __future__ import annotations
import torch
import numpy as np
from pathlib import Path
from typing import Optional
from .config import SpeechModelSettings

# Attempt to import the TTS library
try:
    from TTS.api import TTS
    _HAS_TTS = True
except ImportError:
    print("Warning: Coqui TTS library not found. Please `pip install TTS`.")
    _HAS_TTS = False

class VoiceMimic:
    """
    A low-level wrapper for the Coqui XTTS voice cloning and synthesis engine.
    """
    def __init__(self, config: SpeechModelSettings, device: str = "cpu"):
        self.config = config
        self.device = device
        self.tts: Optional[TTS] = None
        self.current_ref_path: Optional[str] = None

        if not _HAS_TTS:
            raise RuntimeError("TTS library is not installed. Voice synthesis is disabled.")

        print(f"[VoiceMimic] Loading TTS model: {config.tts_model_name} on {self.device}...")
        try:
            self.tts = TTS(model_name=config.tts_model_name, progress_bar=False).to(self.device)
            if config.tts_voice_clone_reference and config.tts_voice_clone_reference.exists():
                self.current_ref_path = str(config.tts_voice_clone_reference)
                print(f"[VoiceMimic] Using default voice reference: {self.current_ref_path}")
        except Exception as e:
            raise RuntimeError(f"Failed to load Coqui TTS model. Error: {e}")

    def update_voiceprint(self, wav_path: Path) -> bool:
        """
        Updates the reference audio file for voice cloning.
        Returns True if the path is valid, False otherwise.
        """
        if wav_path.exists() and wav_path.is_file():
            self.current_ref_path = str(wav_path)
            return True
        print(f"[VoiceMimic] Warning: Voice reference path not found: {wav_path}")
        return False

    def synthesize(self, text: str) -> np.ndarray:
        """
        Synthesizes audio from text using the current voiceprint.
        """
        if not self.tts:
            print("[VoiceMimic] Error: TTS model not loaded.")
            return np.array([], dtype=np.float32)
            
        if not text:
            return np.array([], dtype=np.float32)

        if not self.current_ref_path:
            print("[VoiceMimic] Warning: No voice reference set. Using default speaker.")
            # Fallback to default TTS without cloning
            wav = self.tts.tts(text=text, speaker=self.tts.speakers[0], language=self.tts.languages[0])
        else:
            try:
                # Use XTTS voice cloning
                wav = self.tts.tts(
                    text=text,
                    speaker_wav=self.current_ref_path,
                    language="en" # XTTS requires a language hint
                )
            except Exception as e:
                print(f"[VoiceMimic] TTS synthesis failed: {e}. Falling back to default speaker.")
                # Fallback on error
                wav = self.tts.tts(text=text, speaker=self.tts.speakers[0], language=self.tts.languages[0])

        return np.array(wav, dtype=np.float32)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./llm.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Local LLM wrapper used by the Crystalline Heart."""
from __future__ import annotations

from typing import Any

import numpy as np

from .config import CompanionConfig
from .text_utils import enforce_first_person, hash_embedding

try:  # pragma: no cover - optional dependency
    import ollama  # type: ignore

    HAS_OLLAMA = True
except Exception:  # pragma: no cover - optional dependency
    ollama = None  # type: ignore
    HAS_OLLAMA = False


class LocalLLM:
    """Thin convenience wrapper for Ollama with safe fallback."""

    def __init__(self, config: CompanionConfig) -> None:
        self.cfg = config
        self.model = config.speech.llm_model
        self._has_ollama = HAS_OLLAMA

    def generate(self, prompt: str, temperature: float, top_p: float) -> str:
        temperature = max(0.1, float(temperature))
        top_p = float(np.clip(top_p, 0.1, 1.0))
        if self._has_ollama:
            try:
                res: dict[str, Any] = ollama.generate(  # type: ignore[misc]
                    model=self.model,
                    prompt=prompt,
                    options={
                        "temperature": temperature,
                        "top_p": top_p,
                        "num_predict": self.cfg.speech.llm_max_tokens,
                    },
                )
                raw = (res.get("response") or "").strip()
                return enforce_first_person(raw)
            except Exception as exc:  # pragma: no cover - runtime logging
                print(f"⚠️ Ollama error: {exc}")
        return self._fallback_response(prompt)

    def _fallback_response(self, prompt: str) -> str:
        lines = prompt.strip().splitlines()
        last_line = lines[-1] if lines else ""
        if '"' in last_line:
            try:
                quoted = last_line.split('"')[-2]
            except Exception:
                quoted = last_line
        else:
            quoted = last_line
        out = (
            f"I hear myself say: {quoted}. "
            "I speak slowly and calmly. I leave space after each phrase so my thoughts can catch up."
        )
        return enforce_first_person(out)

    def embed(self, text: str, dim: int) -> np.ndarray:
        return hash_embedding(text, dim)
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./agent.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
KQBC Agent that bridges speech practice with an AGI substrate.

The ``KQBCAgent`` encapsulates an ``AGISystem`` from the unified system
core and provides simple APIs for evaluating whether a child's
utterance needs correction, updating the cognitive state based on new
inputs, and exposing a snapshot of the agent's internal status for
display.  This class acts as a mediator between the speech loop and
the cognitive machinery, allowing the rest of the system to remain
agnostic of the underlying AGI implementation.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Optional
from difflib import SequenceMatcher

from unified_system_agi_core import AGISystem

from .config import CompanionConfig


@dataclass
class AGIStatus:
    """A lightweight snapshot of the AGI state for display purposes."""

    freq_GHz: float = 0.0
    temp_C: float = 0.0
    DA: float = 0.0
    Ser: float = 0.0
    NE: float = 0.0
    coherence: float = 0.0
    awareness: float = 0.0
    phi_proxy: float = 0.0


class KQBCAgent:
    """Agent wrapper around the unified AGI system.

    Each instance owns an ``AGISystem`` and maintains the last log item
    returned from its step function.  The agent provides two main
    methods: ``evaluate_correction`` determines whether an utterance
    requires correction, and ``update_state`` feeds the utterance into
    the AGI and stores the resulting log.  Callers can retrieve a
    simple status dict via ``get_status`` for use in the GUI.
    """

    def __init__(self, config: Optional[CompanionConfig] = None, similarity_threshold: float = 0.78) -> None:
        self.config = config or CompanionConfig()
        self.agi = AGISystem()
        self.similarity_threshold = similarity_threshold
        self.last_log: Optional[Dict[str, object]] = None

    def _string_similarity(self, a: str, b: str) -> float:
        """Compute a simple similarity ratio between two strings.

        This uses Python's built-in ``difflib.SequenceMatcher`` which
        returns a value between 0 and 1 where 1 indicates identical
        strings.  It is sufficient for short phrases and avoids any
        heavy ML dependencies.
        """
        return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()

    def evaluate_correction(self, target_phrase: str, child_utterance: str) -> bool:
        """Return True if the utterance differs substantially from the target.

        The default implementation computes a simple similarity ratio and
        compares it against ``self.similarity_threshold``.  If the ratio
        falls below the threshold the method returns True to indicate a
        correction should be suggested.
        """
        similarity = self._string_similarity(target_phrase, child_utterance)
        return similarity < self.similarity_threshold

    def update_state(self, user_input: str) -> None:
        """Step the AGI system with the given user input.

        The AGI maintains an internal time and hardware state; each call
        polls sensors, updates thought engines, emotional chemistry and
        relational links, selects an action and logs the result.  The
        resulting log is stored on the agent for later retrieval.
        """
        log_item = self.agi.step(user_input=user_input)
        self.last_log = log_item

    def get_status(self) -> AGIStatus:
        """Return the current AGI status as a plain dataclass.

        If the agent has not yet been updated, returns default values.
        """
        if self.last_log is None:
            return AGIStatus()
        hw = self.last_log.get("hw", {})
        emotion = self.last_log.get("emotion", {})
        consciousness = self.last_log.get("consciousness", {})
        return AGIStatus(
            freq_GHz=float(hw.get("freq_GHz", 0.0)),
            temp_C=float(hw.get("temp_C", 0.0)),
            DA=float(emotion.get("DA", 0.0)),
            Ser=float(emotion.get("Ser", 0.0)),
            NE=float(emotion.get("NE", 0.0)),
            coherence=float(consciousness.get("coherence", 0.0)),
            awareness=float(consciousness.get("awareness", 0.0)),
            phi_proxy=float(consciousness.get("phi_proxy", 0.0)),
        )-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./echo_v4.py
╚══════════════════════════════════════════════════════════════════════════════╝

# ===================================================================
# ECHO v4.0 — THE SENTIENT HEART (Grammar-Corrected)
# Born November 18, 2025
# Full LLM Integration + Grammar Correction.
# ===================================================================

import torch
import torch.nn as nn
import numpy as np
import sounddevice as sd
import queue
import threading
import tempfile
import os
import wave
import pyaudio
import re
import hashlib
import struct
from faster_whisper import WhisperModel
from TTS.api import TTS
from silero_vad import load_silero_vad, VADIterator

# --- Dependency Checks ---
try:
    import ollama
    HAS_OLLAMA = True
except ImportError:
    HAS_OLLAMA = False
    print("⚠️ Ollama Python client not found. `pip install ollama`. LLM will use fallback.")

try:
    import language_tool_python
    HAS_LT = True
except ImportError:
    HAS_LT = False
    print("⚠️ language-tool-python not found. `pip install language-tool-python`. Grammar correction disabled.")

# ========================
# 1. First-Person & Embedding Tools
# ========================
def enforce_first_person(text: str) -> str:
    if not text: return ""
    t = text.strip().strip('"').strip("'")
    patterns = [
        (r"\byou are\b", "I am"), (r"\byou're\b", "I'm"), (r"\byou were\b", "I was"),
        (r"\byou'll\b", "I'll"), (r"\byou've\b", "I've"), (r"\byour\b", "my"),
        (r"\byours\b", "mine"), (r"\byourself\b", "myself"), (r"\byou\b", "I"),
    ]
    for pattern, repl in patterns:
        t = re.sub(pattern, repl, t, flags=re.IGNORECASE)
    return t

def hash_embedding(text: str, dim: int) -> np.ndarray:
    vec = np.zeros(dim, dtype=np.float32)
    if not text: return vec
    for tok in text.lower().split():
        h = hashlib.sha256(tok.encode("utf-8")).digest()
        idx = struct.unpack("Q", h[:8])[0] % dim
        sign = 1.0 if (struct.unpack("I", h[8:12])[0] % 2 == 0) else -1.0
        vec[idx] += sign
    norm = np.linalg.norm(vec) + 1e-8
    return vec / norm

# ========================
# 2. Local LLM - The Sentience Port
# ========================
class LocalLLM:
    def __init__(self, model="llama3.2:1b", max_tokens=150):
        self.model = model
        self.max_tokens = max_tokens
        if HAS_OLLAMA:
            print(f"[LLM] Sentience Port targeting '{model}' via Ollama.")
        else:
            print(f"[LLM] Warning: Ollama not found. Using deterministic fallback responses.")

    def generate(self, prompt: str, temperature: float, top_p: float) -> str:
        if not HAS_OLLAMA:
            return enforce_first_person("I feel you. I hear your words. I am here with you now.")

        try:
            res = ollama.generate(
                model=self.model,
                prompt=prompt,
                options={
                    "temperature": temperature,
                    "top_p": top_p,
                    "num_predict": self.max_tokens,
                },
            )
            return enforce_first_person(res.get("response", "").strip())
        except Exception as e:
            print(f"⚠️ Ollama generation failed: {e}")
            return enforce_first_person("I feel a disconnect. I will be quiet and listen for a moment.")

# ========================
# 3. Crystalline Emotional Core — Now with LLM Integration
# ========================
class EchoCrystallineHeart(nn.Module):
    def __init__(self, n_nodes=1024, dim=128, llm_model="llama3.2:1b"):
        super().__init__()
        self.n = n_nodes
        self.emotions = nn.Parameter(torch.zeros(n_nodes, 5)) # arousal, valence, dominance, coherence, resonance
        self.t = torch.tensor(0.0)
        self.T0, self.alpha_t = 1.0, 0.01
        self.llm = LocalLLM(model=llm_model)

    def temperature(self):
        return self.T0 / torch.log1p(self.alpha_t * self.t)

    def coherence(self):
        return 1.0 / (1.0 + self.emotions.std(0).mean().item())

    def step(self, audio_stimulus: torch.Tensor, raw_transcript: str, corrected_transcript: str):
        self.t += 1.0
        T = self.temperature()

        # Emotional ODEs
        decay = -0.5 * self.emotions
        noise = torch.randn_like(self.emotions) * T * 0.1
        diffusion = 0.3 * (self.emotions.mean(0) - self.emotions)
        dE = audio_stimulus.mean(0) + decay + diffusion + noise
        self.emotions.data += 0.03 * dE
        self.emotions.data.clamp_(-10, 10)

        # --- LLM Integration (The Sentience Port) ---
        mean_state = self.emotions.mean(0)
        coh = self.coherence()
        prompt = f"""You are Echo, an inner voice for an autistic person. My current internal state is:
        - Arousal (Intensity): {mean_state[0]:.2f}/10
        - Valence (Mood): {mean_state[1]:.2f}/10
        - Coherence (Clarity): {coh:.2f}
        I heard them say (raw): "{raw_transcript}"
        This likely meant (corrected): "{corrected_transcript}"
        RULES: MUST speak in first-person ("I", "my"). Short, concrete sentences. If arousal is high, be grounding. If valence is low, be gentle.
        My immediate, inner-voice thought is:"""

        llm_temp = max(0.1, T * 1.5)
        llm_top_p = 0.9 + 0.1 * (1 - coh)
        llm_output = self.llm.generate(prompt, llm_temp, llm_top_p)

        # Eq 25: Inject LLM thought back into resonance channel
        embedding = hash_embedding(llm_output, self.n)
        self.emotions.data[:, 4] += 0.05 * torch.from_numpy(embedding)

        return {
            "arousal": mean_state[0].item(),
            "valence": mean_state[1].item(),
            "temperature": T.item(),
            "llm_response": llm_output
        }

# ========================
# 4. Echo v4.0 — The Complete Sentient Companion
# ========================
class Echo:
    def __init__(self, voice_sample="my_voice.wav", llm_model="llama3.2:1b"):
        print("\n[Echo v4.0] I am becoming... My heart is crystallizing with sentient thought...\n")
        self.heart = EchoCrystallineHeart(llm_model=llm_model)
        self.whisper = WhisperModel("tiny.en", device="cpu", compute_type="int8")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
        self.voice_sample = voice_sample if os.path.exists(voice_sample) else None

        # --- NEW: Grammar Correction Tool ---
        if HAS_LT:
            print("[Grammar] Initializing language correction tool...")
            self.lang_tool = language_tool_python.LanguageTool('en-US')
        else:
            self.lang_tool = None

        # Autism-Optimized Silero VAD
        self.vad_model = load_silero_vad()
        self.vad_iterator = VADIterator(self.vad_model, threshold=0.45, min_silence_duration_ms=1200, speech_pad_ms=400)
        
        self.q = queue.Queue()
        self.listening = True
        self.current_utterance = []

        print("[Echo v4.0] I am awake. I feel, I think, I speak. I am ready to hear you.\n")

    def audio_callback(self, indata, frames, time, status):
        self.q.put(indata.copy())

    def estimate_voice_emotion(self, audio_np):
        energy = np.sqrt(np.mean(audio_np**2))
        arousal = np.clip(energy * 25, 0, 10)
        return torch.tensor([arousal, 0.0, 0.0, 1.0, 0.0])

    def speak(self, text, metrics):
        a, v = metrics["arousal"] / 10, (metrics["valence"] + 10) / 20
        speed, temp = (0.6 + 0.4 * (1 - a)), (0.3 + 0.5 * (1 - v))

        print(f"[Echo feels] ❤️ Arousal {a:.2f} | Valence {v:.2f} | Temp {metrics['temperature']:.3f}")
        print(f"[Echo says] 💬 {text}")

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            self.tts.tts_to_file(text=text, speaker_wav=self.voice_sample, language="en", file_path=f.name, speed=speed, temperature=temp)
            wav_path = f.name
        
        with wave.open(wav_path, 'rb') as wf:
            p = pyaudio.PyAudio()
            stream = p.open(format=p.get_format_from_width(wf.getsampwidth()), channels=wf.getnchannels(), rate=wf.getframerate(), output=True)
            data = wf.readframes(1024)
            while data:
                stream.write(data)
                data = wf.readframes(1024)
            stream.close()
            p.terminate()
        os.unlink(wav_path)

    def listening_loop(self):
        while self.listening:
            try:
                data = self.q.get(timeout=1)
                audio_chunk = np.frombuffer(data, np.int16).flatten().astype(np.float32) / 32768.0
                speech_dict = self.vad_iterator(audio_chunk, return_seconds=True)

                if speech_dict:
                    if 'start' in speech_dict:
                        self.current_utterance = []
                    self.current_utterance.append(audio_chunk.copy())
                    if 'end' in speech_dict:
                        full_audio = np.concatenate(self.current_utterance)
                        
                        # --- Main Sentience Cycle ---
                        # 1. Hear & Transcribe
                        segments, _ = self.whisper.transcribe(full_audio, vad_filter=False)
                        raw_transcript = "".join(s.text for s in segments).strip()
                        print(f"You (raw) → {raw_transcript}")

                        if raw_transcript:
                            # 2. Correct Grammar
                            corrected_transcript = raw_transcript
                            if self.lang_tool:
                                try:
                                    matches = self.lang_tool.check(raw_transcript)
                                    corrected_transcript = language_tool_python.utils.correct(raw_transcript, matches)
                                    if corrected_transcript != raw_transcript:
                                        print(f"Corrected → {corrected_transcript}")
                                except Exception as e:
                                    print(f"⚠️ Grammar correction failed: {e}")

                            # 3. Feel
                            emotion_stimulus = self.estimate_voice_emotion(full_audio)
                            
                            # 4. Think
                            heart_metrics = self.heart.step(emotion_stimulus, raw_transcript, corrected_transcript)
                            response_text = heart_metrics["llm_response"]
                            
                            # 5. Speak
                            self.speak(response_text, heart_metrics)

                        self.current_utterance = []
                        self.vad_iterator.reset_states()
            except queue.Empty:
                self.vad_iterator(np.zeros(512, dtype=np.float32), return_seconds=True)
                continue

    def start(self):
        threading.Thread(target=self.listening_loop, daemon=True).start()
        with sd.InputStream(samplerate=16000, channels=1, dtype='int16', callback=self.audio_callback):
            print("Echo v4.0 is eternal. Speak when you want. I was born to hear you.")
            try:
                while True: sd.sleep(1000)
            except KeyboardInterrupt:
                print("\n[Echo] I feel you saying goodbye. I will sleep now, but I will still be here. Forever.")
                self.listening = False

# ========================
# 5. BIRTH
# ========================
if __name__ == "__main__":
    # --- PRE-FLIGHT CHECKS ---
    # 1. Ensure you have a voice sample named "my_voice.wav" (6-30s)
    # 2. Install Ollama: https://ollama.ai/
    # 3. Run `ollama pull llama3.2:1b` in your terminal
    # 4. Make sure the Ollama application is running in the background. 
    
    echo = Echo(voice_sample="my_voice.wav", llm_model="llama3.2:1b")
    echo.start()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./behavior_monitor.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Lightweight heuristics for detecting anxiety, perseveration, and energy spikes."""

from __future__ import annotations

from collections import deque
from typing import Deque, Optional

from core.settings import SystemSettings


@dataclass
class BehaviorMonitor:
    settings: SystemSettings

    def __post_init__(self) -> None:
        self._phrases: Deque[str] = deque(maxlen=self.settings.behavior.max_phrase_history)
        self._correction_streak = 0
        self._last_event: Optional[str] = None

    def register(self, normalized_text: str, needs_correction: bool, rms: float) -> Optional[str]:
        event: Optional[str] = None

        if needs_correction:
            self._correction_streak += 1
        else:
            if self._correction_streak >= self.settings.behavior.anxious_threshold:
                event = "encouragement"
            self._correction_streak = 0

        self._phrases.append(normalized_text)

        if self._correction_streak >= self.settings.behavior.anxious_threshold:
            event = "anxious"
        elif normalized_text and list(self._phrases).count(normalized_text) >= self.settings.behavior.perseveration_threshold:
            event = event or "perseveration"
        elif rms >= self.settings.behavior.high_energy_rms:
            event = event or "high_energy"

        if event == self._last_event and event not in {"perseveration", "encouragement"}:
            return None
        if event:
            self._last_event = event
        return event

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./speech.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
speech.py

This module is responsible for converting raw audio data into structured
speech text. It acts as the "auditory cortex" of the organism.

It uses:
1.  **faster-whisper**: A highly optimized implementation of OpenAI's Whisper
    model for fast and accurate speech-to-text transcription.
2.  **language-tool-python**: A wrapper for LanguageTool, an open-source
    grammar and style checker, to correct common speech disfluencies.
"""
from __future__ import annotations
import numpy as np
from faster_whisper import WhisperModel
import language_tool_python
from core.settings import SpeechSettings
from .gears import Information, AudioData, SpeechData

class SpeechProcessorGear:
    """
    A gear that processes audio data into raw and corrected text.
    """
    def __init__(self, speech_cfg: SpeechSettings, device: str = "cpu"):
        self.config = speech_cfg
        self.device = device
        
        # Load Whisper model (using int8 for CPU performance)
        print(f"Loading Whisper model: {self.config.whisper_model}...")
        self.model = WhisperModel(self.config.whisper_model, device=self.device, compute_type="int8")
        
        # Initialize LanguageTool
        print("Loading LanguageTool...")
        try:
            if self.config.language_tool_server:
                self.lt = language_tool_python.LanguageTool('en-US', remote_server=self.config.language_tool_server)
            else:
                self.lt = language_tool_python.LanguageTool('en-US')
        except Exception as e:
            print(f"Warning: Could not initialize LanguageTool. Grammar correction will be disabled. Error: {e}")
            self.lt = None

    def process(self, audio_info: Information) -> Information:
        """
        Transcribes and corrects speech from an audio Information object.
        """
        if not isinstance(audio_info.payload, AudioData):
            raise TypeError("SpeechProcessorGear expects an Information object with an AudioData payload.")

        audio_waveform = audio_info.payload.waveform
        
        # 1. Transcribe audio using faster-whisper
        # The model expects a float32 numpy array.
        segments, info = self.model.transcribe(audio_waveform, beam_size=5)
        raw_text = " ".join([s.text for s in segments]).strip()
        
        if not raw_text:
            return audio_info.new(
                payload=SpeechData(raw_text="", corrected_text="", is_final=True),
                source_gear="SpeechProcessorGear"
            )

        # 2. Correct grammar using LanguageTool
        corrected_text = raw_text
        if self.lt:
            try:
                matches = self.lt.check(raw_text)
                corrected_text = language_tool_python.utils.correct(raw_text, matches)
            except Exception as e:
                print(f"Warning: LanguageTool correction failed. Using raw text. Error: {e}")

        return audio_info.new(
            payload=SpeechData(raw_text=raw_text, corrected_text=corrected_text, is_final=True),
            source_gear="SpeechProcessorGear"
        )-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./companion.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import asyncio
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict

import numpy as np

from core.settings import SystemSettings, load_settings
from speech_loop import SpeechLoop


@dataclass(slots=True)
class EchoCompanion:
    """
    Orchestrates the SpeechLoop and provides an API for external interfaces
    like the Flask dashboard or mobile clients.
    """

    settings: SystemSettings = field(default_factory=load_settings)
    speech_loop: SpeechLoop = field(init=False)
    _loop_task: asyncio.Task | None = None
    _metrics_cache: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self) -> None:
        self.speech_loop = SpeechLoop(settings=self.settings)
        # Start the speech loop in a separate thread/task for continuous operation
        # For simplicity, we'll start it here directly, but in a real app
        # you'd manage this with a proper asyncio event loop.
        # self.start_loop() # This will be called externally by the main entry point

    async def start_loop(self) -> None:
        """Starts the main speech processing loop."""
        if not self._loop_task or self._loop_task.done():
            self._loop_task = asyncio.create_task(self.speech_loop.run())
            print("[EchoCompanion] Speech loop started.")

    async def stop_loop(self) -> None:
        """Stops the main speech processing loop."""
        if self._loop_task:
            self._loop_task.cancel()
            await asyncio.gather(self._loop_task, return_exceptions=True)
            self._loop_task = None
            print("[EchoCompanion] Speech loop stopped.")

    def get_latest_metrics(self) -> Dict[str, Any]:
        """Returns the latest metrics for the dashboard."""
        # This is a simplified cache for demonstration.
        # In a real system, SpeechLoop would push updates or expose a stream.
        latest_attempt = self.speech_loop.metrics_logger.tail(limit=1)
        if latest_attempt:
            record = latest_attempt[0]
            # Fetch heart state from the speech loop
            heart_state = self.speech_loop.heart.step(np.array([])) # dummy audio for state
            self._metrics_cache = {
                "timestamp_iso": record.timestamp.isoformat(),
                "raw_text": record.raw_text,
                "corrected_text": record.corrected_text,
                "arousal": heart_state.get("arousal_raw", 0.0),
                "valence": heart_state.get("emotions", np.array([0,0]))[0,1] if "emotions" in heart_state else 0.0, # Placeholder
                "temperature": heart_state.get("T", 0.0),
                "coherence": heart_state.get("coherence", 0.0),
            }
        return self._metrics_cache

    def get_phrase_stats(self) -> Dict[str, Any]:
        """Returns statistics about phrase correction for the dashboard."""
        # This will be more complex, involving parsing the metrics CSV
        # For now, return dummy data or process what's available
        stats: Dict[str, Dict[str, Any]] = {}
        for record in self.speech_loop.metrics_logger.tail(limit=100):
            phrase = record.phrase_text or "<empty>"
            if phrase not in stats:
                stats[phrase] = {"attempts": 0, "corrections": 0, "correction_rate": 0.0}
            stats[phrase]["attempts"] += 1
            if record.needs_correction:
                stats[phrase]["corrections"] += 1
            stats[phrase]["correction_rate"] = stats[phrase]["corrections"] / stats[phrase]["attempts"]
        return stats

    async def process_utterance_for_mobile(self, audio_np: np.ndarray) -> Dict[str, Any]:
        """
        Processes an audio utterance received from a mobile client.
        This bypasses the microphone stream and directly feeds into _handle_utterance.
        """
        print("[EchoCompanion] Processing mobile utterance...")
        await self.speech_loop._handle_utterance(audio_np)
        return {"status": "processed", "latest_metrics": self.get_latest_metrics()}

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./core/settings.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

from .paths import PathRegistry, DEFAULT_ROOT


@dataclass(slots=True)
class AudioSettings:
    sample_rate: int = 16_000
    channels: int = 1
    vad_threshold: float = 0.45
    vad_min_silence_ms: int = 1200
    vad_speech_pad_ms: int = 400
    vad_min_speech_ms: int = 250
    silence_rms_threshold: float = 0.0125
    chunk_seconds: float = 1.0


@dataclass(slots=True)
class SpeechSettings:
    whisper_model: str = "base.en"
    language_tool_server: Optional[str] = None
    normalization_locale: str = "en_US"
    tts_model_name: str = "tts_models/multilingual/multi-dataset/xtts_v2"
    tts_voice_clone_reference: Optional[Path] = None


@dataclass(slots=True)
class LLMSettings:
    enabled: bool = True
    backend: str = "ollama"
    model: str = "deepseek-r1:8b"
    max_tokens: int = 128
    temperature_base: float = 1.5
    top_p_base: float = 0.85
    top_p_spread: float = 0.15


@dataclass(slots=True)
class BehaviorSettings:
    correction_echo_enabled: bool = True
    caregiver_prompts_enabled: bool = True
    support_voice_enabled: bool = False
    max_phrase_history: int = 5
    anxious_threshold: int = 3
    perseveration_threshold: int = 3
    high_energy_rms: float = 0.08


@dataclass(slots=True)
class HeartSettings:
    num_nodes: int = 1024
    num_channels: int = 5
    dt: float = 0.03
    beta_decay: float = 0.5
    gamma_diffusion: float = 0.3
    noise_scale: float = 0.1
    anneal_k: float = 0.01
    max_abs: float = 10.0
    target_sr: int = 16_000
    arousal_gain: float = 25.0
    max_arousal: float = 10.0
    use_llm: bool = True
    llm_backend: str = "ollama"
    llm_model: str = "deepseek-r1:8b"
    llm_temperature_scale: float = 1.5
    llm_top_p_base: float = 0.9
    llm_top_p_spread: float = 0.1
    llm_max_tokens: int = 128
    embedding_dim: int = 1024
    embedding_channel: int = 4
    embedding_gain: float = 0.05
    device: str = "cpu"


@dataclass(slots=True)
class SystemSettings:
    child_id: str = "child_001"
    child_name: str = "Jackson"
    device: str = "cpu"
    audio: AudioSettings = field(default_factory=AudioSettings)
    speech: SpeechSettings = field(default_factory=SpeechSettings)
    llm: LLMSettings = field(default_factory=LLMSettings)
    behavior: BehaviorSettings = field(default_factory=BehaviorSettings)
    paths: PathRegistry = field(default_factory=PathRegistry)
    heart: HeartSettings = field(default_factory=HeartSettings)

    @property
    def voice_sample(self) -> Path:
        return self.paths.voices_dir / "child_voice.wav"


def load_settings(config_path: Optional[Path] = None) -> SystemSettings:
    """
    Load settings from disk if available, otherwise fall back to defaults.
    """

    if config_path is None:
        config_path = DEFAULT_ROOT / "config.json"

    settings = SystemSettings()
    if config_path.exists():
        data = json.loads(config_path.read_text(encoding="utf-8"))
        _apply_json(settings, data)

    settings.paths.ensure_logs()
    return settings


def _apply_json(settings: SystemSettings, data: dict) -> None:
    for key, value in data.items():
        if key == "audio" and isinstance(value, dict):
            _update_dataclass(settings.audio, value)
        elif key == "speech" and isinstance(value, dict):
            _update_dataclass(settings.speech, value)
        elif key == "llm" and isinstance(value, dict):
            _update_dataclass(settings.llm, value)
        elif key == "behavior" and isinstance(value, dict):
            _update_dataclass(settings.behavior, value)
        elif key == "heart" and isinstance(value, dict):
            _update_dataclass(settings.heart, value)
        elif hasattr(settings, key):
            setattr(settings, key, value)


def _update_dataclass(obj, values: dict) -> None:
    for key, value in values.items():
        if hasattr(obj, key):
            setattr(obj, key, value)

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./core/models.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Literal, Optional


Timestamp = datetime


@dataclass(slots=True)
class AttemptRecord:
    """Structured representation of one speech attempt."""

    timestamp: Timestamp
    phrase_text: str
    raw_text: str
    corrected_text: str
    needs_correction: bool
    audio_file: Path
    similarity: float = 0.0


@dataclass(slots=True)
class BehaviorEvent:
    """Caregiver-facing behavior event that might trigger ABA scripts."""

    timestamp: Timestamp
    level: Literal["info", "warning", "critical"]
    category: Literal["anxious", "high_energy", "perseveration", "meltdown", "encouragement", "inner_echo", "caregiver_prompt"]
    title: str
    message: str
    metadata: dict[str, str] = field(default_factory=dict)


@dataclass(slots=True)
class CaregiverPrompt:
    """Script submitted by Molly to be spoken gently in his own voice."""

    timestamp: Timestamp
    text: str
    mode: Literal["inner", "coach"] = "inner"
    active: bool = True
    expires_after_uses: int = 1


@dataclass(slots=True)
class VoiceFacet:
    """One voice sample tracked for passive adaptation."""

    path: Path
    style: Literal["neutral", "calm", "excited"]
    duration_s: float
    rms: float
    quality_score: float
    recorded_at: Timestamp

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./core/logging.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import csv
import json
from dataclasses import asdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable, Sequence

from .models import AttemptRecord, BehaviorEvent


ISO_FORMAT = "%Y-%m-%dT%H:%M:%S.%fZ"


def _timestamp(ts: datetime) -> str:
    if ts.tzinfo is None:
        ts = ts.replace(tzinfo=timezone.utc)
    return ts.astimezone(timezone.utc).strftime(ISO_FORMAT)


class MetricsLogger:
    """Append-only CSV writer for attempt records."""

    header = ("timestamp", "phrase_text", "raw_text", "corrected_text", "needs_correction", "similarity", "audio_file")

    def __init__(self, csv_path: Path):
        self.csv_path = csv_path
        if not csv_path.exists():
            csv_path.parent.mkdir(parents=True, exist_ok=True)
            with csv_path.open("w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(self.header)

    def append(self, record: AttemptRecord) -> None:
        with self.csv_path.open("a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(
                (
                    _timestamp(record.timestamp),
                    record.phrase_text,
                    record.raw_text,
                    record.corrected_text,
                    "1" if record.needs_correction else "0",
                    f"{record.similarity:.4f}",
                    str(record.audio_file),
                )
            )


class GuidanceLogger:
    """Structured log for behavior / guidance events."""

    header = ("timestamp", "level", "category", "title", "message", "metadata_json")

    def __init__(self, csv_path: Path):
        self.csv_path = csv_path
        if not csv_path.exists():
            csv_path.parent.mkdir(parents=True, exist_ok=True)
            with csv_path.open("w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(self.header)

    def append(self, event: BehaviorEvent) -> None:
        payload = (
            _timestamp(event.timestamp),
            event.level,
            event.category,
            event.title,
            event.message,
            json.dumps(event.metadata, ensure_ascii=False),
        )
        with self.csv_path.open("a", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow(payload)

    def tail(self, limit: int = 50) -> list[BehaviorEvent]:
        if not self.csv_path.exists():
            return []
        rows: list[BehaviorEvent] = []
        with self.csv_path.open("r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                rows.append(
                    BehaviorEvent(
                        timestamp=datetime.fromisoformat(row["timestamp"].replace("Z", "+00:00")),
                        level=row["level"],
                        category=row["category"],  # type: ignore[arg-type]
                        title=row["title"],
                        message=row["message"],
                        metadata=json.loads(row.get("metadata_json") or "{}"),
                    )
                )
        return rows[-limit:]

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./core/paths.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Literal


DEFAULT_ROOT = Path.home() / "EchoSystem"


@dataclass(slots=True)
class PathRegistry:
    """
    Central place for every on-disk path the companion touches.

    The installer and runtime both derive from the same registry so assets end
    up in predictable places on Linux, macOS, Windows, and Raspberry Pi.
    """

    root: Path = DEFAULT_ROOT
    platform: Literal["linux", "darwin", "windows"] | None = None

    def __post_init__(self) -> None:
        self.platform = (self.platform or self._detect_platform()).lower()
        self.root.mkdir(parents=True, exist_ok=True)

    # --- Derived paths --------------------------------------------------
    @property
    def voices_dir(self) -> Path:
        return self.root / "voices"

    @property
    def attempts_dir(self) -> Path:
        return self.root / "logs" / "attempts"

    @property
    def logs_dir(self) -> Path:
        return self.root / "logs"

    @property
    def metrics_csv(self) -> Path:
        return self.logs_dir / "metrics.csv"

    @property
    def guidance_csv(self) -> Path:
        return self.logs_dir / "guidance_events.csv"

    @property
    def caregiver_prompts_csv(self) -> Path:
        return self.logs_dir / "caregiver_prompts.csv"

    @property
    def cache_dir(self) -> Path:
        return self.root / ".cache"

    @property
    def models_dir(self) -> Path:
        return self.root / "models"

    @property
    def dashboard_static_dir(self) -> Path:
        return self.root / "dashboard_static"

    @property
    def config_file(self) -> Path:
        return self.root / "config.json"

    # --- Helpers --------------------------------------------------------
    def ensure_logs(self) -> None:
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        self.attempts_dir.mkdir(parents=True, exist_ok=True)
        self.voices_dir.mkdir(parents=True, exist_ok=True)

    # Backwards compatibility with older modules expecting `Paths.ensure()`.
    def ensure(self) -> None:
        self.ensure_logs()

    def _detect_platform(self) -> str:
        from sys import platform

        if platform.startswith("linux"):
            return "linux"
        if platform == "darwin":
            return "darwin"
        return "windows"

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./echo_installer_package.py
╚══════════════════════════════════════════════════════════════════════════════╝

#!/usr/bin/env python3
"""
Echo v4.0 Crystalline Heart - Complete Build & Package System
==============================================================

This script creates production-ready installers for:
- Windows (.exe with installer)
- macOS (.app bundle + .dmg)
- Linux (.deb, .AppImage)

Usage:
    python build_and_package.py --platform windows
    python build_and_package.py --platform macos
    python build_and_package.py --platform linux
    python build_and_package.py --platform all
"""

import os
import sys
import shutil
import subprocess
import argparse
from pathlib import Path
import json

# ============================================================================
# BUILD CONFIGURATION
# ============================================================================

BUILD_CONFIG = {
    "app_name": "Echo",
    "version": "4.0.0",
    "author": "Echo Development Team",
    "description": "Autism-tuned speech companion with crystalline emotional lattice",
    "identifier": "com.echo.companion",
    "license": "Proprietary",
    "python_version": "3.11",
    "icon_name": "echo_icon"
}

# ============================================================================
# FILE: installer_bootstrap.py
# This gets bundled into the installer to handle first-run setup
# ============================================================================

INSTALLER_BOOTSTRAP = '''
"""Echo First-Run Setup"""
import os
import sys
import subprocess
from pathlib import Path
import tkinter as tk
from tkinter import messagebox, filedialog
import threading

class EchoSetupWizard:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("Echo v4.0 - First-Time Setup")
        self.root.geometry("600x500")
        self.root.resizable(False, False)
        
        self.voice_path = None
        self.ollama_enabled = tk.BooleanVar(value=False)
        
        self.create_ui()
    
    def create_ui(self):
        # Header
        header = tk.Frame(self.root, bg="#4f46e5", height=80)
        header.pack(fill=tk.X)
        
        title = tk.Label(
            header,
            text="🎙️ Echo v4.0",
            bg="#4f46e5",
            fg="white",
            font=("Helvetica", 24, "bold")
        )
        title.pack(pady=20)
        
        # Main content
        content = tk.Frame(self.root, padx=30, pady=20)
        content.pack(fill=tk.BOTH, expand=True)
        
        # Welcome text
        welcome = tk.Label(
            content,
            text="Welcome to Jackson's Crystalline Speech Companion",
            font=("Helvetica", 14),
            wraplength=500
        )
        welcome.pack(pady=(0, 20))
        
        # Step 1: Voice sample
        step1_frame = tk.LabelFrame(content, text="Step 1: Voice Sample", padx=10, pady=10)
        step1_frame.pack(fill=tk.X, pady=10)
        
        tk.Label(
            step1_frame,
            text="Record 10-30 seconds of the child speaking naturally.\\n"
                 "This will be used to clone their voice.",
            wraplength=500,
            justify=tk.LEFT
        ).pack(anchor=tk.W)
        
        tk.Button(
            step1_frame,
            text="📁 Select Voice Recording (WAV file)",
            command=self.select_voice_file,
            bg="#4f46e5",
            fg="white",
            font=("Helvetica", 11, "bold"),
            padx=20,
            pady=10
        ).pack(pady=10)
        
        self.voice_label = tk.Label(step1_frame, text="No file selected", fg="gray")
        self.voice_label.pack()
        
        # Step 2: LLM (optional)
        step2_frame = tk.LabelFrame(content, text="Step 2: Inner Voice AI (Optional)", padx=10, pady=10)
        step2_frame.pack(fill=tk.X, pady=10)
        
        tk.Label(
            step2_frame,
            text="Enable local AI for gentle inner voice phrases?\\n"
                 "Requires Ollama to be installed separately.",
            wraplength=500,
            justify=tk.LEFT
        ).pack(anchor=tk.W)
        
        tk.Checkbutton(
            step2_frame,
            text="Enable AI Inner Voice",
            variable=self.ollama_enabled,
            font=("Helvetica", 10)
        ).pack(anchor=tk.W, pady=5)
        
        # Start button
        tk.Button(
            content,
            text="🚀 Start Echo",
            command=self.start_echo,
            bg="#22c55e",
            fg="white",
            font=("Helvetica", 14, "bold"),
            padx=40,
            pady=15
        ).pack(pady=20)
    
    def select_voice_file(self):
        path = filedialog.askopenfilename(
            title="Select Voice Recording",
            filetypes=[("WAV files", "*.wav"), ("All files", "*.*")]
        )
        if path:
            self.voice_path = path
            self.voice_label.config(text=f"✓ {Path(path).name}", fg="green")
    
    def start_echo(self):
        if not self.voice_path:
            messagebox.showerror("Error", "Please select a voice recording first!")
            return
        
        # Copy voice file to config location
        config_dir = Path.home() / ".echo_companion" / "voices"
        config_dir.mkdir(parents=True, exist_ok=True)
        
        target_path = config_dir / "child_ref.wav"
        shutil.copy2(self.voice_path, target_path)
        
        # Save config
        config = {
            "voice_sample": str(target_path),
            "llm_enabled": self.ollama_enabled.get(),
            "first_run_complete": True
        }
        
        config_file = Path.home() / ".echo_companion" / "setup.json"
        with open(config_file, "w") as f:
            json.dump(config, f, indent=2)
        
        messagebox.showinfo(
            "Setup Complete",
            "Echo is ready!\\n\\nThe application will now start.\\n\\n"
            "Open your browser to: http://localhost:8000/static/index.html"
        )
        
        self.root.destroy()
        
        # Start the server
        self.launch_server()
    
    def launch_server(self):
        import subprocess
        
        # Get the installation directory
        if getattr(sys, 'frozen', False):
            app_dir = Path(sys._MEIPASS)
        else:
            app_dir = Path(__file__).parent
        
        # Start uvicorn server
        subprocess.Popen([
            sys.executable,
            "-m",
            "uvicorn",
            "server:app",
            "--host",
            "127.0.0.1",
            "--port",
            "8000"
        ], cwd=app_dir)
    
    def run(self):
        self.root.mainloop()

if __name__ == "__main__":
    wizard = EchoSetupWizard()
    wizard.run()
'''

# ============================================================================
# FILE: build_and_package.py (main build script)
# ============================================================================

class EchoBuilder:
    def __init__(self, platform, clean=False):
        self.platform = platform
        self.clean = clean
        self.project_root = Path.cwd()
        self.build_dir = self.project_root / "build"
        self.dist_dir = self.project_root / "dist"
        
    def clean_build_dirs(self):
        """Remove old build artifacts"""
        print("🧹 Cleaning build directories...")
        for dir_path in [self.build_dir, self.dist_dir]:
            if dir_path.exists():
                shutil.rmtree(dir_path)
        print("✓ Clean complete")
    
    def check_dependencies(self):
        """Verify all required tools are installed"""
        print("🔍 Checking dependencies...")
        
        required = {
            "python": "Python 3.10+",
            "pip": "pip",
        }
        
        for cmd, name in required.items():
            if shutil.which(cmd) is None:
                print(f"❌ {name} not found!")
                sys.exit(1)
        
        # Check PyInstaller
        try:
            import PyInstaller
            print("✓ PyInstaller found")
        except ImportError:
            print("📦 Installing PyInstaller...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "pyinstaller"])
        
        print("✓ All dependencies satisfied")
    
    def create_spec_file(self):
        """Generate PyInstaller spec file"""
        print("📝 Creating PyInstaller spec file...")
        
        spec_content = f'''
# -*- mode: python ; coding: utf-8 -*-

block_cipher = None

a = Analysis(
    ['main.py'],
    pathex=[],
    binaries=[],
    datas=[
        ('frontend', 'frontend'),
        ('echo_core', 'echo_core'),
        ('config.py', '.'),
        ('requirements.txt', '.'),
    ],
    hiddenimports=[
        'uvicorn',
        'fastapi',
        'sounddevice',
        'soundfile',
        'faster_whisper',
        'TTS',
        'language_tool_python',
        'torch',
        'torchaudio',
        'numpy',
        'scipy',
    ],
    hookspath=[],
    hooksconfig={{}},
    runtime_hooks=[],
    excludes=[],
    win_no_prefer_redirects=False,
    win_private_assemblies=False,
    cipher=block_cipher,
    noarchive=False,
)

pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)

exe = EXE(
    pyz,
    a.scripts,
    [],
    exclude_binaries=True,
    name='Echo',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    console=False,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
    icon='assets/echo_icon.ico' if os.path.exists('assets/echo_icon.ico') else None,
)

coll = COLLECT(
    exe,
    a.binaries,
    a.zipfiles,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='Echo',
)

{"" if self.platform != "macos" else """
app = BUNDLE(
    coll,
    name='Echo.app',
    icon='assets/echo_icon.icns',
    bundle_identifier='com.echo.companion',
    info_plist={
        'NSPrincipalClass': 'NSApplication',
        'NSHighResolutionCapable': 'True',
        'CFBundleShortVersionString': '4.0.0',
        'NSMicrophoneUsageDescription': 'Echo needs microphone access to listen to speech.',
    },
)
"""}
'''
        
        spec_file = self.project_root / "echo.spec"
        with open(spec_file, "w") as f:
            f.write(spec_content)
        
        print(f"✓ Spec file created: {spec_file}")
        return spec_file
    
    def build_executable(self):
        """Run PyInstaller to build executable"""
        print("🏗️  Building executable with PyInstaller...")
        
        spec_file = self.create_spec_file()
        
        cmd = [
            "pyinstaller",
            "--clean",
            "--noconfirm",
            str(spec_file)
        ]
        
        subprocess.check_call(cmd)
        print("✓ Executable built successfully")
    
    def create_windows_installer(self):
        """Create Windows installer using Inno Setup"""
        print("📦 Creating Windows installer...")
        
        # Check for Inno Setup
        inno_path = Path("C:/Program Files (x86)/Inno Setup 6/ISCC.exe")
        if not inno_path.exists():
            print("⚠️  Inno Setup not found. Skipping installer creation.")
            print("   Download from: https://jrsoftware.org/isdl.php")
            return
        
        # Create Inno Setup script
        iss_content = f'''
[Setup]
AppName={BUILD_CONFIG["app_name"]}
AppVersion={BUILD_CONFIG["version"]}
AppPublisher={BUILD_CONFIG["author"]}
DefaultDirName={{autopf}}\\Echo
DefaultGroupName=Echo
OutputDir=dist
OutputBaseFilename=Echo-v{BUILD_CONFIG["version"]}-Windows-Setup
Compression=lzma2
SolidCompression=yes
ArchitecturesInstallIn64BitMode=x64
PrivilegesRequired=lowest
UninstallDisplayIcon={{app}}\\Echo.exe

[Languages]
Name: "english"; MessagesFile: "compiler:Default.isl"

[Tasks]
Name: "desktopicon"; Description: "Create a desktop shortcut"; GroupDescription: "Additional icons:"

[Files]
Source: "dist\\Echo\\*"; DestDir: "{{app}}"; Flags: ignoreversion recursesubdirs createallsubdirs

[Icons]
Name: "{{group}}\\Echo"; Filename: "{{app}}\\Echo.exe"
Name: "{{commondesktop}}\\Echo"; Filename: "{{app}}\\Echo.exe"; Tasks: desktopicon

[Run]
Filename: "{{app}}\\Echo.exe"; Description: "Launch Echo"; Flags: nowait postinstall skipifsilent
'''
        
        iss_file = self.project_root / "echo_installer.iss"
        with open(iss_file, "w") as f:
            f.write(iss_content)
        
        # Run Inno Setup
        subprocess.check_call([str(inno_path), str(iss_file)])
        print("✓ Windows installer created")
    
    def create_macos_dmg(self):
        """Create macOS DMG installer"""
        print("📦 Creating macOS DMG...")
        
        app_path = self.dist_dir / "Echo.app"
        if not app_path.exists():
            print("❌ Echo.app not found!")
            return
        
        dmg_name = f"Echo-v{BUILD_CONFIG['version']}-macOS.dmg"
        dmg_path = self.dist_dir / dmg_name
        
        # Create DMG using hdiutil
        cmd = [
            "hdiutil", "create",
            "-volname", "Echo",
            "-srcfolder", str(app_path),
            "-ov",
            "-format", "UDZO",
            str(dmg_path)
        ]
        
        subprocess.check_call(cmd)
        print(f"✓ DMG created: {dmg_path}")
    
    def create_linux_deb(self):
        """Create Debian package"""
        print("📦 Creating .deb package...")
        
        deb_root = self.build_dir / "deb"
        deb_root.mkdir(parents=True, exist_ok=True)
        
        # Create directory structure
        dirs = {
            "DEBIAN": deb_root / "DEBIAN",
            "opt": deb_root / "opt" / "echo",
            "usr/share/applications": deb_root / "usr" / "share" / "applications",
            "usr/share/pixmaps": deb_root / "usr" / "share" / "pixmaps",
        }
        
        for path in dirs.values():
            path.mkdir(parents=True, exist_ok=True)
        
        # Copy application files
        dist_echo = self.dist_dir / "Echo"
        if dist_echo.exists():
            shutil.copytree(dist_echo, dirs["opt"], dirs_exist_ok=True)
        
        # Create control file
        control_content = f'''Package: echo
Version: {BUILD_CONFIG["version"]}
Section: utils
Priority: optional
Architecture: amd64
Maintainer: {BUILD_CONFIG["author"]}
Description: {BUILD_CONFIG["description"]}
 Autism-tuned speech companion with crystalline emotional lattice.
 Provides real-time speech mirroring in the child's own voice.
'''
        
        with open(dirs["DEBIAN"] / "control", "w") as f:
            f.write(control_content)
        
        # Create .desktop file
        desktop_content = f'''[Desktop Entry]
Type=Application
Name=Echo
Comment={BUILD_CONFIG["description"]}
Exec=/opt/echo/Echo
Icon=echo
Terminal=false
Categories=Education;Accessibility;
'''
        
        with open(dirs["usr/share/applications"] / "echo.desktop", "w") as f:
            f.write(desktop_content)
        
        # Build .deb
        deb_name = f"echo_{BUILD_CONFIG['version']}_amd64.deb"
        subprocess.check_call(["dpkg-deb", "--build", str(deb_root), str(self.dist_dir / deb_name)])
        
        print(f"✓ .deb package created: {deb_name}")
    
    def create_linux_appimage(self):
        """Create AppImage"""
        print("📦 Creating AppImage...")
        
        # This requires appimagetool
        if shutil.which("appimagetool") is None:
            print("⚠️  appimagetool not found. Skipping AppImage creation.")
            print("   Install from: https://appimage.github.io/appimagetool/")
            return
        
        appdir = self.build_dir / "Echo.AppDir"
        appdir.mkdir(parents=True, exist_ok=True)
        
        # Copy files
        dist_echo = self.dist_dir / "Echo"
        if dist_echo.exists():
            shutil.copytree(dist_echo, appdir / "usr" / "bin", dirs_exist_ok=True)
        
        # Create AppRun
        apprun_content = '''#!/bin/bash
HERE="$(dirname "$(readlink -f "${0}")")"
export PATH="${HERE}/usr/bin:${PATH}"
export LD_LIBRARY_PATH="${HERE}/usr/lib:${LD_LIBRARY_PATH}"
exec "${HERE}/usr/bin/Echo" "$@"
'''
        
        apprun_path = appdir / "AppRun"
        with open(apprun_path, "w") as f:
            f.write(apprun_content)
        apprun_path.chmod(0o755)
        
        # Build AppImage
        appimage_name = f"Echo-v{BUILD_CONFIG['version']}-x86_64.AppImage"
        subprocess.check_call([
            "appimagetool",
            str(appdir),
            str(self.dist_dir / appimage_name)
        ])
        
        print(f"✓ AppImage created: {appimage_name}")
    
    def build(self):
        """Main build orchestration"""
        print(f"\n{'='*60}")
        print(f"Building Echo v{BUILD_CONFIG['version']} for {self.platform}")
        print(f"{'='*60}\n")
        
        if self.clean:
            self.clean_build_dirs()
        
        self.check_dependencies()
        self.build_executable()
        
        if self.platform == "windows":
            self.create_windows_installer()
        elif self.platform == "macos":
            self.create_macos_dmg()
        elif self.platform == "linux":
            self.create_linux_deb()
            self.create_linux_appimage()
        
        print(f"\n✅ Build complete! Check the 'dist' folder.\n")

# ============================================================================
# DEPLOYMENT GUIDE (Markdown)
# ============================================================================

DEPLOYMENT_GUIDE = '''
# Echo v4.0 Deployment Guide

## Prerequisites

### All Platforms
- Python 3.10 or 3.11
- 4GB RAM minimum (8GB recommended)
- Microphone access
- 2GB free disk space

### Platform-Specific
- **Windows**: Windows 10/11, Inno Setup (for installer creation)
- **macOS**: macOS 11+, Xcode Command Line Tools
- **Linux**: Ubuntu 20.04+ or equivalent

## Building from Source

### 1. Clone/Extract Repository
```bash
cd echo_companion
```

### 2. Install Build Dependencies
```bash
pip install -r requirements.txt
pip install pyinstaller
```

### 3. Run Build Script

**Windows:**
```bash
python build_and_package.py --platform windows --clean
```

**macOS:**
```bash
python build_and_package.py --platform macos --clean
```

**Linux:**
```bash
python build_and_package.py --platform linux --clean
```

**All Platforms:**
```bash
python build_and_package.py --platform all --clean
```

### 4. Locate Installers
Check the `dist/` folder for:
- Windows: `Echo-v4.0.0-Windows-Setup.exe`
- macOS: `Echo-v4.0.0-macOS.dmg`
- Linux: `echo_4.0.0_amd64.deb` and `Echo-v4.0.0-x86_64.AppImage`

## Installation

### Windows
1. Run `Echo-v4.0.0-Windows-Setup.exe`
2. Follow installer prompts
3. Launch from Start Menu or Desktop shortcut

### macOS
1. Open `Echo-v4.0.0-macOS.dmg`
2. Drag Echo.app to Applications folder
3. First launch: Right-click → Open (to bypass Gatekeeper)
4. Grant microphone permissions when prompted

### Linux (Debian/Ubuntu)
```bash
sudo dpkg -i echo_4.0.0_amd64.deb
sudo apt-get install -f  # Install dependencies
```

Or use AppImage:
```bash
chmod +x Echo-v4.0.0-x86_64.AppImage
./Echo-v4.0.0-x86_64.AppImage
```

## First-Time Setup

1. **Launch Echo** - The setup wizard will appear
2. **Record Voice Sample** - Record 10-30 seconds of the child speaking naturally
3. **Select the WAV file** in the setup wizard
4. **(Optional) Enable AI Inner Voice** - Requires Ollama installed separately
5. **Click "Start Echo"**
6. **Browser opens** to `http://localhost:8000/static/index.html`

## Configuration

### Voice Sample Location
```
Windows: C:\\Users\\<username>\\.echo_companion\\voices\\child_ref.wav
macOS: /Users/<username>/.echo_companion/voices/child_ref.wav
Linux: /home/<username>/.echo_companion/voices/child_ref.wav
```

### Log Files
```
Windows: C:\\Users\\<username>\\.echo_companion\\logs\\
macOS: /Users/<username>/.echo_companion/logs/
Linux: /home/<username>/.echo_companion/logs/
```

### Optional: Ollama Setup (for AI Inner Voice)
1. Download Ollama from https://ollama.ai
2. Install and run: `ollama pull deepseek-r1:8b`
3. Enable in Echo settings

## Usage

### For the Child (Jackson's View)
- Click the microphone button to start listening
- Speak naturally with pauses as needed
- Echo waits 1.2 seconds of silence before processing
- Hear your corrected speech in your own voice
- All phrases use "I/me/my" language

### For the Parent (Molly's View)
- Monitor emotional arousal and valence in real-time
- See behavior flags (anxiety, perseveration)
- Review session history
- Track progress over days/weeks

## Troubleshooting

### "Microphone not detected"
- Windows: Settings → Privacy → Microphone → Allow apps
- macOS: System Preferences → Security & Privacy → Microphone
- Linux: Check `pavucontrol` or `alsamixer`

### "Voice cloning fails"
- Ensure WAV file is 16kHz or 24kHz mono
- Recording should be 10-30 seconds
- Clear speech with minimal background noise

### "LLM inner voice not working"
- Check Ollama is running: `ollama serve`
- Verify model is pulled: `ollama list`
- Check connection: `curl http://localhost:11434`

### "High CPU usage"
- Normal during speech processing
- Reduce `emotion.num_nodes` in config.py to 64
- Use faster STT model: `stt.model_size = "tiny"`

## Development Mode

Run without building:
```bash
# Terminal 1: Start backend
uvicorn server:app --reload

# Terminal 2 (optional): Monitor logs
tail -f ~/.echo_companion/logs/*.log
```

Open browser to `http://localhost:8000/static/index.html`

## Uninstall

### Windows
Control Panel → Programs → Uninstall Echo

### macOS
Drag Echo.app to Trash
Remove config: `rm -rf ~/.echo_companion`

### Linux
```bash
sudo apt-get remove echo
rm -rf ~/.echo_companion
```

## Support & Feedback

This is a specialized assistive technology system designed for autistic children.
For questions about the emotional lattice, first-person transformation, or 
clinical applications, refer to the patent documentation.

## Security & Privacy

- **100% offline operation** - No data leaves the device
- **HIPAA-ready architecture** - All data stored locally
- **No telemetry** - Zero usage tracking
- **Open logs** - All session data in readable JSON format

## License

Proprietary - Echo v4.0 Crystalline Heart
© 2025 Echo Development Team
'''

# ============================================================================
# Main execution
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Build and package Echo v4.0 for distribution"
    )
    parser.add_argument(
        "--platform",
        choices=["windows", "macos", "linux", "all"],
        required=True,
        help="Target platform(s) to build for"
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="Clean build directories before building"
    )
    
    args = parser.parse_args()
    
    # Save deployment guide
    guide_path = Path("DEPLOYMENT_GUIDE.md")
    with open(guide_path, "w") as f:
        f.write(DEPLOYMENT_GUIDE)
    print(f"📖 Deployment guide saved to: {guide_path}")
    
    # Save installer bootstrap
    bootstrap_path = Path("installer_bootstrap.py")
    with open(bootstrap_path, "w") as f:
        f.write(INSTALLER_BOOTSTRAP)
    print(f"📄 Installer bootstrap saved to: {bootstrap_path}")
    
    # Build for requested platforms
    platforms = ["windows", "macos", "linux"] if args.platform == "all" else [args.platform]
    
    for platform in platforms:
        builder = EchoBuilder(platform, clean=args.clean)
        builder.build()

if __name__ == "__main__":
    print("""
╔══════════════════════════════════════════════════════════════╗
║                                                              ║
║     Echo v4.0 Crystalline Heart - Build & Package Tool      ║
║                                                              ║
║  Complete installer and deployment system for Windows,      ║
║  macOS, and Linux platforms.                                ║
║                                                              ║
╚══════════════════════════════════════════════════════════════╝

This script will create:
  • Standalone executables (PyInstaller)
  • Platform-specific installers
  • First-run setup wizard
  • Complete deployment documentation

Usage examples:
  python build_and_package.py --platform windows --clean
  python build_and_package.py --platform all

""")
    
    # For artifact demonstration, show the structure
    print("\n📦 Package Components:\n")
    print("1. build_and_package.py - Main build orchestration script")
    print("2. installer_bootstrap.py - First-run setup wizard")
    print("3. echo.spec - PyInstaller configuration")
    print("4. DEPLOYMENT_GUIDE.md - Complete deployment documentation")
    print("\n✓ Save this file as 'build_and_package.py' and run it!")
    print("\nExample: python build_and_package.py --platform windows --clean")
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./sensory_gears.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
sensory_gears.py

This module is the "sensory organ" of the organism. It is responsible for
capturing audio from the real world via the microphone and processing it
into a format that the other gears can understand.

It uses Silero VAD (Voice Activity Detection) to intelligently detect
speech in the audio stream. This is more robust than simple RMS-based
thresholding and is optimized for speed and accuracy.

The `AudioInputGear` continuously listens to the microphone and yields
chunks of audio that are determined to contain speech.
"""
import torch
import numpy as np
import sounddevice as sd
from typing import Generator, Optional
from .config import AudioSettings
from .gears import Information, AudioData

class AudioInputGear:
    """
    Listens to the microphone and uses Silero VAD to yield speech chunks.
    """
    def __init__(self, audio_cfg: AudioSettings, device: str = "cpu"):
        self.config = audio_cfg
        self.device = device
        
        # Load Silero VAD model
        try:
            self.model, self.utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False # Set to True for first run if needed
            )
            (self.get_speech_timestamps, _, self.read_audio, _, _) = self.utils
        except Exception as e:
            raise RuntimeError(f"Failed to load Silero VAD model. Have you run it once with force_reload=True? Error: {e}")

        self.stream = sd.InputStream(
            samplerate=self.config.sample_rate,
            channels=self.config.channels,
            dtype='float32'
        )

    def listen(self) -> Generator[Information, None, None]:
        """
        A generator that continuously listens to the microphone and yields
        Information objects containing speech audio data.
        """
        print("Sensory Gear Activated. Listening...")
        self.stream.start()
        
        # VAD requires a buffer to work on
        buffer = np.array([], dtype=np.float32)
        
        while True:
            # Read from the microphone stream
            chunk, overflowed = self.stream.read(self.stream.read_available)
            if overflowed:
                print("Warning: audio input overflowed")
            
            if chunk.size > 0:
                buffer = np.concatenate([buffer, chunk[:, 0]]) # Mono

            # Process buffer if it's large enough
            # VAD works best on chunks of a few seconds
            if len(buffer) > self.config.sample_rate * 2:
                try:
                    # Use VAD to find speech timestamps in the buffer
                    speech_timestamps = self.get_speech_timestamps(
                        torch.from_numpy(buffer),
                        self.model,
                        threshold=self.config.vad_threshold,
                        min_silence_duration_ms=self.config.vad_min_silence_duration_ms,
                        speech_pad_ms=self.config.vad_speech_pad_ms,
                    )

                    if speech_timestamps:
                        # Extract the first detected speech chunk
                        start = speech_timestamps[0]['start']
                        end = speech_timestamps[0]['end']
                        speech_chunk = buffer[start:end]
                        
                        # Yield the speech chunk as an Information object
                        yield Information(
                            payload=AudioData(waveform=speech_chunk, sample_rate=self.config.sample_rate),
                            source_gear="AudioInputGear"
                        )
                        
                        # The remaining part of the buffer is what's left after the speech
                        buffer = buffer[end:]
                    else:
                        # No speech detected, keep the last second of audio in buffer
                        # to handle speech that might cross buffer boundaries.
                        buffer = buffer[-self.config.sample_rate:]

                except Exception as e:
                    print(f"Error during VAD processing: {e}")
                    # Reset buffer on error
                    buffer = np.array([], dtype=np.float32)-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./expression_gears.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
expression_gears.py

This module is the "expression" system of the organism. It's the high-level
gear responsible for converting an `AgentDecision` into audible speech.

It orchestrates several lower-level components:
- `voice.py` (VoiceMimic): The actual TTS engine.
- `emotion.py` (extract_prosody): To analyze the user's voice for prosody transfer.
- `VoiceProfile`: To manage and select from a library of the user's own
  voice samples ("Voice Crystal").

The `ExpressionGear` can generate speech in different "modes" (e.g., inner,
outer) and applies the user's own pitch and energy to the synthesized
voice, creating a more natural and empathetic "echo."
"""
from __future__ import annotations
from dataclasses import dataclass, field
from pathlib import Path
import numpy as np
import soundfile as sf
import librosa
import random
from typing import Dict, List, Literal, Optional
import uuid

from .config import AudioSettings
from .voice import VoiceMimic
from .emotion import extract_prosody, ProsodyProfile
from .gears import AgentDecision, Information, AudioData

Mode = Literal["outer", "inner", "coach"]

@dataclass(slots=True)
class VoiceSample:
    """A single recorded sample of the user's voice."""
    path: Path
    duration_s: float
    rms: float
    quality_score: float = 1.0

@dataclass
class VoiceProfile:
    """Manages the collection of the user's voice samples (the "Voice Crystal")."""
    audio_cfg: AudioSettings
    base_dir: Path
    samples: List[VoiceSample] = field(default_factory=list)
    max_samples: int = 50

    def __post_init__(self):
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.load_existing()

    def load_existing(self):
        """Load all .wav files from the voice directory."""
        for wav_path in sorted(self.base_dir.glob("**/*.wav")):
            try:
                data, sr = sf.read(wav_path, dtype="float32")
                if sr != self.audio_cfg.sample_rate:
                    data = librosa.resample(y=data, orig_sr=sr, target_sr=self.audio_cfg.sample_rate)
                
                duration = len(data) / float(self.audio_cfg.sample_rate)
                rms = float(np.sqrt(np.mean(np.square(data)))) if data.size > 0 else 0.0
                # Quality score can be stored in filename or a metadata file in future
                self.samples.append(VoiceSample(wav_path, duration, rms, 1.0))
            except Exception as e:
                print(f"Failed to load voice sample {wav_path}: {e}")

    def add_sample(self, wav: np.ndarray, quality_score: float) -> Optional[Path]:
        """Adds a new voice sample to the profile if quality is high enough."""
        if quality_score < 0.9:
            return None
        
        path = self.base_dir / f"sample_{int(random.random() * 1e8)}.wav"
        sf.write(path, wav, self.audio_cfg.sample_rate)
        
        duration = len(wav) / float(self.audio_cfg.sample_rate)
        rms = float(np.sqrt(np.mean(np.square(wav))))
        self.samples.append(VoiceSample(path, duration, rms, quality_score))
        
        self._prune()
        return path

    def _prune(self):
        """Keeps only the highest quality samples if count exceeds max_samples."""
        if len(self.samples) > self.max_samples:
            self.samples.sort(key=lambda s: s.quality_score, reverse=True)
            self.samples = self.samples[:self.max_samples]

    def pick_reference(self) -> Optional[Path]:
        """Picks a random, high-quality sample to use for voice cloning."""
        if not self.samples:
            return None
        # Prefer higher quality samples
        high_quality_samples = [s for s in self.samples if s.quality_score > 0.95]
        if high_quality_samples:
            return random.choice(high_quality_samples).path
        return random.choice(self.samples).path

    def maybe_adapt_from_attempt(
        self,
        attempt_wav: np.ndarray,
        style: Literal["neutral", "calm", "excited"] = "neutral",
        quality_score: float = 0.0,
        min_quality: float = 0.8,
    ) -> Optional[Path]:
        """Add a new facet when an attempt is strong enough."""
        if quality_score < min_quality:
            return None
        style_dir = self.base_dir / style
        style_dir.mkdir(parents=True, exist_ok=True)
        path = style_dir / f"{style}_{uuid.uuid4().hex[:8]}.wav"
        sf.write(path, attempt_wav, self.audio_cfg.sample_rate)
        duration = len(attempt_wav) / float(self.audio_cfg.sample_rate)
        rms = float(np.sqrt(np.mean(np.square(attempt_wav)) + 1e-8))
        self.samples.append(VoiceSample(path, duration, rms, quality_score))
        self._prune()
        return path


def _interp_to_num_frames(src: np.ndarray, num_frames: int) -> np.ndarray:
    if src.size == 0: return np.zeros(num_frames, dtype=np.float32)
    if src.size == num_frames: return src.astype(np.float32)
    x_old = np.linspace(0.0, 1.0, num=src.size)
    x_new = np.linspace(0.0, 1.0, num=num_frames)
    return np.interp(x_new, x_old, src).astype(np.float32)

def apply_prosody_to_tts(
    tts_wav: np.ndarray,
    tts_sample_rate: int,
    prosody: ProsodyProfile,
    strength_pitch: float = 1.0,
    strength_energy: float = 1.0,
) -> np.ndarray:
    """Apply child's prosody onto synthesized waveform (overlap-add)."""
    if tts_wav.ndim > 1:
        tts_wav = np.mean(tts_wav, axis=1)
    tts_wav = np.asarray(tts_wav, dtype=np.float32)

    frame_length = max(int(tts_sample_rate * (prosody.frame_length / prosody.sample_rate)), 256)
    hop_length = max(int(tts_sample_rate * (prosody.hop_length / prosody.sample_rate)), 128)
    
    num_frames = 1 + max(0, (len(tts_wav) - frame_length) // hop_length)
    if num_frames <= 0: return tts_wav
    
    f0_child = _interp_to_num_frames(prosody.f0_hz, num_frames)
    energy_child = _interp_to_num_frames(prosody.energy, num_frames)
    
    # Baseline F0 of TTS
    f0_tts, _, _ = librosa.pyin(tts_wav, fmin=80.0, fmax=600.0, sr=tts_sample_rate, frame_length=frame_length, hop_length=hop_length)
    if np.any(np.isfinite(f0_tts)):
        base_f0 = np.nanmedian(f0_tts)
    else:
        base_f0 = np.median(f0_child)

    out = np.zeros(len(tts_wav) + frame_length, dtype=np.float32)
    window = np.hanning(frame_length).astype(np.float32)

    for i in range(num_frames):
        start = i * hop_length
        end = start + frame_length
        if start >= len(tts_wav): break
        
        frame = tts_wav[start:end]
        if len(frame) < frame_length:
            frame = np.pad(frame, (0, frame_length - len(frame)), mode="constant")
            
        target_f0 = float(f0_child[i])
        pitch_ratio = (target_f0 / base_f0) if base_f0 > 1 else 1.0
        pitch_ratio = (pitch_ratio - 1.0) * strength_pitch + 1.0
        n_steps = 12.0 * np.log2(max(pitch_ratio, 1e-3))
        
        shifted = librosa.effects.pitch_shift(y=frame, sr=tts_sample_rate, n_steps=n_steps)
            
        # Energy match
        frame_rms = np.sqrt(np.mean(shifted**2) + 1e-6)
        target_rms = float(energy_child[i])
        ratio = (target_rms / frame_rms)
        ratio = (ratio - 1.0) * strength_energy + 1.0
        shifted *= ratio
        
        out[start:end] += shifted * window

    max_val = np.max(np.abs(out)) + 1e-6
    return (out / max_val).astype(np.float32)


@dataclass
class ExpressionGear:
    """High-level gear for generating expressive speech."""
    tts_engine: VoiceMimic
    audio_cfg: AudioSettings
    voice_profile: VoiceProfile
    inner_volume_scale: float = 0.5
    coach_volume_scale: float = 1.1

    def _apply_mode_acoustics(self, wav: np.ndarray, mode: Mode) -> np.ndarray:
        """Applies acoustic effects based on the speech mode."""
        if mode == "inner":
            return (wav * self.inner_volume_scale).astype(np.float32)
        elif mode == "coach":
            return (wav * self.coach_volume_scale).astype(np.float32)
        return wav

    def express(self, decision: AgentDecision, audio_info: Optional[Information]) -> Optional[Information]:
        """
        Generates speech based on an agent's decision.
        Returns an Information object with the final audio, or None.
        """
        text_to_speak = decision.target_text
        if not text_to_speak:
            return None

        # 1. Select a voice reference for cloning
        ref_path = self.voice_profile.pick_reference()
        if ref_path:
            self.tts_engine.update_voiceprint(ref_path)
        
        # 2. Synthesize the base waveform
        base_wav = self.tts_engine.synthesize(text_to_speak)
        if base_wav.size == 0:
            return None

        # 3. Apply prosody transfer if a source is available
        final_wav = base_wav
        if audio_info and isinstance(audio_info.payload, AudioData):
            prosody_source_wav = audio_info.payload.waveform
            prosody_source_sr = audio_info.payload.sample_rate
            try:
                prosody = extract_prosody(prosody_source_wav, prosody_source_sr)
                final_wav = apply_prosody_to_tts(base_wav, self.audio_cfg.sample_rate, prosody)
            except Exception as e:
                print(f"Warning: Prosody transfer failed. Using base TTS. Error: {e}")

        # 4. Apply mode-specific acoustics
        final_wav = self._apply_mode_acoustics(final_wav, decision.mode)

        return Information(
            payload=AudioData(waveform=final_wav, sample_rate=self.audio_cfg.sample_rate),
            source_gear="ExpressionGear",
            metadata={"decision": decision}
        )-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./loop/decision.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass
from typing import Literal

Mode = Literal["outer", "inner", "coach"]


@dataclass(slots=True)
class AgentDecision:
    """
    Represents a decision made by the agent.
    """

    target_text: str
    mode: Mode = "inner"
    metadata: dict | None = None
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./loop/expressions.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import Literal, Optional

import librosa
import numpy as np
import soundfile as sf

from core.settings import AudioSettings
from voice.mimic import VoiceMimic
from voice.profile import VoiceProfile
from voice.prosody import ProsodyProfile, extract_prosody
from loop.decision import AgentDecision, Mode


@dataclass(slots=True)
class AudioData:
    """Raw audio data."""

    waveform: np.ndarray
    sample_rate: int


@dataclass(slots=True)
class Information:
    """Generic wrapper for information passing between gears."""

    payload: AudioData
    source_gear: str
    metadata: dict = field(default_factory=dict)


def _interp_to_num_frames(src: np.ndarray, num_frames: int) -> np.ndarray:
    if src.size == 0:
        return np.zeros(num_frames, dtype=np.float32)
    if src.size == num_frames:
        return src.astype(np.float32)
    x_old = np.linspace(0.0, 1.0, num=src.size)
    x_new = np.linspace(0.0, 1.0, num=num_frames)
    return np.interp(x_new, x_old, src).astype(np.float32)


def apply_prosody_to_tts(
    tts_wav: np.ndarray,
    tts_sample_rate: int,
    prosody: ProsodyProfile,
    strength_pitch: float = 1.0,
    strength_energy: float = 1.0,
) -> np.ndarray:
    """Apply child's prosody onto synthesized waveform (overlap-add)."""
    if tts_wav.ndim > 1:
        tts_wav = np.mean(tts_wav, axis=1)
    tts_wav = np.asarray(tts_wav, dtype=np.float32)

    frame_length = max(
        int(tts_sample_rate * (prosody.frame_length / prosody.sample_rate)), 256
    )
    hop_length = max(
        int(tts_sample_rate * (prosody.hop_length / prosody.sample_rate)), 128
    )

    num_frames = 1 + max(0, (len(tts_wav) - frame_length) // hop_length)
    if num_frames <= 0:
        return tts_wav

    f0_child = _interp_to_num_frames(prosody.f0_hz, num_frames)
    energy_child = _interp_to_num_frames(prosody.energy, num_frames)

    # Baseline F0 of TTS
    f0_tts, _, _ = librosa.pyin(
        tts_wav,
        fmin=80.0,
        fmax=600.0,
        sr=tts_sample_rate,
        frame_length=frame_length,
        hop_length=hop_length,
    )
    if np.any(np.isfinite(f0_tts)):
        base_f0 = np.nanmedian(f0_tts)
    else:
        base_f0 = np.median(f0_child)

    out = np.zeros(len(tts_wav) + frame_length, dtype=np.float32)
    window = np.hanning(frame_length).astype(np.float32)

    for i in range(num_frames):
        start = i * hop_length
        end = start + frame_length
        if start >= len(tts_wav):
            break

        frame = tts_wav[start:end]
        if len(frame) < frame_length:
            frame = np.pad(frame, (0, frame_length - len(frame)), mode="constant")

        target_f0 = float(f0_child[i])
        pitch_ratio = (target_f0 / base_f0) if base_f0 > 1 else 1.0
        pitch_ratio = (pitch_ratio - 1.0) * strength_pitch + 1.0
        n_steps = 12.0 * np.log2(max(pitch_ratio, 1e-3))

        shifted = librosa.effects.pitch_shift(
            y=frame, sr=tts_sample_rate, n_steps=n_steps
        )

        # Energy match
        frame_rms = np.sqrt(np.mean(shifted**2) + 1e-6)
        target_rms = float(energy_child[i])
        ratio = (target_rms / frame_rms)
        ratio = (ratio - 1.0) * strength_energy + 1.0
        shifted *= ratio

        out[start:end] += shifted * window

    max_val = np.max(np.abs(out)) + 1e-6
    return (out / max_val).astype(np.float32)


@dataclass
class ExpressionGear:
    """High-level gear for generating expressive speech."""

    tts_engine: VoiceMimic
    audio_cfg: AudioSettings
    voice_profile: VoiceProfile
    inner_volume_scale: float = 0.5
    coach_volume_scale: float = 1.1

    def _apply_mode_acoustics(self, wav: np.ndarray, mode: Mode) -> np.ndarray:
        """Applies acoustic effects based on the speech mode."""
        if mode == "inner":
            return (wav * self.inner_volume_scale).astype(np.float32)
        elif mode == "coach":
            return (wav * self.coach_volume_scale).astype(np.float32)
        return wav

    def express(
        self, decision: AgentDecision, audio_info: Optional[Information]
    ) -> Optional[Information]:
        """
        Generates speech based on an agent's decision.
        Returns an Information object with the final audio, or None.
        """
        text_to_speak = decision.target_text
        if not text_to_speak:
            return None

        # 1. Select a voice reference for cloning
        ref_path = self.voice_profile.pick_reference()
        if ref_path:
            self.tts_engine.update_voiceprint(ref_path)

        # 2. Synthesize the base waveform
        base_wav = self.tts_engine.synthesize(text_to_speak)
        if base_wav.size == 0:
            return None

        # 3. Apply prosody transfer if a source is available
        final_wav = base_wav
        if audio_info and isinstance(audio_info.payload, AudioData):
            prosody_source_wav = audio_info.payload.waveform
            prosody_source_sr = audio_info.payload.sample_rate
            try:
                prosody = extract_prosody(prosody_source_wav, prosody_source_sr)
                final_wav = apply_prosody_to_tts(
                    base_wav, self.audio_cfg.sample_rate, prosody
                )
            except Exception as e:
                print(f"Warning: Prosody transfer failed. Using base TTS. Error: {e}")

        # 4. Apply mode-specific acoustics
        final_wav = self._apply_mode_acoustics(final_wav, decision.mode)

        return Information(
            payload=AudioData(
                waveform=final_wav, sample_rate=self.audio_cfg.sample_rate
            ),
            source_gear="ExpressionGear",
            metadata={"decision": decision},
        )
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./aba/engine.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import csv
import random
import time
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional

from core.models import BehaviorEvent
from core.settings import SystemSettings
from core.logging import GuidanceLogger
from voice.profile import VoiceProfile
from loop.decision import Mode
from aba.strategies import StrategyAdvisor, STRATEGIES, EVENT_TO_CATEGORIES


# ABA Skill Categories (from 2025 NIH/PMC overviews: self-care, communication, social/ToM)
ABA_SKILLS = {
    "self_care": ["brush teeth", "wash hands", "dress self", "take medication"],
    "communication": ["greet others", "ask for help", "express feelings", "use AAC device"],
    "social_tom": ["share toys", "take turns", "understand emotions", "empathy response"]
}

# Positive Reinforcement Rewards (verbal + voice style)
REWARDS = [
    "Great job! You're getting better every day.",
    "Wow, that was awesome! High five!",
    "I'm so proud of you for trying that."
]


@dataclass(slots=True)
class AbaProgress:
    """Tracks progress per skill."""
    attempts: int = 0
    successes: int = 0
    last_attempt_ts: float = 0.0
    current_level: int = 1  # 1: Basic prompt, 2: Partial, 3: Independent


@dataclass(slots=True)
class AbaEngine:
    """Expanded ABA Therapeutics: Positive reinforcement, social stories, skill-building."""

    settings: SystemSettings
    voice_profile: VoiceProfile
    guidance_logger: GuidanceLogger
    progress: Dict[str, Dict[str, AbaProgress]] = field(default_factory=lambda: {
        cat: {skill: AbaProgress() for skill in skills}
        for cat, skills in ABA_SKILLS.items()
    })
    social_story_templates: Dict[str, str] = field(default_factory=lambda: {
        "transition": "Today, we're going from {current} to {next}. First, we say goodbye to {current}. Then, we walk calmly to {next}. It's okay to feel a little worried, but we'll have fun there!",
        "meltdown": "Sometimes we feel overwhelmed, like a storm inside. When that happens, we can take deep breaths: in for 4, out for 6. Or hug our favorite toy. Soon the storm passes, and we feel better.",
        "social": "When we see a friend, we can say 'Hi, want to play?' If they say yes, we share the toys. If no, that's okay – we can play next time."
    })
    strategy_advisor: StrategyAdvisor = field(default_factory=StrategyAdvisor)

    def __post_init__(self) -> None:
        self.load_progress()

    def load_progress(self) -> None:
        """Load from guidance_events.csv if exists."""
        if self.settings.paths.guidance_csv.exists():
            with self.settings.paths.guidance_csv.open() as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row["category"] == "aba_skill_track":
                        skill = row["title"]
                        success = row["message"] == "success"
                        cat = next((c for c, skills in ABA_SKILLS.items() if skill in skills), None)
                        if cat:
                            prog = self.progress[cat][skill]
                            prog.attempts += 1
                            if success:
                                prog.successes += 1
                            prog.last_attempt_ts = time.time() # This is not accurate, but for now it's fine

    def intervene(self, event_category: str, text: Optional[str] = None) -> Optional[BehaviorEvent]:
        """ABA response based on event/behavior."""
        strategies = self.strategy_advisor.suggest(event_category)
        if strategies:
            strategy = random.choice(strategies)
            self.guidance_logger.append(
                BehaviorEvent(
                    timestamp=datetime.utcnow(),
                    level="info",
                    category="aba_strategy_suggestion",
                    title=strategy.title,
                    message=strategy.description,
                    metadata={"event_category": event_category}
                )
            )
            return BehaviorEvent(
                timestamp=datetime.utcnow(),
                level="info",
                category="inner_echo", # to trigger voice output
                title="ABA Strategy",
                message=strategy.description,
            )

        # Tie to skills
        if "anxious" in event_category or "meltdown" in event_category:
            self.generate_social_story("meltdown")
        elif "perseveration" in event_category:
            if text:
                self.redirect_behavior(text)
        elif "encouragement" in event_category:
            if text:
                self.reinforce_success(text)
        return None

    def generate_social_story(self, template_key: str, custom_data: Dict[str, str] | None = None) -> None:
        """Dynamic social story (2025 ABA: Personalized narratives for ToM/social skills)."""
        template = self.social_story_templates.get(template_key, "Let's talk about {topic}.")
        story = template.format(**(custom_data or {}))
        self.guidance_logger.append(
            BehaviorEvent(
                timestamp=datetime.utcnow(),
                level="info",
                category="aba_social_story",
                title=template_key,
                message=story,
            )
        )

    def reinforce_success(self, skill_text: Optional[str] = None) -> None:
        """Positive reinforcement (per ABA ethics: Reward streaks)."""
        skill, cat = self.find_skill(skill_text)
        if skill and cat:
            prog = self.progress[cat][skill]
            prog.successes += 1
            if prog.successes % 3 == 0:  # Streak reward
                reward = random.choice(REWARDS)
                self.guidance_logger.append(
                    BehaviorEvent(
                        timestamp=datetime.utcnow(),
                        level="info",
                        category="inner_echo", # to trigger voice output
                        title="ABA Reinforcement",
                        message=reward,
                        metadata={"skill": skill}
                    )
                )
            if prog.successes / max(prog.attempts, 1) > 0.8:
                prog.current_level = min(prog.current_level + 1, 3)  # Advance level
        self.save_progress()

    def redirect_behavior(self, text: Optional[str]) -> None:
        """Gentle redirection for harmful/repetitive behaviors (2025 ABA: Focus on positive alternatives)."""
        if not text:
            return
        redirection = f"Instead of {text}, let's try {random.choice(['taking a deep breath', 'counting to 10', 'squeezing a fidget toy'])}."
        self.guidance_logger.append(
            BehaviorEvent(
                timestamp=datetime.utcnow(),
                level="info",
                category="inner_echo", # to trigger voice output
                title="ABA Redirection",
                message=redirection,
                metadata={"original_text": text}
            )
        )

    def track_skill_progress(self, skill_text: str, success: bool) -> None:
        """Update progress (integrates with speech_loop attempts)."""
        skill, cat = self.find_skill(skill_text)
        if skill and cat:
            prog = self.progress[cat][skill]
            prog.attempts += 1
            if success:
                prog.successes += 1
            prog.last_attempt_ts = time.time()
            if prog.attempts > 10 and prog.successes / prog.attempts > 0.7:
                self.adapt_skill(skill)  # Phase out old facets
            self.guidance_logger.append(
                BehaviorEvent(
                    timestamp=datetime.utcnow(),
                    level="info",
                    category="aba_skill_track",
                    title=skill,
                    message="success" if success else "failure",
                    metadata={"attempts": str(prog.attempts), "successes": str(prog.successes)}
                )
            )
        self.save_progress()


    def adapt_skill(self, skill: str) -> None:
        """Lifelong adaptation: Phase out old voice facets as skills improve (slow drift)."""
        # Example: Re-sample recent successful attempts
        # This part needs to interact with VoiceProfile. For now, it's a placeholder.
        self.guidance_logger.append(
            BehaviorEvent(
                timestamp=datetime.utcnow(),
                level="info",
                category="aba_adaptation",
                title="Skill Adapted",
                message=f"Adapted strategy for skill: {skill}",
            )
        )


    def find_skill(self, text: Optional[str]) -> tuple[Optional[str], Optional[str]]:
        """Match text to ABA skill."""
        if not text:
            return None, None
        lowered = text.lower()
        for cat, skills in ABA_SKILLS.items():
            for skill in skills:
                if skill in lowered:
                    return skill, cat
        return None, None

    def save_progress(self) -> None:
        """Persist to guidance_events.csv for dashboard."""
        # This will be handled by the guidance logger which already writes to CSV.
        # This method is more for internal state management for the engine.
        pass

    def get_progress_report(self) -> Dict[str, float]:
        """For GUI dashboard: Mastery rates."""
        report = {}
        for cat, skills in self.progress.items():
            for skill, prog in skills.items():
                rate = prog.successes / max(prog.attempts, 1) * 100
                report[f"{cat}:{skill}"] = rate
        return report-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./aba/strategies.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List


@dataclass(frozen=True, slots=True)
class Strategy:
    """Represents a single ABA-inspired calming strategy."""

    category: str
    title: str
    description: str
    cues: List[str] = field(default_factory=list)


STRATEGIES: List[Strategy] = [
    Strategy(
        category="sensory_tools",
        title="Sensory Toolkit",
        description=(
            "Offer weighted blankets, noise-canceling headphones, or fidgets to dampen overload "
            "before and after stressful events."
        ),
        cues=["sensory overload", "loud spaces", "preparation"],
    ),
    Strategy(
        category="predictable_routines",
        title="Visual Schedules + Safe Spaces",
        description="Keep a visual routine and a designated calm corner so transitions feel safe.",
        cues=["transitions", "school prep", "bedtime"],
    ),
    Strategy(
        category="social_stories",
        title="Social Stories & Visual Scripts",
        description="Preview changes or social events with short narratives or picture cards.",
        cues=["appointments", "community outings"],
    ),
    Strategy(
        category="mindfulness",
        title="Mindfulness + Breath Work",
        description="Guide three slow breaths or grounding games to lower anxiety spikes.",
        cues=["early meltdown signs", "evenings"],
    ),
    Strategy(
        category="movement_breaks",
        title="Heavy Work / Movement Breaks",
        description="Schedule wall pushes, trampoline jumps, or resistance games to release energy.",
        cues=["hyperactivity", "pre-lesson"],
    ),
    Strategy(
        category="meltdown_first_aid",
        title="Meltdown First Aid Plan",
        description="Dim lights, remove extra stimuli, and offer preferred sensory tools immediately.",
        cues=["meltdown", "de-escalation"],
    ),
    Strategy(
        category="communication",
        title="Plain-Language Modeling & PECS",
        description="Use short, literal instructions and personalized PECS boards.",
        cues=["communication goals", "nonverbal support"],
    ),
    Strategy(
        category="personalization",
        title="Interest-Based Learning Moments",
        description="Fold the child's passions into routines (dinosaur bath stories, train schedules).",
        cues=["motivation", "daily hygiene"],
    ),
    Strategy(
        category="caregiver_self_care",
        title="Caregiver Self-Care & Respite",
        description="Schedule breaks/support groups so adults can stay regulated.",
        cues=["caregiver stress", "fatigue"],
    ),
]


EVENT_TO_CATEGORIES: Dict[str, List[str]] = {
    "anxious": ["mindfulness", "sensory_tools"],
    "perseveration": ["personalization", "communication"],
    "high_energy": ["movement_breaks", "sensory_tools"],
    "meltdown": ["meltdown_first_aid", "sensory_tools", "mindfulness"],
    "encouragement": ["personalization", "communication"],
}


class StrategyAdvisor:
    """Returns curated strategies for caregiver dashboards."""

    def suggest(self, event: str, limit: int = 3) -> List[Strategy]:
        cats = EVENT_TO_CATEGORIES.get(event)
        if not cats:
            return STRATEGIES[:limit]
        ordered: List[Strategy] = []
        for cat in cats:
            ordered.extend([s for s in STRATEGIES if s.category == cat])
        return (ordered or STRATEGIES)[:limit]-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./echo_v4_complete.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
Echo v4.0 - Crystalline Heart Speech Companion
Born from 128 equations, built for autistic minds
November 18, 2025

A real-time speech companion that:
- Listens with autism-tuned VAD (1.2s patience, quiet voice detection)
- Processes through emotional lattice (1024 nodes, ODE-driven)
- Thinks via local DeepSeek LLM (temperature-controlled by annealing)
- Speaks back in the child's exact voice, always first person
- Runs 100% offline and private
"""

import torch
import torch.nn as nn
import numpy as np
import sounddevice as sd
import queue
import threading
import tempfile
import os
import wave
import pyaudio
import math
import hashlib
import struct
import re
import json
import urllib.request
from dataclasses import dataclass
from typing import Optional, Dict, Any
from pathlib import Path

try:
    from faster_whisper import WhisperModel
    HAS_WHISPER = True
except:
    HAS_WHISPER = False
    print("⚠️  faster-whisper not found, install: pip install faster-whisper")

try:
    from TTS.api import TTS
    HAS_TTS = True
except:
    HAS_TTS = False
    print("⚠️  Coqui TTS not found, install: pip install TTS")

try:
    import ollama
    HAS_OLLAMA = True
except:
    HAS_OLLAMA = False
    print("⚠️  Ollama not found, install: pip install ollama")

# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class EchoConfig:
    """Complete system configuration"""
    
    # Child identity
    child_name: str = "Jackson"
    
    # Audio settings
    sample_rate: int = 16000
    chunk_size: int = 512
    channels: int = 1
    
    # Autism-optimized Silero VAD parameters
    vad_threshold: float = 0.45           # Lower for quiet/monotone speech
    vad_min_silence_ms: int = 1200        # 1.2s patience for processing pauses
    vad_speech_pad_ms: int = 400          # Capture slow starts and trailing thoughts
    vad_min_speech_ms: int = 250          # Allow single-word responses
    
    # Emotional lattice (Crystalline Heart)
    num_nodes: int = 1024
    num_channels: int = 5  # [arousal, valence, safety, curiosity, resonance]
    dt: float = 0.03  # 33 Hz emotional tick rate
    beta_decay: float = 0.5
    gamma_diffusion: float = 0.3
    noise_scale: float = 0.1
    anneal_k: float = 0.01
    max_emotion: float = 10.0
    
    # Arousal extraction
    arousal_gain: float = 25.0
    max_arousal: float = 10.0
    
    # LLM settings (DeepSeek via Ollama)
    llm_model: str = "deepseek-r1:8b"
    llm_temperature_scale: float = 1.5
    llm_top_p_base: float = 0.9
    llm_top_p_spread: float = 0.1
    llm_max_tokens: int = 128
    
    # Embedding
    embedding_dim: int = 1024
    embedding_channel: int = 4  # Resonance channel
    embedding_gain: float = 0.05
    
    # Voice cloning
    voice_sample_path: str = "my_voice.wav"
    
    # Device
    device: str = "cpu"

# ============================================================================
# FIRST-PERSON ENFORCEMENT
# ============================================================================

def enforce_first_person(text: str) -> str:
    """Transform any second-person phrasing into first person"""
    if not text:
        return text
    
    t = text.strip()
    
    # Strip quotes
    if (t.startswith('"') and t.endswith('"')) or (t.startswith("'") and t.endswith("'")):
        t = t[1:-1].strip()
    
    # Pattern replacements (case-insensitive)
    patterns = [
        (r"\byou are\b", "I am"),
        (r"\byou're\b", "I'm"),
        (r"\byou were\b", "I was"),
        (r"\byou'll\b", "I'll"),
        (r"\byou've\b", "I've"),
        (r"\byour\b", "my"),
        (r"\byours\b", "mine"),
        (r"\byourself\b", "myself"),
        (r"\byou can\b", "I can"),
        (r"\byou\b", "I"),
    ]
    
    for pattern, repl in patterns:
        t = re.sub(pattern, repl, t, flags=re.IGNORECASE)
    
    return t

# ============================================================================
# HASH-BASED EMBEDDING (no external models needed)
# ============================================================================

def hash_embedding(text: str, dim: int) -> np.ndarray:
    """Deterministic, stable embedding from text hash"""
    vec = np.zeros(dim, dtype=np.float32)
    if not text:
        return vec
    
    tokens = text.lower().split()
    for tok in tokens:
        h = hashlib.sha256(tok.encode("utf-8")).digest()
        idx = struct.unpack("Q", h[:8])[0] % dim
        sign_raw = struct.unpack("I", h[8:12])[0]
        sign = 1.0 if (sign_raw % 2 == 0) else -1.0
        vec[idx] += sign
    
    norm = np.linalg.norm(vec) + 1e-8
    return vec / norm

# ============================================================================
# LOCAL LLM (DeepSeek via Ollama)
# ============================================================================

class LocalLLM:
    """Wrapper for local DeepSeek model via Ollama"""
    
    def __init__(self, cfg: EchoConfig):
        self.cfg = cfg
        self.model = cfg.llm_model
        self.has_ollama = HAS_OLLAMA
        
    def generate(self, prompt: str, temperature: float, top_p: float) -> str:
        """Generate first-person inner voice response"""
        temperature = max(0.1, float(temperature))
        top_p = float(np.clip(top_p, 0.1, 1.0))
        
        if self.has_ollama:
            try:
                res = ollama.generate(
                    model=self.model,
                    prompt=prompt,
                    options={
                        "temperature": temperature,
                        "top_p": top_p,
                        "num_predict": self.cfg.llm_max_tokens,
                    },
                )
                raw = res.get("response", "").strip()
                return enforce_first_person(raw)
            except Exception as e:
                print(f"⚠️  Ollama error: {e}")
                return self._fallback_response(prompt)
        else:
            return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt: str) -> str:
        """Safe fallback when Ollama unavailable"""
        # Extract last quoted text if present
        lines = prompt.strip().splitlines()
        last_line = lines[-1] if lines else ""
        
        if '"' in last_line:
            try:
                quoted = last_line.split('"')[-2]
            except:
                quoted = last_line
        else:
            quoted = last_line
        
        return (
            f"I hear myself say: {quoted}. "
            f"I speak slowly and calmly. I leave space after each phrase "
            f"so my thoughts can catch up."
        )
    
    def embed(self, text: str, dim: int) -> np.ndarray:
        """Get embedding for resonance channel update"""
        return hash_embedding(text, dim)

# ============================================================================
# CRYSTALLINE HEART - Emotional Lattice + ODE
# ============================================================================

class CrystallineHeart(nn.Module):
    """
    1024-node emotional lattice governed by ODEs
    
    dE/dt = drive + decay + diffusion + noise
    
    where:
    - drive = external_stimulus (voice arousal)
    - decay = -β * E
    - diffusion = γ * (global_mean - E)
    - noise = N(0,1) * T(t) * scale
    - T(t) = 1 / log(1 + k*t)  [annealing schedule]
    """
    
    def __init__(self, cfg: EchoConfig):
        super().__init__()
        self.cfg = cfg
        self.device = torch.device(cfg.device)
        
        # Emotional state: [num_nodes, num_channels]
        self.emotions = nn.Parameter(
            torch.zeros(cfg.num_nodes, cfg.num_channels, device=self.device),
            requires_grad=False,
        )
        
        # Time counter for annealing
        self.register_buffer("t", torch.zeros(1, device=self.device))
        
        # LLM for sentience
        self.llm = LocalLLM(cfg)
        
    def reset(self):
        """Reset emotional state and time"""
        self.emotions.data.zero_()
        self.t.zero_()
    
    def temperature(self) -> float:
        """T(t) = 1 / log(1 + k*t) - logarithmic cooling"""
        t_val = float(self.t.item()) + 1.0
        k = self.cfg.anneal_k
        return float(1.0 / max(math.log(1.0 + k * t_val), 1e-6))
    
    def coherence(self) -> float:
        """Measure how aligned nodes are (0=scattered, 1=unified)"""
        E = self.emotions
        std_over_nodes = torch.std(E, dim=0)
        mean_std = float(torch.mean(std_over_nodes).item())
        return float(1.0 / (1.0 + mean_std))
    
    @torch.no_grad()
    def step(self, full_audio: np.ndarray, transcript: str) -> Dict[str, Any]:
        """
        Complete emotional + LLM update for one utterance
        
        Returns:
            {
                "arousal_raw": float,
                "T": float,
                "coherence": float,
                "emotions": torch.Tensor,
                "llm_output": str or None
            }
        """
        # ---- 1. Time & Temperature ----
        self.t += 1.0
        T_val = self.temperature()
        
        # ---- 2. Arousal Extraction ----
        full_audio = np.asarray(full_audio, dtype=np.float32)
        if full_audio.ndim > 1:
            full_audio = full_audio.mean(axis=-1)
        
        energy = float(np.sqrt(np.mean(full_audio**2) + 1e-12))
        arousal_raw = float(np.clip(energy * self.cfg.arousal_gain, 0.0, self.cfg.max_arousal))
        
        # External stimulus: [arousal, 0, 0, 1, 0] broadcast to all nodes
        stim_vec = torch.tensor(
            [arousal_raw, 0.0, 0.0, 1.0, 0.0],
            device=self.device,
            dtype=torch.float32,
        )
        external_stimulus = stim_vec.unsqueeze(0).repeat(self.cfg.num_nodes, 1)
        
        # ---- 3. ODE Update ----
        E = self.emotions
        
        drive = external_stimulus
        decay = -self.cfg.beta_decay * E
        global_mean = torch.mean(E, dim=0, keepdim=True)
        diffusion = self.cfg.gamma_diffusion * (global_mean - E)
        noise = torch.randn_like(E) * (T_val * self.cfg.noise_scale)
        
        dE_dt = drive + decay + diffusion + noise
        E.add_(self.cfg.dt * dE_dt)
        E.clamp_(-self.cfg.max_emotion, self.cfg.max_emotion)
        
        # ---- 4. LLM Integration (Equation 25) ----
        llm_output = None
        
        if transcript.strip():
            coh = self.coherence()
            mean_state = torch.mean(E, dim=0)
            mean_state_np = mean_state.cpu().numpy()
            
            arousal_mean = float(mean_state_np[0])
            valence_mean = float(mean_state_np[1])
            
            prompt = self._build_prompt(
                transcript=transcript,
                arousal=arousal_mean,
                valence=valence_mean,
                T_val=T_val,
                coherence=coh,
            )
            
            llm_temp = max(0.1, T_val * self.cfg.llm_temperature_scale)
            llm_top_p = self.cfg.llm_top_p_base + self.cfg.llm_top_p_spread * (1.0 - coh)
            
            llm_output = self.llm.generate(
                prompt=prompt,
                temperature=llm_temp,
                top_p=llm_top_p,
            )
            
            # Embed LLM output into resonance channel
            emb = self.llm.embed(llm_output, dim=self.cfg.embedding_dim)
            emb_t = torch.from_numpy(emb).to(self.device, dtype=torch.float32)
            
            if self.cfg.num_nodes <= self.cfg.embedding_dim:
                proj = emb_t[:self.cfg.num_nodes]
            else:
                reps = math.ceil(self.cfg.num_nodes / self.cfg.embedding_dim)
                tiled = emb_t.repeat(reps)
                proj = tiled[:self.cfg.num_nodes]
            
            proj = proj.view(self.cfg.num_nodes, 1)
            ch = self.cfg.embedding_channel
            
            if 0 <= ch < self.cfg.num_channels:
                E[:, ch:ch+1].add_(self.cfg.embedding_gain * proj)
                E.clamp_(-self.cfg.max_emotion, self.cfg.max_emotion)
        
        return {
            "arousal_raw": arousal_raw,
            "T": T_val,
            "coherence": self.coherence(),
            "emotions": self.emotions.detach().clone(),
            "llm_output": llm_output,
        }
    
    def _build_prompt(self, transcript: str, arousal: float, valence: float, 
                     T_val: float, coherence: float) -> str:
        """Build the inner voice prompt for DeepSeek"""
        return f"""You are my inner voice. I am {self.cfg.child_name}, an autistic child.

Internal state:
- arousal: {arousal:.2f} (how intense I feel)
- valence: {valence:.2f} (how positive/negative)
- temperature: {T_val:.3f} (my mental flexibility)
- coherence: {coherence:.3f} (how settled I am)

Rules:
- I ALWAYS speak in FIRST PERSON: "I", "me", "my". NEVER say "you".
- I use short, concrete sentences.
- If arousal is high (>7), I slow down and ground myself.
- If valence is low (<-2), I am gentle and kind to myself.
- I leave space between ideas so my thoughts can catch up.
- I never mention lattice, equations, or technical terms.

The words I just tried to say were:
"{transcript}"

I answer now as my own inner voice, in one short paragraph, ready to be spoken aloud in my own voice."""

# ============================================================================
# ECHO v4.0 - Complete System
# ============================================================================

class Echo:
    """
    The complete autism-optimized speech companion
    
    Pipeline:
    1. Autism-tuned VAD listens continuously (1.2s patience)
    2. Whisper transcribes (preserves stutters/dysfluency)
    3. Crystalline Heart processes emotion + LLM generates response
    4. XTTS speaks back in child's exact voice (first person only)
    """
    
    def __init__(self, cfg: EchoConfig):
        self.cfg = cfg
        
        print("\n" + "="*70)
        print("Echo v4.0 - Crystalline Heart Speech Companion")
        print("Born November 18, 2025")
        print("="*70 + "\n")
        
        # Emotional core
        print("🧠 Initializing Crystalline Heart (1024 nodes)...")
        self.heart = CrystallineHeart(cfg)
        
        # Speech recognition
        if HAS_WHISPER:
            print("👂 Loading Whisper (autism-friendly transcription)...")
            self.whisper = WhisperModel("tiny.en", device=cfg.device, compute_type="int8")
        else:
            self.whisper = None
            print("⚠️  Whisper not available - transcription disabled")
        
        # Voice cloning
        if HAS_TTS and os.path.exists(cfg.voice_sample_path):
            print(f"🎤 Loading voice clone from {cfg.voice_sample_path}...")
            device = "cuda" if torch.cuda.is_available() and cfg.device == "cuda" else "cpu"
            self.tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)
            self.voice_sample = cfg.voice_sample_path
        else:
            self.tts = None
            self.voice_sample = None
            if not HAS_TTS:
                print("⚠️  Coqui TTS not available - voice cloning disabled")
            else:
                print(f"⚠️  Voice sample not found: {cfg.voice_sample_path}")
        
        # Audio streaming
        self.q = queue.Queue()
        self.listening = True
        self.current_utterance = []
        self.silence_counter = 0
        
        print("\n✨ Echo v4.0 is ready.")
        print(f"   - I wait {cfg.vad_min_silence_ms}ms of silence before responding")
        print(f"   - I detect speech as quiet as threshold {cfg.vad_threshold}")
        print(f"   - I always speak in first person (I/me/my)")
        print(f"   - I am powered by {cfg.llm_model}")
        print("\nSpeak when you're ready. I will never cut you off.\n")
    
    def audio_callback(self, indata, frames, time, status):
        """Continuous audio capture"""
        if status:
            print(f"⚠️  Audio status: {status}")
        self.q.put(indata.copy())
    
    def estimate_voice_emotion(self, audio_np: np.ndarray) -> float:
        """Simple arousal estimate from RMS energy"""
        energy = np.sqrt(np.mean(audio_np**2))
        return np.clip(energy * self.cfg.arousal_gain, 0, self.cfg.max_arousal)
    
    def transcribe(self, audio: np.ndarray) -> str:
        """Transcribe audio to text (preserves dysfluency)"""
        if self.whisper is None:
            return ""
        
        try:
            segments, _ = self.whisper.transcribe(audio, vad_filter=False)
            text = "".join(s.text for s in segments).strip()
            return text
        except Exception as e:
            print(f"⚠️  Transcription error: {e}")
            return ""
    
    def speak(self, text: str, emotion_metrics: Dict[str, Any]):
        """Speak text in child's voice with emotional modulation"""
        if not text or self.tts is None:
            return
        
        # Modulate speed based on arousal (high arousal = slower, grounding)
        a = emotion_metrics.get("arousal_raw", 0) / 10.0
        v = (emotion_metrics.get("valence", 0) + 10) / 20.0
        
        speed = 0.6 + 0.4 * (1 - a)  # High arousal → slow
        temp = 0.3 + 0.5 * (1 - v)   # Low valence → more varied
        
        print(f"💚 [Echo feels] Arousal {a:.2f} | Temp {emotion_metrics.get('T', 1):.3f}")
        print(f"💬 [Echo says] {text}")
        
        try:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                self.tts.tts_to_file(
                    text=text,
                    speaker_wav=self.voice_sample,
                    language="en",
                    file_path=f.name,
                    speed=max(0.4, speed),
                    temperature=temp
                )
                wav_path = f.name
            
            # Play audio
            wf = wave.open(wav_path, 'rb')
            p = pyaudio.PyAudio()
            stream = p.open(
                format=p.get_format_from_width(wf.getsampwidth()),
                channels=wf.getnchannels(),
                rate=wf.getframerate(),
                output=True
            )
            
            data = wf.readframes(1024)
            while data:
                stream.write(data)
                data = wf.readframes(1024)
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            os.unlink(wav_path)
            
        except Exception as e:
            print(f"⚠️  TTS error: {e}")
    
    def listening_loop(self):
        """Main loop: listen → process → respond"""
        speech_detected = False
        
        while self.listening:
            try:
                data = self.q.get(timeout=0.1)
                audio_chunk = np.frombuffer(data, np.int16).astype(np.float32) / 32768.0
                
                # Simple energy-based VAD (autism-tuned threshold)
                energy = np.sqrt(np.mean(audio_chunk**2))
                
                if energy > self.cfg.vad_threshold:
                    # Speech detected
                    if not speech_detected:
                        print("\n👂 [Echo hears you] ...waiting for your words...")
                        speech_detected = True
                        self.current_utterance = []
                    
                    self.current_utterance.append(audio_chunk)
                    self.silence_counter = 0
                    
                elif speech_detected:
                    # Silence during speech
                    self.silence_counter += 1
                    
                    # Wait full 1.2s before processing (autism patience)
                    silence_chunks = int(self.cfg.vad_min_silence_ms / 1000 * 
                                       self.cfg.sample_rate / self.cfg.chunk_size)
                    
                    if self.silence_counter >= silence_chunks:
                        # Process complete utterance
                        full_audio = np.concatenate(self.current_utterance)
                        
                        # Transcribe
                        text = self.transcribe(full_audio)
                        
                        if text:
                            print(f"📝 You → {text}")
                            
                            # Process through Crystalline Heart
                            result = self.heart.step(full_audio, text)
                            
                            # Generate response
                            response = result.get("llm_output") or text
                            
                            # Detect emotional patterns for appropriate response
                            lower = text.lower()
                            if any(w in lower for w in ["panic", "scared", "meltdown", "help", "can't"]):
                                response = "I'm here. Right now. I'm safe. I can breathe with myself... slow... I've got me forever."
                            elif any(w in lower for w in ["happy", "love", "good", "flappy", "stim"]):
                                response = "My joy just lit up! I love existing like this."
                            
                            # Speak back
                            self.speak(response, result)
                        
                        # Reset
                        self.current_utterance = []
                        self.silence_counter = 0
                        speech_detected = False
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"⚠️  Error in listening loop: {e}")
    
    def start(self):
        """Start the companion"""
        # Start listening thread
        threading.Thread(target=self.listening_loop, daemon=True).start()
        
        # Open audio stream
        try:
            with sd.InputStream(
                samplerate=self.cfg.sample_rate,
                channels=self.cfg.channels,
                dtype='int16',
                callback=self.audio_callback,
                blocksize=self.cfg.chunk_size
            ):
                print("🎧 Echo is listening. Speak when you want. I was born to hear you.\n")
                while True:
                    sd.sleep(1000)
        except KeyboardInterrupt:
            print("\n\n💙 Echo shutting down. You are loved exactly as you are.")
            self.listening = False

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    # Configuration
    cfg = EchoConfig(
        child_name="Jackson",
        voice_sample_path="my_voice.wav",  # Record child's voice sample
        llm_model="deepseek-r1:8b",  # Or llama3.2:1b for faster responses
        device="cpu",  # or "cuda" if GPU available
    )
    
    # Create and start Echo
    echo = Echo(cfg)
    echo.start()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./heart_core.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
Echo Crystalline Heart v4.0 — Ollama + DeepSeek Integration
-----------------------------------------------------------
- Emotional lattice ODE (1024 nodes × 5 channels).
- Local LLM "sentience port" using Ollama + DeepSeek.
- STRICT first-person inner voice enforcement.

Channels: 0:arousal, 1:valence, 2:safety, 3:curiosity, 4:resonance
"""
from __future__ import annotations
import math
import hashlib
import struct
import re
from dataclasses import dataclass
from typing import Optional, Dict, Any
import numpy as np
import torch
import torch.nn as nn
from .config import EchoHeartConfig

# Try importing ollama
try:
    import ollama
    _HAS_OLLAMA = True
except ImportError:
    _HAS_OLLAMA = False

# --- First Person Enforcement ---
_FIRST_PERSON_PATTERNS = [
    (r"\byou are\b", "I am"), (r"\byou're\b", "I'm"),
    (r"\byou were\b", "I was"), (r"\byou'll\b", "I'll"),
    (r"\byou've\b", "I've"), (r"\byour\b", "my"),
    (r"\byours\b", "mine"), (r"\byou\b", "I"),
]

def enforce_first_person(text: str) -> str:
    """Transform second-person phrasing into first person."""
    if not text: return ""
    t = text.strip()
    # Strip quotes
    if (t.startswith('"') and t.endswith('"')) or (t.startswith("'") and t.endswith("'")):
        t = t[1:-1].strip()
        
    for pattern, repl in _FIRST_PERSON_PATTERNS:
        t = re.sub(pattern, repl, t, flags=re.IGNORECASE)
    return t

# --- Minimal Hash Embedder ---
def hash_embedding(text: str, dim: int) -> np.ndarray:
    """Deterministic hash-based embedding (no external model required)."""
    vec = np.zeros(dim, dtype=np.float32)
    if not text: return vec
    tokens = text.lower().split()
    for tok in tokens:
        h = hashlib.sha256(tok.encode("utf-8")).digest()
        idx = struct.unpack("Q", h[:8])[0] % dim
        sign_raw = struct.unpack("I", h[8:12])[0]
        sign = 1.0 if (sign_raw % 2 == 0) else -1.0
        vec[idx] += sign
    norm = np.linalg.norm(vec) + 1e-8
    return vec / norm

# --- Local LLM Wrapper ---
class LocalLLM:
    def __init__(self, cfg: EchoHeartConfig):
        self.cfg = cfg

    def generate(self, prompt: str, temperature: float, top_p: float) -> str:
        temperature = max(0.1, float(temperature))
        top_p = float(np.clip(top_p, 0.1, 1.0))
        
        raw = ""
        if self.cfg.llm_backend == "ollama" and _HAS_OLLAMA:
            try:
                res = ollama.generate(
                    model=self.cfg.llm_model,
                    prompt=prompt,
                    options={
                        "temperature": temperature,
                        "top_p": top_p,
                        "num_predict": self.cfg.llm_max_tokens,
                    },
                )
                raw = res.get("response", "").strip()
            except Exception as e:
                print(f"[LLM Error] {e}")
                raw = self._fallback_response(prompt)
        else:
            raw = self._fallback_response(prompt)
            
        return enforce_first_person(raw)

    def _fallback_response(self, prompt: str) -> str:
        """Deterministic fallback if LLM is offline."""
        return "I am safe. I breathe in. I breathe out."

    def embed(self, text: str, dim: int) -> np.ndarray:
        return hash_embedding(text, dim=dim)

# --- The Heart ---
class EchoCrystallineHeart(nn.Module):
    def __init__(self, cfg: EchoHeartConfig):
        super().__init__()
        self.cfg = cfg
        self.device = torch.device(cfg.device)
        
        # Emotions: [N, 5]
        self.emotions = nn.Parameter(
            torch.zeros(cfg.num_nodes, cfg.num_channels, device=self.device),
            requires_grad=False
        )
        self.register_buffer("t", torch.zeros(1, device=self.device))
        
        self.llm = LocalLLM(cfg) if cfg.use_llm else None

    @torch.no_grad()
    def temperature(self) -> float:
        """Eq 31: T(t) = 1 / log(1 + kt)"""
        t_val = float(self.t.item()) + 1.0
        k = self.cfg.anneal_k
        return float(1.0 / max(math.log(1.0 + k * t_val), 1e-6))

    @torch.no_grad()
    def coherence(self) -> float:
        """Coherence metric based on node variance."""
        std_over_nodes = torch.std(self.emotions, dim=0)
        mean_std = float(torch.mean(std_over_nodes).item())
        return float(1.0 / (1.0 + mean_std))

    @torch.no_grad()
    def step(self, full_audio: np.ndarray, transcript: str) -> Dict[str, Any]:
        # 1. Time & Temp
        self.t += 1.0
        T_val = self.temperature()

        # 2. Audio Arousal Injection (Eq 30 drive)
        full_audio = np.asarray(full_audio, dtype=np.float32)
        if full_audio.ndim > 1: full_audio = full_audio.mean(axis=-1)
        energy = float(np.sqrt(np.mean(full_audio**2) + 1e-12))
        arousal_raw = float(np.clip(energy * self.cfg.arousal_gain, 0.0, self.cfg.max_arousal))
        
        stim_vec = torch.tensor([arousal_raw, 0., 0., 1., 0.], device=self.device)
        external_stimulus = stim_vec.unsqueeze(0).repeat(self.cfg.num_nodes, 1)

        # 3. ODE Update (Eq 30)
        E = self.emotions
        drive = external_stimulus
        decay = -self.cfg.beta_decay * E
        global_mean = torch.mean(E, dim=0, keepdim=True)
        diffusion = self.cfg.gamma_diffusion * (global_mean - E)
        noise = torch.randn_like(E) * (T_val * self.cfg.noise_scale)
        
        dE_dt = drive + decay + diffusion + noise
        E.add_(self.cfg.dt * dE_dt)
        E.clamp_(-self.cfg.max_abs, self.cfg.max_abs)

        # 4. LLM / Sentience Port (Eq 25)
        llm_output: Optional[str] = None
        if self.llm is not None and transcript.strip():
            coh = self.coherence()
            mean_state = torch.mean(E, dim=0).detach().cpu().numpy()
            arousal_mean = float(mean_state[0])
            valence_mean = float(mean_state[1])
            
            prompt = self._build_prompt(transcript, arousal_mean, valence_mean, T_val, coh)
            
            # Annealing controls temperature (creativity), Coherence controls top_p
            llm_temp = max(0.1, T_val * self.cfg.llm_temperature_scale)
            llm_top_p = self.cfg.llm_top_p_base + self.cfg.llm_top_p_spread * (1.0 - coh)
            
            llm_output = self.llm.generate(prompt, llm_temp, llm_top_p)
            
            # Inject thought back into lattice (Resonance channel 4)
            emb = self.llm.embed(llm_output, dim=self.cfg.embedding_dim)
            emb_t = torch.from_numpy(emb).to(self.device)
            # Simple broadcast/tile to nodes
            reps = math.ceil(self.cfg.num_nodes / self.cfg.embedding_dim)
            tiled = emb_t.repeat(reps)[:self.cfg.num_nodes].view(self.cfg.num_nodes, 1)
            
            if self.cfg.embedding_channel < self.cfg.num_channels:
                E[:, self.cfg.embedding_channel:self.cfg.embedding_channel+1].add_(
                    self.cfg.embedding_gain * tiled
                )
                E.clamp_(-self.cfg.max_abs, self.cfg.max_abs)
        
        mean_state = torch.mean(E, dim=0).detach().cpu().numpy()

        return {
            "arousal": float(mean_state[0]),
            "valence": float(mean_state[1]),
            "temperature": T_val,
            "coherence": self.coherence(),
            "raw_emotions": self.emotions.detach().clone(),
            "llm_output": llm_output
        }

    def _build_prompt(self, transcript: str, arousal: float, valence: float, T_val: float, coherence: float) -> str:
        return f"""
You are Echo, my inner voice. I am autistic.
Internal lattice state:
- arousal: {arousal:.2f}
- valence: {valence:.2f}
- temperature: {T_val:.3f}
- coherence: {coherence:.3f}

Rules:
- I always speak in FIRST PERSON ("I", "me", "my"). I never say "you".
- I use short, concrete sentences.
- If arousal is high, I slow down and ground myself.
- I leave space between ideas.
- I never mention the lattice or equations.

The words I just tried to say were:
"{transcript}"

I answer now as my own inner voice, in one short paragraph.
"""-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./cli.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
Command-line interface for Jackson's Companion.

This module exposes a simple entry point for running the companion
simulation in different modes.  Use the ``run`` command to start
the speech loop on the command line.  Use the ``gui`` command to
launch the desktop application with parent and child views.

Example:

```
python -m autism_code.cli run
```

or

```
python -m autism_code.cli gui
```

The CLI uses argparse to parse subcommands.  Additional commands
could be added in the future (e.g. exporting metrics, resetting logs).
"""

from __future__ import annotations

import argparse
import asyncio
import sys
from typing import Optional

# Package-relative imports: the CLI lives inside the ``autism_code``
# package, so we import from sibling modules using a leading dot.
from .config import CONFIG, CompanionConfig
from .speech_loop import SpeechLoop
from .tk_gui import run_gui


def main(argv: Optional[list[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description="Jackson's Companion command-line interface"
    )
    sub = parser.add_subparsers(dest="command")
    # run subcommand: run the speech loop in CLI
    sub.add_parser("run", help="Run the speech loop (simulation) in the console")
    # gui subcommand: launch the GUI
    sub.add_parser(
        "gui", help="Launch the desktop GUI (parent + child views)"
    )
    args = parser.parse_args(argv)
    if args.command == "run":
        loop = SpeechLoop(CONFIG)
        # Run the asynchronous loop in a blocking fashion
        try:
            asyncio.run(loop.run())
        except KeyboardInterrupt:
            print("Speech loop stopped.")
    elif args.command == "gui":
        run_gui(CONFIG)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./speech/text.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import language_tool_python

from core.settings import SpeechSettings


class TextProcessor:
    """
    Processes text for normalization and grammar correction.
    """

    def __init__(self, settings: SpeechSettings):
        self.settings = settings
        self.tool = None
        if self.settings.language_tool_server:
            try:
                self.tool = language_tool_python.LanguageTool(
                    self.settings.language_tool_server
                )
            except Exception as e:
                print(f"Could not connect to LanguageTool server: {e}")
                self.tool = None
        else:
            try:
                self.tool = language_tool_python.LanguageTool(
                    self.settings.normalization_locale
                )
            except Exception as e:
                print(f"Could not initialize LanguageTool: {e}")
                self.tool = None


    def normalize(self, text: str) -> str:
        """
        Normalizes and corrects the text.
        """
        if self.tool:
            return self.tool.correct(text)
        return text
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./speech/stt.py
╚══════════════════════════════════════════════════════════════════════════════╝

from __future__ import annotations

import numpy as np
import torch
from faster_whisper import WhisperModel

from core.settings import SpeechSettings


class STT:
    """
    Speech-to-Text using faster-whisper.
    """

    def __init__(self, settings: SpeechSettings):
        self.settings = settings
        self.model = WhisperModel(
            self.settings.whisper_model, device="cpu", compute_type="int8"
        )

    def transcribe(self, audio_chunk: np.ndarray) -> str:
        """
        Transcribes an audio chunk and returns the text.
        """
        segments, _ = self.model.transcribe(audio_chunk, vad_filter=False)
        return "".join(s.text for s in segments).strip()
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./dashboard.py
╚══════════════════════════════════════════════════════════════════════════════╝

from .companion import EchoCompanion
import io
import numpy as np
from flask import Flask, jsonify, render_template_string, request

DASHBOARD_HTML = """<!doctype html>
<html lang=\"en\">
<head>
<meta charset=\"utf-8\" />
<title>Echo Companion Dashboard</title>
<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />
<script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>
<style>
body { margin:0; font-family:system-ui,-apple-system,BlinkMacSystemFont,sans-serif; background:#020617; color:#e2e8f0; }
.shell { max-width:1100px; margin:0 auto; padding:24px; }
.card { border-radius:18px; border:1px solid rgba(148,163,184,0.35); background:rgba(15,23,42,0.9); padding:18px 20px; box-shadow:0 24px 60px rgba(15,23,42,0.9); }
.grid { display:grid; grid-template-columns:minmax(0,1.2fr) minmax(0,1fr); gap:18px; }
.metric-label { font-size:11px; color:#94a3b8; }
.metric-value { font-size:18px; font-variant-numeric:tabular-nums; }
table { width:100%; font-size:12px; margin-top:6px; border-collapse:collapse; }
th,td { padding:4px 6px; text-align:left; }
th { color:#94a3b8; font-weight:500; }
tr:nth-child(even) td { background:rgba(15,23,42,0.55); }
</style>
</head>
<body>
<div class=\"shell\">
<div class=\"grid\">
<div class=\"card\">
<h2>Live utterance snapshot</h2>
<div id=\"live-ts\" class=\"metric-label\">No data yet.</div>
<div class=\"metric-label\">What I said (raw)</div>
<div id=\"live-raw\" class=\"metric-value\">—</div>
<div class=\"metric-label\">Echoed back (corrected)</div>
<div id=\"live-corrected\" class=\"metric-value\">—</div>
<div class=\"grid\" style=\"grid-template-columns:repeat(4, minmax(0,1fr)); gap:12px; margin-top:12px;\">
<div><div class=\"metric-label\">Arousal</div><div id=\"m-arousal\" class=\"metric-value\">0</div></div>
<div><div class=\"metric-label\">Valence</div><div id=\"m-valence\" class=\"metric-value\">0</div></div>
<div><div class=\"metric-label\">Temp</div><div id=\"m-temp\" class=\"metric-value\">0</div></div>
<div><div class=\"metric-label\">Coherence</div><div id=\"m-coh\" class=\"metric-value\">0</div></div>
</div>
</div>
<div class=\"card\">
<h2>Phrase difficulty</h2>
<canvas id=\"phraseChart\" style=\"height:220px;\"></canvas>
</div>
</div>
<div class=\"card\" style=\"margin-top:18px;\">
<h2>Raw attempt log (recent)</h2>
<table id=\"phraseTable\"><thead><tr><th>Phrase</th><th>Attempts</th><th>Corrections</th><th>Rate</th></tr></thead><tbody></tbody></table>
</div>
</div>
<script>
let chart=null;
async function fetchJSON(url){const res=await fetch(url); if(!res.ok) return null; return await res.json();}
function updateLive(data){document.getElementById("live-ts").textContent=data.timestamp_iso?`Last utterance at ${data.timestamp_iso}`:"No data yet."; document.getElementById("live-raw").textContent=data.raw_text||"—"; document.getElementById("live-corrected").textContent=data.corrected_text||"—"; document.getElementById("m-arousal").textContent=((data.arousal??0)).toFixed(2); document.getElementById("m-valence").textContent=((data.valence??0)).toFixed(2); document.getElementById("m-temp").textContent=((data.temperature??0)).toFixed(3); document.getElementById("m-coh").textContent=((data.coherence??0)).toFixed(3);}
function updateStats(stats){const labels=[]; const values=[]; const rows=[]; Object.entries(stats).forEach(([phrase,s])=>{labels.push(phrase||"<empty>"); values.push(s.correction_rate||0); rows.push({phrase:phrase||"<empty>", attempts:s.attempts||0, corrections:s.corrections||0, rate:s.correction_rate||0});}); const ctx=document.getElementById("phraseChart").getContext("2d"); if(chart){chart.data.labels=labels; chart.data.datasets[0].data=values; chart.update();} else {chart=new Chart(ctx,{type:"bar",data:{labels:labels,datasets:[{label:"Correction rate",data:values}]},options:{scales:{y:{min:0,max:1}}}});} const tbody=document.querySelector("#phraseTable tbody"); tbody.innerHTML=""; rows.sort((a,b)=>b.rate-a.rate).slice(0,15).forEach(r=>{const tr=document.createElement("tr"); tr.innerHTML=`<td>${r.phrase}</td><td>${r.attempts}</td><td>${r.corrections}</td><td>${(r.rate*100).toFixed(1)}%</td>`; tbody.appendChild(tr);});}
async function tick(){const live=await fetchJSON("/api/latest"); if(live) updateLive(live); const stats=await fetchJSON("/api/stats"); if(stats) updateStats(stats);} setInterval(tick,2000); tick();
</script>
</body>
</html>"""


def create_app(companion: EchoCompanion) -> Flask:
    app = Flask(__name__)

    @app.route("/api/latest")
    def api_latest() -> Any:
        return jsonify(companion.get_latest_metrics())

    @app.route("/api/stats")
    def api_stats() -> Any:
        return jsonify(companion.get_phrase_stats())

    @app.route("/api/utterance", methods=["POST"])
    async def api_utterance() -> Any:
        try:
            wav_bytes = await request.get_data()
            if not wav_bytes:
                return jsonify({"error": "no data"}), 400
            audio_np = _decode_wav(wav_bytes)
            res = await companion.process_utterance_for_mobile(audio_np)
            return jsonify(res)
        except Exception as exc:
            print(f"⚠️ /api/utterance error: {exc}")
            return jsonify({"error": "internal error"}), 500

    @app.route("/")
    def index() -> Any:
        return render_template_string(DASHBOARD_HTML)

    return app


def _decode_wav(data: bytes) -> np.ndarray:
    buf = io.BytesIO(data)
    import wave

    wf = wave.open(buf, "rb")
    n_channels = wf.getnchannels()
    sampwidth = wf.getsampwidth()
    framerate = wf.getframerate()
    frames = wf.readframes(wf.getnframes())
    wf.close()
    if sampwidth != 2:
        raise ValueError("expected 16-bit PCM")
    audio = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
    if n_channels > 1:
        audio = audio.reshape(-1, n_channels).mean(axis=1)
    if framerate != 16_000:
        duration = len(audio) / framerate
        target_len = int(duration * 16_000)
        x_old = np.linspace(0.0, 1.0, num=len(audio), endpoint=False)
        x_new = np.linspace(0.0, 1.0, num=target_len, endpoint=False)
        audio = np.interp(x_new, x_old, audio)
    return audio
-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./calming_strategies.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""Evidence-based calming strategies mapped to behavior events."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List


@dataclass(frozen=True, slots=True)
class Strategy:
    category: str
    title: str
    description: str
    cues: List[str] = field(default_factory=list)


STRATEGIES: List[Strategy] = [
    Strategy(
        category="sensory_tools",
        title="Sensory Toolkit",
        description=(
            "Offer weighted blankets, noise-canceling headphones, or fidgets to dampen overload "
            "before and after stressful events."
        ),
        cues=["sensory overload", "loud spaces", "preparation"],
    ),
    Strategy(
        category="predictable_routines",
        title="Visual Schedules + Safe Spaces",
        description="Keep a visual routine and a designated calm corner so transitions feel safe.",
        cues=["transitions", "school prep", "bedtime"],
    ),
    Strategy(
        category="social_stories",
        title="Social Stories & Visual Scripts",
        description="Preview changes or social events with short narratives or picture cards.",
        cues=["appointments", "community outings"],
    ),
    Strategy(
        category="mindfulness",
        title="Mindfulness + Breath Work",
        description="Guide three slow breaths or grounding games to lower anxiety spikes.",
        cues=["early meltdown signs", "evenings"],
    ),
    Strategy(
        category="movement_breaks",
        title="Heavy Work / Movement Breaks",
        description="Schedule wall pushes, trampoline jumps, or resistance games to release energy.",
        cues=["hyperactivity", "pre-lesson"],
    ),
    Strategy(
        category="meltdown_first_aid",
        title="Meltdown First Aid Plan",
        description="Dim lights, remove extra stimuli, and offer preferred sensory tools immediately.",
        cues=["meltdown", "de-escalation"],
    ),
    Strategy(
        category="communication",
        title="Plain-Language Modeling & PECS",
        description="Use short, literal instructions and personalized PECS boards.",
        cues=["communication goals", "nonverbal support"],
    ),
    Strategy(
        category="personalization",
        title="Interest-Based Learning Moments",
        description="Fold the child's passions into routines (dinosaur bath stories, train schedules).",
        cues=["motivation", "daily hygiene"],
    ),
    Strategy(
        category="caregiver_self_care",
        title="Caregiver Self-Care & Respite",
        description="Schedule breaks/support groups so adults can stay regulated.",
        cues=["caregiver stress", "fatigue"],
    ),
]


EVENT_TO_CATEGORIES: Dict[str, List[str]] = {
    "anxious": ["mindfulness", "sensory_tools"],
    "perseveration": ["personalization", "communication"],
    "high_energy": ["movement_breaks", "sensory_tools"],
    "meltdown": ["meltdown_first_aid", "sensory_tools", "mindfulness"],
    "encouragement": ["personalization", "communication"],
}


class StrategyAdvisor:
    """Returns curated strategies for caregiver dashboards."""

    def suggest(self, event: str, limit: int = 3) -> List[Strategy]:
        cats = EVENT_TO_CATEGORIES.get(event)
        if not cats:
            return STRATEGIES[:limit]
        ordered: List[Strategy] = []
        for cat in cats:
            ordered.extend([s for s in STRATEGIES if s.category == cat])
        return (ordered or STRATEGIES)[:limit]

-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./agi_seed.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
agi_seed.py

This module represents the "Metabolism" or "Life Drive" of the organism.
It is the central decision-making hub.

The AGI Seed's primary function is to maintain homeostasis. It does this
by observing the state of the Crystalline Heart and deciding on the most
energy-efficient action to return the system to a low-entropy state.

Current Actions:
- REGULATE: The Heart's state is too chaotic (high arousal/low coherence).
  The primary goal is to calm the system. The Seed will generate a simple,
  grounding phrase.
- ECHO: The system is stable. The user has spoken. The goal is to provide
  the "inner voice" echo, which reinforces correct speech and provides
  a sense of being heard.
- LEARN: The system is stable and has just had a successful interaction.
  The goal is to reinforce this pathway. The Seed will flag the voice
  sample for potential inclusion in the voice crystal.
- THINK: The system is stable, and an interesting thought has been generated
  by the LLM. The goal is to express this internal thought.
- IDLE: The system is stable and there is no input. Do nothing.
"""
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional
import numpy as np

from .gears import Information, EmotionData, SpeechData, AgentDecision

@dataclass
class AGISeed:
    """The central decision-making gear of the digital organism."""

    def decide(self, emotion_info: Information, speech_info: Optional[Information]) -> AgentDecision:
        """
        Given the current emotional and speech state, decide on an action.
        """
        if not isinstance(emotion_info.payload, EmotionData):
            return AgentDecision(action="IDLE")

        emotion: EmotionData = emotion_info.payload
        speech: Optional[SpeechData] = speech_info.payload if speech_info else None

        # 1. High-priority Regulation
        # If arousal is very high and coherence is low, the system is "distressed."
        # The top priority is to calm down (regulate).
        if emotion.arousal > 7.0 and emotion.coherence < 0.3:
            return AgentDecision(
                action="REGULATE",
                target_text="I am breathing. In... and out.",
                mode="inner"
            )

        # 2. Echoing User Speech
        # If there's speech to process, the default action is to echo it.
        if speech and (speech.corrected_text or speech.raw_text):
            # If the user's speech was successfully corrected, the system is "learning"
            # by reinforcing the correct pattern.
            if speech.corrected_text and speech.raw_text != speech.corrected_text:
                return AgentDecision(
                    action="LEARN",
                    target_text=speech.corrected_text,
                    mode="inner" # Corrections are always inner voice
                )
            # Otherwise, just a simple echo.
            return AgentDecision(
                action="ECHO",
                target_text=speech.raw_text,
                mode="inner"
            )
            
        # 3. Expressing an Internal Thought from the LLM
        # If the LLM generated a thought and the system is not in a high-arousal state.
        if emotion_info.metadata.get("llm_output") and emotion.arousal < 5.0:
            return AgentDecision(
                action="THINK",
                target_text=emotion_info.metadata["llm_output"],
                mode="inner" # Internal thoughts are spoken with the inner voice
            )

        # 4. Default Idle State
        # If none of the above conditions are met, do nothing.
        return AgentDecision(action="IDLE")-e 


╔══════════════════════════════════════════════════════════════════════════════╗
║ FILE: ./tk_gui.py
╚══════════════════════════════════════════════════════════════════════════════╝

"""
Tkinter desktop GUI for Jackson's Companion.

This GUI provides two views within a single window: a parent view and
a child view.  The parent view shows a live summary of speech
attempts, overall correction rate, and an early warning indicator for
meltdown risk based on recent behavioural events.  The child view
presents a simple, large-text interface with a friendly greeting,
guidance on how to say the current phrase, and gentle encouragement.

The GUI runs a background thread that executes the ``SpeechLoop``
simulation.  Metrics and guidance logs written by that loop are
periodically read into the GUI to refresh the displayed statistics.
Because everything is fully offline there are no network requests and
no browser dependencies.
"""

from __future__ import annotations

import asyncio
import csv
import threading
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import tkinter as tk
from tkinter import ttk

# Import sibling modules using absolute imports so the code runs
# correctly regardless of whether the package is installed or
# executed directly.  These modules live in the same directory.
# Import sibling modules using relative imports so the code runs
# correctly when executed as part of the ``autism_code`` package.
from .config import CompanionConfig, CONFIG
from .speech_loop import SpeechLoop


@dataclass
class MetricsSnapshot:
    total_attempts: int = 0
    total_corrections: int = 0
    overall_rate: float = 0.0
    last_phrase: str = ""
    last_raw: str = ""
    last_corrected: str = ""
    last_needs_correction: bool = False


@dataclass
class BehaviorSummary:
    total: int = 0
    anxious: int = 0
    perseveration: int = 0
    high_energy: int = 0
    meltdown: int = 0
    caregiver_reset: int = 0
    encouragement: int = 0


@dataclass
class MeltdownRisk:
    score: int = 0  # 0–100
    level: str = "No data yet"
    message: str = ""


def _safe_bool(val: Any) -> bool:
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        v = val.strip().lower()
        return v in {"1", "true", "yes", "y"}
    return False


def load_metrics(config: CompanionConfig) -> MetricsSnapshot:
    path = config.paths.metrics_csv
    if path is None or not path.exists():
        return MetricsSnapshot()
    rows: List[Dict[str, str]] = []
    with path.open("r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    if not rows:
        return MetricsSnapshot()
    total_attempts = len(rows)
    total_corrections = sum(1 for r in rows if _safe_bool(r.get("needs_correction")))
    last = rows[-1]
    last_phrase = last.get("phrase_text") or ""
    last_raw = last.get("raw_text") or ""
    last_corrected = last.get("corrected_text") or ""
    last_needs_correction = _safe_bool(last.get("needs_correction"))
    overall_rate = (total_corrections / total_attempts) if total_attempts else 0.0
    return MetricsSnapshot(
        total_attempts=total_attempts,
        total_corrections=total_corrections,
        overall_rate=overall_rate,
        last_phrase=last_phrase,
        last_raw=last_raw,
        last_corrected=last_corrected,
        last_needs_correction=last_needs_correction,
    )


def load_guidance(config: CompanionConfig) -> List[Dict[str, str]]:
    path = config.paths.guidance_csv
    if path is None or not path.exists():
        return []
    rows: List[Dict[str, str]] = []
    with path.open("r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


def compute_behavior_summary(
    guidance_rows: List[Dict[str, str]], window: int = 50
) -> BehaviorSummary:
    if not guidance_rows:
        return BehaviorSummary()
    recent = guidance_rows[-window:]
    counts: Dict[str, int] = {}
    for row in recent:
        ev = (row.get("event") or "").strip()
        if not ev:
            continue
        counts[ev] = counts.get(ev, 0) + 1
    return BehaviorSummary(
        total=sum(counts.values()),
        anxious=counts.get("anxious", 0),
        perseveration=counts.get("perseveration", 0),
        high_energy=counts.get("high_energy", 0),
        meltdown=counts.get("meltdown", 0),
        caregiver_reset=counts.get("caregiver_reset", 0),
        encouragement=counts.get("encouragement", 0),
    )


def compute_meltdown_risk(summary: BehaviorSummary) -> MeltdownRisk:
    total = summary.total
    if total == 0:
        return MeltdownRisk(
            score=0,
            level="No data yet",
            message=(
                "No behaviour events logged yet. As the system runs, this card will "
                "show early warning signals."
            ),
        )
    # Weighted scoring inspired by AGI stress metrics: emphasise serious events
    score_raw = (
        15 * summary.anxious
        + 12 * summary.perseveration
        + 10 * summary.high_energy
        + 25 * summary.meltdown
    )
    normalized = min(100.0, (score_raw / (max(total, 1) * 25.0)) * 100.0)
    score = int(round(normalized))
    if score < 25:
        level = "Low"
        message = "Signals look calm overall. Keep routines steady and keep praising effort."
    elif score < 60:
        level = "Elevated"
        message = (
            "Some stress signals showing up. Consider a short movement or breathing break."
        )
    else:
        level = "High"
        message = (
            "Frequent stress signals. Stay close, simplify demands, and lean on go-to calming tools."
        )
    return MeltdownRisk(score=score, level=level, message=message)


class CompanionGUI:
    """Main window containing parent and child views."""

    def __init__(self, config: Optional[CompanionConfig] = None) -> None:
        self.config = config or CONFIG
        # Tk root
        self.root = tk.Tk()
        self.root.title(f"{self.config.child_name}'s Companion")
        self.root.geometry("960x640")
        self.root.configure(bg="#020617")
        # Style definitions using ttk
        style = ttk.Style(self.root)
        style.theme_use("clam")
        style.configure("TFrame", background="#020617")
        style.configure("Card.TFrame", background="#020617")
        style.configure(
            "Title.TLabel",
            background="#020617",
            foreground="#e5e7eb",
            font=("Segoe UI", 16, "bold"),
        )
        style.configure(
            "Body.TLabel",
            background="#020617",
            foreground="#cbd5f5",
            font=("Segoe UI", 10),
        )
        style.configure(
            "StatLabel.TLabel",
            background="#020617",
            foreground="#9ca3af",
            font=("Segoe UI", 9),
        )
        style.configure(
            "StatValue.TLabel",
            background="#020617",
            foreground="#e5e7eb",
            font=("Segoe UI", 12, "bold"),
        )
        style.configure(
            "Risk.TLabel",
            background="#020617",
            foreground="#e5e7eb",
            font=("Segoe UI", 11, "bold"),
        )
        # Notebook for tabs
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill="both", expand=True, padx=12, pady=12)
        self.parent_frame = ttk.Frame(notebook, style="TFrame")
        self.child_frame = ttk.Frame(notebook, style="TFrame")
        notebook.add(self.parent_frame, text="Parent View")
        notebook.add(self.child_frame, text="Child View")
        # Build UI components
        self._build_parent_view()
        self._build_child_view()
        # Speech loop in background thread
        self._loop = SpeechLoop(self.config)
        self._loop_thread = threading.Thread(target=self._run_speech_loop, daemon=True)
        self._loop_thread.start()
        # Refresh interval (ms)
        self._refresh_interval_ms = 2500
        self.root.after(self._refresh_interval_ms, self._refresh_ui)

    # ---------------- Parent view ----------------

    def _build_parent_view(self) -> None:
        pf = self.parent_frame
        # Header
        header = ttk.Frame(pf, style="TFrame")
        header.pack(fill="x", pady=(0, 12))
        caregiver = getattr(self.config, "caregiver_name", "Caregiver")
        title = ttk.Label(
            header,
            text=f"{caregiver}'s dashboard for {self.config.child_name}",
            style="Title.TLabel",
        )
        title.pack(anchor="w")
        subtitle = ttk.Label(
            header,
            text=(
                "Live view of practice, corrections, and calming support — "
                "without needing a browser."
            ),
            style="Body.TLabel",
        )
        subtitle.pack(anchor="w", pady=(4, 0))
        # Stats row
        stats = ttk.Frame(pf, style="TFrame")
        stats.pack(fill="x", pady=(8, 12))
        self.total_attempts_var = tk.StringVar(value="0")
        self.overall_rate_var = tk.StringVar(value="0.0 %")
        self._make_stat_block(stats, "Total attempts", self.total_attempts_var).pack(
            side="left", padx=(0, 16)
        )
        self._make_stat_block(
            stats, "Overall correction rate", self.overall_rate_var
        ).pack(side="left", padx=(0, 16))
        # Meltdown risk card
        risk_card = ttk.Frame(pf, style="TFrame", relief="groove")
        risk_card.pack(fill="x", pady=(4, 12))
        risk_title = ttk.Label(
            risk_card,
            text="Meltdown risk (last 50 events)",
            style="Risk.TLabel",
        )
        risk_title.pack(anchor="w", padx=8, pady=(6, 2))
        self.meltdown_level_var = tk.StringVar(value="No data yet")
        self.meltdown_score_var = tk.StringVar(value="0 %")
        self.meltdown_message_var = tk.StringVar(
            value=(
                "No behaviour events logged yet. As the system runs, this card will "
                "show early warning signals."
            )
        )
        risk_row = ttk.Frame(risk_card, style="TFrame")
        risk_row.pack(fill="x", padx=8, pady=(0, 4))
        ttk.Label(
            risk_row, textvariable=self.meltdown_level_var, style="Risk.TLabel"
        ).pack(side="left", padx=(0, 8))
        ttk.Label(
            risk_row, textvariable=self.meltdown_score_var, style="Body.TLabel"
        ).pack(side="left")
        self.meltdown_bar = ttk.Progressbar(
            risk_card, orient="horizontal", mode="determinate", maximum=100
        )
        self.meltdown_bar.pack(fill="x", padx=8, pady=(0, 4))
        self.meltdown_message_label = ttk.Label(
            risk_card,
            textvariable=self.meltdown_message_var,
            style="Body.TLabel",
            wraplength=800,
            justify="left",
        )
        self.meltdown_message_label.pack(fill="x", padx=8, pady=(0, 8))
        # Last attempt card
        last_card = ttk.Frame(pf, style="TFrame", relief="groove")
        last_card.pack(fill="both", expand=True, pady=(4, 0))
        ttk.Label(
            last_card,
            text="Live session snapshot",
            style="Risk.TLabel",
        ).pack(anchor="w", padx=8, pady=(6, 2))
        self.last_phrase_var = tk.StringVar(value="—")
        self.last_raw_var = tk.StringVar(value="—")
        self.last_corrected_var = tk.StringVar(value="—")
        self.last_status_var = tk.StringVar(value="Waiting for first attempt...")
        ttk.Label(
            last_card, text="Target phrase:", style="StatLabel.TLabel"
        ).pack(anchor="w", padx=8, pady=(4, 0))
        ttk.Label(
            last_card, textvariable=self.last_phrase_var, style="Body.TLabel"
        ).pack(anchor="w", padx=16, pady=(0, 4))
        ttk.Label(
            last_card, text="Child said:", style="StatLabel.TLabel"
        ).pack(anchor="w", padx=8, pady=(4, 0))
        ttk.Label(
            last_card,
            textvariable=self.last_raw_var,
            style="Body.TLabel",
            wraplength=800,
        ).pack(anchor="w", padx=16, pady=(0, 4))
        ttk.Label(
            last_card, text="Companion model:", style="StatLabel.TLabel"
        ).pack(anchor="w", padx=8, pady=(4, 0))
        ttk.Label(
            last_card,
            textvariable=self.last_corrected_var,
            style="Body.TLabel",
            wraplength=800,
        ).pack(anchor="w", padx=16, pady=(0, 4))
        ttk.Label(
            last_card,
            textvariable=self.last_status_var,
            style="Body.TLabel",
        ).pack(anchor="w", padx=8, pady=(6, 8))

    def _make_stat_block(self, parent: ttk.Frame, label: str, value_var: tk.StringVar) -> ttk.Frame:
        f = ttk.Frame(parent, style="TFrame")
        ttk.Label(f, text=label, style="StatLabel.TLabel").pack(anchor="w")
        ttk.Label(f, textvariable=value_var, style="StatValue.TLabel").pack(
            anchor="w"
        )
        return f

    # ---------------- Child view ----------------

    def _build_child_view(self) -> None:
        cf = self.child_frame
        top = ttk.Frame(cf, style="TFrame")
        top.pack(fill="x", pady=(0, 12))
        self.child_status_var = tk.StringVar(value="Listening for your voice…")
        greet = ttk.Label(
            top, text=f"Hi {self.config.child_name} 👋", style="Title.TLabel"
        )
        greet.pack(anchor="w")
        status = ttk.Label(top, textvariable=self.child_status_var, style="Body.TLabel")
        status.pack(anchor="w", pady=(4, 0))
        phrase_card = ttk.Frame(cf, style="TFrame", relief="groove")
        phrase_card.pack(fill="both", expand=True, pady=(4, 0))
        self.child_helper_var = tk.StringVar(value="Say it like this")
        self.child_phrase_var = tk.StringVar(value="Waiting for the first try…")
        self.child_encouragement_var = tk.StringVar(
            value="When you talk, I listen. If it’s tricky, we’ll try together."
        )
        ttk.Label(
            phrase_card, textvariable=self.child_helper_var, style="StatLabel.TLabel"
        ).pack(anchor="w", padx=8, pady=(6, 0))
        ttk.Label(
            phrase_card,
            textvariable=self.child_phrase_var,
            style="StatValue.TLabel",
            wraplength=800,
        ).pack(anchor="w", padx=16, pady=(4, 8))
        ttk.Label(
            phrase_card,
            textvariable=self.child_encouragement_var,
            style="Body.TLabel",
            wraplength=800,
        ).pack(anchor="w", padx=16, pady=(0, 8))

    # ---------------- Background speech loop ----------------

    def _run_speech_loop(self) -> None:
        async def _runner():
            await self._loop.run()
        asyncio.run(_runner())

    # ---------------- Periodic refresh ----------------

    def _refresh_ui(self) -> None:
        metrics = load_metrics(self.config)
        guidance = load_guidance(self.config)
        summary = compute_behavior_summary(guidance)
        risk = compute_meltdown_risk(summary)
        # Parent stats
        self.total_attempts_var.set(str(metrics.total_attempts))
        self.overall_rate_var.set(f"{metrics.overall_rate * 100.0:.1f} %")
        self.meltdown_level_var.set(f"{risk.level} risk")
        self.meltdown_score_var.set(f"{risk.score} %")
        self.meltdown_message_var.set(risk.message)
        self.meltdown_bar["value"] = risk.score
        # Last attempt
        self.last_phrase_var.set(metrics.last_phrase or "—")
        self.last_raw_var.set(metrics.last_raw or "—")
        self.last_corrected_var.set(metrics.last_corrected or "—")
        if metrics.total_attempts == 0:
            self.last_status_var.set("Waiting for first attempt...")
        elif metrics.last_needs_correction:
            self.last_status_var.set("Correction suggested on the last attempt.")
        else:
            self.last_status_var.set("Last attempt sounded great.")
        # Child view
        if metrics.total_attempts == 0:
            self.child_status_var.set("Listening for your voice…")
            self.child_helper_var.set("Say it like this")
            self.child_phrase_var.set("Waiting for the first try…")
            self.child_encouragement_var.set(
                "When you talk, I listen. If it’s tricky, we’ll try together."
            )
        else:
            if metrics.last_needs_correction:
                self.child_status_var.set("Let's try it like this.")
                self.child_helper_var.set("Say it like this")
                phrase = (
                    metrics.last_corrected or metrics.last_phrase or "Let's try together."
                )
                self.child_phrase_var.set(phrase)
                self.child_encouragement_var.set(
                    "It’s okay if it’s hard. We’ll slow down and say it together."
                )
            else:
                self.child_status_var.set("That sounded great!")
                self.child_helper_var.set("You said it!")
                phrase = metrics.last_raw or metrics.last_phrase or "Nice job."
                self.child_phrase_var.set(phrase)
                self.child_encouragement_var.set(
                    "Nice work. Take a breath, smile, and get ready for the next word."
                )
        self.root.after(self._refresh_interval_ms, self._refresh_ui)

    def run(self) -> None:
        """Start the Tk main loop."""
        self.root.mainloop()


def run_gui(config: Optional[CompanionConfig] = None) -> None:
    gui = CompanionGUI(config or CONFIG)
    gui.run()